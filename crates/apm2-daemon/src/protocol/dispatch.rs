//! Privileged endpoint dispatcher for RFC-0017 control-plane IPC.
//!
//! This module implements the privileged endpoint dispatcher per DD-001 and
//! DD-009. Privileged endpoints (ClaimWork, SpawnEpisode, IssueCapability,
//! Shutdown) are only accessible via the operator socket. Session socket
//! connections receive `PERMISSION_DENIED` for all privileged requests.
//!
//! # Security Invariants
//!
//! - [INV-0001] An agent cannot execute privileged IPC operations
//! - [TB-002] Privilege separation boundary: session connections blocked from
//!   privileged handlers
//! - [TCK-00253] Actor_id derived from credential, not user input
//!
//! # Message Flow
//!
//! ```text
//! ┌─────────────────┐     ┌─────────────────┐     ┌─────────────────┐
//! │ operator.sock   │────▶│ PrivilegedDispatch │──▶│ Handler Stubs  │
//! └─────────────────┘     └─────────────────┘     └─────────────────┘
//!                                │
//!                                │ `PERMISSION_DENIED`
//!                                ▼
//! ┌─────────────────┐     ┌─────────────────┐
//! │ session.sock    │────▶│  (Rejected)     │
//! └─────────────────┘     └─────────────────┘
//! ```

use std::collections::VecDeque;
use std::sync::Arc;
use std::time::{Duration, SystemTime};

use apm2_core::credentials::{
    AuthMethod, CredentialProfile as CoreCredentialProfile, CredentialStore, ProfileId, Provider,
};
use apm2_core::determinism::canonicalize_json;
use apm2_core::events::{DefectRecorded, Validate};
use apm2_core::evidence::ContentAddressedStore;
use apm2_core::fac::{
    AttestationLevel, AttestationRequirements, CHANGESET_PUBLISHED_PREFIX,
    REVIEW_RECEIPT_RECORDED_PREFIX, ReceiptAttestation, ReceiptKind, RiskTier, builtin_profiles,
    validate_receipt_attestation,
};
use apm2_core::process::ProcessState;
use bytes::Bytes;
use prost::Message;
use secrecy::SecretString;
use subtle::ConstantTimeEq;
use tracing::{debug, error, info, warn};

use super::credentials::PeerCredentials;
use super::error::{ProtocolError, ProtocolResult};
use super::messages::{
    AddCredentialRequest,
    AddCredentialResponse,
    BoundedDecode,
    ClaimWorkRequest,
    ClaimWorkResponse,
    ConsensusByzantineEvidenceRequest,
    ConsensusByzantineEvidenceResponse,
    ConsensusErrorCode,
    ConsensusMetricsRequest,
    ConsensusMetricsResponse,
    ConsensusStatusRequest,
    ConsensusStatusResponse,
    ConsensusValidatorsRequest,
    ConsensusValidatorsResponse,
    CredentialAuthMethod as ProtoAuthMethod,
    CredentialProvider as ProtoProvider,
    DecodeConfig,
    // TCK-00340: Sublease delegation types
    DelegateSubleaseRequest,
    DelegateSubleaseResponse,
    EndSessionRequest,
    EndSessionResponse,
    // TCK-00389: Review receipt ingestion types
    IngestReviewReceiptRequest,
    IngestReviewReceiptResponse,
    IssueCapabilityRequest,
    IssueCapabilityResponse,
    ListCredentialsRequest,
    ListCredentialsResponse,
    ListProcessesRequest,
    ListProcessesResponse,
    LoginCredentialRequest,
    LoginCredentialResponse,
    PatternRejection,
    PrivilegedError,
    PrivilegedErrorCode,
    ProcessInfo,
    ProcessStateEnum,
    ProcessStatusRequest,
    ProcessStatusResponse,
    // TCK-00394: ChangeSet publishing types
    PublishChangeSetRequest,
    PublishChangeSetResponse,
    RefreshCredentialRequest,
    RefreshCredentialResponse,
    ReloadProcessRequest,
    ReloadProcessResponse,
    RemoveCredentialRequest,
    RemoveCredentialResponse,
    RestartProcessRequest,
    RestartProcessResponse,
    ReviewReceiptVerdict,
    ShutdownRequest,
    ShutdownResponse,
    SpawnEpisodeRequest,
    SpawnEpisodeResponse,
    StartProcessRequest,
    StartProcessResponse,
    StopProcessRequest,
    StopProcessResponse,
    SubscribePulseRequest,
    SubscribePulseResponse,
    SwitchCredentialRequest,
    SwitchCredentialResponse,
    TerminationOutcome,
    UnsubscribePulseRequest,
    UnsubscribePulseResponse,
    UpdateStopFlagsRequest,
    UpdateStopFlagsResponse,
    WorkRole,
    WorkStatusRequest,
    WorkStatusResponse,
};
use super::pulse_acl::{
    AclDecision, AclError, PulseAclEvaluator, validate_client_sub_id, validate_subscription_id,
};
use super::resource_governance::{
    SharedSubscriptionRegistry, SubscriptionRegistry, SubscriptionState,
};
use super::session_dispatch::InMemoryManifestStore;
use super::session_token::TokenMinter;
use crate::episode::registry::InMemorySessionRegistry;
use crate::episode::{
    CapabilityManifest, CustodyDomainError, CustodyDomainId, EpisodeId, EpisodeRuntime,
    EpisodeRuntimeConfig, InMemoryCasManifestLoader, LeaseIssueDenialReason, ManifestLoader,
    TerminationClass, validate_custody_domain_overlap,
};
use crate::governance::GovernanceFreshnessMonitor;
use crate::htf::{ClockConfig, HolonicClock};
use crate::metrics::SharedMetricsRegistry;
use crate::session::{EphemeralHandle, SessionRegistry, SessionState};
use crate::state::SharedState;

// ============================================================================
// Ledger Event Emitter Interface (TCK-00253)
// ============================================================================

/// A signed ledger event for persistence.
///
/// Per acceptance criteria: "`WorkClaimed` event signed and persisted"
/// This struct represents a signed event ready for ledger ingestion.
#[derive(Debug, Clone)]
#[non_exhaustive]
pub struct SignedLedgerEvent {
    /// Unique event identifier.
    pub event_id: String,

    /// Event type discriminant.
    pub event_type: String,

    /// Work ID this event relates to.
    pub work_id: String,

    /// Actor ID that produced this event.
    pub actor_id: String,

    /// Canonical event payload (JSON).
    pub payload: Vec<u8>,

    /// Ed25519 signature over canonical bytes.
    pub signature: Vec<u8>,

    /// Timestamp in nanoseconds since epoch (HTF-compliant).
    pub timestamp_ns: u64,
}

/// Error type for ledger event emission.
#[derive(Debug, Clone, PartialEq, Eq)]
pub enum LedgerEventError {
    /// Signing operation failed.
    SigningFailed {
        /// Error message.
        message: String,
    },

    /// Ledger persistence failed.
    PersistenceFailed {
        /// Error message.
        message: String,
    },

    /// Validation failed (TCK-00307 MAJOR 4).
    ///
    /// Per REQ-VAL-0001: All event payloads must be validated before emission
    /// to prevent denial-of-service via unbounded strings/bytes.
    ValidationFailed {
        /// Error message describing the validation failure.
        message: String,
    },
}

impl std::fmt::Display for LedgerEventError {
    fn fmt(&self, f: &mut std::fmt::Formatter<'_>) -> std::fmt::Result {
        match self {
            Self::SigningFailed { message } => write!(f, "signing failed: {message}"),
            Self::PersistenceFailed { message } => write!(f, "persistence failed: {message}"),
            Self::ValidationFailed { message } => write!(f, "validation failed: {message}"),
        }
    }
}

impl std::error::Error for LedgerEventError {}

/// Error type for HTF timestamp generation (TCK-00289).
///
/// # Security (Fail-Closed)
///
/// Per RFC-0016 and the security policy, HTF timestamp errors must be
/// propagated rather than returning a fallback value. Returning 0 would
/// violate fail-closed security posture and could allow operations to
/// proceed with invalid timestamps.
#[derive(Debug, Clone, PartialEq, Eq)]
pub enum HtfTimestampError {
    /// HLC is not enabled on the clock.
    HlcNotEnabled,
    /// Clock error occurred.
    ClockError {
        /// Error message from the clock.
        message: String,
    },
}

impl std::fmt::Display for HtfTimestampError {
    fn fmt(&self, f: &mut std::fmt::Formatter<'_>) -> std::fmt::Result {
        match self {
            Self::HlcNotEnabled => write!(f, "HLC not enabled on clock"),
            Self::ClockError { message } => write!(f, "clock error: {message}"),
        }
    }
}

impl std::error::Error for HtfTimestampError {}

/// Parameters for a work state transition event (TCK-00395).
///
/// Bundles the parameters needed to emit a `WorkTransitioned` event to the
/// ledger. This struct reduces the number of arguments passed to
/// `emit_work_transitioned`.
#[derive(Debug, Clone)]
pub struct WorkTransition<'a> {
    /// The work ID being transitioned.
    pub work_id: &'a str,
    /// The state before the transition.
    pub from_state: &'a str,
    /// The state after the transition.
    pub to_state: &'a str,
    /// Why the transition occurred (e.g., `work_claimed_via_ipc`).
    pub rationale_code: &'a str,
    /// The work item's `transition_count` before this transition.
    pub previous_transition_count: u32,
    /// The actor performing the transition.
    pub actor_id: &'a str,
    /// HTF-compliant timestamp in nanoseconds since epoch.
    pub timestamp_ns: u64,
}

/// Parameters for a stop-flag mutation audit event (TCK-00351).
///
/// This captures the immutable evidence payload for `UpdateStopFlags` so
/// emergency/governance stop mutations are queryable from the ledger.
#[derive(Debug, Clone)]
#[allow(clippy::struct_excessive_bools)] // Captures explicit before/after stop-flag state for audit evidence.
pub struct StopFlagsMutation<'a> {
    /// Actor identity derived from peer credentials.
    pub actor_id: &'a str,
    /// Emergency stop flag value before mutation.
    pub emergency_stop_previous: bool,
    /// Emergency stop flag value after mutation.
    pub emergency_stop_current: bool,
    /// Governance stop flag value before mutation.
    pub governance_stop_previous: bool,
    /// Governance stop flag value after mutation.
    pub governance_stop_current: bool,
    /// HTF-compliant timestamp in nanoseconds since epoch.
    pub timestamp_ns: u64,
    /// Request-scoped context (connection identity + requested fields).
    pub request_context: &'a serde_json::Value,
}

/// Trait for emitting signed events to the ledger.
///
/// Per TCK-00253 acceptance criteria:
/// - "`WorkClaimed` event signed and persisted"
/// - "Ledger query returns signed event"
///
/// # Implementers
///
/// - `StubLedgerEventEmitter`: In-memory storage for testing
/// - `DurableLedgerEventEmitter`: SQLite-backed persistence with HTF timestamps
///   (TCK-00289)
pub trait LedgerEventEmitter: Send + Sync {
    /// Emits a signed `WorkClaimed` event to the ledger.
    ///
    /// # Arguments
    ///
    /// * `claim` - The work claim to record
    /// * `timestamp_ns` - HTF-compliant timestamp in nanoseconds since epoch
    ///
    /// # Returns
    ///
    /// The signed event that was persisted.
    ///
    /// # Errors
    ///
    /// Returns `LedgerEventError` if signing or persistence fails.
    fn emit_work_claimed(
        &self,
        claim: &WorkClaim,
        timestamp_ns: u64,
    ) -> Result<SignedLedgerEvent, LedgerEventError>;

    /// Emits a signed `SessionStarted` event to the ledger (TCK-00289).
    ///
    /// Per TCK-00348, the `SessionStarted` event is the **authoritative**
    /// record for contract binding metadata. When `contract_binding` is
    /// `Some`, the binding fields (client/server hashes, mismatch waived,
    /// risk tier) are included in the signed payload. Persistence failure
    /// MUST be propagated as an error (fail-closed).
    ///
    /// # Arguments
    ///
    /// * `session_id` - The session ID being started
    /// * `work_id` - The work ID this session is associated with
    /// * `lease_id` - The lease ID authorizing this session
    /// * `actor_id` - The actor starting the session
    /// * `adapter_profile_hash` - CAS hash of the adapter profile used
    /// * `role_spec_hash` - CAS hash of the role spec (if available)
    /// * `timestamp_ns` - HTF-compliant timestamp in nanoseconds since epoch
    /// * `contract_binding` - Contract binding from handshake (if available)
    /// * `identity_proof_profile_hash` - Active identity proof profile hash for
    ///   the session identity context (when available)
    ///
    /// # Returns
    ///
    /// The signed event that was persisted.
    ///
    /// # Errors
    ///
    /// Returns `LedgerEventError` if signing or persistence fails.
    #[allow(clippy::too_many_arguments)]
    fn emit_session_started(
        &self,
        session_id: &str,
        work_id: &str,
        lease_id: &str,
        actor_id: &str,
        adapter_profile_hash: &[u8; 32],
        role_spec_hash: Option<&[u8; 32]>,
        timestamp_ns: u64,
        contract_binding: Option<&crate::hsi_contract::SessionContractBinding>,
        identity_proof_profile_hash: Option<&[u8; 32]>,
    ) -> Result<SignedLedgerEvent, LedgerEventError>;

    /// Emits a generic session event to the ledger (TCK-00290).
    ///
    /// This method handles arbitrary session events from `EmitEvent` requests,
    /// preserving the actual `event_type` and payload from the request rather
    /// than coercing all events into `session_started` events.
    ///
    /// # Arguments
    ///
    /// * `session_id` - The session ID emitting the event
    /// * `event_type` - The actual event type from the request
    /// * `payload` - The event payload bytes from the request
    /// * `actor_id` - The actor emitting the event (session ID or agent ID)
    /// * `timestamp_ns` - HTF-compliant timestamp in nanoseconds since epoch
    ///
    /// # Returns
    ///
    /// The signed event that was persisted.
    ///
    /// # Errors
    ///
    /// Returns `LedgerEventError` if signing or persistence fails.
    fn emit_session_event(
        &self,
        session_id: &str,
        event_type: &str,
        payload: &[u8],
        actor_id: &str,
        timestamp_ns: u64,
    ) -> Result<SignedLedgerEvent, LedgerEventError>;

    /// Returns whether emitted events are durably persisted.
    ///
    /// `false` indicates in-memory/test-only emitters where audit evidence
    /// does not survive process restart.
    fn has_durable_storage(&self) -> bool {
        true
    }

    /// Emits a signed `StopFlagsMutated` event to the ledger (TCK-00351).
    ///
    /// This records runtime stop-flag mutations performed by the privileged
    /// `UpdateStopFlags` endpoint.
    fn emit_stop_flags_mutated(
        &self,
        mutation: &StopFlagsMutation<'_>,
    ) -> Result<SignedLedgerEvent, LedgerEventError>;

    /// Emits a signed `DefectRecorded` event to the ledger (TCK-00307).
    fn emit_defect_recorded(
        &self,
        defect: &DefectRecorded,
        timestamp_ns: u64,
    ) -> Result<SignedLedgerEvent, LedgerEventError>;

    /// Queries a signed event by event ID.
    fn get_event(&self, event_id: &str) -> Option<SignedLedgerEvent>;

    /// Queries events by work ID.
    fn get_events_by_work_id(&self, work_id: &str) -> Vec<SignedLedgerEvent>;

    /// Queries a signed event by `receipt_id` embedded in the payload.
    ///
    /// This searches for events of type `review_receipt_recorded` or
    /// `review_blocked_recorded` whose JSON payload contains a matching
    /// `receipt_id` field. Used by `handle_ingest_review_receipt` for
    /// idempotency: if a receipt with the same `receipt_id` was already
    /// ingested, the existing event is returned to avoid duplicate ledger
    /// entries.
    ///
    /// # Why not `get_event`?
    ///
    /// `get_event` looks up by `event_id` (the random `EVT-<uuid>` primary
    /// key), which is different from `receipt_id` (the caller-supplied
    /// idempotency key). Using `get_event(&receipt_id)` would never match
    /// because `receipt_id` is not stored as the `event_id`.
    ///
    /// # Default
    ///
    /// Returns `None`. Implementations that support receipt-based lookup
    /// should override this.
    fn get_event_by_receipt_id(&self, receipt_id: &str) -> Option<SignedLedgerEvent> {
        let _ = receipt_id;
        None
    }

    /// Emits an episode lifecycle event to the ledger (TCK-00321).
    ///
    /// Per REQ-0005, episode events must be streamed directly to the ledger
    /// as they occur, rather than buffered in memory. This enables:
    /// - Events survive daemon restart (ledger-backed durability)
    /// - Receipt event appended atomically at completion
    /// - CAS-before-ledger ordering for events referencing CAS hashes
    ///
    /// # Arguments
    ///
    /// * `episode_id` - The episode ID for this event
    /// * `event_type` - The event type discriminant (e.g., "episode.created")
    /// * `payload` - The JSON-serialized event payload
    /// * `timestamp_ns` - HTF-compliant timestamp in nanoseconds since epoch
    ///
    /// # Returns
    ///
    /// The signed event that was persisted.
    ///
    /// # Errors
    ///
    /// Returns `LedgerEventError` if signing or persistence fails.
    fn emit_episode_event(
        &self,
        episode_id: &str,
        event_type: &str,
        payload: &[u8],
        timestamp_ns: u64,
    ) -> Result<SignedLedgerEvent, LedgerEventError>;

    /// Emits a `ReviewReceiptRecorded` ledger event (TCK-00321).
    ///
    /// Per REQ-0005, receipt events must be emitted atomically at episode
    /// completion. This method:
    /// - Validates that referenced CAS artifacts exist (CAS-before-event)
    /// - Persists the receipt to the ledger atomically
    /// - Returns the signed event for verification
    ///
    /// # Ledger Event vs Protocol Event
    ///
    /// This produces a **ledger event** (JCS-canonicalized JSON) for
    /// persistence and audit, which is distinct from the FAC protocol's
    /// `ReviewReceiptRecorded` event (binary canonical format) defined in
    /// `apm2_core::fac::review_receipt`.
    ///
    /// The ledger event format includes:
    /// - `event_type`: Event discriminant for querying
    /// - `timestamp_ns`: HTF-compliant timestamp in the signed payload
    /// - All required fields for audit trail reconstruction
    ///
    /// Both formats use the same domain prefix (`REVIEW_RECEIPT_RECORDED:`) to
    /// ensure namespace consistency, but serve different purposes in the
    /// system.
    ///
    /// # Arguments
    ///
    /// * `episode_id` - The episode that produced this receipt
    /// * `receipt_id` - Unique receipt identifier
    /// * `changeset_digest` - BLAKE3 digest of the reviewed changeset
    /// * `artifact_bundle_hash` - CAS hash of the artifact bundle
    /// * `reviewer_actor_id` - Actor ID of the reviewer
    /// * `timestamp_ns` - HTF-compliant timestamp in nanoseconds since epoch
    /// * `identity_proof_hash` - Identity proof hash binding (32 bytes)
    ///
    /// # Returns
    ///
    /// The signed event that was persisted.
    ///
    /// # Errors
    ///
    /// Returns `LedgerEventError` if signing, CAS validation, or persistence
    /// fails.
    #[allow(clippy::too_many_arguments)]
    fn emit_review_receipt(
        &self,
        episode_id: &str,
        receipt_id: &str,
        changeset_digest: &[u8; 32],
        artifact_bundle_hash: &[u8; 32],
        reviewer_actor_id: &str,
        timestamp_ns: u64,
        identity_proof_hash: &[u8; 32],
    ) -> Result<SignedLedgerEvent, LedgerEventError>;

    /// Emits a `ReviewBlockedRecorded` ledger event (TCK-00389).
    ///
    /// Records a blocked review outcome in the ledger. This is emitted when
    /// an external reviewer reports that the review was blocked due to tool
    /// failure, apply failure, or similar issues.
    ///
    /// # Arguments
    ///
    /// * `lease_id` - Gate lease identifier associated with this review
    /// * `receipt_id` - Unique receipt identifier (used as `blocked_id`)
    /// * `changeset_digest` - BLAKE3 digest of the reviewed changeset
    /// * `artifact_bundle_hash` - CAS hash of the artifact bundle
    /// * `reason_code` - Numeric reason code for the blocked review
    /// * `blocked_log_hash` - CAS hash of blocked logs
    /// * `reviewer_actor_id` - Actor ID of the reviewer
    /// * `timestamp_ns` - HTF-compliant timestamp in nanoseconds since epoch
    /// * `identity_proof_hash` - Identity proof hash binding (32 bytes)
    ///
    /// # Returns
    ///
    /// The signed event that was persisted.
    ///
    /// # Errors
    ///
    /// Returns `LedgerEventError` if signing or persistence fails.
    #[allow(clippy::too_many_arguments)]
    fn emit_review_blocked_receipt(
        &self,
        lease_id: &str,
        receipt_id: &str,
        changeset_digest: &[u8; 32],
        artifact_bundle_hash: &[u8; 32],
        reason_code: u32,
        blocked_log_hash: &[u8; 32],
        reviewer_actor_id: &str,
        timestamp_ns: u64,
        identity_proof_hash: &[u8; 32],
    ) -> Result<SignedLedgerEvent, LedgerEventError>;

    /// Returns the number of `work_transitioned` events for a given work ID.
    ///
    /// This is the authoritative source for the `previous_transition_count`
    /// field in `WorkTransitioned` events. Callers MUST use this method
    /// rather than hardcoding transition counts.
    ///
    /// # Arguments
    ///
    /// * `work_id` - The work ID to count transitions for
    ///
    /// # Returns
    ///
    /// The number of `work_transitioned` events for the given work ID.
    fn get_work_transition_count(&self, work_id: &str) -> u32;

    /// Emits a signed `WorkTransitioned` event to the ledger (TCK-00395).
    ///
    /// Records a work item state transition in the ledger, enabling the FAC
    /// CLI (`apm2 fac work status`) to observe work lifecycle state changes
    /// and the `GateOrchestrator` (TCK-00388) to trigger gate lifecycle.
    ///
    /// # Arguments
    ///
    /// * `work_id` - The work ID being transitioned
    /// * `from_state` - The state before the transition
    /// * `to_state` - The state after the transition
    /// * `rationale_code` - Why the transition occurred (e.g.,
    ///   `work_claimed_via_ipc`)
    /// * `previous_transition_count` - The work item's `transition_count`
    ///   before this transition (replay protection)
    /// * `actor_id` - The actor performing the transition
    /// * `timestamp_ns` - HTF-compliant timestamp in nanoseconds since epoch
    ///
    /// # Returns
    ///
    /// The signed event that was persisted.
    ///
    /// # Errors
    ///
    /// Returns `LedgerEventError` if signing or persistence fails.
    fn emit_work_transitioned(
        &self,
        transition: &WorkTransition<'_>,
    ) -> Result<SignedLedgerEvent, LedgerEventError>;

    /// Emits a signed `SessionTerminated` event to the ledger (TCK-00395).
    ///
    /// Records session termination in the ledger, enabling the
    /// `GateOrchestrator` (TCK-00388) to trigger gate lifecycle after
    /// session completion.
    ///
    /// # Arguments
    ///
    /// * `session_id` - The session that terminated
    /// * `work_id` - The work ID this session was associated with
    /// * `exit_code` - The process exit code (0 = success)
    /// * `termination_reason` - Human-readable termination reason
    /// * `actor_id` - The actor associated with this session
    /// * `timestamp_ns` - HTF-compliant timestamp in nanoseconds since epoch
    ///
    /// # Returns
    ///
    /// The signed event that was persisted.
    ///
    /// # Errors
    ///
    /// Returns `LedgerEventError` if signing or persistence fails.
    fn emit_session_terminated(
        &self,
        session_id: &str,
        work_id: &str,
        exit_code: i32,
        termination_reason: &str,
        actor_id: &str,
        timestamp_ns: u64,
    ) -> Result<SignedLedgerEvent, LedgerEventError>;

    /// Atomically emits a `WorkClaimed` event followed by a
    /// `WorkTransitioned`(Open -> Claimed) event (TCK-00395).
    ///
    /// # Atomicity Contract
    ///
    /// Both events are persisted atomically per operation: either both
    /// succeed or neither is committed. This prevents partial state where
    /// `work_claimed` is persisted but the corresponding transition is not.
    ///
    /// For in-memory emitters, atomicity is trivially guaranteed. For
    /// durable emitters, this must be implemented with a database
    /// transaction.
    ///
    /// # Returns
    ///
    /// The signed `WorkClaimed` event on success.
    ///
    /// # Errors
    ///
    /// Returns `LedgerEventError` if signing or persistence of either
    /// event fails. On error, neither event is committed.
    fn emit_claim_lifecycle(
        &self,
        claim: &WorkClaim,
        actor_id: &str,
        timestamp_ns: u64,
    ) -> Result<SignedLedgerEvent, LedgerEventError> {
        // Default implementation: emit sequentially.
        // Override in durable emitters for transactional atomicity.
        let transition_count = self.get_work_transition_count(&claim.work_id);
        let claimed_event = self.emit_work_claimed(claim, timestamp_ns)?;
        if let Err(e) = self.emit_work_transitioned(&WorkTransition {
            work_id: &claim.work_id,
            from_state: "Open",
            to_state: "Claimed",
            rationale_code: "work_claimed_via_ipc",
            previous_transition_count: transition_count,
            actor_id,
            timestamp_ns,
        }) {
            // Partial failure: work_claimed was persisted but transition was not.
            // Log and propagate the error.
            warn!(
                error = %e,
                work_id = %claim.work_id,
                "Partial commit: work_claimed persisted but work_transitioned failed"
            );
            return Err(e);
        }
        Ok(claimed_event)
    }

    /// Atomically emits a `SessionStarted` event followed by a
    /// `WorkTransitioned`(Claimed -> `InProgress`) event (TCK-00395).
    ///
    /// # Atomicity Contract
    ///
    /// Both events are persisted atomically per operation. See
    /// [`Self::emit_claim_lifecycle`] for details.
    ///
    /// # Returns
    ///
    /// The signed `SessionStarted` event on success.
    ///
    /// # Errors
    ///
    /// Returns `LedgerEventError` if signing or persistence of either
    /// event fails.
    #[allow(clippy::too_many_arguments)]
    fn emit_spawn_lifecycle(
        &self,
        session_id: &str,
        work_id: &str,
        lease_id: &str,
        actor_id: &str,
        adapter_profile_hash: &[u8; 32],
        role_spec_hash: Option<&[u8; 32]>,
        timestamp_ns: u64,
        contract_binding: Option<&crate::hsi_contract::SessionContractBinding>,
        identity_proof_profile_hash: Option<&[u8; 32]>,
    ) -> Result<SignedLedgerEvent, LedgerEventError> {
        // Default implementation: emit sequentially.
        let session_event = self.emit_session_started(
            session_id,
            work_id,
            lease_id,
            actor_id,
            adapter_profile_hash,
            role_spec_hash,
            timestamp_ns,
            contract_binding,
            identity_proof_profile_hash,
        )?;
        let transition_count = self.get_work_transition_count(work_id);
        if let Err(e) = self.emit_work_transitioned(&WorkTransition {
            work_id,
            from_state: "Claimed",
            to_state: "InProgress",
            rationale_code: "episode_spawned_via_ipc",
            previous_transition_count: transition_count,
            actor_id,
            timestamp_ns,
        }) {
            warn!(
                error = %e,
                work_id = %work_id,
                "Partial commit: session_started persisted but work_transitioned failed"
            );
            return Err(e);
        }
        Ok(session_event)
    }

    /// Emits an `EpisodeRunAttributed` event to the ledger (TCK-00330).
    ///
    /// This method records attribution for an episode run, binding:
    /// - `work_id`: The work item this run is associated with
    /// - `episode_id`: The episode identifier
    /// - `session_id`: The session that executed the run
    /// - `adapter_profile_hash`: CAS hash of the `AgentAdapterProfileV1` used
    ///
    /// Per REQ-0009, ledger events must include `adapter_profile_hash`
    /// attribution to enable audit trail reconstruction and
    /// profile-specific analysis.
    ///
    /// # Arguments
    ///
    /// * `work_id` - The work ID this run is associated with
    /// * `episode_id` - The episode ID for this run
    /// * `session_id` - The session ID that executed the run
    /// * `adapter_profile_hash` - CAS hash of the `AgentAdapterProfileV1`
    /// * `timestamp_ns` - HTF-compliant timestamp in nanoseconds since epoch
    ///
    /// # Returns
    ///
    /// The signed event that was persisted.
    ///
    /// # Errors
    ///
    /// Returns `LedgerEventError` if signing or persistence fails.
    fn emit_episode_run_attributed(
        &self,
        work_id: &str,
        episode_id: &str,
        session_id: &str,
        adapter_profile_hash: &[u8; 32],
        timestamp_ns: u64,
    ) -> Result<SignedLedgerEvent, LedgerEventError>;

    /// Emits a signed `ChangeSetPublished` event to the ledger (TCK-00394).
    ///
    /// Records that a changeset bundle was published to CAS and anchored in
    /// the ledger, enabling downstream gate orchestration (TCK-00388) to
    /// bind gate leases to the changeset.
    ///
    /// # Arguments
    ///
    /// * `work_id` - The work ID this changeset belongs to
    /// * `changeset_digest` - BLAKE3 digest of the canonical bundle
    /// * `cas_hash` - CAS hash of the stored bundle artifact
    /// * `actor_id` - The actor who published the changeset
    /// * `timestamp_ns` - HTF-compliant timestamp in nanoseconds since epoch
    ///
    /// # Returns
    ///
    /// The signed event that was persisted.
    ///
    /// # Errors
    ///
    /// Returns `LedgerEventError` if signing or persistence fails.
    fn emit_changeset_published(
        &self,
        work_id: &str,
        changeset_digest: &[u8; 32],
        cas_hash: &[u8; 32],
        actor_id: &str,
        timestamp_ns: u64,
    ) -> Result<SignedLedgerEvent, LedgerEventError>;

    /// Emits a signed receipt with envelope bindings (TCK-00350).
    ///
    /// Per REQ-0004, all authoritative effect receipts MUST carry
    /// `envelope_hash`, `capability_manifest_hash`, and
    /// `view_commitment_hash`. This method validates bindings before
    /// emission (fail-closed) and includes them in the signed payload.
    ///
    /// # Fail-closed
    ///
    /// Returns [`LedgerEventError::ValidationFailed`] if any binding
    /// hash is zero. Receipts MUST NOT be emitted without valid bindings.
    ///
    /// # Arguments
    ///
    /// * `episode_id` - The episode that produced this receipt
    /// * `receipt_id` - Unique receipt identifier
    /// * `changeset_digest` - BLAKE3 digest of the reviewed changeset
    /// * `artifact_bundle_hash` - CAS hash of the artifact bundle
    /// * `reviewer_actor_id` - Actor ID of the reviewer
    /// * `timestamp_ns` - HTF-compliant timestamp in nanoseconds
    /// * `bindings` - Envelope bindings to include in the receipt
    /// * `identity_proof_hash` - Identity proof hash binding (32 bytes)
    ///
    /// # Errors
    ///
    /// Returns `LedgerEventError` if validation, signing, or persistence
    /// fails.
    #[allow(clippy::too_many_arguments)]
    fn emit_receipt_with_bindings(
        &self,
        episode_id: &str,
        receipt_id: &str,
        changeset_digest: &[u8; 32],
        artifact_bundle_hash: &[u8; 32],
        reviewer_actor_id: &str,
        timestamp_ns: u64,
        bindings: &crate::episode::EnvelopeBindings,
        identity_proof_hash: &[u8; 32],
    ) -> Result<SignedLedgerEvent, LedgerEventError> {
        // Fail-closed: validate bindings before emission
        bindings
            .validate()
            .map_err(|e| LedgerEventError::ValidationFailed {
                message: format!("envelope binding validation failed: {e}"),
            })?;

        // Default implementation delegates to emit_review_receipt.
        // Implementations that persist bindings should override this.
        self.emit_review_receipt(
            episode_id,
            receipt_id,
            changeset_digest,
            artifact_bundle_hash,
            reviewer_actor_id,
            timestamp_ns,
            identity_proof_hash,
        )
    }
}

/// Domain separation prefix for `WorkClaimed` events.
///
/// Per RFC-0017 and TCK-00264: domain prefixes prevent cross-context replay.
pub const WORK_CLAIMED_DOMAIN_PREFIX: &[u8] = b"apm2.event.work_claimed:";

/// Domain separation prefix for `DefectRecorded` events.
pub const DEFECT_RECORDED_DOMAIN_PREFIX: &[u8] = b"apm2.event.defect_recorded:";

/// Domain separation prefix for episode lifecycle events (TCK-00321).
///
/// Per RFC-0017 and TCK-00264: domain prefixes prevent cross-context replay.
pub const EPISODE_EVENT_DOMAIN_PREFIX: &[u8] = b"apm2.event.episode:";

/// Domain separation prefix for episode run attribution events (TCK-00330).
///
/// Per RFC-0017 and TCK-00264: domain prefixes prevent cross-context replay.
/// This prefix is used for events that attribute episode runs to specific
/// adapter profiles via their CAS hash.
pub const EPISODE_RUN_ATTRIBUTED_PREFIX: &[u8] = b"apm2.event.episode_run_attributed:";

/// Domain separation prefix for `WorkTransitioned` ledger events (TCK-00395).
///
/// Per RFC-0017 DD-006: domain prefixes prevent cross-context replay.
/// This prefix is used when emitting work state transition events to the
/// ledger, enabling the FAC CLI and `GateOrchestrator` to observe work
/// lifecycle state changes.
pub const WORK_TRANSITIONED_DOMAIN_PREFIX: &[u8] = b"apm2.event.work_transitioned:";

/// Domain separation prefix for `SessionTerminated` ledger events (TCK-00395).
///
/// Per RFC-0017 DD-006: domain prefixes prevent cross-context replay.
/// This prefix is used when emitting session termination events to the
/// ledger, enabling the `GateOrchestrator` to trigger gate lifecycle
/// after session completion.
///
/// # Note
///
/// This is distinct from `SESSION_TERMINATED_DOMAIN_PREFIX` in
/// `apm2_core::events::canonical` which is used for kernel event signing.
/// This prefix is for ledger-level JCS-canonicalized JSON events.
pub const SESSION_TERMINATED_LEDGER_DOMAIN_PREFIX: &[u8] = b"apm2.event.session_terminated_ledger:";

/// Domain separation prefix for `StopFlagsMutated` ledger events (TCK-00351).
pub const STOP_FLAGS_MUTATED_DOMAIN_PREFIX: &[u8] = b"apm2.event.stop_flags_mutated:";

/// Ledger indexing key for daemon stop-flag mutation events.
pub const STOP_FLAGS_MUTATED_WORK_ID: &str = "daemon.stop_flags";

/// Domain separation prefix for `ChangeSetPublished` ledger events (TCK-00394).
///
/// Per RFC-0017 DD-006: domain prefixes prevent cross-context replay.
/// This prefix is used when emitting changeset publication events to the
/// ledger, enabling gate orchestration (TCK-00388) to bind gate leases
/// to published changesets.
pub const CHANGESET_PUBLISHED_LEDGER_DOMAIN_PREFIX: &[u8] = CHANGESET_PUBLISHED_PREFIX;

// Note: `REVIEW_RECEIPT_RECORDED_PREFIX` is imported from `apm2_core::fac`
// to ensure protocol compatibility across the daemon/core boundary (TCK-00321).
// See `apm2_core::fac::domain_separator` for the canonical definition.

/// Domain separation prefix for `ReviewBlockedRecorded` ledger events
/// (TCK-00389).
///
/// Per RFC-0017 DD-006: domain prefixes prevent cross-context replay.
/// This prefix is used when emitting review blocked events to the ledger.
pub const REVIEW_BLOCKED_RECORDED_LEDGER_PREFIX: &[u8] = b"apm2.event.review_blocked_recorded:";

/// Maximum length for ID fields (`work_id`, `lease_id`, etc.).
///
/// Per SEC-SCP-FAC-0020: Unbounded input processing can lead to
/// denial-of-service via OOM. This limit prevents attackers from supplying
/// multi-GB ID strings.
pub const MAX_ID_LENGTH: usize = 256;

/// Maximum allowed length for the `reason` field in `EndSessionRequest`.
///
/// Per SEC-SCP-FAC-0020: Caller-controlled text is written into signed ledger
/// payloads. Unbounded reason strings enable denial-of-service via OOM and
/// bloated ledger entries. This limit constrains the reason to a reasonable
/// diagnostic string.
pub const MAX_REASON_LENGTH: usize = 1024;

/// Maximum allowed length for `SpawnEpisodeRequest.escalation_predicate`.
///
/// Per SEC-SCP-FAC-0020: caller-controlled free-form predicates must be
/// bounded before persistence to prevent oversized payload retention.
pub const MAX_ESCALATION_PREDICATE_LEN: usize = 1024;

/// Builds the canonical JSON payload for a `SessionStarted` ledger event.
///
/// Extracted to a single helper to eliminate triplicated payload construction
/// across `StubLedgerEventEmitter::emit_session_started`,
/// `SqliteLedgerEventEmitter::emit_session_started`, and
/// `SqliteLedgerEventEmitter::emit_spawn_lifecycle`.
///
/// TCK-00348: Includes contract binding fields when available.
#[allow(clippy::too_many_arguments)]
pub fn build_session_started_payload(
    session_id: &str,
    work_id: &str,
    lease_id: &str,
    actor_id: &str,
    adapter_profile_hash: &[u8; 32],
    role_spec_hash: Option<&[u8; 32]>,
    contract_binding: Option<&crate::hsi_contract::SessionContractBinding>,
    identity_proof_profile_hash: Option<&[u8; 32]>,
) -> serde_json::Value {
    let mut payload = serde_json::json!({
        "event_type": "session_started",
        "session_id": session_id,
        "work_id": work_id,
        "lease_id": lease_id,
        "actor_id": actor_id,
        "adapter_profile_hash": hex::encode(adapter_profile_hash),
    });
    if let Some(hash) = role_spec_hash {
        payload.as_object_mut().expect("payload is object").insert(
            "role_spec_hash".to_string(),
            serde_json::Value::String(hex::encode(hash)),
        );
    } else {
        payload.as_object_mut().expect("payload is object").insert(
            "waiver_id".to_string(),
            serde_json::Value::String("WVR-0002".to_string()),
        );
        payload.as_object_mut().expect("payload is object").insert(
            "role_spec_hash_absent".to_string(),
            serde_json::Value::Bool(true),
        );
    }
    if let Some(binding) = contract_binding {
        let obj = payload.as_object_mut().expect("payload is object");
        obj.insert(
            "cli_contract_hash".to_string(),
            serde_json::Value::String(binding.cli_contract_hash.clone()),
        );
        obj.insert(
            "server_contract_hash".to_string(),
            serde_json::Value::String(binding.server_contract_hash.clone()),
        );
        obj.insert(
            "mismatch_waived".to_string(),
            serde_json::Value::Bool(binding.mismatch_waived),
        );
        obj.insert(
            "risk_tier".to_string(),
            serde_json::to_value(binding.risk_tier).expect("RiskTier serializes"),
        );
        obj.insert(
            "client_canonicalizers".to_string(),
            serde_json::to_value(&binding.client_canonicalizers)
                .expect("CanonicalizerInfo serializes"),
        );
    }
    if let Some(profile_hash) = identity_proof_profile_hash {
        payload.as_object_mut().expect("payload is object").insert(
            "identity_proof_profile_hash".to_string(),
            serde_json::Value::String(hex::encode(profile_hash)),
        );
    }
    payload
}

/// Maximum number of events stored in `StubLedgerEventEmitter`.
///
/// Per CTR-1303: In-memory stores must have `max_entries` limit with O(1)
/// eviction. This prevents denial-of-service via memory exhaustion from
/// unbounded event emission.
pub const MAX_LEDGER_EVENTS: usize = 10_000;

/// Stub ledger event emitter for testing.
///
/// Stores events in memory with Ed25519 signatures using a test signing key.
/// In production, this will be replaced with `SqliteLedgerEventEmitter`.
///
/// # Capacity Limits (CTR-1303)
///
/// This emitter enforces a maximum of [`MAX_LEDGER_EVENTS`] entries to prevent
/// memory exhaustion. When the limit is reached, the oldest entry (by insertion
/// order) is evicted to make room for the new event.
#[derive(Debug)]
pub struct StubLedgerEventEmitter {
    /// Events stored with insertion order for LRU eviction.
    /// The `Vec` maintains insertion order; oldest entries are at the front.
    events: std::sync::RwLock<(
        Vec<String>,
        std::collections::HashMap<String, SignedLedgerEvent>,
    )>,
    /// Events indexed by work ID for efficient querying.
    events_by_work_id: std::sync::RwLock<std::collections::HashMap<String, Vec<String>>>,
    /// Signing key for event signatures (test key).
    signing_key: ed25519_dalek::SigningKey,
}

impl Default for StubLedgerEventEmitter {
    fn default() -> Self {
        Self::new()
    }
}

impl StubLedgerEventEmitter {
    /// Creates a new stub emitter with a random test signing key.
    #[must_use]
    pub fn new() -> Self {
        use rand::rngs::OsRng;
        Self {
            events: std::sync::RwLock::new((
                Vec::with_capacity(MAX_LEDGER_EVENTS.min(1000)), // Pre-allocate reasonably
                std::collections::HashMap::with_capacity(MAX_LEDGER_EVENTS.min(1000)),
            )),
            events_by_work_id: std::sync::RwLock::new(std::collections::HashMap::new()),
            signing_key: ed25519_dalek::SigningKey::generate(&mut OsRng),
        }
    }

    /// Creates a new stub emitter with a specific signing key.
    #[must_use]
    pub fn with_signing_key(signing_key: ed25519_dalek::SigningKey) -> Self {
        Self {
            events: std::sync::RwLock::new((
                Vec::with_capacity(MAX_LEDGER_EVENTS.min(1000)), // Pre-allocate reasonably
                std::collections::HashMap::with_capacity(MAX_LEDGER_EVENTS.min(1000)),
            )),
            events_by_work_id: std::sync::RwLock::new(std::collections::HashMap::new()),
            signing_key,
        }
    }

    /// Returns the verifying (public) key for signature verification.
    #[must_use]
    pub fn verifying_key(&self) -> ed25519_dalek::VerifyingKey {
        self.signing_key.verifying_key()
    }
}

impl LedgerEventEmitter for StubLedgerEventEmitter {
    fn emit_work_claimed(
        &self,
        claim: &WorkClaim,
        timestamp_ns: u64,
    ) -> Result<SignedLedgerEvent, LedgerEventError> {
        use ed25519_dalek::Signer;

        // Generate unique event ID
        let event_id = format!("EVT-{}", uuid::Uuid::new_v4());

        // Build canonical payload (deterministic JSON)
        let payload = serde_json::json!({
            "event_type": "work_claimed",
            "work_id": claim.work_id,
            "lease_id": claim.lease_id,
            "actor_id": claim.actor_id,
            "role": format!("{:?}", claim.role),
            "policy_resolved_ref": claim.policy_resolution.policy_resolved_ref,
            "capability_manifest_hash": hex::encode(claim.policy_resolution.capability_manifest_hash),
            "context_pack_hash": hex::encode(claim.policy_resolution.context_pack_hash),
        });

        let payload_bytes =
            serde_json::to_vec(&payload).map_err(|e| LedgerEventError::SigningFailed {
                message: format!("payload serialization failed: {e}"),
            })?;

        // Build canonical bytes for signing (domain prefix + payload)
        let mut canonical_bytes =
            Vec::with_capacity(WORK_CLAIMED_DOMAIN_PREFIX.len() + payload_bytes.len());
        canonical_bytes.extend_from_slice(WORK_CLAIMED_DOMAIN_PREFIX);
        canonical_bytes.extend_from_slice(&payload_bytes);

        // Sign the canonical bytes
        let signature = self.signing_key.sign(&canonical_bytes);

        // TCK-00289: Use provided HTF-compliant timestamp from HolonicClock.
        // The timestamp_ns parameter is now provided by the caller from
        // HolonicClock.now_hlc() ensuring RFC-0016 HTF compliance.

        let signed_event = SignedLedgerEvent {
            event_id: event_id.clone(),
            event_type: "work_claimed".to_string(),
            work_id: claim.work_id.clone(),
            actor_id: claim.actor_id.clone(),
            payload: payload_bytes,
            signature: signature.to_bytes().to_vec(),
            timestamp_ns,
        };

        // CTR-1303: Persist to bounded in-memory store with LRU eviction
        {
            let mut guard = self.events.write().expect("lock poisoned");
            let mut events_by_work = self.events_by_work_id.write().expect("lock poisoned");
            let (order, events) = &mut *guard;

            // Evict oldest entries if at capacity, also pruning events_by_work_id index
            while events.len() >= MAX_LEDGER_EVENTS {
                if let Some(oldest_key) = order.first().cloned() {
                    order.remove(0);
                    // Remove from events and prune the events_by_work_id index
                    if let Some(evicted_event) = events.remove(&oldest_key) {
                        // Remove from work_id index
                        if let Some(work_id_events) = events_by_work.get_mut(&evicted_event.work_id)
                        {
                            work_id_events.retain(|id| id != &oldest_key);
                            // Remove the entry entirely if no events remain for this work_id
                            if work_id_events.is_empty() {
                                events_by_work.remove(&evicted_event.work_id);
                            }
                        }
                    }
                    debug!(
                        evicted_event_id = %oldest_key,
                        "Evicted oldest ledger event to maintain capacity limit"
                    );
                } else {
                    break;
                }
            }

            order.push(event_id.clone());
            events.insert(event_id.clone(), signed_event.clone());
            events_by_work
                .entry(claim.work_id.clone())
                .or_default()
                .push(event_id);
        }

        info!(
            event_id = %signed_event.event_id,
            work_id = %signed_event.work_id,
            actor_id = %signed_event.actor_id,
            "WorkClaimed event signed and persisted"
        );

        Ok(signed_event)
    }

    fn get_event(&self, event_id: &str) -> Option<SignedLedgerEvent> {
        let guard = self.events.read().expect("lock poisoned");
        guard.1.get(event_id).cloned()
    }

    fn emit_session_started(
        &self,
        session_id: &str,
        work_id: &str,
        lease_id: &str,
        actor_id: &str,
        adapter_profile_hash: &[u8; 32],
        role_spec_hash: Option<&[u8; 32]>,
        timestamp_ns: u64,
        contract_binding: Option<&crate::hsi_contract::SessionContractBinding>,
        identity_proof_profile_hash: Option<&[u8; 32]>,
    ) -> Result<SignedLedgerEvent, LedgerEventError> {
        use ed25519_dalek::Signer;

        // Domain prefix for session events (must be at function start per clippy)
        const SESSION_STARTED_DOMAIN_PREFIX: &[u8] = b"apm2.event.session_started:";

        // Generate unique event ID
        let event_id = format!("EVT-{}", uuid::Uuid::new_v4());

        let payload = build_session_started_payload(
            session_id,
            work_id,
            lease_id,
            actor_id,
            adapter_profile_hash,
            role_spec_hash,
            contract_binding,
            identity_proof_profile_hash,
        );

        let payload_bytes =
            serde_json::to_vec(&payload).map_err(|e| LedgerEventError::SigningFailed {
                message: format!("payload serialization failed: {e}"),
            })?;

        // Build canonical bytes for signing (domain prefix + payload)
        let mut canonical_bytes =
            Vec::with_capacity(SESSION_STARTED_DOMAIN_PREFIX.len() + payload_bytes.len());
        canonical_bytes.extend_from_slice(SESSION_STARTED_DOMAIN_PREFIX);
        canonical_bytes.extend_from_slice(&payload_bytes);

        // Sign the canonical bytes
        let signature = self.signing_key.sign(&canonical_bytes);

        let signed_event = SignedLedgerEvent {
            event_id: event_id.clone(),
            event_type: "session_started".to_string(),
            work_id: work_id.to_string(),
            actor_id: actor_id.to_string(),
            payload: payload_bytes,
            signature: signature.to_bytes().to_vec(),
            timestamp_ns,
        };

        // CTR-1303: Persist to bounded in-memory store with LRU eviction
        {
            let mut guard = self.events.write().expect("lock poisoned");
            let mut events_by_work = self.events_by_work_id.write().expect("lock poisoned");
            let (order, events) = &mut *guard;

            // Evict oldest entries if at capacity
            while events.len() >= MAX_LEDGER_EVENTS {
                if let Some(oldest_key) = order.first().cloned() {
                    order.remove(0);
                    if let Some(evicted_event) = events.remove(&oldest_key) {
                        if let Some(work_id_events) = events_by_work.get_mut(&evicted_event.work_id)
                        {
                            work_id_events.retain(|id| id != &oldest_key);
                            if work_id_events.is_empty() {
                                events_by_work.remove(&evicted_event.work_id);
                            }
                        }
                    }
                } else {
                    break;
                }
            }

            order.push(event_id.clone());
            events.insert(event_id.clone(), signed_event.clone());
            events_by_work
                .entry(work_id.to_string())
                .or_default()
                .push(event_id);
        }

        info!(
            event_id = %signed_event.event_id,
            session_id = %session_id,
            work_id = %signed_event.work_id,
            "SessionStarted event signed and persisted"
        );

        Ok(signed_event)
    }

    fn emit_session_event(
        &self,
        session_id: &str,
        event_type: &str,
        payload: &[u8],
        actor_id: &str,
        timestamp_ns: u64,
    ) -> Result<SignedLedgerEvent, LedgerEventError> {
        use ed25519_dalek::Signer;

        // Domain prefix for generic session events (TCK-00290)
        const SESSION_EVENT_DOMAIN_PREFIX: &[u8] = b"apm2.event.session_event:";

        // Generate unique event ID
        let event_id = format!("EVT-{}", uuid::Uuid::new_v4());

        // Build payload as JSON with actual event type and hex-encoded payload
        let payload_json = serde_json::json!({
            "event_type": event_type,
            "session_id": session_id,
            "actor_id": actor_id,
            "payload": hex::encode(payload),
        });

        // MAJOR 1 FIX (TCK-00290): Use JCS (RFC 8785) canonicalization for signing.
        // This matches the production SqliteLedgerEventEmitter and ensures
        // deterministic JSON representation per RFC-0016. Using
        // serde_json::to_vec is non-deterministic because it does not guarantee
        // key ordering.
        let payload_string = payload_json.to_string();
        let canonical_payload =
            canonicalize_json(&payload_string).map_err(|e| LedgerEventError::SigningFailed {
                message: format!("JCS canonicalization failed: {e}"),
            })?;
        let payload_bytes = canonical_payload.as_bytes().to_vec();

        // Build canonical bytes for signing (domain prefix + JCS payload)
        let mut canonical_bytes =
            Vec::with_capacity(SESSION_EVENT_DOMAIN_PREFIX.len() + payload_bytes.len());
        canonical_bytes.extend_from_slice(SESSION_EVENT_DOMAIN_PREFIX);
        canonical_bytes.extend_from_slice(&payload_bytes);

        // Sign the canonical bytes
        let signature = self.signing_key.sign(&canonical_bytes);

        let signed_event = SignedLedgerEvent {
            event_id: event_id.clone(),
            event_type: event_type.to_string(),
            work_id: session_id.to_string(), // Use session_id as work_id for indexing
            actor_id: actor_id.to_string(),
            payload: payload_bytes,
            signature: signature.to_bytes().to_vec(),
            timestamp_ns,
        };

        // CTR-1303: Persist to bounded in-memory store with LRU eviction
        {
            let mut guard = self.events.write().expect("lock poisoned");
            let mut events_by_work = self.events_by_work_id.write().expect("lock poisoned");
            let (order, events) = &mut *guard;

            // Evict oldest entries if at capacity
            while events.len() >= MAX_LEDGER_EVENTS {
                if let Some(oldest_key) = order.first().cloned() {
                    order.remove(0);
                    if let Some(evicted_event) = events.remove(&oldest_key) {
                        if let Some(work_id_events) = events_by_work.get_mut(&evicted_event.work_id)
                        {
                            work_id_events.retain(|id| id != &oldest_key);
                            if work_id_events.is_empty() {
                                events_by_work.remove(&evicted_event.work_id);
                            }
                        }
                    }
                } else {
                    break;
                }
            }

            order.push(event_id.clone());
            events.insert(event_id.clone(), signed_event.clone());
            events_by_work
                .entry(session_id.to_string())
                .or_default()
                .push(event_id);
        }

        info!(
            event_id = %signed_event.event_id,
            session_id = %session_id,
            event_type = %event_type,
            actor_id = %actor_id,
            "SessionEvent signed and persisted"
        );

        Ok(signed_event)
    }

    fn has_durable_storage(&self) -> bool {
        false
    }

    fn emit_stop_flags_mutated(
        &self,
        mutation: &StopFlagsMutation<'_>,
    ) -> Result<SignedLedgerEvent, LedgerEventError> {
        use ed25519_dalek::Signer;

        let event_id = format!("EVT-{}", uuid::Uuid::new_v4());

        let payload_json = serde_json::json!({
            "event_type": "stop_flags_mutated",
            "actor_id": mutation.actor_id,
            "emergency_stop_previous": mutation.emergency_stop_previous,
            "emergency_stop_current": mutation.emergency_stop_current,
            "governance_stop_previous": mutation.governance_stop_previous,
            "governance_stop_current": mutation.governance_stop_current,
            "request_context": mutation.request_context,
            "timestamp_ns": mutation.timestamp_ns,
        });

        let payload_string = payload_json.to_string();
        let canonical_payload =
            canonicalize_json(&payload_string).map_err(|e| LedgerEventError::SigningFailed {
                message: format!("JCS canonicalization failed: {e}"),
            })?;
        let payload_bytes = canonical_payload.as_bytes().to_vec();

        let mut canonical_bytes =
            Vec::with_capacity(STOP_FLAGS_MUTATED_DOMAIN_PREFIX.len() + payload_bytes.len());
        canonical_bytes.extend_from_slice(STOP_FLAGS_MUTATED_DOMAIN_PREFIX);
        canonical_bytes.extend_from_slice(&payload_bytes);

        let signature = self.signing_key.sign(&canonical_bytes);

        let signed_event = SignedLedgerEvent {
            event_id: event_id.clone(),
            event_type: "stop_flags_mutated".to_string(),
            work_id: STOP_FLAGS_MUTATED_WORK_ID.to_string(),
            actor_id: mutation.actor_id.to_string(),
            payload: payload_bytes,
            signature: signature.to_bytes().to_vec(),
            timestamp_ns: mutation.timestamp_ns,
        };

        {
            let mut guard = self.events.write().expect("lock poisoned");
            let mut events_by_work = self.events_by_work_id.write().expect("lock poisoned");
            let (order, events) = &mut *guard;

            while events.len() >= MAX_LEDGER_EVENTS {
                if let Some(oldest_key) = order.first().cloned() {
                    order.remove(0);
                    if let Some(evicted_event) = events.remove(&oldest_key) {
                        if let Some(work_id_events) = events_by_work.get_mut(&evicted_event.work_id)
                        {
                            work_id_events.retain(|id| id != &oldest_key);
                            if work_id_events.is_empty() {
                                events_by_work.remove(&evicted_event.work_id);
                            }
                        }
                    }
                } else {
                    break;
                }
            }

            order.push(event_id.clone());
            events.insert(event_id.clone(), signed_event.clone());
            events_by_work
                .entry(STOP_FLAGS_MUTATED_WORK_ID.to_string())
                .or_default()
                .push(event_id);
        }

        info!(
            event_id = %signed_event.event_id,
            actor_id = %mutation.actor_id,
            emergency_stop_previous = mutation.emergency_stop_previous,
            emergency_stop_current = mutation.emergency_stop_current,
            governance_stop_previous = mutation.governance_stop_previous,
            governance_stop_current = mutation.governance_stop_current,
            "StopFlagsMutated event signed and persisted"
        );

        Ok(signed_event)
    }

    fn emit_defect_recorded(
        &self,
        defect: &DefectRecorded,
        timestamp_ns: u64,
    ) -> Result<SignedLedgerEvent, LedgerEventError> {
        use ed25519_dalek::Signer;

        // TCK-00307 MAJOR 4: Call validate() to enforce DoS protections
        defect
            .validate()
            .map_err(|e| LedgerEventError::ValidationFailed { message: e })?;

        // Generate unique event ID
        let event_id = format!("EVT-{}", uuid::Uuid::new_v4());

        // TCK-00307 BLOCKER: Use JCS/JSON wire format (not ProtoBuf) to match
        // SqliteLedgerEventEmitter and ensure consumer uniformity.
        // Include time_envelope_ref for temporal binding (MAJOR 1).
        let time_envelope_ref_hex = defect
            .time_envelope_ref
            .as_ref()
            .map(|ter| hex::encode(&ter.hash));

        let payload = serde_json::json!({
            "event_type": "defect_recorded",
            "defect_id": defect.defect_id,
            "defect_type": defect.defect_type,
            "cas_hash": hex::encode(&defect.cas_hash),
            "source": defect.source,
            "work_id": defect.work_id,
            "severity": defect.severity,
            "detected_at": defect.detected_at,
            "time_envelope_ref": time_envelope_ref_hex,
        });

        // Use JCS (RFC 8785) canonicalization for deterministic signing
        let payload_json = payload.to_string();
        let canonical_payload =
            canonicalize_json(&payload_json).map_err(|e| LedgerEventError::SigningFailed {
                message: format!("JCS canonicalization failed: {e}"),
            })?;
        let payload_bytes = canonical_payload.as_bytes().to_vec();

        // Build canonical bytes for signing (domain prefix + JCS payload)
        let mut canonical_bytes =
            Vec::with_capacity(DEFECT_RECORDED_DOMAIN_PREFIX.len() + payload_bytes.len());
        canonical_bytes.extend_from_slice(DEFECT_RECORDED_DOMAIN_PREFIX);
        canonical_bytes.extend_from_slice(&payload_bytes);

        // Sign the canonical bytes
        let signature = self.signing_key.sign(&canonical_bytes);

        let signed_event = SignedLedgerEvent {
            event_id: event_id.clone(),
            event_type: "defect_recorded".to_string(),
            work_id: defect.work_id.clone(),
            actor_id: "system".to_string(),
            payload: payload_bytes,
            signature: signature.to_bytes().to_vec(),
            timestamp_ns,
        };

        // CTR-1303: Persist to bounded in-memory store with LRU eviction
        {
            let mut guard = self.events.write().expect("lock poisoned");
            let mut events_by_work = self.events_by_work_id.write().expect("lock poisoned");
            let (order, events) = &mut *guard;

            // Evict oldest entries if at capacity
            while events.len() >= MAX_LEDGER_EVENTS {
                if let Some(oldest_key) = order.first().cloned() {
                    order.remove(0);
                    if let Some(evicted_event) = events.remove(&oldest_key) {
                        if let Some(work_id_events) = events_by_work.get_mut(&evicted_event.work_id)
                        {
                            work_id_events.retain(|id| id != &oldest_key);
                            if work_id_events.is_empty() {
                                events_by_work.remove(&evicted_event.work_id);
                            }
                        }
                    }
                } else {
                    break;
                }
            }

            order.push(event_id.clone());
            events.insert(event_id.clone(), signed_event.clone());
            events_by_work
                .entry(defect.work_id.clone())
                .or_default()
                .push(event_id);
        }

        info!(
            event_id = %signed_event.event_id,
            defect_id = %defect.defect_id,
            work_id = %signed_event.work_id,
            "DefectRecorded event signed and persisted"
        );

        Ok(signed_event)
    }

    fn get_events_by_work_id(&self, work_id: &str) -> Vec<SignedLedgerEvent> {
        let events_by_work = self.events_by_work_id.read().expect("lock poisoned");
        let guard = self.events.read().expect("lock poisoned");

        events_by_work
            .get(work_id)
            .map(|event_ids| {
                event_ids
                    .iter()
                    .filter_map(|id| guard.1.get(id).cloned())
                    .collect()
            })
            .unwrap_or_default()
    }

    fn get_event_by_receipt_id(&self, receipt_id: &str) -> Option<SignedLedgerEvent> {
        let guard = self.events.read().expect("lock poisoned");
        // Search all review receipt events for a matching receipt_id in the
        // JCS-canonicalized JSON payload. Both `review_receipt_recorded` and
        // `review_blocked_recorded` events embed `receipt_id` in the payload.
        for event in guard.1.values() {
            if event.event_type != "review_receipt_recorded"
                && event.event_type != "review_blocked_recorded"
            {
                continue;
            }
            // The payload is JCS-canonicalized JSON stored as bytes.
            if let Ok(payload) = serde_json::from_slice::<serde_json::Value>(&event.payload) {
                if payload
                    .get("receipt_id")
                    .and_then(serde_json::Value::as_str)
                    == Some(receipt_id)
                {
                    return Some(event.clone());
                }
            }
        }
        None
    }

    fn get_work_transition_count(&self, work_id: &str) -> u32 {
        let events_by_work = self.events_by_work_id.read().expect("lock poisoned");
        let guard = self.events.read().expect("lock poisoned");

        events_by_work.get(work_id).map_or(0, |event_ids| {
            #[allow(clippy::cast_possible_truncation)]
            let count = event_ids
                .iter()
                .filter_map(|id| guard.1.get(id))
                .filter(|e| e.event_type == "work_transitioned")
                .count() as u32;
            count
        })
    }

    fn emit_episode_event(
        &self,
        episode_id: &str,
        event_type: &str,
        payload: &[u8],
        timestamp_ns: u64,
    ) -> Result<SignedLedgerEvent, LedgerEventError> {
        use ed25519_dalek::Signer;

        // Generate unique event ID
        let event_id = format!("EVT-{}", uuid::Uuid::new_v4());

        // Build payload as JSON with episode event metadata
        // The payload is already JSON-serialized episode event data
        // SECURITY: timestamp_ns is included in signed payload to prevent temporal
        // malleability per LAW-09 (Temporal Pinning & Freshness) and RS-40
        // (Time & Monotonicity)
        let payload_json = serde_json::json!({
            "event_type": event_type,
            "episode_id": episode_id,
            "payload": hex::encode(payload),
            "timestamp_ns": timestamp_ns,
        });

        // Use JCS (RFC 8785) canonicalization for deterministic signing
        let payload_string = payload_json.to_string();
        let canonical_payload =
            canonicalize_json(&payload_string).map_err(|e| LedgerEventError::SigningFailed {
                message: format!("JCS canonicalization failed: {e}"),
            })?;
        let payload_bytes = canonical_payload.as_bytes().to_vec();

        // Build canonical bytes for signing (domain prefix + JCS payload)
        let mut canonical_bytes =
            Vec::with_capacity(EPISODE_EVENT_DOMAIN_PREFIX.len() + payload_bytes.len());
        canonical_bytes.extend_from_slice(EPISODE_EVENT_DOMAIN_PREFIX);
        canonical_bytes.extend_from_slice(&payload_bytes);

        // Sign the canonical bytes
        let signature = self.signing_key.sign(&canonical_bytes);

        let signed_event = SignedLedgerEvent {
            event_id: event_id.clone(),
            event_type: event_type.to_string(),
            work_id: episode_id.to_string(), // Use episode_id as work_id for indexing
            actor_id: "daemon".to_string(),  // Episode events are daemon-authored
            payload: payload_bytes,
            signature: signature.to_bytes().to_vec(),
            timestamp_ns,
        };

        // CTR-1303: Persist to bounded in-memory store with LRU eviction
        {
            let mut guard = self.events.write().expect("lock poisoned");
            let mut events_by_work = self.events_by_work_id.write().expect("lock poisoned");
            let (order, events) = &mut *guard;

            // Evict oldest entries if at capacity
            while events.len() >= MAX_LEDGER_EVENTS {
                if let Some(oldest_key) = order.first().cloned() {
                    order.remove(0);
                    if let Some(evicted_event) = events.remove(&oldest_key) {
                        if let Some(work_id_events) = events_by_work.get_mut(&evicted_event.work_id)
                        {
                            work_id_events.retain(|id| id != &oldest_key);
                            if work_id_events.is_empty() {
                                events_by_work.remove(&evicted_event.work_id);
                            }
                        }
                    }
                } else {
                    break;
                }
            }

            order.push(event_id.clone());
            events.insert(event_id.clone(), signed_event.clone());
            events_by_work
                .entry(episode_id.to_string())
                .or_default()
                .push(event_id);
        }

        info!(
            event_id = %signed_event.event_id,
            episode_id = %episode_id,
            event_type = %event_type,
            "EpisodeEvent signed and persisted"
        );

        Ok(signed_event)
    }

    fn emit_review_receipt(
        &self,
        episode_id: &str,
        receipt_id: &str,
        changeset_digest: &[u8; 32],
        artifact_bundle_hash: &[u8; 32],
        reviewer_actor_id: &str,
        timestamp_ns: u64,
        identity_proof_hash: &[u8; 32],
    ) -> Result<SignedLedgerEvent, LedgerEventError> {
        use ed25519_dalek::Signer;

        // Generate unique event ID
        let event_id = format!("EVT-{}", uuid::Uuid::new_v4());

        // Build payload as JSON with review receipt data
        // SECURITY: timestamp_ns is included in signed payload to prevent temporal
        // malleability per LAW-09 (Temporal Pinning & Freshness) and RS-40
        // (Time & Monotonicity)
        //
        // SECURITY (TCK-00356 Fix 1): identity_proof_hash is included in
        // the signed payload so it is audit-bound and cannot be stripped
        // post-signing.
        let payload_json = serde_json::json!({
            "event_type": "review_receipt_recorded",
            "episode_id": episode_id,
            "lease_id": episode_id,
            "receipt_id": receipt_id,
            "changeset_digest": hex::encode(changeset_digest),
            "artifact_bundle_hash": hex::encode(artifact_bundle_hash),
            "verdict": "APPROVE",
            "reviewer_actor_id": reviewer_actor_id,
            "timestamp_ns": timestamp_ns,
            "identity_proof_hash": hex::encode(identity_proof_hash),
        });

        // Use JCS (RFC 8785) canonicalization for deterministic signing
        let payload_string = payload_json.to_string();
        let canonical_payload =
            canonicalize_json(&payload_string).map_err(|e| LedgerEventError::SigningFailed {
                message: format!("JCS canonicalization failed: {e}"),
            })?;
        let payload_bytes = canonical_payload.as_bytes().to_vec();

        // Build canonical bytes for signing (domain prefix + JCS payload)
        // TCK-00321: Use REVIEW_RECEIPT_RECORDED_PREFIX from apm2_core::fac for
        // protocol compatibility across daemon/core boundary.
        let mut canonical_bytes =
            Vec::with_capacity(REVIEW_RECEIPT_RECORDED_PREFIX.len() + payload_bytes.len());
        canonical_bytes.extend_from_slice(REVIEW_RECEIPT_RECORDED_PREFIX);
        canonical_bytes.extend_from_slice(&payload_bytes);

        // Sign the canonical bytes
        let signature = self.signing_key.sign(&canonical_bytes);

        let signed_event = SignedLedgerEvent {
            event_id: event_id.clone(),
            event_type: "review_receipt_recorded".to_string(),
            work_id: episode_id.to_string(),
            actor_id: reviewer_actor_id.to_string(),
            payload: payload_bytes,
            signature: signature.to_bytes().to_vec(),
            timestamp_ns,
        };

        // CTR-1303: Persist to bounded in-memory store with LRU eviction
        {
            let mut guard = self.events.write().expect("lock poisoned");
            let mut events_by_work = self.events_by_work_id.write().expect("lock poisoned");
            let (order, events) = &mut *guard;

            // Evict oldest entries if at capacity
            while events.len() >= MAX_LEDGER_EVENTS {
                if let Some(oldest_key) = order.first().cloned() {
                    order.remove(0);
                    if let Some(evicted_event) = events.remove(&oldest_key) {
                        if let Some(work_id_events) = events_by_work.get_mut(&evicted_event.work_id)
                        {
                            work_id_events.retain(|id| id != &oldest_key);
                            if work_id_events.is_empty() {
                                events_by_work.remove(&evicted_event.work_id);
                            }
                        }
                    }
                } else {
                    break;
                }
            }

            order.push(event_id.clone());
            events.insert(event_id.clone(), signed_event.clone());
            events_by_work
                .entry(episode_id.to_string())
                .or_default()
                .push(event_id);
        }

        info!(
            event_id = %signed_event.event_id,
            episode_id = %episode_id,
            receipt_id = %receipt_id,
            "ReviewReceiptRecorded event signed and persisted"
        );

        Ok(signed_event)
    }

    #[allow(clippy::too_many_arguments)]
    fn emit_review_blocked_receipt(
        &self,
        lease_id: &str,
        receipt_id: &str,
        changeset_digest: &[u8; 32],
        artifact_bundle_hash: &[u8; 32],
        reason_code: u32,
        blocked_log_hash: &[u8; 32],
        reviewer_actor_id: &str,
        timestamp_ns: u64,
        identity_proof_hash: &[u8; 32],
    ) -> Result<SignedLedgerEvent, LedgerEventError> {
        use ed25519_dalek::Signer;

        let event_id = format!("EVT-{}", uuid::Uuid::new_v4());

        // SECURITY (TCK-00356 Fix 2): identity_proof_hash is included in
        // the signed payload so it is audit-bound and cannot be stripped
        // post-signing, matching the APPROVE path's payload binding.
        let payload_json = serde_json::json!({
            "event_type": "review_blocked_recorded",
            "lease_id": lease_id,
            "receipt_id": receipt_id,
            "changeset_digest": hex::encode(changeset_digest),
            "artifact_bundle_hash": hex::encode(artifact_bundle_hash),
            "verdict": "BLOCKED",
            "blocked_reason_code": reason_code,
            // Preserve legacy field for backward compatibility with old readers.
            "reason_code": reason_code,
            "blocked_log_hash": hex::encode(blocked_log_hash),
            "reviewer_actor_id": reviewer_actor_id,
            "timestamp_ns": timestamp_ns,
            "identity_proof_hash": hex::encode(identity_proof_hash),
        });

        let payload_string = payload_json.to_string();
        let canonical_payload =
            canonicalize_json(&payload_string).map_err(|e| LedgerEventError::SigningFailed {
                message: format!("JCS canonicalization failed: {e}"),
            })?;
        let payload_bytes = canonical_payload.as_bytes().to_vec();

        let mut canonical_bytes =
            Vec::with_capacity(REVIEW_BLOCKED_RECORDED_LEDGER_PREFIX.len() + payload_bytes.len());
        canonical_bytes.extend_from_slice(REVIEW_BLOCKED_RECORDED_LEDGER_PREFIX);
        canonical_bytes.extend_from_slice(&payload_bytes);

        let signature = self.signing_key.sign(&canonical_bytes);

        let signed_event = SignedLedgerEvent {
            event_id: event_id.clone(),
            event_type: "review_blocked_recorded".to_string(),
            work_id: receipt_id.to_string(),
            actor_id: reviewer_actor_id.to_string(),
            payload: payload_bytes,
            signature: signature.to_bytes().to_vec(),
            timestamp_ns,
        };

        // CTR-1303: Persist to bounded in-memory store with LRU eviction
        {
            let mut guard = self.events.write().expect("lock poisoned");
            let mut events_by_work = self.events_by_work_id.write().expect("lock poisoned");
            let (order, events) = &mut *guard;

            while events.len() >= MAX_LEDGER_EVENTS {
                if let Some(oldest_key) = order.first().cloned() {
                    order.remove(0);
                    if let Some(evicted_event) = events.remove(&oldest_key) {
                        if let Some(work_id_events) = events_by_work.get_mut(&evicted_event.work_id)
                        {
                            work_id_events.retain(|id| id != &oldest_key);
                            if work_id_events.is_empty() {
                                events_by_work.remove(&evicted_event.work_id);
                            }
                        }
                    }
                } else {
                    break;
                }
            }

            order.push(event_id.clone());
            events.insert(event_id.clone(), signed_event.clone());
            events_by_work
                .entry(receipt_id.to_string())
                .or_default()
                .push(event_id);
        }

        info!(
            event_id = %signed_event.event_id,
            receipt_id = %receipt_id,
            reason_code = %reason_code,
            "ReviewBlockedRecorded event signed and persisted"
        );

        Ok(signed_event)
    }

    fn emit_episode_run_attributed(
        &self,
        work_id: &str,
        episode_id: &str,
        session_id: &str,
        adapter_profile_hash: &[u8; 32],
        timestamp_ns: u64,
    ) -> Result<SignedLedgerEvent, LedgerEventError> {
        use ed25519_dalek::Signer;

        // Generate unique event ID
        let event_id = format!("EVT-{}", uuid::Uuid::new_v4());

        // Build payload as JSON with run attribution data
        // SECURITY: timestamp_ns is included in signed payload to prevent temporal
        // malleability per LAW-09 (Temporal Pinning & Freshness) and RS-40
        // (Time & Monotonicity)
        // TCK-00330: adapter_profile_hash provides ledger attribution for profile-based
        // auditing
        let payload_json = serde_json::json!({
            "event_type": "episode_run_attributed",
            "work_id": work_id,
            "episode_id": episode_id,
            "session_id": session_id,
            "adapter_profile_hash": hex::encode(adapter_profile_hash),
            "timestamp_ns": timestamp_ns,
        });

        // Use JCS (RFC 8785) canonicalization for deterministic signing
        let payload_string = payload_json.to_string();
        let canonical_payload =
            canonicalize_json(&payload_string).map_err(|e| LedgerEventError::SigningFailed {
                message: format!("JCS canonicalization failed: {e}"),
            })?;
        let payload_bytes = canonical_payload.as_bytes().to_vec();

        // Build canonical bytes for signing (domain prefix + JCS payload)
        let mut canonical_bytes =
            Vec::with_capacity(EPISODE_RUN_ATTRIBUTED_PREFIX.len() + payload_bytes.len());
        canonical_bytes.extend_from_slice(EPISODE_RUN_ATTRIBUTED_PREFIX);
        canonical_bytes.extend_from_slice(&payload_bytes);

        // Sign the canonical bytes
        let signature = self.signing_key.sign(&canonical_bytes);

        let signed_event = SignedLedgerEvent {
            event_id: event_id.clone(),
            event_type: "episode_run_attributed".to_string(),
            work_id: work_id.to_string(),
            actor_id: session_id.to_string(), // Session is the actor for run attribution
            payload: payload_bytes,
            signature: signature.to_bytes().to_vec(),
            timestamp_ns,
        };

        // CTR-1303: Persist to bounded in-memory store with LRU eviction
        {
            let mut guard = self.events.write().expect("lock poisoned");
            let mut events_by_work = self.events_by_work_id.write().expect("lock poisoned");
            let (order, events) = &mut *guard;

            // Evict oldest entries if at capacity
            while events.len() >= MAX_LEDGER_EVENTS {
                if let Some(oldest_key) = order.first().cloned() {
                    order.remove(0);
                    if let Some(evicted_event) = events.remove(&oldest_key) {
                        if let Some(work_id_events) = events_by_work.get_mut(&evicted_event.work_id)
                        {
                            work_id_events.retain(|id| id != &oldest_key);
                            if work_id_events.is_empty() {
                                events_by_work.remove(&evicted_event.work_id);
                            }
                        }
                    }
                } else {
                    break;
                }
            }

            order.push(event_id.clone());
            events.insert(event_id.clone(), signed_event.clone());
            events_by_work
                .entry(work_id.to_string())
                .or_default()
                .push(event_id);
        }

        info!(
            event_id = %signed_event.event_id,
            work_id = %work_id,
            episode_id = %episode_id,
            session_id = %session_id,
            adapter_profile_hash = %hex::encode(adapter_profile_hash),
            "EpisodeRunAttributed event signed and persisted"
        );

        Ok(signed_event)
    }

    fn emit_work_transitioned(
        &self,
        transition: &WorkTransition<'_>,
    ) -> Result<SignedLedgerEvent, LedgerEventError> {
        use ed25519_dalek::Signer;

        // Generate unique event ID
        let event_id = format!("EVT-{}", uuid::Uuid::new_v4());

        // Build payload as JSON with work transition data
        // SECURITY: timestamp_ns is included in signed payload to prevent temporal
        // malleability per LAW-09 (Temporal Pinning & Freshness)
        let payload_json = serde_json::json!({
            "event_type": "work_transitioned",
            "work_id": transition.work_id,
            "from_state": transition.from_state,
            "to_state": transition.to_state,
            "rationale_code": transition.rationale_code,
            "previous_transition_count": transition.previous_transition_count,
            "actor_id": transition.actor_id,
            "timestamp_ns": transition.timestamp_ns,
        });

        // TCK-00395: Use JCS (RFC 8785) canonicalization for signing.
        let payload_string = payload_json.to_string();
        let canonical_payload =
            canonicalize_json(&payload_string).map_err(|e| LedgerEventError::SigningFailed {
                message: format!("JCS canonicalization failed: {e}"),
            })?;
        let payload_bytes = canonical_payload.as_bytes().to_vec();

        // Build canonical bytes for signing (domain prefix + JCS payload)
        let mut canonical_bytes =
            Vec::with_capacity(WORK_TRANSITIONED_DOMAIN_PREFIX.len() + payload_bytes.len());
        canonical_bytes.extend_from_slice(WORK_TRANSITIONED_DOMAIN_PREFIX);
        canonical_bytes.extend_from_slice(&payload_bytes);

        // Sign the canonical bytes
        let signature = self.signing_key.sign(&canonical_bytes);

        let signed_event = SignedLedgerEvent {
            event_id: event_id.clone(),
            event_type: "work_transitioned".to_string(),
            work_id: transition.work_id.to_string(),
            actor_id: transition.actor_id.to_string(),
            payload: payload_bytes,
            signature: signature.to_bytes().to_vec(),
            timestamp_ns: transition.timestamp_ns,
        };

        // CTR-1303: Persist to bounded in-memory store with LRU eviction
        {
            let mut guard = self.events.write().expect("lock poisoned");
            let mut events_by_work = self.events_by_work_id.write().expect("lock poisoned");
            let (order, events) = &mut *guard;

            // Evict oldest entries if at capacity
            while events.len() >= MAX_LEDGER_EVENTS {
                if let Some(oldest_key) = order.first().cloned() {
                    order.remove(0);
                    if let Some(evicted_event) = events.remove(&oldest_key) {
                        if let Some(work_id_events) = events_by_work.get_mut(&evicted_event.work_id)
                        {
                            work_id_events.retain(|id| id != &oldest_key);
                            if work_id_events.is_empty() {
                                events_by_work.remove(&evicted_event.work_id);
                            }
                        }
                    }
                } else {
                    break;
                }
            }

            order.push(event_id.clone());
            events.insert(event_id.clone(), signed_event.clone());
            events_by_work
                .entry(transition.work_id.to_string())
                .or_default()
                .push(event_id);
        }

        info!(
            event_id = %signed_event.event_id,
            work_id = %transition.work_id,
            from_state = %transition.from_state,
            to_state = %transition.to_state,
            rationale_code = %transition.rationale_code,
            "WorkTransitioned event signed and persisted"
        );

        Ok(signed_event)
    }

    fn emit_session_terminated(
        &self,
        session_id: &str,
        work_id: &str,
        exit_code: i32,
        termination_reason: &str,
        actor_id: &str,
        timestamp_ns: u64,
    ) -> Result<SignedLedgerEvent, LedgerEventError> {
        use ed25519_dalek::Signer;

        // Generate unique event ID
        let event_id = format!("EVT-{}", uuid::Uuid::new_v4());

        // Build payload as JSON with session termination data
        // SECURITY: timestamp_ns is included in signed payload to prevent temporal
        // malleability per LAW-09 (Temporal Pinning & Freshness)
        let payload_json = serde_json::json!({
            "event_type": "session_terminated",
            "session_id": session_id,
            "work_id": work_id,
            "exit_code": exit_code,
            "termination_reason": termination_reason,
            "actor_id": actor_id,
            "timestamp_ns": timestamp_ns,
        });

        // TCK-00395: Use JCS (RFC 8785) canonicalization for signing.
        let payload_string = payload_json.to_string();
        let canonical_payload =
            canonicalize_json(&payload_string).map_err(|e| LedgerEventError::SigningFailed {
                message: format!("JCS canonicalization failed: {e}"),
            })?;
        let payload_bytes = canonical_payload.as_bytes().to_vec();

        // Build canonical bytes for signing (domain prefix + JCS payload)
        let mut canonical_bytes =
            Vec::with_capacity(SESSION_TERMINATED_LEDGER_DOMAIN_PREFIX.len() + payload_bytes.len());
        canonical_bytes.extend_from_slice(SESSION_TERMINATED_LEDGER_DOMAIN_PREFIX);
        canonical_bytes.extend_from_slice(&payload_bytes);

        // Sign the canonical bytes
        let signature = self.signing_key.sign(&canonical_bytes);

        let signed_event = SignedLedgerEvent {
            event_id: event_id.clone(),
            event_type: "session_terminated".to_string(),
            work_id: work_id.to_string(),
            actor_id: actor_id.to_string(),
            payload: payload_bytes,
            signature: signature.to_bytes().to_vec(),
            timestamp_ns,
        };

        // CTR-1303: Persist to bounded in-memory store with LRU eviction
        {
            let mut guard = self.events.write().expect("lock poisoned");
            let mut events_by_work = self.events_by_work_id.write().expect("lock poisoned");
            let (order, events) = &mut *guard;

            // Evict oldest entries if at capacity
            while events.len() >= MAX_LEDGER_EVENTS {
                if let Some(oldest_key) = order.first().cloned() {
                    order.remove(0);
                    if let Some(evicted_event) = events.remove(&oldest_key) {
                        if let Some(work_id_events) = events_by_work.get_mut(&evicted_event.work_id)
                        {
                            work_id_events.retain(|id| id != &oldest_key);
                            if work_id_events.is_empty() {
                                events_by_work.remove(&evicted_event.work_id);
                            }
                        }
                    }
                } else {
                    break;
                }
            }

            order.push(event_id.clone());
            events.insert(event_id.clone(), signed_event.clone());
            events_by_work
                .entry(work_id.to_string())
                .or_default()
                .push(event_id);
        }

        info!(
            event_id = %signed_event.event_id,
            session_id = %session_id,
            work_id = %work_id,
            exit_code = %exit_code,
            termination_reason = %termination_reason,
            "SessionTerminated event signed and persisted"
        );

        Ok(signed_event)
    }

    fn emit_changeset_published(
        &self,
        work_id: &str,
        changeset_digest: &[u8; 32],
        cas_hash: &[u8; 32],
        actor_id: &str,
        timestamp_ns: u64,
    ) -> Result<SignedLedgerEvent, LedgerEventError> {
        use ed25519_dalek::Signer;

        // Generate unique event ID
        let event_id = format!("EVT-{}", uuid::Uuid::new_v4());

        // Build payload as JSON with changeset publication data.
        // SECURITY: timestamp_ns is included in signed payload to prevent
        // temporal malleability per LAW-09.
        let payload_json = serde_json::json!({
            "event_type": "changeset_published",
            "work_id": work_id,
            "changeset_digest": hex::encode(changeset_digest),
            "cas_hash": hex::encode(cas_hash),
            "actor_id": actor_id,
            "timestamp_ns": timestamp_ns,
        });

        // TCK-00394: Use JCS (RFC 8785) canonicalization for signing.
        let payload_string = payload_json.to_string();
        let canonical_payload =
            canonicalize_json(&payload_string).map_err(|e| LedgerEventError::SigningFailed {
                message: format!("JCS canonicalization failed: {e}"),
            })?;
        let payload_bytes = canonical_payload.as_bytes().to_vec();

        // Build canonical bytes for signing (domain prefix + JCS payload)
        let mut canonical_bytes = Vec::with_capacity(
            CHANGESET_PUBLISHED_LEDGER_DOMAIN_PREFIX.len() + payload_bytes.len(),
        );
        canonical_bytes.extend_from_slice(CHANGESET_PUBLISHED_LEDGER_DOMAIN_PREFIX);
        canonical_bytes.extend_from_slice(&payload_bytes);

        // Sign the canonical bytes
        let signature = self.signing_key.sign(&canonical_bytes);

        let signed_event = SignedLedgerEvent {
            event_id: event_id.clone(),
            event_type: "changeset_published".to_string(),
            work_id: work_id.to_string(),
            actor_id: actor_id.to_string(),
            payload: payload_bytes,
            signature: signature.to_bytes().to_vec(),
            timestamp_ns,
        };

        // CTR-1303: Persist to bounded in-memory store with LRU eviction
        {
            let mut guard = self.events.write().expect("lock poisoned");
            let mut events_by_work = self.events_by_work_id.write().expect("lock poisoned");
            let (order, events) = &mut *guard;

            // Evict oldest entries if at capacity
            while events.len() >= MAX_LEDGER_EVENTS {
                if let Some(oldest_key) = order.first().cloned() {
                    order.remove(0);
                    if let Some(evicted_event) = events.remove(&oldest_key) {
                        if let Some(work_id_events) = events_by_work.get_mut(&evicted_event.work_id)
                        {
                            work_id_events.retain(|id| id != &oldest_key);
                            if work_id_events.is_empty() {
                                events_by_work.remove(&evicted_event.work_id);
                            }
                        }
                    }
                } else {
                    break;
                }
            }

            order.push(event_id.clone());
            events.insert(event_id.clone(), signed_event.clone());
            events_by_work
                .entry(work_id.to_string())
                .or_default()
                .push(event_id);
        }

        info!(
            event_id = %signed_event.event_id,
            work_id = %work_id,
            changeset_digest = %hex::encode(changeset_digest),
            cas_hash = %hex::encode(cas_hash),
            "ChangeSetPublished event signed and persisted"
        );

        Ok(signed_event)
    }

    /// TCK-00350: Emits a receipt with envelope bindings in the stub
    /// implementation.
    #[allow(clippy::too_many_arguments)]
    fn emit_receipt_with_bindings(
        &self,
        episode_id: &str,
        receipt_id: &str,
        changeset_digest: &[u8; 32],
        artifact_bundle_hash: &[u8; 32],
        reviewer_actor_id: &str,
        timestamp_ns: u64,
        bindings: &crate::episode::EnvelopeBindings,
        identity_proof_hash: &[u8; 32],
    ) -> Result<SignedLedgerEvent, LedgerEventError> {
        use ed25519_dalek::Signer;

        // Fail-closed: validate bindings before emission
        bindings
            .validate()
            .map_err(|e| LedgerEventError::ValidationFailed {
                message: format!("envelope binding validation failed: {e}"),
            })?;

        let event_id = format!("EVT-{}", uuid::Uuid::new_v4());

        // Include bindings in the signed payload
        // SECURITY (TCK-00356 Fix 1): identity_proof_hash is included in
        // the signed payload so it is audit-bound.
        let (env_hex, cap_hex, view_hex) = bindings.to_hex_map();
        let payload_json = serde_json::json!({
            "event_type": "review_receipt_recorded",
            "episode_id": episode_id,
            "receipt_id": receipt_id,
            "changeset_digest": hex::encode(changeset_digest),
            "artifact_bundle_hash": hex::encode(artifact_bundle_hash),
            "reviewer_actor_id": reviewer_actor_id,
            "timestamp_ns": timestamp_ns,
            "envelope_hash": env_hex,
            "capability_manifest_hash": cap_hex,
            "view_commitment_hash": view_hex,
            "identity_proof_hash": hex::encode(identity_proof_hash),
        });

        let payload_bytes =
            serde_json::to_vec(&payload_json).map_err(|e| LedgerEventError::SigningFailed {
                message: format!("payload serialization failed: {e}"),
            })?;

        let mut canonical_bytes =
            Vec::with_capacity(REVIEW_RECEIPT_RECORDED_PREFIX.len() + payload_bytes.len());
        canonical_bytes.extend_from_slice(REVIEW_RECEIPT_RECORDED_PREFIX);
        canonical_bytes.extend_from_slice(&payload_bytes);

        let signature = self.signing_key.sign(&canonical_bytes);

        let signed_event = SignedLedgerEvent {
            event_id: event_id.clone(),
            event_type: "review_receipt_recorded".to_string(),
            work_id: episode_id.to_string(),
            actor_id: reviewer_actor_id.to_string(),
            payload: payload_bytes,
            signature: signature.to_bytes().to_vec(),
            timestamp_ns,
        };

        // CTR-1303: Persist to bounded in-memory store with LRU eviction
        {
            let mut guard = self.events.write().expect("lock poisoned");
            let mut events_by_work = self.events_by_work_id.write().expect("lock poisoned");
            let (order, events) = &mut *guard;

            while events.len() >= MAX_LEDGER_EVENTS {
                if let Some(oldest_key) = order.first().cloned() {
                    order.remove(0);
                    if let Some(evicted_event) = events.remove(&oldest_key) {
                        if let Some(work_id_events) = events_by_work.get_mut(&evicted_event.work_id)
                        {
                            work_id_events.retain(|id| id != &oldest_key);
                            if work_id_events.is_empty() {
                                events_by_work.remove(&evicted_event.work_id);
                            }
                        }
                    }
                } else {
                    break;
                }
            }

            order.push(event_id.clone());
            events.insert(event_id.clone(), signed_event.clone());
            events_by_work
                .entry(episode_id.to_string())
                .or_default()
                .push(event_id);
        }

        Ok(signed_event)
    }
}

// ============================================================================
// Policy Resolver Interface (TCK-00253)
// ============================================================================

use serde::{Deserialize, Serialize};

// ... (existing code)

/// Result of a policy resolution request.
///
/// Per DD-002, the daemon delegates policy resolution to the governance holon.
/// This struct captures the resolved policy state for work claiming.
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct PolicyResolution {
    /// Unique reference to the `PolicyResolvedForChangeSet` event.
    pub policy_resolved_ref: String,

    /// BLAKE3 hash of the resolved policy.
    pub resolved_policy_hash: [u8; 32],

    /// BLAKE3 hash of the capability manifest derived from policy.
    pub capability_manifest_hash: [u8; 32],

    /// BLAKE3 hash of the sealed context pack.
    pub context_pack_hash: [u8; 32],

    /// Resolved risk tier (0-4) from `PolicyResolvedForChangeSet`.
    ///
    /// Used by `handle_ingest_review_receipt` to enforce attestation
    /// ratcheting: higher tiers require stronger attestation levels.
    /// SECURITY: Defaults to `4` (Tier4, most restrictive) via
    /// `risk_tier_fail_closed` to prevent attestation downgrade via
    /// schema drift or tampered persistence (fail-closed semantics).
    #[serde(default = "risk_tier_fail_closed")]
    pub resolved_risk_tier: u8,

    /// Policy-resolved scope baseline for strict-subset validation.
    ///
    /// SECURITY (MAJOR 1 v3 fix): The scope baseline MUST come from an
    /// authoritative policy source (the policy resolver), NOT from the
    /// candidate manifest being validated. Building the baseline from the
    /// manifest itself makes the subset check tautological (always passes).
    ///
    /// When `None` (or absent from serialized data), V1 minting is
    /// denied (fail-closed) because no independent baseline is available
    /// to validate the candidate manifest against.
    #[serde(default)]
    pub resolved_scope_baseline: Option<crate::episode::capability::ScopeBaseline>,

    /// Policy-bound adapter profile hash (BLOCKER security fix).
    ///
    /// When present, `SpawnEpisode` MUST use this exact adapter profile hash.
    /// Any request supplying a different hash is rejected (confused-deputy
    /// prevention). CAS existence alone is NOT authorization -- the profile
    /// must be policy-bound to the caller's scope.
    ///
    /// When `None` (rollout waiver WVR-0003), callers may supply any
    /// CAS-present hash or fall back to the role-based default. This field
    /// will become mandatory once governance populates it.
    #[serde(default)]
    pub expected_adapter_profile_hash: Option<[u8; 32]>,
}

/// Serde default for `resolved_risk_tier`: returns `4` (Tier4, most
/// restrictive).
///
/// SECURITY: This ensures that missing or omitted `resolved_risk_tier` values
/// fail closed to the most restrictive tier, preventing attestation downgrade
/// via schema drift or tampered persistence.
const fn risk_tier_fail_closed() -> u8 {
    4
}

/// Error type for policy resolution operations.
#[derive(Debug, Clone, PartialEq, Eq)]
pub enum PolicyResolutionError {
    /// Policy resolution not found for the given work/role combination.
    NotFound {
        /// The work ID that was queried.
        work_id: String,
        /// The role that was queried.
        role: WorkRole,
    },

    /// Policy resolution failed due to governance error.
    GovernanceFailed {
        /// Error message from governance.
        message: String,
    },

    /// Invalid credential for policy resolution.
    InvalidCredential {
        /// Error message describing the credential issue.
        message: String,
    },
}

impl std::fmt::Display for PolicyResolutionError {
    fn fmt(&self, f: &mut std::fmt::Formatter<'_>) -> std::fmt::Result {
        match self {
            Self::NotFound { work_id, role } => {
                write!(
                    f,
                    "policy resolution not found for work_id={work_id}, role={role:?}"
                )
            },
            Self::GovernanceFailed { message } => {
                write!(f, "governance failed: {message}")
            },
            Self::InvalidCredential { message } => {
                write!(f, "invalid credential: {message}")
            },
        }
    }
}

impl std::error::Error for PolicyResolutionError {}

/// Trait for policy resolution delegation to governance.
///
/// Per DD-002, the daemon does not embed governance logic. It delegates
/// policy resolution to the governance holon and mints capability manifests
/// based on the returned resolution.
///
/// # Implementers
///
/// - `StubPolicyResolver`: Returns stub data for testing
/// - `GovernancePolicyResolver`: Delegates to actual governance holon (future)
pub trait PolicyResolver: Send + Sync {
    /// Resolves policy for a work claim.
    ///
    /// # Arguments
    ///
    /// * `work_id` - Generated work ID for this claim
    /// * `role` - The role being claimed
    /// * `actor_id` - The authoritative actor ID (derived from credential)
    ///
    /// # Returns
    ///
    /// `PolicyResolution` containing the resolved policy hashes and references.
    ///
    /// # Errors
    ///
    /// Returns `PolicyResolutionError` if resolution fails.
    fn resolve_for_claim(
        &self,
        work_id: &str,
        role: WorkRole,
        actor_id: &str,
    ) -> Result<PolicyResolution, PolicyResolutionError>;
}

/// Stub policy resolver for testing and development.
///
/// Returns deterministic stub data. In production, this will be replaced
/// with a real governance holon integration.
///
/// # TCK-00255: Context Pack Sealing
///
/// This stub demonstrates the sealing pattern required by RFC-0017:
/// 1. Create a `ContextPackManifest` with work-specific entries
/// 2. Call `seal()` to get the deterministic content hash
/// 3. Return the seal hash in the `PolicyResolution.context_pack_hash`
///
/// # TCK-00317: Role-Based Manifest Hash Resolution
///
/// Per DOD item 2 (Policy Resolution Bypass fix), this resolver now returns
/// the canonical `reviewer_v0_manifest_hash()` for the Reviewer role. This
/// ensures `SpawnEpisode` loads the correct manifest from CAS using the hash
/// from `PolicyResolution`, rather than selecting manifests by role name.
#[derive(Debug, Clone, Default)]
pub struct StubPolicyResolver;

impl PolicyResolver for StubPolicyResolver {
    fn resolve_for_claim(
        &self,
        work_id: &str,
        role: WorkRole,
        actor_id: &str,
    ) -> Result<PolicyResolution, PolicyResolutionError> {
        use apm2_core::context::{AccessLevel, ContextPackManifestBuilder, ManifestEntryBuilder};

        use crate::episode::reviewer_manifest::reviewer_v0_manifest_hash;

        // Generate deterministic hash for policy
        let policy_hash = blake3::hash(format!("policy:{work_id}:{actor_id}").as_bytes());

        // TCK-00317: Return role-appropriate manifest hash
        //
        // Per DOD item 2, the policy resolver must return the correct manifest
        // hash for each role. SpawnEpisode uses this hash to load the manifest
        // from CAS, ensuring the manifest is not bypassed by role selection.
        //
        // - Reviewer: Use canonical reviewer v0 manifest hash
        // - Other roles: Use deterministic stub hash (fail-closed on CAS lookup)
        //
        // MAJOR 1 v3 fix: Also resolve the authoritative scope baseline and
        // risk tier ceiling from the policy (not from the candidate manifest).
        let (manifest_hash, resolved_scope_baseline) = if role == WorkRole::Reviewer {
            let reviewer = crate::episode::reviewer_manifest::reviewer_v0_manifest();
            let baseline = crate::episode::capability::ScopeBaseline {
                tools: reviewer.tool_allowlist.clone(),
                write_paths: reviewer.write_allowlist.clone(),
                shell_patterns: reviewer.shell_allowlist.clone(),
            };
            (*reviewer_v0_manifest_hash(), Some(baseline))
        } else {
            // For non-reviewer roles, generate a deterministic hash that will
            // fail closed when loaded from CAS (hash doesn't exist in store).
            // The fallback manifest (`from_hash_with_default_allowlist`) has
            // empty allowlists, so we provide a matching empty baseline. This
            // is NOT fail-open: the empty baseline means no tools, write paths,
            // or shell patterns are permitted, which matches the empty fallback
            // manifest. Any manifest with non-empty allowlists would be rejected
            // as exceeding this baseline.
            let hash =
                *blake3::hash(format!("manifest:{work_id}:{actor_id}").as_bytes()).as_bytes();
            (
                hash,
                Some(crate::episode::capability::ScopeBaseline::default()),
            )
        };

        // TCK-00255: Create and seal a context pack manifest
        // In production, this would be populated with actual file entries from
        // the work definition. For the stub, we create a deterministic manifest
        // based on work_id and actor_id.
        let content_hash = blake3::hash(format!("content:{work_id}:{actor_id}").as_bytes());
        let context_pack = ContextPackManifestBuilder::new(
            format!("manifest:{work_id}"),
            format!("profile:{actor_id}"),
        )
        .add_entry(
            ManifestEntryBuilder::new(
                format!("/work/{work_id}/context.yaml"),
                *content_hash.as_bytes(),
            )
            .stable_id("work-context")
            .access_level(AccessLevel::Read)
            .build(),
        )
        .build();

        // TCK-00255: Call seal() to get the context pack hash
        // This ensures the hash is deterministic and verifiable.
        let context_pack_hash =
            context_pack
                .seal()
                .map_err(|e| PolicyResolutionError::GovernanceFailed {
                    message: format!("context pack sealing failed: {e}"),
                })?;

        Ok(PolicyResolution {
            policy_resolved_ref: format!("PolicyResolvedForChangeSet:{work_id}"),
            resolved_policy_hash: *policy_hash.as_bytes(),
            capability_manifest_hash: manifest_hash,
            context_pack_hash,
            resolved_risk_tier: 0, // Stub resolver: Tier0 default
            resolved_scope_baseline,
            expected_adapter_profile_hash: None, // TODO(TCK-00399): populate from governance
        })
    }
}

// ============================================================================
// Work Registry Interface (TCK-00253)
// ============================================================================

/// A claimed work item with its associated metadata.
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct WorkClaim {
    /// Unique work identifier.
    pub work_id: String,

    /// Lease identifier for this claim.
    pub lease_id: String,

    /// Authoritative actor ID (derived from credential).
    pub actor_id: String,

    /// Role claimed for this work.
    #[serde(with = "work_role_serde")]
    pub role: WorkRole,

    /// Policy resolution for this claim.
    pub policy_resolution: PolicyResolution,

    /// Custody domains associated with the executor (for `SoD` validation).
    ///
    /// Per TCK-00258, these are the domains assigned to the actor claiming
    /// the work. For `GATE_EXECUTOR` roles, spawn will be rejected if these
    /// domains overlap with author domains.
    pub executor_custody_domains: Vec<String>,

    /// Custody domains associated with changeset authors (for `SoD`
    /// validation).
    ///
    /// Per TCK-00258, these are the domains of the actors who authored the
    /// changeset being reviewed.
    pub author_custody_domains: Vec<String>,
}

mod work_role_serde {
    use serde::{Deserialize, Deserializer, Serializer};

    use super::WorkRole;

    // Serde requires `&T` for custom serializers via `serialize_with`.
    #[allow(clippy::trivially_copy_pass_by_ref)]
    pub fn serialize<S>(role: &WorkRole, serializer: S) -> Result<S::Ok, S::Error>
    where
        S: Serializer,
    {
        serializer.serialize_i32(*role as i32)
    }

    pub fn deserialize<'de, D>(deserializer: D) -> Result<WorkRole, D::Error>
    where
        D: Deserializer<'de>,
    {
        let val = i32::deserialize(deserializer)?;
        WorkRole::try_from(val).map_err(serde::de::Error::custom)
    }
}

/// Trait for persisting and querying work claims.
///
/// The work registry tracks claimed work items and their associated
/// policy resolutions. It also handles `WorkClaimed` event signing.
pub trait WorkRegistry: Send + Sync {
    /// Registers a new work claim.
    ///
    /// # Arguments
    ///
    /// * `claim` - The work claim to register
    ///
    /// # Returns
    ///
    /// The registered `WorkClaim` (may be enriched with additional metadata).
    ///
    /// # Errors
    ///
    /// Returns an error if registration fails.
    fn register_claim(&self, claim: WorkClaim) -> Result<WorkClaim, WorkRegistryError>;

    /// Queries a work claim by work ID.
    fn get_claim(&self, work_id: &str) -> Option<WorkClaim>;
}

/// Error type for work registry operations.
#[derive(Debug, Clone, PartialEq, Eq)]
pub enum WorkRegistryError {
    /// Work ID already exists.
    DuplicateWorkId {
        /// The duplicate work ID.
        work_id: String,
    },

    /// Registration failed.
    RegistrationFailed {
        /// Error message.
        message: String,
    },
}

impl std::fmt::Display for WorkRegistryError {
    fn fmt(&self, f: &mut std::fmt::Formatter<'_>) -> std::fmt::Result {
        match self {
            Self::DuplicateWorkId { work_id } => {
                write!(f, "duplicate work_id: {work_id}")
            },
            Self::RegistrationFailed { message } => {
                write!(f, "registration failed: {message}")
            },
        }
    }
}

impl std::error::Error for WorkRegistryError {}

/// Maximum number of work claims stored in `StubWorkRegistry`.
///
/// Per CTR-1303: In-memory stores must have `max_entries` limit with O(1)
/// eviction. This prevents denial-of-service via memory exhaustion from
/// unbounded claim registration.
pub const MAX_WORK_CLAIMS: usize = 10_000;

/// In-memory stub work registry for testing.
///
/// # Capacity Limits (CTR-1303)
///
/// This registry enforces a maximum of [`MAX_WORK_CLAIMS`] entries to prevent
/// memory exhaustion. When the limit is reached, the oldest entry (by insertion
/// order) is evicted to make room for the new claim.
///
/// # Performance
///
/// Uses `VecDeque` for O(1) eviction via `pop_front()` instead of
/// `Vec::remove(0)` which is O(n).
#[derive(Debug)]
pub struct StubWorkRegistry {
    /// Claims stored with insertion order for LRU eviction.
    /// Uses `VecDeque` for O(1) eviction of oldest entries.
    claims: std::sync::RwLock<(
        VecDeque<String>,
        std::collections::HashMap<String, WorkClaim>,
    )>,
}

impl Default for StubWorkRegistry {
    fn default() -> Self {
        Self {
            claims: std::sync::RwLock::new((
                VecDeque::with_capacity(MAX_WORK_CLAIMS.min(1000)), // Pre-allocate reasonably
                std::collections::HashMap::with_capacity(MAX_WORK_CLAIMS.min(1000)),
            )),
        }
    }
}

impl WorkRegistry for StubWorkRegistry {
    fn register_claim(&self, claim: WorkClaim) -> Result<WorkClaim, WorkRegistryError> {
        let mut guard = self.claims.write().expect("lock poisoned");
        let (order, claims) = &mut *guard;

        if claims.contains_key(&claim.work_id) {
            return Err(WorkRegistryError::DuplicateWorkId {
                work_id: claim.work_id,
            });
        }

        // CTR-1303: Evict oldest entry if at capacity (O(1) via pop_front)
        while claims.len() >= MAX_WORK_CLAIMS {
            if let Some(oldest_key) = order.pop_front() {
                claims.remove(&oldest_key);
                debug!(
                    evicted_work_id = %oldest_key,
                    "Evicted oldest work claim to maintain capacity limit"
                );
            } else {
                break;
            }
        }

        let work_id = claim.work_id.clone();
        order.push_back(work_id.clone());
        claims.insert(work_id, claim.clone());
        Ok(claim)
    }

    fn get_claim(&self, work_id: &str) -> Option<WorkClaim> {
        let guard = self.claims.read().expect("lock poisoned");
        guard.1.get(work_id).cloned()
    }
}

// ============================================================================
// Actor ID Derivation (TCK-00253)
// ============================================================================

/// Derives the authoritative actor ID from peer credentials.
///
/// Per DD-001 and the proto definition, the `actor_id` in the request is a
/// "display hint" only. The authoritative `actor_id` is derived from the
/// credential. This implementation uses a fingerprint of the UID and GID
/// to create a **stable** identifier that does not change per request.
///
/// # Stability
///
/// The actor ID is derived ONLY from stable credential fields (UID, GID).
/// It intentionally excludes:
/// - PID: Changes per process
/// - Nonce: Changes per request
///
/// This ensures the same user always maps to the same `actor_id`.
///
/// # Arguments
///
/// * `credentials` - The peer credentials from `SO_PEERCRED`
///
/// # Returns
///
/// A stable actor ID string derived from the credential.
///
/// # TODO
///
/// - TCK-00253: `credential_signature` field is currently ignored. Integration
///   with credential verification infrastructure will allow deriving `actor_id`
///   from cryptographic identity rather than Unix UID/GID.
#[must_use]
pub fn derive_actor_id(credentials: &PeerCredentials) -> String {
    // Create a fingerprint from UID and GID only (stable across requests)
    // Per code quality review: exclude PID (changes per process) and nonce (changes
    // per request)
    let mut hasher = blake3::Hasher::new();
    hasher.update(&credentials.uid.to_le_bytes());
    hasher.update(&credentials.gid.to_le_bytes());

    let hash = hasher.finalize();

    // Use first 8 bytes (16 hex chars) for a shorter identifier
    // Per code quality review: use blake3::Hash::to_hex() instead of manual
    // formatting
    let hex = hash.to_hex();
    format!("actor:{}", &hex[..16])
}

/// Generates a unique work ID.
///
/// Uses UUID v4 for uniqueness per RFC-0016 (Hybrid Time Framework compliance).
/// HTF prohibits `SystemTime::now()` to ensure deterministic replay.
#[must_use]
pub fn generate_work_id() -> String {
    // RFC-0016 HTF compliance: Use UUID v4 instead of SystemTime::now()
    let uuid = uuid::Uuid::new_v4();
    format!("W-{uuid}")
}

/// Generates a unique lease ID.
///
/// Uses UUID v4 for uniqueness per RFC-0016 (Hybrid Time Framework compliance).
/// HTF prohibits `SystemTime::now()` to ensure deterministic replay.
#[must_use]
pub fn generate_lease_id() -> String {
    // RFC-0016 HTF compliance: Use UUID v4 instead of SystemTime::now()
    let uuid = uuid::Uuid::new_v4();
    format!("L-{uuid}")
}

/// Extracts replay-critical fields from a persisted `SubleaseIssued` payload.
///
/// Supports both payload shapes used by emitters:
/// - wrapper JSON with hex-encoded inner payload in `"payload"` (stub emitter)
/// - direct top-level fields (`SQLite` emitter)
fn extract_sublease_replay_bindings(event_payload: &[u8]) -> Result<(String, [u8; 32]), String> {
    let wrapper = serde_json::from_slice::<serde_json::Value>(event_payload)
        .map_err(|e| format!("cannot parse SubleaseIssued payload wrapper: {e}"))?;

    let inner_payload = wrapper
        .get("payload")
        .and_then(serde_json::Value::as_str)
        .and_then(|hex_payload| hex::decode(hex_payload).ok())
        .and_then(|inner_bytes| serde_json::from_slice::<serde_json::Value>(&inner_bytes).ok());

    let lookup_field = |field: &str| {
        inner_payload
            .as_ref()
            .and_then(|inner| inner.get(field).and_then(serde_json::Value::as_str))
            .or_else(|| wrapper.get(field).and_then(serde_json::Value::as_str))
    };

    let parent_lease_id = lookup_field("parent_lease_id")
        .map(str::to_owned)
        .ok_or_else(|| "missing parent_lease_id".to_string())?;

    let identity_proof_hash_hex = lookup_field("identity_proof_hash")
        .ok_or_else(|| "missing identity_proof_hash".to_string())?;
    let identity_proof_hash_vec = hex::decode(identity_proof_hash_hex)
        .map_err(|e| format!("identity_proof_hash is not valid hex: {e}"))?;
    crate::identity::validate_identity_proof_hash(&identity_proof_hash_vec)
        .map_err(|e| format!("identity_proof_hash validation failed: {e}"))?;
    let identity_proof_hash: [u8; 32] = identity_proof_hash_vec
        .as_slice()
        .try_into()
        .expect("validated to 32 bytes by validate_identity_proof_hash");

    Ok((parent_lease_id, identity_proof_hash))
}

/// Replay-critical fields persisted for `IngestReviewReceipt` idempotency.
#[derive(Debug, Clone, PartialEq, Eq)]
struct ReceiptReplayBindings {
    lease_id: String,
    changeset_digest: [u8; 32],
    verdict: String,
    identity_proof_hash: [u8; 32],
    artifact_bundle_hash: [u8; 32],
    blocked_reason_code: Option<u32>,
    blocked_log_hash: Option<[u8; 32]>,
}

fn normalize_receipt_replay_verdict(verdict: &str) -> Result<String, String> {
    let canonical = verdict.trim().to_ascii_uppercase();
    match canonical.as_str() {
        "APPROVE" | "APPROVED" => Ok("APPROVE".to_string()),
        "BLOCKED" => Ok("BLOCKED".to_string()),
        _ => Err(format!("unsupported verdict value: {verdict}")),
    }
}

/// Extracts replay-critical fields from a persisted review receipt payload.
///
/// Supports both payload shapes used by emitters:
/// - wrapper JSON with hex-encoded inner payload in `"payload"` (stub emitter)
/// - direct top-level fields (`SQLite` emitter)
fn extract_receipt_replay_bindings(event_payload: &[u8]) -> Result<ReceiptReplayBindings, String> {
    let wrapper = serde_json::from_slice::<serde_json::Value>(event_payload)
        .map_err(|e| format!("cannot parse receipt payload wrapper: {e}"))?;

    let inner_payload = wrapper
        .get("payload")
        .and_then(serde_json::Value::as_str)
        .and_then(|hex_payload| hex::decode(hex_payload).ok())
        .and_then(|inner_bytes| serde_json::from_slice::<serde_json::Value>(&inner_bytes).ok());

    let lookup_value = |field: &str| {
        inner_payload
            .as_ref()
            .and_then(|inner| inner.get(field))
            .or_else(|| wrapper.get(field))
    };
    let lookup_field = |field: &str| lookup_value(field).and_then(serde_json::Value::as_str);
    let decode_hash32 = |field: &str, value: &str| -> Result<[u8; 32], String> {
        let decoded = hex::decode(value).map_err(|e| format!("{field} is not valid hex: {e}"))?;
        decoded
            .as_slice()
            .try_into()
            .map_err(|_| format!("{field} must decode to 32 bytes, got {}", decoded.len()))
    };

    let lease_id = lookup_field("lease_id")
        .or_else(|| lookup_field("episode_id"))
        .map(str::to_owned)
        .ok_or_else(|| "missing lease_id".to_string())?;
    if lease_id.is_empty() {
        return Err("lease_id is empty".to_string());
    }

    let changeset_digest_hex =
        lookup_field("changeset_digest").ok_or_else(|| "missing changeset_digest".to_string())?;
    let changeset_digest = decode_hash32("changeset_digest", changeset_digest_hex)?;

    let verdict_value = lookup_field("verdict")
        .map(str::to_owned)
        .or_else(|| {
            lookup_field("event_type").and_then(|event_type| match event_type {
                "review_receipt_recorded" | "ReviewReceiptRecorded" => Some("APPROVE".to_string()),
                "review_blocked_recorded" | "ReviewBlockedRecorded" => Some("BLOCKED".to_string()),
                _ => None,
            })
        })
        .ok_or_else(|| "missing verdict".to_string())?;
    let verdict = normalize_receipt_replay_verdict(&verdict_value)?;

    let identity_proof_hash_hex = lookup_field("identity_proof_hash")
        .ok_or_else(|| "missing identity_proof_hash".to_string())?;
    let identity_proof_hash_vec = hex::decode(identity_proof_hash_hex)
        .map_err(|e| format!("identity_proof_hash is not valid hex: {e}"))?;
    crate::identity::validate_identity_proof_hash(&identity_proof_hash_vec)
        .map_err(|e| format!("identity_proof_hash validation failed: {e}"))?;
    let identity_proof_hash: [u8; 32] = identity_proof_hash_vec
        .as_slice()
        .try_into()
        .expect("validated to 32 bytes by validate_identity_proof_hash");

    let artifact_bundle_hash_hex = lookup_field("artifact_bundle_hash")
        .ok_or_else(|| "missing artifact_bundle_hash".to_string())?;
    let artifact_bundle_hash = decode_hash32("artifact_bundle_hash", artifact_bundle_hash_hex)?;

    let blocked_reason_code = lookup_value("blocked_reason_code")
        .or_else(|| lookup_value("reason_code"))
        .map(|value| match value {
            serde_json::Value::Number(num) => {
                let reason_u64 = num.as_u64().ok_or_else(|| {
                    "blocked_reason_code must be a non-negative integer".to_string()
                })?;
                u32::try_from(reason_u64)
                    .map_err(|_| format!("blocked_reason_code must fit in u32, got {reason_u64}"))
            },
            serde_json::Value::String(text) => text
                .parse::<u32>()
                .map_err(|e| format!("blocked_reason_code is not a valid u32: {e}")),
            other => Err(format!(
                "blocked_reason_code has unsupported JSON type: {other}"
            )),
        })
        .transpose()?;

    let blocked_log_hash = lookup_field("blocked_log_hash")
        .map(|value| decode_hash32("blocked_log_hash", value))
        .transpose()?;

    Ok(ReceiptReplayBindings {
        lease_id,
        changeset_digest,
        verdict,
        identity_proof_hash,
        artifact_bundle_hash,
        blocked_reason_code,
        blocked_log_hash,
    })
}

// ============================================================================
// Connection Context
// ============================================================================

/// Connection context tracking privilege level and authentication state.
///
/// Per DD-001 (`privilege_predicate`), connections are classified as privileged
/// based on the socket path:
/// - operator.sock: `is_privileged = true`
/// - session.sock: `is_privileged = false`
///
/// # TCK-00303: Connection Lifecycle Management
///
/// The `connection_id` field is used to track connections in the subscription
/// registry. When a connection closes, the connection handler MUST call
/// `subscription_registry.unregister_connection(connection_id)` to free
/// resources and prevent connection slot leaks.
#[derive(Debug, Clone)]
pub struct ConnectionContext {
    /// Whether this connection is privileged (operator socket).
    is_privileged: bool,

    /// Peer credentials extracted via `SO_PEERCRED`.
    peer_credentials: Option<PeerCredentials>,

    /// Session ID for session-scoped connections (None for operator
    /// connections).
    session_id: Option<String>,

    /// Connection ID for subscription registry tracking (TCK-00303).
    ///
    /// Generated once when the connection is established and used consistently
    /// across all subscribe/unsubscribe operations. Must be passed to
    /// `unregister_connection` when the connection closes to prevent leaks.
    connection_id: String,

    /// Contract binding metadata from the handshake (TCK-00348).
    ///
    /// Stored at connection time and threaded into `SessionStarted` events
    /// during `SpawnEpisode` so the authoritative record uses the real
    /// session ID (not a surrogate connection ID).
    contract_binding: Option<crate::hsi_contract::SessionContractBinding>,

    /// Active identity proof profile hash for this connection's identity
    /// context.
    ///
    /// This is persisted into `SessionStarted` payloads when present so
    /// session-open audit records bind to the verifier economics contract
    /// (REQ-0012). Full identity proof dereference wiring is deferred under
    /// WVR-0103 / TCK-00361.
    identity_proof_profile_hash: Option<[u8; 32]>,

    /// Connection phase for session-typed state machine (TCK-00349).
    ///
    /// Per REQ-0003, authority-bearing operations require valid session-state
    /// progression. The phase starts at `Connected`, advances to
    /// `HandshakeComplete` after successful handshake, then to `SessionOpen`
    /// when the connection is ready for IPC dispatch.
    ///
    /// The [`PrivilegedDispatcher::dispatch`] and
    /// [`SessionDispatcher::dispatch`] methods check this phase to ensure
    /// no dispatch occurs before `SessionOpen`.
    phase: crate::protocol::connection_handler::ConnectionPhase,
}

impl ConnectionContext {
    /// Creates a new privileged connection context (operator socket).
    ///
    /// # TCK-00303: Connection ID Generation
    ///
    /// The `connection_id` is generated from peer credentials (PID-based for
    /// operator connections) or a UUID if credentials are unavailable.
    #[must_use]
    pub fn privileged(peer_credentials: Option<PeerCredentials>) -> Self {
        let connection_id = peer_credentials.as_ref().and_then(|c| c.pid).map_or_else(
            || format!("CONN-OP-{}", uuid::Uuid::new_v4()),
            |pid| format!("CONN-OP-{pid}"),
        );
        Self {
            is_privileged: true,
            peer_credentials,
            session_id: None,
            connection_id,
            contract_binding: None,
            identity_proof_profile_hash: None,
            // TCK-00349: New connections start in Connected phase.
            // Callers MUST advance to SessionOpen before dispatch.
            phase: crate::protocol::connection_handler::ConnectionPhase::Connected,
        }
    }

    /// Creates a new session-scoped connection context (session socket).
    ///
    /// # TCK-00303: Connection ID Generation
    ///
    /// The `connection_id` is generated from the session ID (if available)
    /// or peer credentials (PID-based), or a UUID if neither is available.
    #[must_use]
    pub fn session(peer_credentials: Option<PeerCredentials>, session_id: Option<String>) -> Self {
        // For session connections, prefer session_id-based connection ID,
        // but fall back to PID or UUID if session_id is not yet known
        // (it may be set later via session token validation)
        let connection_id = session_id.as_ref().map_or_else(
            || {
                peer_credentials.as_ref().and_then(|c| c.pid).map_or_else(
                    || format!("CONN-SESS-{}", uuid::Uuid::new_v4()),
                    |pid| format!("CONN-SESS-{pid}"),
                )
            },
            |sid| format!("CONN-SESS-{sid}"),
        );
        Self {
            is_privileged: false,
            peer_credentials,
            session_id,
            connection_id,
            contract_binding: None,
            identity_proof_profile_hash: None,
            // TCK-00349: New connections start in Connected phase.
            // Callers MUST advance to SessionOpen before dispatch.
            phase: crate::protocol::connection_handler::ConnectionPhase::Connected,
        }
    }

    /// Returns `true` if this connection has privileged access.
    #[must_use]
    pub const fn is_privileged(&self) -> bool {
        self.is_privileged
    }

    /// Returns the peer credentials if available.
    #[must_use]
    pub const fn peer_credentials(&self) -> Option<&PeerCredentials> {
        self.peer_credentials.as_ref()
    }

    /// Returns the session ID for session-scoped connections.
    #[must_use]
    pub fn session_id(&self) -> Option<&str> {
        self.session_id.as_deref()
    }

    /// Returns the connection ID for subscription registry tracking.
    ///
    /// # TCK-00303: Connection Lifecycle
    ///
    /// This ID must be passed to `unregister_connection` when the connection
    /// closes to free subscription registry slots and prevent `DoS` via
    /// connection slot exhaustion.
    #[must_use]
    pub fn connection_id(&self) -> &str {
        &self.connection_id
    }

    /// Sets the contract binding metadata from the handshake (TCK-00348).
    ///
    /// Called after `perform_handshake` succeeds. The binding is later
    /// threaded into `SessionStarted` events during `emit_spawn_lifecycle`.
    pub fn set_contract_binding(&mut self, binding: crate::hsi_contract::SessionContractBinding) {
        self.contract_binding = Some(binding);
    }

    /// Returns the contract binding if set.
    #[must_use]
    pub const fn contract_binding(&self) -> Option<&crate::hsi_contract::SessionContractBinding> {
        self.contract_binding.as_ref()
    }

    /// Sets the active identity proof profile hash for this connection.
    ///
    /// Called from the production session-open path in `main.rs` when
    /// the active identity proof profile is resolved after handshake
    /// (TCK-00358). Also used by test code to inject specific profile
    /// hashes for verification. The spawn path emits a warning and
    /// falls back to the baseline SMT-256 10^12 profile hash if this
    /// method was not called at session-open time.
    pub fn set_identity_proof_profile_hash(
        &mut self,
        profile_hash: [u8; 32],
    ) -> Result<(), crate::identity::IdentityProofError> {
        if profile_hash == [0u8; 32] {
            return Err(crate::identity::IdentityProofError::InvalidField {
                field: "identity_proof_profile_hash",
                reason: "must be non-zero".to_string(),
            });
        }
        self.identity_proof_profile_hash = Some(profile_hash);
        Ok(())
    }

    /// Returns the active identity proof profile hash, if known.
    #[must_use]
    pub const fn identity_proof_profile_hash(&self) -> Option<&[u8; 32]> {
        self.identity_proof_profile_hash.as_ref()
    }

    /// Returns the current connection phase (TCK-00349).
    #[must_use]
    pub const fn phase(&self) -> crate::protocol::connection_handler::ConnectionPhase {
        self.phase
    }

    /// Advances the connection phase to `HandshakeComplete` (TCK-00349).
    ///
    /// Called after `perform_handshake` succeeds. Must be called before
    /// `advance_to_session_open`.
    ///
    /// # Errors
    ///
    /// Returns `ConnectionPhaseError::IllegalTransition` if the current
    /// phase is not `Connected`.
    pub fn advance_to_handshake_complete(
        &mut self,
    ) -> Result<(), crate::protocol::connection_handler::ConnectionPhaseError> {
        self.phase = self.phase.advance_to_handshake_complete()?;
        Ok(())
    }

    /// Advances the connection phase to `SessionOpen` (TCK-00349).
    ///
    /// Called after handshake completion, before entering the message
    /// dispatch loop. This is the gate that permits authority-bearing
    /// IPC operations.
    ///
    /// # Errors
    ///
    /// Returns `ConnectionPhaseError::IllegalTransition` if the current
    /// phase is not `HandshakeComplete`.
    pub fn advance_to_session_open(
        &mut self,
    ) -> Result<(), crate::protocol::connection_handler::ConnectionPhaseError> {
        self.phase = self.phase.advance_to_session_open()?;
        Ok(())
    }

    /// Creates a privileged connection context already in `SessionOpen` phase.
    ///
    /// This is a convenience constructor for tests and situations where the
    /// handshake has already been validated externally (e.g., unit tests
    /// that exercise dispatch directly without a socket handshake).
    ///
    /// # Panics
    ///
    /// Panics if the phase transitions fail (should never happen for a fresh
    /// context).
    #[must_use]
    pub fn privileged_session_open(peer_credentials: Option<PeerCredentials>) -> Self {
        let mut ctx = Self::privileged(peer_credentials);
        ctx.advance_to_handshake_complete()
            .expect("fresh context should transition to HandshakeComplete");
        ctx.advance_to_session_open()
            .expect("HandshakeComplete should transition to SessionOpen");
        ctx
    }

    /// Creates a session-scoped connection context already in `SessionOpen`
    /// phase.
    ///
    /// This is a convenience constructor for tests and situations where the
    /// handshake has already been validated externally.
    ///
    /// # Panics
    ///
    /// Panics if the phase transitions fail (should never happen for a fresh
    /// context).
    #[must_use]
    pub fn session_open(
        peer_credentials: Option<PeerCredentials>,
        session_id: Option<String>,
    ) -> Self {
        let mut ctx = Self::session(peer_credentials, session_id);
        ctx.advance_to_handshake_complete()
            .expect("fresh context should transition to HandshakeComplete");
        ctx.advance_to_session_open()
            .expect("HandshakeComplete should transition to SessionOpen");
        ctx
    }
}

// ============================================================================
// Message Type Tags (for routing)
// ============================================================================

/// Message type tags for privileged endpoint routing.
///
/// These tags are used to identify the message type before decoding,
/// allowing the dispatcher to route to the appropriate handler.
///
/// # Consensus Query Tag Range (CTR-PROTO-011)
///
/// Consensus query messages use tags 5-8 per RFC-0014/TCK-00345:
/// - 5 = `ConsensusStatus`
/// - 6 = `ConsensusValidators`
/// - 7 = `ConsensusByzantineEvidence`
/// - 8 = `ConsensusMetrics`
///
/// # HEF Tag Range (CTR-PROTO-010)
///
/// HEF messages use tag range 64-79 per RFC-0018:
/// - 64 = `SubscribePulse`
/// - 65 = `SubscribePulseResponse` (response only)
/// - 66 = `UnsubscribePulse`
/// - 67 = `UnsubscribePulseResponse` (response only)
/// - 68 = `PulseEvent` (server->client only)
#[derive(Debug, Clone, Copy, PartialEq, Eq, Hash)]
#[repr(u8)]
pub enum PrivilegedMessageType {
    /// `ClaimWork` request (IPC-PRIV-001)
    ClaimWork           = 1,
    /// `SpawnEpisode` request (IPC-PRIV-002)
    SpawnEpisode        = 2,
    /// `IssueCapability` request (IPC-PRIV-003)
    IssueCapability     = 3,
    /// Shutdown request (IPC-PRIV-004)
    Shutdown            = 4,
    // --- Process Management (CTR-PROTO-011, TCK-00342) ---
    /// `ListProcesses` request (IPC-PRIV-005)
    ListProcesses       = 5,
    /// `ProcessStatus` request (IPC-PRIV-006)
    ProcessStatus       = 6,
    /// `StartProcess` request (IPC-PRIV-007)
    StartProcess        = 7,
    /// `StopProcess` request (IPC-PRIV-008)
    StopProcess         = 8,
    /// `RestartProcess` request (IPC-PRIV-009)
    RestartProcess      = 9,
    /// `ReloadProcess` request (IPC-PRIV-010)
    ReloadProcess       = 10,
    // --- Consensus Query Endpoints (CTR-PROTO-011, RFC-0014, TCK-00345) ---
    /// `ConsensusStatus` request (IPC-PRIV-011)
    ConsensusStatus     = 11,
    /// `ConsensusValidators` request (IPC-PRIV-012)
    ConsensusValidators = 12,
    /// `ConsensusByzantineEvidence` request (IPC-PRIV-013)
    ConsensusByzantineEvidence = 13,
    /// `ConsensusMetrics` request (IPC-PRIV-014)
    ConsensusMetrics    = 14,
    /// `WorkStatus` request (IPC-PRIV-015, TCK-00344)
    WorkStatus          = 15,
    /// `EndSession` request (IPC-PRIV-016, TCK-00395)
    EndSession          = 16,
    /// `IngestReviewReceipt` request (IPC-PRIV-017, TCK-00389)
    IngestReviewReceipt = 17,
    /// `UpdateStopFlags` request (IPC-PRIV-018, TCK-00351)
    UpdateStopFlags     = 18,
    // --- Credential Management (CTR-PROTO-012, RFC-0018, TCK-00343) ---
    /// `ListCredentials` request (IPC-PRIV-021)
    ListCredentials     = 21,
    /// `AddCredential` request (IPC-PRIV-022)
    AddCredential       = 22,
    /// `RemoveCredential` request (IPC-PRIV-023)
    RemoveCredential    = 23,
    /// `RefreshCredential` request (IPC-PRIV-024)
    RefreshCredential   = 24,
    /// `SwitchCredential` request (IPC-PRIV-025)
    SwitchCredential    = 25,
    /// `LoginCredential` request (IPC-PRIV-026)
    LoginCredential     = 26,
    // --- HEF Pulse Plane (CTR-PROTO-010, RFC-0018) ---
    /// `SubscribePulse` request (IPC-HEF-001)
    SubscribePulse      = 64,
    /// `UnsubscribePulse` request (IPC-HEF-002)
    UnsubscribePulse    = 66,
    /// `PulseEvent` notification (server->client, IPC-HEF-003)
    PulseEvent          = 68,
    // --- ChangeSet Publishing (RFC-0018, TCK-00394) ---
    /// `PublishChangeSet` request (IPC-PRIV-017)
    PublishChangeSet    = 70,
    // --- Sublease Delegation (RFC-0019, TCK-00340) ---
    /// `DelegateSublease` request (IPC-PRIV-072)
    DelegateSublease    = 72,
}

impl PrivilegedMessageType {
    /// Attempts to parse a message type from a tag byte.
    #[must_use]
    pub const fn from_tag(tag: u8) -> Option<Self> {
        match tag {
            1 => Some(Self::ClaimWork),
            2 => Some(Self::SpawnEpisode),
            3 => Some(Self::IssueCapability),
            4 => Some(Self::Shutdown),
            // Process Management tags (5-10)
            5 => Some(Self::ListProcesses),
            6 => Some(Self::ProcessStatus),
            7 => Some(Self::StartProcess),
            8 => Some(Self::StopProcess),
            9 => Some(Self::RestartProcess),
            10 => Some(Self::ReloadProcess),
            // Consensus query tags (11-14, TCK-00345)
            11 => Some(Self::ConsensusStatus),
            12 => Some(Self::ConsensusValidators),
            13 => Some(Self::ConsensusByzantineEvidence),
            14 => Some(Self::ConsensusMetrics),
            // TCK-00344: Work status query
            15 => Some(Self::WorkStatus),
            // TCK-00395: EndSession for session termination
            16 => Some(Self::EndSession),
            // TCK-00389: IngestReviewReceipt for external reviewer results
            17 => Some(Self::IngestReviewReceipt),
            // TCK-00351: stop flags update
            18 => Some(Self::UpdateStopFlags),
            // Credential management tags (21-26, TCK-00343)
            21 => Some(Self::ListCredentials),
            22 => Some(Self::AddCredential),
            23 => Some(Self::RemoveCredential),
            24 => Some(Self::RefreshCredential),
            25 => Some(Self::SwitchCredential),
            26 => Some(Self::LoginCredential),
            // HEF tags (64-68)
            64 => Some(Self::SubscribePulse),
            66 => Some(Self::UnsubscribePulse),
            68 => Some(Self::PulseEvent),
            // ChangeSet publishing (TCK-00394)
            70 => Some(Self::PublishChangeSet),
            // Sublease delegation (TCK-00340)
            72 => Some(Self::DelegateSublease),
            _ => None,
        }
    }

    /// Returns the tag byte for this message type.
    #[must_use]
    pub const fn tag(self) -> u8 {
        self as u8
    }

    /// Returns all request-bearing variants of `PrivilegedMessageType`.
    ///
    /// This excludes response-only and notification-only variants such as
    /// `PulseEvent`. Adding a new request-bearing variant to the enum
    /// without adding it here will cause the HSI manifest completeness
    /// tests to fail.
    #[must_use]
    pub const fn all_request_variants() -> &'static [Self] {
        &[
            Self::ClaimWork,
            Self::SpawnEpisode,
            Self::IssueCapability,
            Self::Shutdown,
            Self::ListProcesses,
            Self::ProcessStatus,
            Self::StartProcess,
            Self::StopProcess,
            Self::RestartProcess,
            Self::ReloadProcess,
            Self::ConsensusStatus,
            Self::ConsensusValidators,
            Self::ConsensusByzantineEvidence,
            Self::ConsensusMetrics,
            Self::WorkStatus,
            Self::EndSession,
            Self::IngestReviewReceipt,
            Self::UpdateStopFlags,
            Self::ListCredentials,
            Self::AddCredential,
            Self::RemoveCredential,
            Self::RefreshCredential,
            Self::SwitchCredential,
            Self::LoginCredential,
            Self::SubscribePulse,
            Self::UnsubscribePulse,
            Self::PublishChangeSet,
            Self::DelegateSublease,
        ]
    }

    /// Returns `true` if this variant represents a client-initiated request,
    /// `false` if it is a server-to-client notification or response-only
    /// variant.
    ///
    /// This method uses an **exhaustive** match (no wildcard `_ =>` arm), so
    /// adding a new variant to the enum forces a compile error until it is
    /// classified here. This provides the non-self-referential completeness
    /// guarantee required by RFC-0020 section 3.1.1.
    #[must_use]
    pub const fn is_client_request(self) -> bool {
        // IMPORTANT: This match MUST remain exhaustive (no `_ =>` wildcard).
        // Adding a new enum variant forces a compile error here, ensuring the
        // developer must classify it as client-request (true) or not (false).
        match self {
            Self::ClaimWork
            | Self::SpawnEpisode
            | Self::IssueCapability
            | Self::Shutdown
            | Self::ListProcesses
            | Self::ProcessStatus
            | Self::StartProcess
            | Self::StopProcess
            | Self::RestartProcess
            | Self::ReloadProcess
            | Self::ConsensusStatus
            | Self::ConsensusValidators
            | Self::ConsensusByzantineEvidence
            | Self::ConsensusMetrics
            | Self::WorkStatus
            | Self::EndSession
            | Self::IngestReviewReceipt
            | Self::UpdateStopFlags
            | Self::ListCredentials
            | Self::AddCredential
            | Self::RemoveCredential
            | Self::RefreshCredential
            | Self::SwitchCredential
            | Self::LoginCredential
            | Self::SubscribePulse
            | Self::UnsubscribePulse
            | Self::PublishChangeSet
            | Self::DelegateSublease => true,
            // Server-to-client notification only — not a client request.
            Self::PulseEvent => false,
        }
    }

    /// Returns the HSI route path for this variant.
    ///
    /// Used by the HSI contract manifest to derive routes directly from the
    /// dispatch enum, ensuring the manifest stays in sync with the actual
    /// dispatch registry.
    #[must_use]
    pub const fn hsi_route(self) -> &'static str {
        match self {
            Self::ClaimWork => "hsi.work.claim",
            Self::SpawnEpisode => "hsi.episode.spawn",
            Self::IssueCapability => "hsi.capability.issue",
            Self::Shutdown => "hsi.daemon.shutdown",
            Self::ListProcesses => "hsi.process.list",
            Self::ProcessStatus => "hsi.process.status",
            Self::StartProcess => "hsi.process.start",
            Self::StopProcess => "hsi.process.stop",
            Self::RestartProcess => "hsi.process.restart",
            Self::ReloadProcess => "hsi.process.reload",
            Self::ConsensusStatus => "hsi.consensus.status",
            Self::ConsensusValidators => "hsi.consensus.validators",
            Self::ConsensusByzantineEvidence => "hsi.consensus.byzantine_evidence",
            Self::ConsensusMetrics => "hsi.consensus.metrics",
            Self::WorkStatus => "hsi.work.status",
            Self::EndSession => "hsi.session.end",
            Self::IngestReviewReceipt => "hsi.review.ingest_receipt",
            Self::UpdateStopFlags => "hsi.stop.update_flags",
            Self::ListCredentials => "hsi.credential.list",
            Self::AddCredential => "hsi.credential.add",
            Self::RemoveCredential => "hsi.credential.remove",
            Self::RefreshCredential => "hsi.credential.refresh",
            Self::SwitchCredential => "hsi.credential.switch",
            Self::LoginCredential => "hsi.credential.login",
            Self::SubscribePulse => "hsi.pulse.subscribe",
            Self::UnsubscribePulse => "hsi.pulse.unsubscribe",
            Self::PublishChangeSet => "hsi.changeset.publish",
            Self::DelegateSublease => "hsi.sublease.delegate",
            Self::PulseEvent => "hsi.pulse.event",
        }
    }

    /// Returns the HSI manifest route ID for this variant.
    #[must_use]
    pub const fn hsi_route_id(self) -> &'static str {
        match self {
            Self::ClaimWork => "CLAIM_WORK",
            Self::SpawnEpisode => "SPAWN_EPISODE",
            Self::IssueCapability => "ISSUE_CAPABILITY",
            Self::Shutdown => "SHUTDOWN",
            Self::ListProcesses => "LIST_PROCESSES",
            Self::ProcessStatus => "PROCESS_STATUS",
            Self::StartProcess => "START_PROCESS",
            Self::StopProcess => "STOP_PROCESS",
            Self::RestartProcess => "RESTART_PROCESS",
            Self::ReloadProcess => "RELOAD_PROCESS",
            Self::ConsensusStatus => "CONSENSUS_STATUS",
            Self::ConsensusValidators => "CONSENSUS_VALIDATORS",
            Self::ConsensusByzantineEvidence => "CONSENSUS_BYZANTINE_EVIDENCE",
            Self::ConsensusMetrics => "CONSENSUS_METRICS",
            Self::WorkStatus => "WORK_STATUS",
            Self::EndSession => "END_SESSION",
            Self::IngestReviewReceipt => "INGEST_REVIEW_RECEIPT",
            Self::UpdateStopFlags => "UPDATE_STOP_FLAGS",
            Self::ListCredentials => "LIST_CREDENTIALS",
            Self::AddCredential => "ADD_CREDENTIAL",
            Self::RemoveCredential => "REMOVE_CREDENTIAL",
            Self::RefreshCredential => "REFRESH_CREDENTIAL",
            Self::SwitchCredential => "SWITCH_CREDENTIAL",
            Self::LoginCredential => "LOGIN_CREDENTIAL",
            Self::SubscribePulse => "SUBSCRIBE_PULSE",
            Self::UnsubscribePulse => "UNSUBSCRIBE_PULSE",
            Self::PublishChangeSet => "PUBLISH_CHANGESET",
            Self::DelegateSublease => "DELEGATE_SUBLEASE",
            Self::PulseEvent => "PULSE_EVENT",
        }
    }

    /// Returns the HSI request schema ID for this variant.
    #[must_use]
    pub const fn hsi_request_schema(self) -> &'static str {
        match self {
            Self::ClaimWork => "apm2.claim_work_request.v1",
            Self::SpawnEpisode => "apm2.spawn_episode_request.v1",
            Self::IssueCapability => "apm2.issue_capability_request.v1",
            Self::Shutdown => "apm2.shutdown_request.v1",
            Self::ListProcesses => "apm2.list_processes_request.v1",
            Self::ProcessStatus => "apm2.process_status_request.v1",
            Self::StartProcess => "apm2.start_process_request.v1",
            Self::StopProcess => "apm2.stop_process_request.v1",
            Self::RestartProcess => "apm2.restart_process_request.v1",
            Self::ReloadProcess => "apm2.reload_process_request.v1",
            Self::ConsensusStatus => "apm2.consensus_status_request.v1",
            Self::ConsensusValidators => "apm2.consensus_validators_request.v1",
            Self::ConsensusByzantineEvidence => "apm2.consensus_byzantine_evidence_request.v1",
            Self::ConsensusMetrics => "apm2.consensus_metrics_request.v1",
            Self::WorkStatus => "apm2.work_status_request.v1",
            Self::EndSession => "apm2.end_session_request.v1",
            Self::IngestReviewReceipt => "apm2.ingest_review_receipt_request.v1",
            Self::UpdateStopFlags => "apm2.update_stop_flags_request.v1",
            Self::ListCredentials => "apm2.list_credentials_request.v1",
            Self::AddCredential => "apm2.add_credential_request.v1",
            Self::RemoveCredential => "apm2.remove_credential_request.v1",
            Self::RefreshCredential => "apm2.refresh_credential_request.v1",
            Self::SwitchCredential => "apm2.switch_credential_request.v1",
            Self::LoginCredential => "apm2.login_credential_request.v1",
            Self::SubscribePulse => "apm2.subscribe_pulse_request.v1",
            Self::UnsubscribePulse => "apm2.unsubscribe_pulse_request.v1",
            Self::PublishChangeSet => "apm2.publish_changeset_request.v1",
            Self::DelegateSublease => "apm2.delegate_sublease_request.v1",
            Self::PulseEvent => "apm2.pulse_event_request.v1",
        }
    }

    /// Returns the HSI response schema ID for this variant.
    #[must_use]
    pub const fn hsi_response_schema(self) -> &'static str {
        match self {
            Self::ClaimWork => "apm2.claim_work_response.v1",
            Self::SpawnEpisode => "apm2.spawn_episode_response.v1",
            Self::IssueCapability => "apm2.issue_capability_response.v1",
            Self::Shutdown => "apm2.shutdown_response.v1",
            Self::ListProcesses => "apm2.list_processes_response.v1",
            Self::ProcessStatus => "apm2.process_status_response.v1",
            Self::StartProcess => "apm2.start_process_response.v1",
            Self::StopProcess => "apm2.stop_process_response.v1",
            Self::RestartProcess => "apm2.restart_process_response.v1",
            Self::ReloadProcess => "apm2.reload_process_response.v1",
            Self::ConsensusStatus => "apm2.consensus_status_response.v1",
            Self::ConsensusValidators => "apm2.consensus_validators_response.v1",
            Self::ConsensusByzantineEvidence => "apm2.consensus_byzantine_evidence_response.v1",
            Self::ConsensusMetrics => "apm2.consensus_metrics_response.v1",
            Self::WorkStatus => "apm2.work_status_response.v1",
            Self::EndSession => "apm2.end_session_response.v1",
            Self::IngestReviewReceipt => "apm2.ingest_review_receipt_response.v1",
            Self::UpdateStopFlags => "apm2.update_stop_flags_response.v1",
            Self::ListCredentials => "apm2.list_credentials_response.v1",
            Self::AddCredential => "apm2.add_credential_response.v1",
            Self::RemoveCredential => "apm2.remove_credential_response.v1",
            Self::RefreshCredential => "apm2.refresh_credential_response.v1",
            Self::SwitchCredential => "apm2.switch_credential_response.v1",
            Self::LoginCredential => "apm2.login_credential_response.v1",
            Self::SubscribePulse => "apm2.subscribe_pulse_response.v1",
            Self::UnsubscribePulse => "apm2.unsubscribe_pulse_response.v1",
            Self::PublishChangeSet => "apm2.publish_changeset_response.v1",
            Self::DelegateSublease => "apm2.delegate_sublease_response.v1",
            Self::PulseEvent => "apm2.pulse_event_response.v1",
        }
    }
}

// ============================================================================
// Response Envelope
// ============================================================================

/// Response envelope for privileged endpoint responses.
///
/// Contains either a successful response or an error.
#[derive(Debug)]
pub enum PrivilegedResponse {
    /// Successful `ClaimWork` response.
    ClaimWork(ClaimWorkResponse),
    /// Successful `SpawnEpisode` response.
    SpawnEpisode(SpawnEpisodeResponse),
    /// Successful `IssueCapability` response.
    IssueCapability(IssueCapabilityResponse),
    /// Successful Shutdown response.
    Shutdown(ShutdownResponse),
    // --- Process Management (TCK-00342) ---
    /// Successful `ListProcesses` response.
    ListProcesses(ListProcessesResponse),
    /// Successful `ProcessStatus` response.
    ProcessStatus(ProcessStatusResponse),
    /// Successful `StartProcess` response.
    StartProcess(StartProcessResponse),
    /// Successful `StopProcess` response.
    StopProcess(StopProcessResponse),
    /// Successful `RestartProcess` response.
    RestartProcess(RestartProcessResponse),
    /// Successful `ReloadProcess` response.
    ReloadProcess(ReloadProcessResponse),
    /// Successful `WorkStatus` response (TCK-00344).
    WorkStatus(WorkStatusResponse),
    /// Successful `EndSession` response (TCK-00395).
    EndSession(EndSessionResponse),
    /// Successful `IngestReviewReceipt` response (TCK-00389).
    IngestReviewReceipt(IngestReviewReceiptResponse),
    /// Successful `UpdateStopFlags` response (TCK-00351).
    UpdateStopFlags(UpdateStopFlagsResponse),
    // --- Credential Management (CTR-PROTO-012, TCK-00343) ---
    /// Successful `ListCredentials` response.
    ListCredentials(ListCredentialsResponse),
    /// Successful `AddCredential` response.
    AddCredential(AddCredentialResponse),
    /// Successful `RemoveCredential` response.
    RemoveCredential(RemoveCredentialResponse),
    /// Successful `RefreshCredential` response.
    RefreshCredential(RefreshCredentialResponse),
    /// Successful `SwitchCredential` response.
    SwitchCredential(SwitchCredentialResponse),
    /// Successful `LoginCredential` response.
    LoginCredential(LoginCredentialResponse),
    /// Successful `SubscribePulse` response (TCK-00302).
    SubscribePulse(SubscribePulseResponse),
    /// Successful `UnsubscribePulse` response (TCK-00302).
    UnsubscribePulse(UnsubscribePulseResponse),
    /// Successful `ConsensusStatus` response (TCK-00345).
    ConsensusStatus(ConsensusStatusResponse),
    /// Successful `ConsensusValidators` response (TCK-00345).
    ConsensusValidators(ConsensusValidatorsResponse),
    /// Successful `ConsensusByzantineEvidence` response (TCK-00345).
    ConsensusByzantineEvidence(ConsensusByzantineEvidenceResponse),
    /// Successful `ConsensusMetrics` response (TCK-00345).
    ConsensusMetrics(ConsensusMetricsResponse),
    /// Successful `PublishChangeSet` response (TCK-00394).
    PublishChangeSet(PublishChangeSetResponse),
    /// Successful `DelegateSublease` response (TCK-00340).
    DelegateSublease(DelegateSubleaseResponse),
    /// Error response.
    Error(PrivilegedError),
}

impl PrivilegedResponse {
    /// Creates a `PERMISSION_DENIED` error response.
    #[must_use]
    pub fn permission_denied() -> Self {
        Self::Error(PrivilegedError {
            code: PrivilegedErrorCode::PermissionDenied.into(),
            message: "permission denied".to_string(),
        })
    }

    /// Creates a custom error response.
    #[must_use]
    pub fn error(code: PrivilegedErrorCode, message: impl Into<String>) -> Self {
        Self::Error(PrivilegedError {
            code: code.into(),
            message: message.into(),
        })
    }

    /// Encodes the response to bytes.
    ///
    /// The format is: [tag: u8][payload: protobuf]
    /// Tag 0 indicates an error response.
    #[must_use]
    pub fn encode(&self) -> Bytes {
        // Response tags for HEF messages (request tag + 1)
        const SUBSCRIBE_PULSE_RESPONSE_TAG: u8 = 65;
        const UNSUBSCRIBE_PULSE_RESPONSE_TAG: u8 = 67;

        let mut buf = Vec::new();
        match self {
            Self::ClaimWork(resp) => {
                buf.push(PrivilegedMessageType::ClaimWork.tag());
                resp.encode(&mut buf).expect("encode cannot fail");
            },
            Self::SpawnEpisode(resp) => {
                buf.push(PrivilegedMessageType::SpawnEpisode.tag());
                resp.encode(&mut buf).expect("encode cannot fail");
            },
            Self::IssueCapability(resp) => {
                buf.push(PrivilegedMessageType::IssueCapability.tag());
                resp.encode(&mut buf).expect("encode cannot fail");
            },
            Self::Shutdown(resp) => {
                buf.push(PrivilegedMessageType::Shutdown.tag());
                resp.encode(&mut buf).expect("encode cannot fail");
            },
            // Process Management (TCK-00342)
            Self::ListProcesses(resp) => {
                buf.push(PrivilegedMessageType::ListProcesses.tag());
                resp.encode(&mut buf).expect("encode cannot fail");
            },
            Self::ProcessStatus(resp) => {
                buf.push(PrivilegedMessageType::ProcessStatus.tag());
                resp.encode(&mut buf).expect("encode cannot fail");
            },
            Self::StartProcess(resp) => {
                buf.push(PrivilegedMessageType::StartProcess.tag());
                resp.encode(&mut buf).expect("encode cannot fail");
            },
            Self::StopProcess(resp) => {
                buf.push(PrivilegedMessageType::StopProcess.tag());
                resp.encode(&mut buf).expect("encode cannot fail");
            },
            Self::RestartProcess(resp) => {
                buf.push(PrivilegedMessageType::RestartProcess.tag());
                resp.encode(&mut buf).expect("encode cannot fail");
            },
            Self::ReloadProcess(resp) => {
                buf.push(PrivilegedMessageType::ReloadProcess.tag());
                resp.encode(&mut buf).expect("encode cannot fail");
            },
            Self::WorkStatus(resp) => {
                buf.push(PrivilegedMessageType::WorkStatus.tag());
                resp.encode(&mut buf).expect("encode cannot fail");
            },
            Self::EndSession(resp) => {
                buf.push(PrivilegedMessageType::EndSession.tag());
                resp.encode(&mut buf).expect("encode cannot fail");
            },
            // TCK-00389: IngestReviewReceipt
            Self::IngestReviewReceipt(resp) => {
                buf.push(PrivilegedMessageType::IngestReviewReceipt.tag());
                resp.encode(&mut buf).expect("encode cannot fail");
            },
            Self::UpdateStopFlags(resp) => {
                buf.push(PrivilegedMessageType::UpdateStopFlags.tag());
                resp.encode(&mut buf).expect("encode cannot fail");
            },
            // Credential Management (CTR-PROTO-012, TCK-00343)
            Self::ListCredentials(resp) => {
                buf.push(PrivilegedMessageType::ListCredentials.tag());
                resp.encode(&mut buf).expect("encode cannot fail");
            },
            Self::AddCredential(resp) => {
                buf.push(PrivilegedMessageType::AddCredential.tag());
                resp.encode(&mut buf).expect("encode cannot fail");
            },
            Self::RemoveCredential(resp) => {
                buf.push(PrivilegedMessageType::RemoveCredential.tag());
                resp.encode(&mut buf).expect("encode cannot fail");
            },
            Self::RefreshCredential(resp) => {
                buf.push(PrivilegedMessageType::RefreshCredential.tag());
                resp.encode(&mut buf).expect("encode cannot fail");
            },
            Self::SwitchCredential(resp) => {
                buf.push(PrivilegedMessageType::SwitchCredential.tag());
                resp.encode(&mut buf).expect("encode cannot fail");
            },
            Self::LoginCredential(resp) => {
                buf.push(PrivilegedMessageType::LoginCredential.tag());
                resp.encode(&mut buf).expect("encode cannot fail");
            },
            Self::SubscribePulse(resp) => {
                buf.push(SUBSCRIBE_PULSE_RESPONSE_TAG);
                resp.encode(&mut buf).expect("encode cannot fail");
            },
            Self::UnsubscribePulse(resp) => {
                buf.push(UNSUBSCRIBE_PULSE_RESPONSE_TAG);
                resp.encode(&mut buf).expect("encode cannot fail");
            },
            Self::ConsensusStatus(resp) => {
                buf.push(PrivilegedMessageType::ConsensusStatus.tag());
                resp.encode(&mut buf).expect("encode cannot fail");
            },
            Self::ConsensusValidators(resp) => {
                buf.push(PrivilegedMessageType::ConsensusValidators.tag());
                resp.encode(&mut buf).expect("encode cannot fail");
            },
            Self::ConsensusByzantineEvidence(resp) => {
                buf.push(PrivilegedMessageType::ConsensusByzantineEvidence.tag());
                resp.encode(&mut buf).expect("encode cannot fail");
            },
            Self::ConsensusMetrics(resp) => {
                buf.push(PrivilegedMessageType::ConsensusMetrics.tag());
                resp.encode(&mut buf).expect("encode cannot fail");
            },
            Self::PublishChangeSet(resp) => {
                buf.push(PrivilegedMessageType::PublishChangeSet.tag());
                resp.encode(&mut buf).expect("encode cannot fail");
            },
            Self::DelegateSublease(resp) => {
                buf.push(PrivilegedMessageType::DelegateSublease.tag());
                resp.encode(&mut buf).expect("encode cannot fail");
            },
            Self::Error(err) => {
                buf.push(0); // Error tag
                err.encode(&mut buf).expect("encode cannot fail");
            },
        }
        Bytes::from(buf)
    }
}

// ============================================================================
// Dispatcher
// ============================================================================

// ============================================================================
// TCK-00257: Gate Lease Validation
// ============================================================================

/// Maximum number of lease entries to store (CTR-1303: bounded capacity).
const MAX_LEASE_ENTRIES: usize = 10_000;

/// Error returned when lease validation fails.
#[derive(Debug, Clone, PartialEq, Eq)]
pub enum LeaseValidationError {
    /// Lease ID was not found in the ledger.
    LeaseNotFound {
        /// The lease ID that was not found.
        lease_id: String,
    },
    /// Lease exists but `work_id` does not match.
    ///
    /// # Security Note (SEC-HYG-001)
    ///
    /// The expected `work_id` is intentionally omitted from this error to
    /// prevent information leakage. Revealing the expected value could
    /// allow an attacker to enumerate valid work IDs.
    WorkIdMismatch {
        /// The actual `work_id` from the request (not the expected one).
        actual: String,
    },
    /// Lease has expired.
    LeaseExpired {
        /// The expired lease ID.
        lease_id: String,
    },
    /// Failed to query the ledger.
    LedgerQueryFailed {
        /// The error message from the ledger.
        message: String,
    },
}

impl std::fmt::Display for LeaseValidationError {
    fn fmt(&self, f: &mut std::fmt::Formatter<'_>) -> std::fmt::Result {
        match self {
            Self::LeaseNotFound { lease_id } => {
                write!(f, "lease not found: {lease_id}")
            },
            Self::WorkIdMismatch { actual } => {
                write!(f, "work_id mismatch for provided value: {actual}")
            },
            Self::LeaseExpired { lease_id } => {
                write!(f, "lease expired: {lease_id}")
            },
            Self::LedgerQueryFailed { message } => {
                write!(f, "ledger query failed: {message}")
            },
        }
    }
}

impl std::error::Error for LeaseValidationError {}

/// Trait for validating gate leases.
///
/// Per RFC-0017 IPC-PRIV-002, `GATE_EXECUTOR` role requires a valid
/// `GateLeaseIssued` event to exist in the ledger for the specified
/// `lease_id` and `work_id`.
///
/// # Security Contract
///
/// - `GATE_EXECUTOR` spawn MUST be rejected if lease validation fails
/// - Lease validation verifies the lease exists and matches the `work_id`
/// - This is a fail-closed check: validation errors reject the spawn
///
/// # Implementers
///
/// - `StubLeaseValidator`: In-memory storage for testing
/// - `LedgerLeaseValidator`: Ledger-backed validation (future)
pub trait LeaseValidator: Send + Sync {
    /// Validates that a gate lease exists and matches the `work_id`.
    ///
    /// # Arguments
    ///
    /// * `lease_id` - The lease ID to validate
    /// * `work_id` - The work ID that must match the lease
    ///
    /// # Returns
    ///
    /// `Ok(())` if the lease is valid and matches the `work_id`.
    ///
    /// # Errors
    ///
    /// Returns `LeaseValidationError` if:
    /// - The lease does not exist (`LeaseNotFound`)
    /// - The lease exists but the `work_id` doesn't match (`WorkIdMismatch`)
    /// - The lease has expired (`LeaseExpired`)
    /// - The ledger query failed (`LedgerQueryFailed`)
    fn validate_gate_lease(
        &self,
        lease_id: &str,
        work_id: &str,
    ) -> Result<(), LeaseValidationError>;

    /// Returns the `executor_actor_id` bound to a lease (TCK-00389).
    ///
    /// This is used by `IngestReviewReceipt` to validate that the reviewer
    /// identity matches the executor authorized by the gate lease.
    ///
    /// # Returns
    ///
    /// `Some(actor_id)` if the lease exists, `None` otherwise.
    fn get_lease_executor_actor_id(&self, lease_id: &str) -> Option<String>;

    /// Registers a gate lease for testing purposes.
    ///
    /// In production, leases are issued through a separate governance flow.
    /// This method exists to support test fixtures.
    fn register_lease(&self, lease_id: &str, work_id: &str, gate_id: &str);

    /// Registers a gate lease with an executor actor ID (TCK-00389).
    ///
    /// This method exists to support test fixtures that need to verify
    /// reviewer identity against the lease's `executor_actor_id`.
    fn register_lease_with_executor(
        &self,
        lease_id: &str,
        work_id: &str,
        gate_id: &str,
        executor_actor_id: &str,
    ) {
        // Default: delegate to register_lease (backward compat)
        let _ = executor_actor_id;
        self.register_lease(lease_id, work_id, gate_id);
    }

    /// Returns the `work_id` bound to a lease (TCK-00340).
    ///
    /// Used by `handle_ingest_review_receipt` to resolve the work claim
    /// and its associated risk tier for attestation ratcheting.
    ///
    /// # Returns
    ///
    /// `Some(work_id)` if the lease exists, `None` otherwise.
    fn get_lease_work_id(&self, lease_id: &str) -> Option<String> {
        let _ = lease_id;
        None
    }

    /// Returns the full `GateLease` for a given lease ID (TCK-00340).
    ///
    /// Used by `DelegateSublease` to retrieve the parent lease for
    /// strict-subset validation and sublease issuance.
    ///
    /// # Trust Model (v7 Finding 1)
    ///
    /// The returned lease is trusted because it comes from the daemon-owned
    /// `SQLite` ledger (same-process trust boundary). The lease's embedded
    /// `issuer_signature` field preserves the original Ed25519 signature
    /// for downstream verification, but callers within the daemon process
    /// do NOT need to call `GateLease::validate_signature()` -- the ledger
    /// integrity is guaranteed by the process trust boundary.
    ///
    /// See `SqliteLeaseValidator::register_full_lease()` for the full trust
    /// model documentation.
    ///
    /// # Returns
    ///
    /// `Some(GateLease)` if the lease exists, `None` otherwise.
    fn get_gate_lease(&self, lease_id: &str) -> Option<apm2_core::fac::GateLease> {
        let _ = lease_id;
        None
    }

    /// Registers a full `GateLease` for sublease delegation (TCK-00340).
    ///
    /// This method exists to support test fixtures and production paths
    /// that need full lease objects for sublease delegation.
    ///
    /// # Errors
    ///
    /// Returns an error string if the lease could not be persisted
    /// (serialization failure, database write failure, etc.). Callers
    /// MUST fail closed when this returns `Err` to preserve the
    /// single-effect guarantee (LAW-11) -- a sublease event emitted
    /// without a corresponding persisted lease anchor would leave the
    /// system in an inconsistent state.
    fn register_full_lease(&self, lease: &apm2_core::fac::GateLease) -> Result<(), String> {
        let _ = lease;
        Ok(())
    }
}

/// Entry for a registered lease.
#[derive(Debug, Clone)]
#[allow(clippy::struct_field_names)]
struct LeaseEntry {
    work_id: String,
    #[allow(dead_code)]
    gate_id: String,
    /// The executor actor ID authorized by this lease (TCK-00389).
    executor_actor_id: String,
}

/// Stub implementation of [`LeaseValidator`] for testing.
///
/// Stores leases in memory with no persistence.
///
/// # Capacity Limits (CTR-1303)
///
/// This validator enforces a maximum of 10,000 entries to prevent memory
/// exhaustion. When the limit is reached, the oldest entry (by insertion order)
/// is evicted to make room for the new lease.
///
/// # Security Notes
///
/// - **SEC-DoS-001**: Duplicate lease IDs are handled by updating in place
///   rather than adding a new entry, preventing unbounded memory growth.
/// - **SEC-DoS-002**: Uses `VecDeque` for O(1) eviction from the front,
///   consistent with the O(1) eviction pattern established in PR 329.
#[derive(Debug, Default)]
pub struct StubLeaseValidator {
    /// Leases stored with insertion order for O(1) LRU eviction.
    ///
    /// Uses `VecDeque` (SEC-DoS-002) for efficient front removal.
    leases: std::sync::RwLock<(
        std::collections::VecDeque<String>,
        std::collections::HashMap<String, LeaseEntry>,
    )>,
    /// Full gate leases for sublease delegation (TCK-00340).
    full_leases: std::sync::RwLock<std::collections::HashMap<String, apm2_core::fac::GateLease>>,
}

impl StubLeaseValidator {
    /// Creates a new empty lease validator.
    #[must_use]
    pub fn new() -> Self {
        Self {
            leases: std::sync::RwLock::new((
                std::collections::VecDeque::new(),
                std::collections::HashMap::new(),
            )),
            full_leases: std::sync::RwLock::new(std::collections::HashMap::new()),
        }
    }
}

impl LeaseValidator for StubLeaseValidator {
    fn validate_gate_lease(
        &self,
        lease_id: &str,
        work_id: &str,
    ) -> Result<(), LeaseValidationError> {
        let guard = self.leases.read().expect("lock poisoned");
        let (_, leases) = &*guard;

        leases.get(lease_id).map_or_else(
            || {
                Err(LeaseValidationError::LeaseNotFound {
                    lease_id: lease_id.to_string(),
                })
            },
            |entry| {
                let work_id_matches = entry.work_id.len() == work_id.len()
                    && bool::from(entry.work_id.as_bytes().ct_eq(work_id.as_bytes()));
                if work_id_matches {
                    Ok(())
                } else {
                    Err(LeaseValidationError::WorkIdMismatch {
                        actual: work_id.to_string(),
                    })
                }
            },
        )
    }

    fn get_lease_executor_actor_id(&self, lease_id: &str) -> Option<String> {
        let guard = self.leases.read().expect("lock poisoned");
        let (_, leases) = &*guard;
        leases
            .get(lease_id)
            .map(|entry| entry.executor_actor_id.clone())
    }

    fn get_lease_work_id(&self, lease_id: &str) -> Option<String> {
        let guard = self.leases.read().expect("lock poisoned");
        let (_, leases) = &*guard;
        leases.get(lease_id).map(|entry| entry.work_id.clone())
    }

    fn register_lease(&self, lease_id: &str, work_id: &str, gate_id: &str) {
        // Delegate to register_lease_with_executor with empty executor ID
        // for backward compatibility with existing tests.
        self.register_lease_with_executor(lease_id, work_id, gate_id, "");
    }

    fn register_lease_with_executor(
        &self,
        lease_id: &str,
        work_id: &str,
        gate_id: &str,
        executor_actor_id: &str,
    ) {
        let mut guard = self.leases.write().expect("lock poisoned");
        let (order, leases) = &mut *guard;

        // SEC-DoS-001: Check for duplicate lease_id and update in place
        if leases.contains_key(lease_id) {
            // Update existing entry without adding to order (already tracked)
            leases.insert(
                lease_id.to_string(),
                LeaseEntry {
                    work_id: work_id.to_string(),
                    gate_id: gate_id.to_string(),
                    executor_actor_id: executor_actor_id.to_string(),
                },
            );
            return;
        }

        // CTR-1303: Evict oldest entry if at capacity
        // SEC-DoS-002: Use pop_front() for O(1) eviction
        while leases.len() >= MAX_LEASE_ENTRIES {
            if let Some(oldest_key) = order.pop_front() {
                leases.remove(&oldest_key);
            } else {
                break;
            }
        }

        order.push_back(lease_id.to_string());
        leases.insert(
            lease_id.to_string(),
            LeaseEntry {
                work_id: work_id.to_string(),
                gate_id: gate_id.to_string(),
                executor_actor_id: executor_actor_id.to_string(),
            },
        );
    }

    fn get_gate_lease(&self, lease_id: &str) -> Option<apm2_core::fac::GateLease> {
        let guard = self.full_leases.read().expect("lock poisoned");
        guard.get(lease_id).cloned()
    }

    fn register_full_lease(&self, lease: &apm2_core::fac::GateLease) -> Result<(), String> {
        let mut guard = self.full_leases.write().expect("lock poisoned");
        // SECURITY (v10 BLOCKER 2): Enforce uniqueness at the persistence
        // layer. If a lease with this ID already exists, return an error
        // so the caller can handle it (idempotent or conflict). This makes
        // the store the source of truth for uniqueness, preventing TOCTOU
        // races where two concurrent requests both pass the in-memory
        // pre-check and create conflicting leases.
        if guard.contains_key(&lease.lease_id) {
            return Err(format!("duplicate lease_id: {}", lease.lease_id));
        }
        guard.insert(lease.lease_id.clone(), lease.clone());
        Ok(())
    }
}

/// Privileged endpoint dispatcher.
///
/// Routes incoming messages to the appropriate handler based on message type.
/// Enforces privilege separation by checking
/// `ConnectionContext::is_privileged()` before dispatching to any handler.
///
/// # Security Contract
///
/// Per INV-0001 and TB-002:
/// - Session connections receive `PERMISSION_DENIED` for ALL privileged
///   requests
/// - No privileged handler logic executes for non-privileged connections
/// - Generic error messages prevent endpoint enumeration (TH-004)
///
/// # TCK-00253 Additions
///
/// - Policy resolver for governance delegation
/// - Work registry for claim persistence
/// - Ledger event emitter for signed event persistence
/// - Actor ID derivation from credentials
///
/// # TCK-00256 Additions
///
/// - Episode runtime for lifecycle management
/// - Session registry for session state persistence
///
/// # TCK-00257 Additions
///
/// - Lease validator for `GATE_EXECUTOR` spawn validation
///
/// # TCK-00289 Additions
///
/// - `HolonicClock` for HTF-compliant timestamps in `IssueCapability`
pub struct PrivilegedDispatcher {
    /// Decode configuration for bounded message decoding.
    decode_config: DecodeConfig,

    /// Policy resolver for governance delegation (TCK-00253).
    policy_resolver: Arc<dyn PolicyResolver>,

    /// Work registry for claim persistence (TCK-00253).
    work_registry: Arc<dyn WorkRegistry>,

    /// Ledger event emitter for signed event persistence (TCK-00253).
    event_emitter: Arc<dyn LedgerEventEmitter>,

    /// Episode runtime for lifecycle management (TCK-00256).
    episode_runtime: Arc<EpisodeRuntime>,

    /// Session registry for session state persistence (TCK-00256).
    session_registry: Arc<dyn SessionRegistry>,

    /// Lease validator for `GATE_EXECUTOR` spawn validation (TCK-00257).
    lease_validator: Arc<dyn LeaseValidator>,

    /// Token minter for session token generation (TCK-00287).
    ///
    /// Shared with `SessionDispatcher` to ensure tokens minted during
    /// `SpawnEpisode` can be validated on session endpoints.
    token_minter: Arc<TokenMinter>,

    /// Manifest store for capability manifest registration (TCK-00287).
    ///
    /// Shared with `SessionDispatcher` so that manifests registered during
    /// `SpawnEpisode` are accessible for tool request validation.
    manifest_store: Arc<InMemoryManifestStore>,

    /// CAS-backed manifest loader for capability manifest retrieval
    /// (TCK-00317).
    ///
    /// Per DOD item 1 (CAS Storage & Hash Loading), manifests are stored in
    /// CAS and loaded by hash. The policy resolver returns the manifest hash,
    /// and `handle_spawn_episode` uses this loader to retrieve the manifest.
    ///
    /// # Security Model
    ///
    /// - Manifests are stored with their BLAKE3 hash as the key
    /// - Load operations verify the hash matches the content
    /// - Missing manifests result in fail-closed rejection
    manifest_loader: Arc<dyn ManifestLoader>,

    /// Prometheus metrics registry for daemon health observability (TCK-00268).
    ///
    /// When present, the dispatcher emits metrics for:
    /// - `session_spawned`: When `SpawnEpisode` succeeds
    /// - `ipc_request_completed`: For each dispatched request
    /// - `capability_granted`: When `IssueCapability` succeeds
    ///
    /// # Integration Status
    ///
    /// **NOTE**: This dispatcher uses the binary protocol
    /// (`PrivilegedMessageType`) which is not currently wired into
    /// `main.rs`. The daemon's main connection handler uses JSON-based
    /// `IpcRequest` via `handlers::dispatch()` instead.
    ///
    /// These metrics will become active when the binary protocol path is
    /// integrated into the daemon's connection handling. Until then, the
    /// JSON-based IPC path in `handlers.rs` correctly records
    /// `ipc_request_completed` metrics.
    ///
    /// TODO(TCK-FUTURE): Wire `PrivilegedDispatcher` into `main.rs` to enable
    /// these metrics for binary protocol requests.
    metrics: Option<SharedMetricsRegistry>,

    /// HTF-compliant clock for timestamps (TCK-00289).
    ///
    /// Used to generate RFC-0016 compliant timestamps for:
    /// - `IssueCapability` `granted_at` / `expires_at` fields
    /// - `WorkClaimed` ledger event timestamps
    ///
    /// # HTF Compliance
    ///
    /// The clock provides:
    /// - Monotonic ticks: Never regress within a process lifetime
    /// - HLC stamps: Hybrid logical clock for cross-node causality
    /// - Wall time bounds: Observational only, with uncertainty interval
    holonic_clock: Arc<HolonicClock>,

    /// Subscription registry for HEF Pulse Plane resource governance
    /// (TCK-00303).
    ///
    /// Tracks per-connection subscription state and enforces limits per
    /// RFC-0018. Shared with `SessionDispatcher` to manage subscriptions
    /// across both operator and session sockets.
    subscription_registry: SharedSubscriptionRegistry,

    /// Shared daemon state for process management (TCK-00342).
    ///
    /// When present, process management handlers (`ListProcesses`,
    /// `ProcessStatus`, `StartProcess`, `StopProcess`, `RestartProcess`,
    /// `ReloadProcess`) query the `Supervisor` within `DaemonState` for
    /// process information. When `None`, handlers return stub responses
    /// (for testing without full daemon context).
    daemon_state: Option<SharedState>,

    /// Node ID for consensus status reporting (TCK-00345).
    ///
    /// Used in consensus query responses to identify this node.
    node_id: String,

    /// Consensus subsystem state handle (TCK-00345).
    ///
    /// When `Some`, consensus queries return real state data.
    /// When `None`, consensus queries return `CONSENSUS_NOT_CONFIGURED` error.
    ///
    /// # Future Work
    ///
    /// This will be wired to actual consensus state (`HotStuffState`,
    /// `ConsensusMetrics`, etc.) when the daemon consensus integration is
    /// complete. For now, presence/absence controls whether the subsystem
    /// is considered "configured".
    consensus_state: Option<()>,

    /// Credential store for secure credential persistence (TCK-00343).
    ///
    /// When present, credential management handlers (`ListCredentials`,
    /// `AddCredential`, `RemoveCredential`, `RefreshCredential`,
    /// `SwitchCredential`, `LoginCredential`) persist credentials to the
    /// `CredentialStore` backed by the OS keyring. When `None`, handlers
    /// return error responses indicating the credential store is not
    /// configured.
    credential_store: Option<Arc<CredentialStore>>,

    /// Session telemetry store for tracking tool calls, events emitted,
    /// and session start time (TCK-00384).
    ///
    /// When present, `SpawnEpisode` registers telemetry for new sessions
    /// with `started_at_ns` set to the current wall time. The store is
    /// shared with `SessionDispatcher` for counter updates and queries.
    telemetry_store: Option<Arc<crate::session::SessionTelemetryStore>>,

    /// Content-addressed store for `ChangeSet` bundle persistence (TCK-00394).
    ///
    /// When present, `PublishChangeSet` stores the canonical bundle bytes in
    /// CAS and returns the content hash for ledger event binding. When `None`,
    /// the handler returns an error indicating CAS is not configured.
    cas: Option<Arc<dyn ContentAddressedStore>>,

    /// Shared V1 manifest store for TCK-00352 scope enforcement.
    ///
    /// Per Security Review MAJOR 2, `handle_spawn_episode` mints a V1
    /// capability manifest and registers it in this store. The
    /// `SessionDispatcher` holds a clone of the same `Arc` and enforces
    /// V1 scope checks in `handle_request_tool`.
    v1_manifest_store: Option<crate::protocol::session_dispatch::SharedV1ManifestStore>,

    /// Gate orchestrator for sublease delegation (TCK-00340).
    ///
    /// When present, `DelegateSublease` delegates to the orchestrator to
    /// issue signed subleases with strict-subset validation. When `None`,
    /// the handler returns an error indicating the orchestrator is not
    /// configured.
    gate_orchestrator: Option<Arc<crate::gate::GateOrchestrator>>,

    /// Per-session stop conditions store for pre-actuation gate enforcement
    /// (TCK-00351 v4).
    ///
    /// When present, `SpawnEpisode` registers stop conditions (from the
    /// episode envelope) for the new session. The `SessionDispatcher` holds
    /// a clone of the same `Arc` and reads conditions in the pre-actuation
    /// gate to enforce `max_episodes` / `escalation_predicate`.
    stop_conditions_store: Option<Arc<crate::session::SessionStopConditionsStore>>,

    /// Shared stop authority used by privileged control-plane handlers to
    /// mutate emergency/governance stop flags at runtime.
    stop_authority: Option<Arc<crate::episode::preactuation::StopAuthority>>,

    /// Governance freshness monitor wired from production `DispatcherState`.
    ///
    /// Successful governance-backed operations call `record_success()`.
    /// Only governance transport/communication failures call
    /// `record_failure()`.
    governance_freshness_monitor: Option<Arc<GovernanceFreshnessMonitor>>,

    /// TCK-00399: Adapter registry for spawning agent CLI processes.
    ///
    /// When present, `handle_spawn_episode` loads the adapter profile from
    /// CAS, builds a `HarnessConfig`, and spawns the agent process via the
    /// appropriate `HarnessAdapter`.
    adapter_registry: Option<Arc<crate::episode::AdapterRegistry>>,
}

impl Default for PrivilegedDispatcher {
    /// Creates a default dispatcher (TEST ONLY).
    ///
    /// # Warning: RSK-2503 Mixed Clock Domain Hazard
    ///
    /// See `new()` for details on clock domain hazards.
    fn default() -> Self {
        Self::new()
    }
}

/// Default session token TTL (5 minutes).
///
/// Per WVR-0002, env-only bootstrap tokens MUST have a TTL <= 300 seconds.
/// This was previously 3600s (1 hour) which violated the waiver constraint.
pub const DEFAULT_SESSION_TOKEN_TTL_SECS: u64 = 300;

/// Stop-condition policy floor for untrusted `SpawnEpisode` inputs.
///
/// This enforces authoritative minimum constraints before requester-supplied
/// stop conditions are persisted.
#[derive(Debug, Clone, Copy)]
struct StopConditionPolicy {
    /// Maximum allowed value for `max_episodes`.
    ///
    /// Values above this policy floor are rejected. A value of `0` means no
    /// floor is configured for this dimension.
    max_episodes_floor: u64,
}

impl StopConditionPolicy {
    /// Fail-closed policy floor used when no governance-provided floor is
    /// available.
    const fn fail_closed_default() -> Self {
        Self {
            // Transitional policy ceiling: requester cannot bypass with
            // unlimited `max_episodes=0` and cannot exceed this floor.
            max_episodes_floor: 10,
        }
    }

    /// Validates request stop conditions against the policy floor and returns
    /// canonicalized conditions to persist.
    ///
    /// # Errors
    ///
    /// Returns `Err` when request values violate the floor.
    fn validate_against_floor(
        self,
        request_max_episodes: Option<u64>,
        request_escalation_predicate: Option<&str>,
    ) -> Result<crate::episode::envelope::StopConditions, String> {
        // Fail-closed request default: missing max_episodes is constrained.
        let resolved_max_episodes = request_max_episodes.unwrap_or(1);

        if self.max_episodes_floor > 0 {
            if resolved_max_episodes == 0 {
                return Err(format!(
                    "max_episodes=0 is not allowed by policy floor; expected 1..={}",
                    self.max_episodes_floor
                ));
            }
            if resolved_max_episodes > self.max_episodes_floor {
                return Err(format!(
                    "max_episodes={} exceeds policy floor max={}",
                    resolved_max_episodes, self.max_episodes_floor
                ));
            }
        }

        Ok(crate::episode::envelope::StopConditions {
            max_episodes: resolved_max_episodes,
            escalation_predicate: request_escalation_predicate.unwrap_or("").to_string(),
            goal_predicate: String::new(),
            failure_predicate: String::new(),
        })
    }
}

impl PrivilegedDispatcher {
    /// Creates a new dispatcher with default decode configuration (TEST ONLY).
    ///
    /// # Warning: RSK-2503 Mixed Clock Domain Hazard
    ///
    /// This constructor creates an internal `HolonicClock` instance. For
    /// production code, use `with_shared_state` or `with_dependencies` to
    /// inject a shared clock and prevent mixed clock domain hazards.
    ///
    /// # Usage
    ///
    /// This constructor is intended for unit tests only. Production code
    /// should use `with_shared_state` or `with_dependencies` with a
    /// properly initialized and shared `HolonicClock`.
    ///
    /// Uses stub implementations for policy resolver, work registry, event
    /// emitter, session registry, and lease validator. No metrics are emitted.
    /// Creates internal stub token minter, manifest store, and HTF clock for
    /// testing.
    #[must_use]
    pub fn new() -> Self {
        // TCK-00289: Create default HolonicClock for HTF-compliant timestamps
        // WARNING: This creates an internal clock which can cause RSK-2503
        // (Mixed Clock Domain Hazard) if used in production alongside other
        // components with their own clocks. Use with_shared_state or
        // with_dependencies for production code.
        let holonic_clock = Arc::new(
            HolonicClock::new(ClockConfig::default(), None)
                .expect("default ClockConfig should always succeed"),
        );
        // TCK-00303: Create subscription registry for HEF resource governance
        let subscription_registry = Arc::new(SubscriptionRegistry::with_defaults());

        Self {
            decode_config: DecodeConfig::default(),
            policy_resolver: Arc::new(StubPolicyResolver),
            work_registry: Arc::new(StubWorkRegistry::default()),
            event_emitter: Arc::new(StubLedgerEventEmitter::new()),
            episode_runtime: Arc::new(EpisodeRuntime::new(EpisodeRuntimeConfig::default())),
            session_registry: Arc::new(InMemorySessionRegistry::default()),
            lease_validator: Arc::new(StubLeaseValidator::new()),
            token_minter: Arc::new(TokenMinter::new(TokenMinter::generate_secret())),
            manifest_store: Arc::new(InMemoryManifestStore::new()),
            // TCK-00317: Pre-seed CAS with reviewer v0 manifest
            manifest_loader: Arc::new(InMemoryCasManifestLoader::with_reviewer_v0_manifest()),
            metrics: None,
            holonic_clock,
            subscription_registry,
            daemon_state: None,
            // TCK-00345: Consensus state not configured in test mode
            node_id: "test-node".to_string(),
            consensus_state: None,
            credential_store: None,
            telemetry_store: None,
            cas: None,
            v1_manifest_store: None,
            gate_orchestrator: None,
            stop_conditions_store: None,
            stop_authority: None,
            governance_freshness_monitor: None,
            adapter_registry: None,
        }
    }

    /// Creates a new dispatcher with custom decode configuration (TEST ONLY).
    ///
    /// # Warning: RSK-2503 Mixed Clock Domain Hazard
    ///
    /// This constructor creates an internal `HolonicClock` instance. For
    /// production code, use `with_shared_state` or `with_dependencies` to
    /// inject a shared clock and prevent mixed clock domain hazards.
    ///
    /// # Usage
    ///
    /// This constructor is intended for unit tests only. Production code
    /// should use `with_shared_state` or `with_dependencies` with a
    /// properly initialized and shared `HolonicClock`.
    ///
    /// Uses stub implementations for policy resolver, work registry, event
    /// emitter, session registry, and lease validator. No metrics are emitted.
    /// Creates internal stub token minter, manifest store, and HTF clock for
    /// testing.
    #[must_use]
    pub fn with_decode_config(decode_config: DecodeConfig) -> Self {
        // TCK-00289: Create default HolonicClock for HTF-compliant timestamps
        // WARNING: This creates an internal clock which can cause RSK-2503
        // (Mixed Clock Domain Hazard) if used in production alongside other
        // components with their own clocks. Use with_shared_state or
        // with_dependencies for production code.
        let holonic_clock = Arc::new(
            HolonicClock::new(ClockConfig::default(), None)
                .expect("default ClockConfig should always succeed"),
        );
        // TCK-00303: Create subscription registry for HEF resource governance
        let subscription_registry = Arc::new(SubscriptionRegistry::with_defaults());

        Self {
            decode_config,
            policy_resolver: Arc::new(StubPolicyResolver),
            work_registry: Arc::new(StubWorkRegistry::default()),
            event_emitter: Arc::new(StubLedgerEventEmitter::new()),
            episode_runtime: Arc::new(EpisodeRuntime::new(EpisodeRuntimeConfig::default())),
            session_registry: Arc::new(InMemorySessionRegistry::default()),
            lease_validator: Arc::new(StubLeaseValidator::new()),
            token_minter: Arc::new(TokenMinter::new(TokenMinter::generate_secret())),
            manifest_store: Arc::new(InMemoryManifestStore::new()),
            // TCK-00317: Pre-seed CAS with reviewer v0 manifest
            manifest_loader: Arc::new(InMemoryCasManifestLoader::with_reviewer_v0_manifest()),
            metrics: None,
            holonic_clock,
            subscription_registry,
            daemon_state: None,
            // TCK-00345: Consensus state not configured in test mode
            node_id: "test-node".to_string(),
            consensus_state: None,
            credential_store: None,
            telemetry_store: None,
            cas: None,
            v1_manifest_store: None,
            gate_orchestrator: None,
            stop_conditions_store: None,
            stop_authority: None,
            governance_freshness_monitor: None,
            adapter_registry: None,
        }
    }

    /// Creates a new dispatcher with custom dependencies (PRODUCTION).
    ///
    /// This is the production constructor for real governance integration.
    /// Does not include metrics; use `with_metrics` to add them.
    ///
    /// # TCK-00289: Clock Injection (RSK-2503 Prevention)
    ///
    /// The `clock` parameter MUST be a shared `HolonicClock` instance that is
    /// also used by other components in the system. This prevents the mixed
    /// clock domain hazard (RSK-2503) that would occur if each component
    /// created its own clock.
    ///
    /// # TCK-00287: State Sharing
    ///
    /// The `token_minter` and `manifest_store` parameters MUST be `Arc::clone`
    /// copies of the same instances used by `SessionDispatcher`. This ensures:
    /// - Tokens minted during `SpawnEpisode` can be validated by
    ///   `SessionDispatcher`
    /// - Capability manifests registered during `SpawnEpisode` are accessible
    ///   for tool request validation
    ///
    /// Callers must ensure proper sharing by cloning the Arcs BEFORE passing
    /// to this constructor:
    /// ```ignore
    /// let token_minter = Arc::new(TokenMinter::new(...));
    /// let manifest_store = Arc::new(InMemoryManifestStore::new());
    /// let priv_dispatcher = PrivilegedDispatcher::with_dependencies(
    ///     ...,
    ///     Arc::clone(&token_minter),  // Clone BEFORE passing
    ///     Arc::clone(&manifest_store), // Clone BEFORE passing
    /// );
    /// let session_dispatcher = SessionDispatcher::with_manifest_store(
    ///     (*token_minter).clone(),
    ///     manifest_store,
    /// );
    /// ```
    #[must_use]
    #[allow(clippy::too_many_arguments)]
    pub fn with_dependencies(
        decode_config: DecodeConfig,
        policy_resolver: Arc<dyn PolicyResolver>,
        work_registry: Arc<dyn WorkRegistry>,
        event_emitter: Arc<dyn LedgerEventEmitter>,
        episode_runtime: Arc<EpisodeRuntime>,
        session_registry: Arc<dyn SessionRegistry>,
        lease_validator: Arc<dyn LeaseValidator>,
        clock: Arc<HolonicClock>,
        token_minter: Arc<TokenMinter>,
        manifest_store: Arc<InMemoryManifestStore>,
        manifest_loader: Arc<dyn ManifestLoader>,
        subscription_registry: SharedSubscriptionRegistry,
    ) -> Self {
        Self {
            decode_config,
            policy_resolver,
            work_registry,
            event_emitter,
            episode_runtime,
            session_registry,
            lease_validator,
            token_minter,
            manifest_store,
            manifest_loader,
            metrics: None,
            holonic_clock: clock,
            subscription_registry,
            daemon_state: None,
            // TCK-00345: Consensus state not configured by default
            node_id: "node-001".to_string(),
            consensus_state: None,
            credential_store: None,
            telemetry_store: None,
            cas: None,
            v1_manifest_store: None,
            gate_orchestrator: None,
            stop_conditions_store: None,
            stop_authority: None,
            governance_freshness_monitor: None,
            adapter_registry: None,
        }
    }

    /// Creates a new dispatcher with shared token minter and manifest store
    /// (PRODUCTION).
    ///
    /// # TCK-00287: State Sharing
    ///
    /// This constructor is used by `DispatcherState` to wire up shared
    /// dependencies between `PrivilegedDispatcher` and `SessionDispatcher`:
    /// - `token_minter`: Ensures tokens minted during `SpawnEpisode` can be
    ///   validated by `SessionDispatcher`
    /// - `manifest_store`: Ensures capability manifests registered during
    ///   `SpawnEpisode` are accessible for tool request validation
    /// - `session_registry`: Uses the global daemon session registry instead of
    ///   an internal stub
    ///
    /// # TCK-00289: Clock Injection (RSK-2503 Prevention)
    ///
    /// The `clock` parameter MUST be a shared `HolonicClock` instance that is
    /// also used by other components in the system. This prevents the mixed
    /// clock domain hazard (RSK-2503) that would occur if each component
    /// created its own clock.
    #[must_use]
    pub fn with_shared_state(
        token_minter: Arc<TokenMinter>,
        manifest_store: Arc<InMemoryManifestStore>,
        session_registry: Arc<dyn SessionRegistry>,
        clock: Arc<HolonicClock>,
        subscription_registry: SharedSubscriptionRegistry,
    ) -> Self {
        Self {
            decode_config: DecodeConfig::default(),
            policy_resolver: Arc::new(StubPolicyResolver),
            work_registry: Arc::new(StubWorkRegistry::default()),
            event_emitter: Arc::new(StubLedgerEventEmitter::new()),
            episode_runtime: Arc::new(EpisodeRuntime::new(EpisodeRuntimeConfig::default())),
            session_registry,
            lease_validator: Arc::new(StubLeaseValidator::new()),
            token_minter,
            manifest_store,
            // TCK-00317: Pre-seed CAS with reviewer v0 manifest
            manifest_loader: Arc::new(InMemoryCasManifestLoader::with_reviewer_v0_manifest()),
            metrics: None,
            holonic_clock: clock,
            subscription_registry,
            daemon_state: None,
            // TCK-00345: Consensus state not configured by default
            node_id: "node-001".to_string(),
            consensus_state: None,
            credential_store: None,
            telemetry_store: None,
            cas: None,
            v1_manifest_store: None,
            gate_orchestrator: None,
            stop_conditions_store: None,
            stop_authority: None,
            governance_freshness_monitor: None,
            adapter_registry: None,
        }
    }

    /// Sets the daemon state for process management (TCK-00342).
    ///
    /// When set, process management handlers (`ListProcesses`,
    /// `ProcessStatus`, `StartProcess`, `StopProcess`, `RestartProcess`,
    /// `ReloadProcess`) query the `Supervisor` within `DaemonState` for
    /// live process information instead of returning stub responses.
    #[must_use]
    pub fn with_daemon_state(mut self, state: SharedState) -> Self {
        self.daemon_state = Some(state);
        self
    }

    /// Sets the credential store for credential management (TCK-00343).
    ///
    /// When set, credential management handlers (`ListCredentials`,
    /// `AddCredential`, `RemoveCredential`, `RefreshCredential`,
    /// `SwitchCredential`) persist credentials via the `CredentialStore`
    /// backed by the OS keyring. When not set, handlers return error
    /// responses indicating the credential store is not configured.
    #[must_use]
    pub fn with_credential_store(mut self, store: Arc<CredentialStore>) -> Self {
        self.credential_store = Some(store);
        self
    }

    /// Sets the session telemetry store for tracking tool calls, events
    /// emitted, and session start time (TCK-00384).
    ///
    /// When set, `SpawnEpisode` registers telemetry for new sessions. The
    /// store should be shared with `SessionDispatcher` for counter
    /// updates and queries.
    #[must_use]
    pub fn with_telemetry_store(
        mut self,
        store: Arc<crate::session::SessionTelemetryStore>,
    ) -> Self {
        self.telemetry_store = Some(store);
        self
    }

    /// Sets the content-addressed store for changeset bundle persistence
    /// (TCK-00394).
    ///
    /// When set, `PublishChangeSet` stores the canonical bundle bytes in CAS
    /// and returns the content hash for ledger event binding. When not set,
    /// the handler returns an error indicating CAS is not configured.
    #[must_use]
    pub fn with_cas(mut self, cas: Arc<dyn ContentAddressedStore>) -> Self {
        self.cas = Some(cas);
        self
    }

    /// Sets the shared V1 manifest store (TCK-00352 Security Review MAJOR 2).
    ///
    /// When set, `handle_spawn_episode` mints a V1 capability manifest and
    /// registers it in this store. The `SessionDispatcher` holds a clone of
    /// the same `Arc` and enforces V1 scope checks in `handle_request_tool`.
    #[must_use]
    pub fn with_v1_manifest_store(
        mut self,
        store: crate::protocol::session_dispatch::SharedV1ManifestStore,
    ) -> Self {
        self.v1_manifest_store = Some(store);
        self
    }

    /// Sets the gate orchestrator for sublease delegation (TCK-00340).
    #[must_use]
    pub fn with_gate_orchestrator(
        mut self,
        orchestrator: Arc<crate::gate::GateOrchestrator>,
    ) -> Self {
        self.gate_orchestrator = Some(orchestrator);
        self
    }

    /// Sets the per-session stop conditions store (TCK-00351 v4).
    ///
    /// When set, `SpawnEpisode` registers stop conditions from the episode
    /// envelope for each new session. The `SessionDispatcher` holds a clone
    /// of the same `Arc` and reads conditions in the pre-actuation gate.
    #[must_use]
    pub fn with_stop_conditions_store(
        mut self,
        store: Arc<crate::session::SessionStopConditionsStore>,
    ) -> Self {
        self.stop_conditions_store = Some(store);
        self
    }

    /// Sets the shared stop authority for runtime stop-flag mutation.
    #[must_use]
    pub fn with_stop_authority(
        mut self,
        authority: Arc<crate::episode::preactuation::StopAuthority>,
    ) -> Self {
        self.stop_authority = Some(authority);
        self
    }

    /// Sets the governance freshness monitor for production probe wiring.
    #[must_use]
    pub fn with_governance_freshness_monitor(
        mut self,
        monitor: Arc<GovernanceFreshnessMonitor>,
    ) -> Self {
        self.governance_freshness_monitor = Some(monitor);
        self
    }

    /// Sets the adapter registry for spawning agent CLI processes (TCK-00399).
    #[must_use]
    pub fn with_adapter_registry(mut self, registry: Arc<crate::episode::AdapterRegistry>) -> Self {
        self.adapter_registry = Some(registry);
        self
    }

    /// Replaces the session registry used by this dispatcher (TEST ONLY).
    ///
    /// This allows tests to inject a concrete `InMemorySessionRegistry`
    /// (or any `SessionRegistry` impl) so they can inspect registry
    /// cardinality and content after dispatch operations.
    #[cfg(test)]
    #[must_use]
    pub fn with_session_registry(mut self, registry: Arc<dyn SessionRegistry>) -> Self {
        self.session_registry = registry;
        self
    }

    /// Adds a metrics registry to the dispatcher (TCK-00268).
    ///
    /// When set, the dispatcher will emit metrics for:
    /// - `session_spawned`: When `SpawnEpisode` succeeds
    /// - `ipc_request_completed`: For each dispatched request
    /// - `capability_granted`: When `IssueCapability` succeeds
    ///
    /// # Integration Status
    ///
    /// **NOTE**: This method is currently only exercised in tests. The binary
    /// protocol path is not yet wired into `main.rs`. See the `metrics` field
    /// documentation for details.
    #[must_use]
    pub fn with_metrics(mut self, metrics: SharedMetricsRegistry) -> Self {
        self.metrics = Some(metrics);
        self
    }

    /// Returns a reference to the event emitter.
    ///
    /// This is useful for testing to verify events were emitted.
    #[must_use]
    pub fn event_emitter(&self) -> &Arc<dyn LedgerEventEmitter> {
        &self.event_emitter
    }

    /// Records a successful governance probe when monitoring is wired.
    fn record_governance_probe_success(&self) {
        if let Some(ref monitor) = self.governance_freshness_monitor {
            monitor.record_success();
        }
    }

    /// Records a governance probe failure when monitoring is wired.
    ///
    /// IMPORTANT: Callers must use this only for real governance service
    /// transport/communication failures, never for local validation/state
    /// precondition errors.
    fn record_governance_probe_failure(&self) {
        if let Some(ref monitor) = self.governance_freshness_monitor {
            monitor.record_failure();
        }
    }

    /// Returns a reference to the episode runtime.
    ///
    /// This is useful for testing to verify episode state.
    #[must_use]
    pub const fn episode_runtime(&self) -> &Arc<EpisodeRuntime> {
        &self.episode_runtime
    }

    /// Returns a reference to the session registry.
    ///
    /// This is useful for testing to verify session state.
    #[must_use]
    pub fn session_registry(&self) -> &Arc<dyn SessionRegistry> {
        &self.session_registry
    }

    /// Returns a reference to the lease validator.
    ///
    /// Primarily for testing purposes to pre-register leases.
    #[must_use]
    pub fn lease_validator(&self) -> &Arc<dyn LeaseValidator> {
        &self.lease_validator
    }

    /// Returns a reference to the token minter.
    ///
    /// # TCK-00287
    ///
    /// This is used to share the token minter with `SessionDispatcher`.
    #[must_use]
    pub const fn token_minter(&self) -> &Arc<TokenMinter> {
        &self.token_minter
    }

    /// Returns a reference to the manifest store.
    ///
    /// # TCK-00287
    ///
    /// This is used to share the manifest store with `SessionDispatcher`.
    #[must_use]
    pub const fn manifest_store(&self) -> &Arc<InMemoryManifestStore> {
        &self.manifest_store
    }

    /// Returns a reference to the HTF-compliant clock (TCK-00289).
    ///
    /// This is primarily for testing to verify clock behavior.
    #[must_use]
    pub const fn holonic_clock(&self) -> &Arc<HolonicClock> {
        &self.holonic_clock
    }

    /// Returns a reference to the subscription registry (TCK-00303).
    ///
    /// # TCK-00303
    ///
    /// This is used to share the subscription registry with
    /// `SessionDispatcher`.
    #[must_use]
    pub const fn subscription_registry(&self) -> &SharedSubscriptionRegistry {
        &self.subscription_registry
    }

    // =========================================================================
    // TCK-00289: HTF-Compliant Timestamp Generation
    // =========================================================================

    /// Returns an HTF-compliant timestamp in nanoseconds since epoch.
    ///
    /// Per RFC-0016, all timestamps must come from the `HolonicClock` to
    /// ensure:
    /// - Monotonicity: Timestamps never regress within a process lifetime
    /// - Causality: HLC provides cross-node causal ordering
    /// - Determinism: Clock source is injectable for replay scenarios
    ///
    /// # Returns
    ///
    /// The current HLC wall time in nanoseconds since epoch. This is a u64
    /// value representing hybrid logical clock time, suitable for ledger
    /// event timestamps and capability grant/expiry times.
    ///
    /// # Panics
    ///
    /// This method expects the `HolonicClock` to have HLC enabled. If HLC is
    /// not enabled, this returns an error (fail-closed).
    ///
    /// # Errors
    ///
    /// Returns `HtfTimestampError` if the clock operation fails.
    ///
    /// # Security (Fail-Closed)
    ///
    /// Per RFC-0016 and TCK-00289 DOD, this method fails closed rather than
    /// returning a fallback value like 0. Returning 0 would violate security
    /// policy and allow operations to proceed with invalid timestamps.
    fn get_htf_timestamp_ns(&self) -> Result<u64, HtfTimestampError> {
        match self.holonic_clock.now_hlc() {
            Ok(hlc) => Ok(hlc.wall_ns),
            Err(e) => {
                // TCK-00289: Fail-closed - do not return 0 as fallback.
                // This is a security-critical operation that must not proceed
                // with invalid timestamps.
                warn!(error = %e, "HLC clock error - failing closed per RFC-0016");
                Err(HtfTimestampError::ClockError {
                    message: e.to_string(),
                })
            },
        }
    }

    // =========================================================================
    // TCK-00258: Custody Domain Resolution (`SoD` Enforcement)
    // =========================================================================

    /// Resolves custody domains for an actor.
    ///
    /// Per TCK-00258, this method maps an actor ID to its custody domains
    /// for `SoD` validation. In production, this would query the `KeyPolicy`
    /// via a `CustodyDomainResolver` trait.
    ///
    /// # Stub Implementation
    ///
    /// Currently returns a single domain derived from the actor ID prefix.
    /// For example, `team-alpha:alice` -> `["team-alpha"]`.
    /// This enables testing of the `SoD` enforcement without full `KeyPolicy`
    /// integration.
    ///
    /// # Security (Fail-Closed)
    ///
    /// Returns an error if the actor ID doesn't match the expected
    /// `domain:actor` schema. This ensures that malformed or non-standard
    /// actor IDs cannot bypass `SoD` validation.
    #[allow(clippy::unused_self)] // Will use self in production for registry access
    fn resolve_actor_custody_domains(&self, actor_id: &str) -> Result<Vec<String>, String> {
        // Stub: derive domain from actor_id prefix (e.g., "team-alpha:alice" ->
        // "team-alpha") In production, this would query
        // KeyPolicy.custody_domains
        if let Some(colon_pos) = actor_id.find(':') {
            let domain = &actor_id[..colon_pos];
            if !domain.is_empty() {
                return Ok(vec![domain.to_string()]);
            }
        }
        // SEC-SoD-001: Fail-closed for malformed actor IDs.
        // If the actor_id doesn't match expected schema (domain:actor), return
        // an error. This prevents attackers from bypassing SoD by using
        // non-standard IDs.
        Err(format!(
            "malformed actor_id: {actor_id} (expected domain:actor)"
        ))
    }

    /// Resolves custody domains for changeset authors.
    ///
    /// Per TCK-00258, this method retrieves the custody domains of all actors
    /// who authored the changeset being reviewed. In production, this would
    /// query the changeset metadata and `KeyPolicy`.
    ///
    /// # Stub Implementation
    ///
    /// Currently returns a placeholder domain based on the `work_id`.
    /// For testing, set `work_id` to include domain information:
    /// e.g., `W-team-alpha-12345` -> `["team-alpha"]`
    ///
    /// # Security (Fail-Closed)
    ///
    /// Returns an error if the `work_id` doesn't match the expected schema.
    /// This ensures that malformed work IDs cannot bypass `SoD` validation.
    #[allow(clippy::unused_self)] // Will use self in production for registry access
    fn resolve_changeset_author_domains(&self, work_id: &str) -> Result<Vec<String>, String> {
        // Stub: derive author domain from work_id (e.g., "W-team-alpha-12345" ->
        // ["team-alpha"]) In production, this would query changeset metadata
        // for author list, then resolve each author's custody domains via
        // KeyPolicy
        if let Some(rest) = work_id.strip_prefix("W-") {
            if let Some(dash_pos) = rest.find('-') {
                let domain = &rest[..dash_pos];
                if !domain.is_empty() {
                    return Ok(vec![domain.to_string()]);
                }
            }
        }
        // SEC-SoD-001: Fail-closed for malformed work_ids.
        // Return an error if the work_id doesn't match the expected schema.
        // This prevents attackers from bypassing SoD by using malformed work_ids
        // that don't map to author domains.
        Err(format!(
            "malformed work_id: {work_id} (expected W-domain-suffix)"
        ))
    }

    /// Dispatches a privileged request to the appropriate handler.
    ///
    /// # Message Format
    ///
    /// The frame format is: [tag: u8][payload: protobuf]
    /// Where tag identifies the message type (see [`PrivilegedMessageType`]).
    ///
    /// # Security
    ///
    /// 1. Validates `ctx.is_privileged()` FIRST
    /// 2. Returns `PERMISSION_DENIED` immediately for non-privileged
    ///    connections
    /// 3. Only then decodes and routes the message
    ///
    /// # Errors
    ///
    /// Returns `Err` for protocol-level errors (malformed frames, etc.).
    /// Application-level errors are returned in [`PrivilegedResponse::Error`].
    pub fn dispatch(
        &self,
        frame: &Bytes,
        ctx: &ConnectionContext,
    ) -> ProtocolResult<PrivilegedResponse> {
        // TCK-00349: Check session phase BEFORE any message processing.
        // No authority-bearing IPC is permitted before SessionOpen.
        if !ctx.phase().allows_dispatch() {
            warn!(
                phase = %ctx.phase(),
                "Dispatch rejected: connection not in SessionOpen phase"
            );
            return Ok(PrivilegedResponse::error(
                PrivilegedErrorCode::PermissionDenied,
                format!(
                    "dispatch rejected: connection is in {} phase, not SessionOpen",
                    ctx.phase()
                ),
            ));
        }

        // INV-0001: Check privilege BEFORE any message processing
        if !ctx.is_privileged() {
            // TH-004: Generic error prevents endpoint enumeration
            debug!(
                peer_pid = ?ctx.peer_credentials().map(|c| c.pid),
                "Non-privileged connection attempted privileged endpoint"
            );
            return Ok(PrivilegedResponse::permission_denied());
        }

        // Validate frame has at least a tag byte
        if frame.is_empty() {
            return Err(ProtocolError::Serialization {
                reason: "empty frame".to_string(),
            });
        }

        let tag = frame[0];
        let payload = &frame[1..];

        // Route based on message type
        let msg_type =
            PrivilegedMessageType::from_tag(tag).ok_or_else(|| ProtocolError::Serialization {
                reason: format!("unknown privileged message type: {tag}"),
            })?;

        let result = match msg_type {
            PrivilegedMessageType::ClaimWork => self.handle_claim_work(payload, ctx),
            PrivilegedMessageType::SpawnEpisode => self.handle_spawn_episode(payload, ctx),
            PrivilegedMessageType::IssueCapability => self.handle_issue_capability(payload, ctx),
            PrivilegedMessageType::Shutdown => self.handle_shutdown(payload, ctx),
            // Process Management (TCK-00342)
            PrivilegedMessageType::ListProcesses => self.handle_list_processes(payload),
            PrivilegedMessageType::ProcessStatus => self.handle_process_status(payload),
            PrivilegedMessageType::StartProcess => self.handle_start_process(payload),
            PrivilegedMessageType::StopProcess => self.handle_stop_process(payload),
            PrivilegedMessageType::RestartProcess => self.handle_restart_process(payload),
            PrivilegedMessageType::ReloadProcess => self.handle_reload_process(payload),
            // Consensus Query Endpoints (TCK-00345)
            PrivilegedMessageType::ConsensusStatus => self.handle_consensus_status(payload),
            PrivilegedMessageType::ConsensusValidators => self.handle_consensus_validators(payload),
            PrivilegedMessageType::ConsensusByzantineEvidence => {
                self.handle_consensus_byzantine_evidence(payload)
            },
            PrivilegedMessageType::ConsensusMetrics => self.handle_consensus_metrics(payload),
            // TCK-00344: Work status query
            PrivilegedMessageType::WorkStatus => self.handle_work_status(payload, ctx),
            // TCK-00395: EndSession for session termination with ledger event
            PrivilegedMessageType::EndSession => self.handle_end_session(payload, ctx),
            // TCK-00389: IngestReviewReceipt for external reviewer results
            PrivilegedMessageType::IngestReviewReceipt => {
                self.handle_ingest_review_receipt(payload, ctx)
            },
            // TCK-00351: Runtime stop-flag mutation
            PrivilegedMessageType::UpdateStopFlags => self.handle_update_stop_flags(payload, ctx),
            // Credential Management (CTR-PROTO-012, TCK-00343)
            PrivilegedMessageType::ListCredentials => self.handle_list_credentials(payload, ctx),
            PrivilegedMessageType::AddCredential => self.handle_add_credential(payload, ctx),
            PrivilegedMessageType::RemoveCredential => self.handle_remove_credential(payload, ctx),
            PrivilegedMessageType::RefreshCredential => {
                self.handle_refresh_credential(payload, ctx)
            },
            PrivilegedMessageType::SwitchCredential => self.handle_switch_credential(payload, ctx),
            PrivilegedMessageType::LoginCredential => self.handle_login_credential(payload, ctx),
            // HEF Pulse Plane (TCK-00302): Operator subscription handlers
            PrivilegedMessageType::SubscribePulse => self.handle_subscribe_pulse(payload, ctx),
            PrivilegedMessageType::UnsubscribePulse => self.handle_unsubscribe_pulse(payload, ctx),
            // PulseEvent is server-to-client only, reject if received from client
            PrivilegedMessageType::PulseEvent => Ok(PrivilegedResponse::error(
                PrivilegedErrorCode::PermissionDenied,
                "PulseEvent is server-to-client only",
            )),
            // TCK-00394: ChangeSet publishing for daemon-anchored submission
            PrivilegedMessageType::PublishChangeSet => self.handle_publish_changeset(payload, ctx),
            // TCK-00340: Sublease delegation for child holon authorization
            PrivilegedMessageType::DelegateSublease => self.handle_delegate_sublease(payload, ctx),
        };

        // TCK-00268: Emit IPC request completion metrics
        if let Some(ref metrics) = self.metrics {
            let endpoint = match msg_type {
                PrivilegedMessageType::ClaimWork => "ClaimWork",
                PrivilegedMessageType::SpawnEpisode => "SpawnEpisode",
                PrivilegedMessageType::IssueCapability => "IssueCapability",
                PrivilegedMessageType::Shutdown => "Shutdown",
                // Process Management (TCK-00342)
                PrivilegedMessageType::ListProcesses => "ListProcesses",
                PrivilegedMessageType::ProcessStatus => "ProcessStatus",
                PrivilegedMessageType::StartProcess => "StartProcess",
                PrivilegedMessageType::StopProcess => "StopProcess",
                PrivilegedMessageType::RestartProcess => "RestartProcess",
                PrivilegedMessageType::ReloadProcess => "ReloadProcess",
                // Consensus Query Endpoints (TCK-00345)
                PrivilegedMessageType::ConsensusStatus => "ConsensusStatus",
                PrivilegedMessageType::ConsensusValidators => "ConsensusValidators",
                PrivilegedMessageType::ConsensusByzantineEvidence => "ConsensusByzantineEvidence",
                PrivilegedMessageType::ConsensusMetrics => "ConsensusMetrics",
                // TCK-00344
                PrivilegedMessageType::WorkStatus => "WorkStatus",
                // TCK-00395
                PrivilegedMessageType::EndSession => "EndSession",
                // TCK-00389
                PrivilegedMessageType::IngestReviewReceipt => "IngestReviewReceipt",
                // TCK-00351
                PrivilegedMessageType::UpdateStopFlags => "UpdateStopFlags",
                // Credential Management (CTR-PROTO-012, TCK-00343)
                PrivilegedMessageType::ListCredentials => "ListCredentials",
                PrivilegedMessageType::AddCredential => "AddCredential",
                PrivilegedMessageType::RemoveCredential => "RemoveCredential",
                PrivilegedMessageType::RefreshCredential => "RefreshCredential",
                PrivilegedMessageType::SwitchCredential => "SwitchCredential",
                PrivilegedMessageType::LoginCredential => "LoginCredential",
                // HEF Pulse Plane (TCK-00300)
                PrivilegedMessageType::SubscribePulse => "SubscribePulse",
                PrivilegedMessageType::UnsubscribePulse => "UnsubscribePulse",
                PrivilegedMessageType::PulseEvent => "PulseEvent",
                // TCK-00394
                PrivilegedMessageType::PublishChangeSet => "PublishChangeSet",
                // TCK-00340
                PrivilegedMessageType::DelegateSublease => "DelegateSublease",
            };
            let status = match &result {
                Ok(PrivilegedResponse::Error(_)) => "error",
                Ok(_) => "success",
                Err(_) => "protocol_error",
            };
            metrics
                .daemon_metrics()
                .ipc_request_completed(endpoint, status);
        }

        result
    }

    /// Handles `ClaimWork` requests (IPC-PRIV-001).
    ///
    /// # TCK-00253 Implementation
    ///
    /// This handler implements the work claim flow per DD-001 and DD-002:
    ///
    /// 1. Validate request structure
    /// 2. Derive authoritative `actor_id` from credential (not user input)
    /// 3. Query governance for `PolicyResolvedForChangeSet`
    /// 4. Mint capability manifest based on resolved policy
    /// 5. Register work claim in registry
    /// 6. Return work assignment
    ///
    /// # Security
    ///
    /// Per DD-001: The `actor_id` in the request is a display hint only.
    /// The authoritative `actor_id` is derived from the peer credential.
    fn handle_claim_work(
        &self,
        payload: &[u8],
        ctx: &ConnectionContext,
    ) -> ProtocolResult<PrivilegedResponse> {
        let request =
            ClaimWorkRequest::decode_bounded(payload, &self.decode_config).map_err(|e| {
                ProtocolError::Serialization {
                    reason: format!("invalid ClaimWorkRequest: {e}"),
                }
            })?;

        let role = WorkRole::try_from(request.role).unwrap_or(WorkRole::Unspecified);

        info!(
            actor_id_hint = %request.actor_id,
            role = ?role,
            peer_pid = ?ctx.peer_credentials().map(|c| c.pid),
            "ClaimWork request received"
        );

        // Validate role is specified
        if role == WorkRole::Unspecified {
            return Ok(PrivilegedResponse::error(
                PrivilegedErrorCode::CapabilityRequestRejected,
                "role is required",
            ));
        }

        // TCK-00253: Derive authoritative actor_id from credential (not user input)
        // Per DD-001: "actor_id is a display hint; authoritative actor_id derived from
        // credential"
        let peer_creds = ctx
            .peer_credentials()
            .ok_or_else(|| ProtocolError::Serialization {
                reason: "peer credentials required for work claim".to_string(),
            })?;

        let actor_id = derive_actor_id(peer_creds);

        debug!(
            actor_id_hint = %request.actor_id,
            derived_actor_id = %actor_id,
            "Actor ID derived from credential"
        );

        // Generate unique work and lease IDs
        let work_id = generate_work_id();
        let lease_id = generate_lease_id();

        // TCK-00253: Query governance for policy resolution
        // Per DD-002: "Daemon calls HOLON-KERNEL-GOVERNANCE for policy resolution"
        let policy_resolution = match self
            .policy_resolver
            .resolve_for_claim(&work_id, role, &actor_id)
        {
            Ok(resolution) => resolution,
            Err(e) => {
                if matches!(&e, PolicyResolutionError::GovernanceFailed { .. }) {
                    // Governance resolver reported a governance-side failure
                    // (transport/communication/service error), so this
                    // qualifies as a governance probe failure signal.
                    self.record_governance_probe_failure();
                }
                warn!(error = %e, "Policy resolution failed");
                // Return application-level error, not protocol error
                // Policy resolution failures are logic errors, not serialization errors
                return Ok(PrivilegedResponse::error(
                    PrivilegedErrorCode::PolicyResolutionFailed,
                    format!("policy resolution failed: {e}"),
                ));
            },
        };

        // Successful policy resolution is a direct governance-path health
        // signal, so refresh the governance probe watermark.
        self.record_governance_probe_success();

        // SEC-SCP-FAC-0020: lease_id is redacted from logs to prevent capability
        // leakage
        info!(
            work_id = %work_id,
            lease_id = "[REDACTED]",
            actor_id = %actor_id,
            policy_resolved_ref = %policy_resolution.policy_resolved_ref,
            "Work claimed with policy resolution"
        );

        // TCK-00258: Extract custody domains for SoD validation
        // For now, use a stub implementation that derives domain from actor_id
        // In production, this would query the KeyPolicy via CustodyDomainResolver
        let executor_custody_domains = match self.resolve_actor_custody_domains(&actor_id) {
            Ok(domains) => domains,
            Err(e) => {
                warn!(error = %e, "Failed to resolve executor custody domains");
                return Ok(PrivilegedResponse::error(
                    PrivilegedErrorCode::CapabilityRequestRejected,
                    format!("failed to resolve executor custody domains: {e}"),
                ));
            },
        };

        let author_custody_domains = match self.resolve_changeset_author_domains(&work_id) {
            Ok(domains) => domains,
            Err(e) => {
                warn!(error = %e, "Failed to resolve author custody domains");
                return Ok(PrivilegedResponse::error(
                    PrivilegedErrorCode::CapabilityRequestRejected,
                    format!("failed to resolve author custody domains: {e}"),
                ));
            },
        };

        // TCK-00395: Clone actor_id before it is moved into WorkClaim,
        // as we need it later for emit_work_transitioned.
        let actor_id_for_transition = actor_id.clone();

        // Register the work claim
        let claim = WorkClaim {
            work_id,
            lease_id,
            actor_id,
            role,
            policy_resolution: policy_resolution.clone(),
            executor_custody_domains,
            author_custody_domains,
        };

        let claim = match self.work_registry.register_claim(claim) {
            Ok(claim) => claim,
            Err(e) => {
                warn!(error = %e, "Work registration failed");
                // Return application-level error, not protocol error
                // Registration failures are logic errors, not serialization errors
                return Ok(PrivilegedResponse::error(
                    PrivilegedErrorCode::CapabilityRequestRejected,
                    format!("work registration failed: {e}"),
                ));
            },
        };

        // TCK-00253: Emit signed WorkClaimed + WorkTransitioned events atomically.
        // Per acceptance criteria: "`WorkClaimed` event signed and persisted"
        // The events are:
        // - Signed with the daemon's signing key (Ed25519)
        // - Includes work_id, lease_id, actor_id, role, and policy_resolved_ref
        // - Persisted to the append-only ledger for audit trail
        //
        // TCK-00289: Use HTF-compliant timestamp from HolonicClock.
        // Per RFC-0016, timestamps must come from the HTF clock source to ensure
        // monotonicity and causal ordering.
        //
        // TCK-00395: Uses emit_claim_lifecycle for atomic dual-event emission.
        // Both work_claimed and work_transitioned(Open->Claimed) are emitted
        // as a single atomic operation to prevent partial state commits.
        let timestamp_ns = match self.get_htf_timestamp_ns() {
            Ok(ts) => ts,
            Err(e) => {
                // TCK-00289: Fail-closed - do not proceed without valid timestamp
                warn!(error = %e, "HTF timestamp generation failed - failing closed");
                return Ok(PrivilegedResponse::error(
                    PrivilegedErrorCode::CapabilityRequestRejected,
                    format!("HTF timestamp error: {e}"),
                ));
            },
        };
        let signed_event = match self.event_emitter.emit_claim_lifecycle(
            &claim,
            &actor_id_for_transition,
            timestamp_ns,
        ) {
            Ok(event) => event,
            Err(e) => {
                warn!(error = %e, "Claim lifecycle event emission failed");
                return Ok(PrivilegedResponse::error(
                    PrivilegedErrorCode::CapabilityRequestRejected,
                    format!("claim lifecycle event emission failed: {e}"),
                ));
            },
        };

        debug!(
            event_id = %signed_event.event_id,
            work_id = %claim.work_id,
            "Claim lifecycle events emitted (work_claimed + work_transitioned)"
        );

        // Return the work assignment
        Ok(PrivilegedResponse::ClaimWork(ClaimWorkResponse {
            work_id: claim.work_id,
            lease_id: claim.lease_id,
            capability_manifest_hash: policy_resolution.capability_manifest_hash.to_vec(),
            policy_resolved_ref: policy_resolution.policy_resolved_ref,
            context_pack_hash: policy_resolution.context_pack_hash.to_vec(),
        }))
    }

    /// Rolls back a partially-completed spawn registration.
    ///
    /// Removes the newly-registered session, cleans up its telemetry/stop
    /// conditions, and restores evicted session/telemetry/manifest/stop
    /// entries captured before failure. Optionally removes the manifest if it
    /// was registered.
    ///
    /// Returns `Some(warning)` if any rollback step failed (indicating
    /// partial failure), or `None` if rollback was clean.
    ///
    /// # TCK-00384 BLOCKER 2
    ///
    /// All rollback operations are explicitly error-checked; none are
    /// silently discarded via `let _ = ...`.
    fn rollback_spawn(
        &self,
        session_id: &str,
        evicted_sessions: &[SessionState],
        evicted_telemetry: &[(String, std::sync::Arc<crate::session::SessionTelemetry>)],
        evicted_manifests: &[(String, std::sync::Arc<crate::episode::CapabilityManifest>)],
        evicted_stop_conditions: &[(String, crate::episode::envelope::StopConditions)],
        remove_manifest: bool,
    ) -> Option<String> {
        let mut warnings: Vec<String> = Vec::new();

        // 1. Remove the newly-registered session from the registry.
        if let Err(e) = self.session_registry.remove_session(session_id) {
            warn!(
                session_id = %session_id,
                error = %e,
                "Rollback: failed to remove session from registry"
            );
            warnings.push(format!("remove_session({session_id}): {e}"));
        }

        // 2. Clean up telemetry for the new session and restore evicted telemetry
        //    entries.
        if let Some(ref store) = self.telemetry_store {
            store.remove(session_id);
            for (sid, telem) in evicted_telemetry {
                if let Err(e) = store.restore(sid, std::sync::Arc::clone(telem)) {
                    warn!(
                        session_id = %sid,
                        error = %e,
                        "Rollback: failed to restore evicted telemetry"
                    );
                    warnings.push(format!("restore_telemetry({sid}): {e}"));
                }
            }
        }

        // 3. Remove the manifest if it was registered.
        if remove_manifest {
            self.manifest_store.remove(session_id);
        }

        // 3a. TCK-00352 MAJOR 2 fix: Remove V1 manifest on rollback.
        // Without this, a failed spawn leaves a stale V1 manifest that
        // could be matched by a recycled session ID (dangling reference).
        if let Some(ref v1_store) = self.v1_manifest_store {
            v1_store.remove(session_id);
        }

        // 3a-ii. TCK-00351 v4: Remove stop conditions on rollback.
        // Prevents a stale stop conditions entry from persisting for a
        // session that was never fully spawned.
        if let Some(ref store) = self.stop_conditions_store {
            store.remove(session_id);
        }

        // 3b. Restore evicted manifests so capacity is not permanently lost.
        for (sid, manifest) in evicted_manifests {
            self.manifest_store
                .restore(sid, std::sync::Arc::clone(manifest));
        }

        // 4. Re-register evicted sessions to restore capacity.
        for evicted in evicted_sessions {
            if let Err(e) = self.session_registry.register_session(evicted.clone()) {
                warn!(
                    session_id = %evicted.session_id,
                    error = %e,
                    "Rollback: failed to re-register evicted session"
                );
                warnings.push(format!("re-register({}): {e}", evicted.session_id));
            }
        }

        // 5. Restore evicted stop conditions captured before eviction cleanup.
        // This keeps rollback atomic across session/telemetry/manifest/stop stores.
        if let Some(ref store) = self.stop_conditions_store {
            for (sid, conditions) in evicted_stop_conditions {
                if let Err(e) = store.restore(sid, conditions.clone()) {
                    warn!(
                        session_id = %sid,
                        error = %e,
                        "Rollback: failed to restore evicted stop conditions"
                    );
                    warnings.push(format!("restore_stop_conditions({sid}): {e}"));
                }
            }
        }

        if warnings.is_empty() {
            None
        } else {
            Some(warnings.join("; "))
        }
    }

    /// Unified post-start rollback: stops a running episode and then rolls
    /// back the session, telemetry, and manifest stores.
    ///
    /// This helper exists to eliminate duplication across the three post-start
    /// failure paths in `handle_spawn_episode` (`update_episode_id` failure,
    /// peer-credentials missing, and `emit_spawn_lifecycle` failure). Every
    /// post-start error path MUST call this to prevent leaking a running
    /// runtime episode.
    ///
    /// The episode is stopped with `TerminationClass::Crashed` because the
    /// failure occurred after the episode was already started — the episode
    /// never ran to completion, so `Crashed` is the correct classification.
    ///
    /// # Arguments
    ///
    /// * `episode_id_opt` — The episode that was started (if any). When `None`
    ///   (e.g. in unit-test mode), only the store rollback is performed.
    /// * `session_id` — Session ID to remove from stores.
    /// * `evicted_sessions` — Sessions evicted during registration, to be
    ///   restored.
    /// * `evicted_telemetry` — Telemetry captured from evicted sessions, to be
    ///   restored.
    /// * `evicted_manifests` — Manifests captured from evicted sessions, to be
    ///   restored.
    /// * `evicted_stop_conditions` — Stop conditions captured from evicted
    ///   sessions, to be restored.
    /// * `timestamp_ns` — HTF timestamp for the episode stop event.
    /// * `context` — Human-readable label for log messages identifying the
    ///   failure site (e.g. "peer credentials failure").
    #[allow(clippy::too_many_arguments)]
    fn rollback_spawn_with_episode_stop(
        &self,
        episode_id_opt: Option<&EpisodeId>,
        session_id: &str,
        evicted_sessions: &[SessionState],
        evicted_telemetry: &[(String, std::sync::Arc<crate::session::SessionTelemetry>)],
        evicted_manifests: &[(String, std::sync::Arc<crate::episode::CapabilityManifest>)],
        evicted_stop_conditions: &[(String, crate::episode::envelope::StopConditions)],
        timestamp_ns: u64,
        context: &str,
    ) -> Option<String> {
        // Step 1: Stop the already-started episode to prevent leak.
        if let Some(episode_id) = episode_id_opt {
            if let Ok(handle) = tokio::runtime::Handle::try_current() {
                let rt = &self.episode_runtime;
                let stop_err = tokio::task::block_in_place(|| {
                    handle.block_on(rt.stop(
                        episode_id,
                        crate::episode::TerminationClass::Crashed,
                        timestamp_ns,
                    ))
                });
                if let Err(stop_e) = stop_err {
                    warn!(
                        error = %stop_e,
                        episode_id = %episode_id,
                        context = %context,
                        "Rollback: failed to stop episode after post-start failure"
                    );
                }
            }
        }

        // Step 2: Rollback session, telemetry, and manifest stores.
        let rollback_warn = self.rollback_spawn(
            session_id,
            evicted_sessions,
            evicted_telemetry,
            evicted_manifests,
            evicted_stop_conditions,
            true,
        );
        if let Some(ref rw) = rollback_warn {
            warn!(
                rollback_errors = %rw,
                context = %context,
                "Partial rollback failure during post-start error recovery"
            );
        }
        rollback_warn
    }

    /// Resolves the adapter profile hash for `SpawnEpisode`.
    ///
    /// If `requested_hash` is provided, validates it is exactly 32 bytes and
    /// exists in CAS (fail-closed). If omitted, resolves a deterministic
    /// built-in default by `WorkRole` and stores it in CAS so that auditors
    /// reading the ledger can resolve the hash (MAJOR-1 security fix).
    ///
    /// # Policy Binding (BLOCKER security fix)
    ///
    /// When the policy resolution carries an `expected_adapter_profile_hash`,
    /// the resolved hash MUST match exactly. This prevents confused-deputy
    /// attacks where an authorized caller substitutes a different CAS-present
    /// profile to gain access to different command/env configurations.
    ///
    /// CAS existence is NOT authorization. The profile must be bound to the
    /// caller's policy scope.
    ///
    /// # Authorization Trust Chain
    ///
    /// The `requested_hash` originates from the `SpawnEpisodeRequest`, whose
    /// caller has already been authenticated and authorized by the time this
    /// method is invoked:
    ///
    /// 1. `ClaimWork` established a policy resolution for the `work_id`,
    ///    binding the caller identity and lease.
    /// 2. `handle_spawn_episode` verified the `work_id` has a valid claim (line
    ///    ~6306), the role matches the claim, and the `lease_id` matches via
    ///    constant-time comparison (line ~6346).
    /// 3. Therefore the `adapter_profile_hash` was submitted by an authorized
    ///    caller. The CAS-existence check here ensures the hash refers to a
    ///    well-formed, previously stored profile -- not that the caller is
    ///    allowed to use it (that was established upstream).
    /// 4. When `expected_adapter_profile_hash` is set in the policy resolution,
    ///    the resolved hash is validated against it using constant-time
    ///    comparison, closing the confused-deputy gap.
    fn resolve_spawn_adapter_profile_hash(
        &self,
        requested_hash: Option<&[u8]>,
        role: WorkRole,
        claim: &WorkClaim,
    ) -> Result<[u8; 32], String> {
        let resolved_hash = if let Some(raw_hash) = requested_hash {
            if raw_hash.len() != 32 {
                return Err(format!(
                    "adapter_profile_hash must be exactly 32 bytes, got {}",
                    raw_hash.len()
                ));
            }

            let mut hash = [0u8; 32];
            hash.copy_from_slice(raw_hash);

            // SECURITY: CAS-existence check. The caller was already authorized
            // via ClaimWork + lease_id verification in handle_spawn_episode
            // (see doc comment above). This check ensures the hash references
            // a valid, previously stored profile -- not that the caller is
            // allowed to use it (that was established upstream).
            let cas = self.cas.as_ref().ok_or_else(|| {
                "adapter_profile_hash validation requires CAS configuration".to_string()
            })?;
            let exists = cas
                .exists(&hash)
                .map_err(|e| format!("adapter_profile_hash CAS validation failed: {e}"))?;
            if !exists {
                return Err(format!(
                    "adapter_profile_hash not found in CAS: {}",
                    hex::encode(hash)
                ));
            }
            hash
        } else {
            self.resolve_default_adapter_profile(role)?
        };

        // SECURITY (BLOCKER fix): Policy-binding validation.
        //
        // When the policy resolution carries an expected_adapter_profile_hash,
        // the resolved hash MUST match exactly. This prevents confused-deputy
        // attacks where an authorized caller substitutes a CAS-present profile
        // hash that differs from the policy-bound one.
        //
        // Uses constant-time comparison to prevent timing side-channels that
        // could leak information about the expected hash.
        if let Some(expected_hash) = claim.policy_resolution.expected_adapter_profile_hash {
            let binding_matches = bool::from(resolved_hash.ct_eq(&expected_hash));
            if !binding_matches {
                return Err(format!(
                    "adapter_profile_hash policy binding mismatch: \
                     resolved {} but policy expects {}",
                    hex::encode(resolved_hash),
                    hex::encode(expected_hash)
                ));
            }
        } else {
            // WVR-0003: Policy-level adapter profile binding is not yet
            // populated by governance resolution. Once governance populates
            // expected_adapter_profile_hash in PolicyResolution, this branch
            // will be removed and mismatches will be hard-rejected.
            // Accepted risk: CAS-existence serves as the authorization check
            // until governance wiring is complete.
            tracing::debug!(
                work_id = %claim.work_id,
                resolved_hash = %hex::encode(resolved_hash),
                "adapter_profile_hash accepted under rollout waiver WVR-0003 \
                 (no policy binding present)"
            );
        }

        Ok(resolved_hash)
    }

    /// Resolves the role-based default adapter profile hash.
    ///
    /// Stores the default profile in CAS so that auditors reading the ledger
    /// can resolve the hash. If CAS is not configured, falls back to
    /// computing the hash without persistence.
    fn resolve_default_adapter_profile(&self, role: WorkRole) -> Result<[u8; 32], String> {
        // TODO(TCK-00397): differentiate per-role profiles post-rollout
        let profile = match role {
            WorkRole::Implementer
            | WorkRole::GateExecutor
            | WorkRole::Reviewer
            | WorkRole::Coordinator
            | WorkRole::Unspecified => builtin_profiles::claude_code_profile(),
        };
        // Store in CAS so the hash is resolvable from the ledger.
        self.cas.as_ref().map_or_else(
            || {
                profile
                    .compute_cas_hash()
                    .map_err(|e| format!("default adapter profile hash computation failed: {e}"))
            },
            |cas| {
                profile
                    .store_in_cas(cas.as_ref())
                    .map_err(|e| format!("default adapter profile CAS storage failed: {e}"))
            },
        )
    }

    /// Derives role spec attribution from daemon-controlled claim context.
    ///
    /// During rollout waiver WVR-0002, policy claims do not always carry an
    /// authoritative role spec binding yet, so this may return `None`.
    const fn derive_role_spec_hash(_claim: &WorkClaim) -> Option<[u8; 32]> {
        None
    }

    /// Builds a `HarnessConfig` from an adapter profile and spawn parameters
    /// (TCK-00399).
    ///
    /// # Security
    ///
    /// - Session token is passed via env only (WVR-0002), NEVER in argv
    /// - Security-critical env vars cannot be overridden by the profile
    /// - `workspace_root` path traversal is prevented by
    ///   `HarnessConfig::validate()`
    fn build_harness_config(
        profile: &apm2_core::fac::AgentAdapterProfileV1,
        episode_id: &str,
        workspace_root: &str,
        prompt: &str,
        model: &str,
        session_token: &secrecy::SecretString,
    ) -> Result<crate::episode::HarnessConfig, String> {
        use secrecy::ExposeSecret;

        /// Environment variable names that MUST NOT be overridden by
        /// adapter profiles (privilege escalation / library injection).
        const FORBIDDEN_ENV_KEYS: &[&str] = &[
            "PATH",
            "LD_PRELOAD",
            "LD_LIBRARY_PATH",
            "DYLD_INSERT_LIBRARIES",
            "DYLD_LIBRARY_PATH",
        ];

        /// Known template placeholder tokens. Only these are flagged as
        /// unresolved after expansion; arbitrary `{...}` strings (e.g.,
        /// JSON literals) are accepted.
        const KNOWN_PLACEHOLDERS: &[&str] = &["{workspace}", "{prompt}", "{model}", "{episode_id}"];

        // Expand args_template placeholders.
        // SECURITY: session_token MUST NEVER appear in argv (WVR-0002).
        #[allow(clippy::literal_string_with_formatting_args)]
        let expanded_args: Vec<String> = profile
            .args_template
            .iter()
            .map(|arg| {
                arg.replace("{workspace}", workspace_root)
                    .replace("{prompt}", prompt)
                    .replace("{model}", model)
            })
            .collect();

        // Defense-in-depth: verify session_token is not in any arg.
        let token_str = session_token.expose_secret();
        for (i, arg) in expanded_args.iter().enumerate() {
            if arg.contains(token_str) {
                return Err(format!(
                    "security violation: session_token found in argv[{i}] \
                     after template expansion"
                ));
            }
        }

        // Fail-closed: reject any unresolved KNOWN template placeholders in args.
        // Only flag known placeholder tokens to avoid rejecting legitimate
        // literal braces (e.g., JSON arguments like `{"key": "value"}`).
        for (i, arg) in expanded_args.iter().enumerate() {
            for placeholder in KNOWN_PLACEHOLDERS {
                if arg.contains(placeholder) {
                    return Err(format!(
                        "unresolved template placeholder {placeholder} in argv[{i}]"
                    ));
                }
            }
        }

        let mut config = crate::episode::HarnessConfig::new(&profile.command, episode_id)
            .with_args(expanded_args)
            .with_cwd(workspace_root);

        // Expand and set env vars from profile template.
        for (key, value_template) in &profile.env_template {
            if FORBIDDEN_ENV_KEYS
                .iter()
                .any(|&k| k.eq_ignore_ascii_case(key))
            {
                return Err(format!(
                    "security violation: adapter profile overrides \
                     forbidden env var '{key}'"
                ));
            }
            #[allow(clippy::literal_string_with_formatting_args)]
            let expanded_value = value_template
                .replace("{workspace}", workspace_root)
                .replace("{prompt}", prompt)
                .replace("{model}", model);
            config = config.with_env(key, expanded_value);
        }

        // WVR-0002: Pass session token via environment only.
        config = config.with_secret_env("APM2_SESSION_TOKEN", session_token.clone());

        // SECURITY: After clearenv() the child has no PATH, so non-absolute
        // commands (claude, gemini, codex, ollama) would fail at exec.
        // Inject a daemon-controlled safe default PATH.  This is NOT
        // profile-controlled (FORBIDDEN_ENV_KEYS still blocks profile
        // overrides) -- it is set by the daemon after all profile env
        // expansion.
        config = config.with_env("PATH", "/usr/local/bin:/usr/bin:/bin");

        if profile.requires_pty {
            config = config.with_pty_size(120, 40);
        }

        config
            .validate()
            .map_err(|e| format!("harness config validation failed: {e}"))?;

        Ok(config)
    }

    /// Handles `SpawnEpisode` requests (IPC-PRIV-002).
    ///
    /// # Security Contract (TCK-00257)
    ///
    /// - `GATE_EXECUTOR` role requires a valid `lease_id` that references a
    ///   `GateLeaseIssued` event in the ledger
    /// - The lease must match the `work_id` in the request
    /// - Non-`GATE_EXECUTOR` roles (`IMPLEMENTER`, `REVIEWER`) do not require
    ///   ledger lease validation (they use claim-based validation)
    ///
    /// # TCK-00256 Implementation
    ///
    /// This handler implements the episode spawn flow per DD-001 and DD-002:
    ///
    /// 1. Validate request structure
    /// 2. Query work registry for `PolicyResolvedForChangeSet`
    /// 3. Validate role matches the claimed role
    /// 4. Validate `lease_id` matches the claimed `lease_id` (SEC-SCP-FAC-0020)
    /// 5. Create episode in runtime with policy constraints
    /// 6. Persist session state for subsequent IPC calls
    /// 7. Return session credentials
    ///
    /// # Security
    ///
    /// - Per SEC-SCP-FAC-0020: `lease_id` is validated against the claim to
    ///   prevent authorization bypass. The `lease_id` is redacted from logs to
    ///   prevent capability leakage.
    /// - Per fail-closed semantics: spawn is rejected if policy resolution is
    ///   missing.
    fn handle_spawn_episode(
        &self,
        payload: &[u8],
        ctx: &ConnectionContext,
    ) -> ProtocolResult<PrivilegedResponse> {
        // TCK-00319: Maximum path length constant (declared at function start per
        // clippy)
        const MAX_PATH_LENGTH: usize = 4096;

        let request =
            SpawnEpisodeRequest::decode_bounded(payload, &self.decode_config).map_err(|e| {
                ProtocolError::Serialization {
                    reason: format!("invalid SpawnEpisodeRequest: {e}"),
                }
            })?;

        // SEC-SCP-FAC-0020: lease_id is redacted from logs to prevent capability
        // leakage
        info!(
            work_id = %request.work_id,
            role = ?WorkRole::try_from(request.role).unwrap_or(WorkRole::Unspecified),
            lease_id = "[REDACTED]",
            peer_pid = ?ctx.peer_credentials().map(|c| c.pid),
            "SpawnEpisode request received"
        );

        // Validate required fields
        if request.work_id.is_empty() {
            return Ok(PrivilegedResponse::error(
                PrivilegedErrorCode::CapabilityRequestRejected,
                "work_id is required",
            ));
        }

        // SEC-SCP-FAC-0020: Enforce maximum length on work_id to prevent DoS via OOM
        if request.work_id.len() > MAX_ID_LENGTH {
            warn!(
                work_id_len = request.work_id.len(),
                max_len = MAX_ID_LENGTH,
                "SpawnEpisode rejected: work_id exceeds maximum length"
            );
            return Ok(PrivilegedResponse::error(
                PrivilegedErrorCode::CapabilityRequestRejected,
                format!("work_id exceeds maximum length of {MAX_ID_LENGTH} bytes"),
            ));
        }

        // SEC-SCP-FAC-0020: Enforce maximum length on lease_id to prevent DoS via OOM
        if let Some(ref lease_id) = request.lease_id {
            if lease_id.len() > MAX_ID_LENGTH {
                warn!(
                    lease_id_len = lease_id.len(),
                    max_len = MAX_ID_LENGTH,
                    "SpawnEpisode rejected: lease_id exceeds maximum length"
                );
                return Ok(PrivilegedResponse::error(
                    PrivilegedErrorCode::CapabilityRequestRejected,
                    format!("lease_id exceeds maximum length of {MAX_ID_LENGTH} bytes"),
                ));
            }
        }

        // SECURITY MAJOR-1: bound escalation predicate length before
        // policy validation/persistence to prevent oversized payload DoS.
        if let Some(ref escalation_predicate) = request.escalation_predicate {
            if escalation_predicate.len() > MAX_ESCALATION_PREDICATE_LEN {
                warn!(
                    escalation_predicate_len = escalation_predicate.len(),
                    max_len = MAX_ESCALATION_PREDICATE_LEN,
                    "SpawnEpisode rejected: escalation_predicate exceeds maximum length"
                );
                return Ok(PrivilegedResponse::error(
                    PrivilegedErrorCode::CapabilityRequestRejected,
                    format!(
                        "escalation_predicate exceeds maximum length of \
                         {MAX_ESCALATION_PREDICATE_LEN} bytes"
                    ),
                ));
            }
        }

        // TCK-00319: Validate workspace_root is provided
        if request.workspace_root.is_empty() {
            return Ok(PrivilegedResponse::error(
                PrivilegedErrorCode::CapabilityRequestRejected,
                "workspace_root is required",
            ));
        }

        // TCK-00319: Validate workspace_root path length (prevent DoS via unbounded
        // paths)
        if request.workspace_root.len() > MAX_PATH_LENGTH {
            warn!(
                workspace_root_len = request.workspace_root.len(),
                max_len = MAX_PATH_LENGTH,
                "SpawnEpisode rejected: workspace_root exceeds maximum length"
            );
            return Ok(PrivilegedResponse::error(
                PrivilegedErrorCode::CapabilityRequestRejected,
                format!("workspace_root exceeds maximum length of {MAX_PATH_LENGTH} bytes"),
            ));
        }

        // TCK-00319: Validate workspace_root is an absolute path
        let workspace_path = std::path::Path::new(&request.workspace_root);
        if !workspace_path.is_absolute() {
            warn!(
                workspace_root = %request.workspace_root,
                "SpawnEpisode rejected: workspace_root must be an absolute path"
            );
            return Ok(PrivilegedResponse::error(
                PrivilegedErrorCode::CapabilityRequestRejected,
                "workspace_root must be an absolute path",
            ));
        }

        // TCK-00319: Validate workspace_root exists and is a directory
        if !workspace_path.exists() {
            warn!(
                workspace_root = %request.workspace_root,
                "SpawnEpisode rejected: workspace_root does not exist"
            );
            return Ok(PrivilegedResponse::error(
                PrivilegedErrorCode::CapabilityRequestRejected,
                format!("workspace_root does not exist: {}", request.workspace_root),
            ));
        }

        if !workspace_path.is_dir() {
            warn!(
                workspace_root = %request.workspace_root,
                "SpawnEpisode rejected: workspace_root is not a directory"
            );
            return Ok(PrivilegedResponse::error(
                PrivilegedErrorCode::CapabilityRequestRejected,
                format!(
                    "workspace_root is not a directory: {}",
                    request.workspace_root
                ),
            ));
        }

        if request.role == WorkRole::Unspecified as i32 {
            return Ok(PrivilegedResponse::error(
                PrivilegedErrorCode::CapabilityRequestRejected,
                "role is required",
            ));
        }

        // GATE_EXECUTOR requires lease_id
        if request.role == WorkRole::GateExecutor as i32 && request.lease_id.is_none() {
            return Ok(PrivilegedResponse::error(
                PrivilegedErrorCode::GateLeaseMissing,
                "lease_id is required for GATE_EXECUTOR role",
            ));
        }

        // TCK-00257: GATE_EXECUTOR requires valid lease in ledger
        // Per RFC-0017 IPC-PRIV-002, the lease must exist as a GateLeaseIssued event
        // and the work_id must match. This is a fail-closed check.
        if request.role == WorkRole::GateExecutor as i32 {
            let lease_id = request
                .lease_id
                .as_ref()
                .expect("lease_id presence checked above");

            if let Err(e) = self
                .lease_validator
                .validate_gate_lease(lease_id, &request.work_id)
            {
                warn!(
                    work_id = %request.work_id,
                    error = %e,
                    "SpawnEpisode rejected: gate lease validation failed"
                );
                return Ok(PrivilegedResponse::error(
                    PrivilegedErrorCode::GateLeaseMissing,
                    format!("gate lease validation failed: {e}"),
                ));
            }
        }

        // TCK-00256: Query work registry for PolicyResolvedForChangeSet
        // Fail-closed: spawn is only allowed if a valid policy resolution exists
        // for the work_id. This is established during ClaimWork.
        let Some(claim) = self.work_registry.get_claim(&request.work_id) else {
            // Local precondition failure (no prior ClaimWork / missing local
            // claim state); this is NOT a governance transport failure.
            warn!(
                work_id = %request.work_id,
                "SpawnEpisode rejected: policy resolution not found for work_id"
            );
            return Ok(PrivilegedResponse::error(
                PrivilegedErrorCode::PolicyResolutionMissing,
                format!(
                    "policy resolution not found for work_id={}; ClaimWork must be called first",
                    request.work_id
                ),
            ));
        };

        // TCK-00256: Validate role matches the claimed role
        // Per DD-001, the role in the spawn request should match the claimed role
        let request_role = WorkRole::try_from(request.role).unwrap_or(WorkRole::Unspecified);
        if claim.role != request_role {
            warn!(
                work_id = %request.work_id,
                claimed_role = ?claim.role,
                request_role = ?request_role,
                "SpawnEpisode rejected: role mismatch"
            );
            return Ok(PrivilegedResponse::error(
                PrivilegedErrorCode::CapabilityRequestRejected,
                format!(
                    "role mismatch: work was claimed as {:?} but spawn requested {:?}",
                    claim.role, request_role
                ),
            ));
        }

        // SEC-SCP-FAC-0020: Validate lease_id matches the claimed lease_id
        // This prevents authorization bypass where a caller provides an arbitrary
        // lease_id. All roles must provide the correct lease_id from ClaimWork.
        // NOTE: Uses constant-time comparison to prevent timing side-channel attacks
        // that could leak information about valid lease_id values.
        let provided_lease_id = request.lease_id.as_deref().unwrap_or("");
        let lease_id_matches = provided_lease_id.len() == claim.lease_id.len()
            && bool::from(
                provided_lease_id
                    .as_bytes()
                    .ct_eq(claim.lease_id.as_bytes()),
            );
        if !lease_id_matches {
            warn!(
                work_id = %request.work_id,
                "SpawnEpisode rejected: lease_id mismatch"
            );
            return Ok(PrivilegedResponse::error(
                PrivilegedErrorCode::CapabilityRequestRejected,
                "lease_id does not match the claimed lease_id",
            ));
        }

        // For GateExecutor, the lease_id is required and must match
        // (Redundant but explicit check preserved for clarity per logic)
        // NOTE: Uses constant-time comparison to prevent timing side-channel attacks.
        if request.role == WorkRole::GateExecutor as i32 {
            if let Some(ref lease_id) = request.lease_id {
                let gate_lease_matches = lease_id.len() == claim.lease_id.len()
                    && bool::from(lease_id.as_bytes().ct_eq(claim.lease_id.as_bytes()));
                if !gate_lease_matches {
                    warn!(
                        work_id = %request.work_id,
                        "SpawnEpisode rejected: GateExecutor lease_id mismatch"
                    );
                    return Ok(PrivilegedResponse::error(
                        PrivilegedErrorCode::CapabilityRequestRejected,
                        "lease_id does not match the claimed lease_id for GATE_EXECUTOR",
                    ));
                }
            }
        }

        // TCK-00351 BLOCKER 2: Validate caller-supplied stop conditions
        // against an authoritative policy floor before persisting.
        let stop_policy = StopConditionPolicy::fail_closed_default();
        let resolved_stop_conditions = match stop_policy.validate_against_floor(
            request.max_episodes,
            request.escalation_predicate.as_deref(),
        ) {
            Ok(conditions) => conditions,
            Err(e) => {
                warn!(
                    work_id = %request.work_id,
                    max_episodes = ?request.max_episodes,
                    "SpawnEpisode rejected: stop-condition policy floor violation"
                );
                return Ok(PrivilegedResponse::error(
                    PrivilegedErrorCode::CapabilityRequestRejected,
                    format!("invalid stop conditions: {e}"),
                ));
            },
        };

        // =====================================================================
        // TCK-00258: SoD Custody Domain Validation
        //
        // Per REQ-DCP-0006, GATE_EXECUTOR spawns MUST enforce Separation of
        // Duties by rejecting when executor custody domains overlap with author
        // custody domains. This prevents self-review attacks.
        // =====================================================================
        if request.role == WorkRole::GateExecutor as i32 {
            // Convert claim domains to CustodyDomainId for validation
            let executor_domains: Vec<CustodyDomainId> = claim
                .executor_custody_domains
                .iter()
                .filter_map(|d| CustodyDomainId::new(d.clone()).ok())
                .collect();

            let author_domains: Vec<CustodyDomainId> = claim
                .author_custody_domains
                .iter()
                .filter_map(|d| CustodyDomainId::new(d.clone()).ok())
                .collect();

            // TCK-00258: Fail-closed SoD validation for GATE_EXECUTOR.
            // If author domains cannot be resolved (empty), DENY the spawn.
            // The absence of author information MUST block, not allow, to prevent
            // attackers from bypassing SoD by using malformed work_ids.
            if author_domains.is_empty() {
                warn!(
                    work_id = %request.work_id,
                    "SpawnEpisode rejected: cannot resolve author custody domains for SoD validation"
                );
                return Ok(PrivilegedResponse::error(
                    PrivilegedErrorCode::SodViolation,
                    "cannot resolve author custody domains; SoD validation requires author information for GATE_EXECUTOR",
                ));
            }

            // Validate SoD: executor domains must not overlap with author domains
            if !executor_domains.is_empty() {
                if let Err(CustodyDomainError::Overlap {
                    executor_domain,
                    author_domain,
                }) = validate_custody_domain_overlap(&executor_domains, &author_domains)
                {
                    warn!(
                        work_id = %request.work_id,
                        executor_domain = %executor_domain,
                        author_domain = %author_domain,
                        "SpawnEpisode rejected: SoD custody domain violation"
                    );

                    // Emit LeaseIssueDenied event for audit logging.
                    // TCK-00289: Use HTF-compliant timestamp per RFC-0016.
                    // Fail-closed: if clock fails, we still reject the spawn (already
                    // doing that) but log at warning level instead of emitting event.
                    let timestamp_ns = match self.get_htf_timestamp_ns() {
                        Ok(ts) => ts,
                        Err(e) => {
                            warn!(error = %e, "HTF timestamp error for LeaseIssueDenied - skipping event emission");
                            0u64 // Use 0 only for best-effort event, spawn is still denied
                        },
                    };

                    // Best-effort event emission - don't fail spawn on event error.
                    // If no Tokio runtime is available (e.g., in unit tests), skip the
                    // async event emission. This is safe because:
                    // 1. The denial is still returned to the caller
                    // 2. The event is only for audit/diagnostic purposes
                    // 3. Production code always has a Tokio runtime
                    if let Ok(handle) = tokio::runtime::Handle::try_current() {
                        let _ = tokio::task::block_in_place(|| {
                            handle.block_on(async {
                                self.episode_runtime
                                    .emit_lease_issue_denied(
                                        request.work_id.clone(),
                                        LeaseIssueDenialReason::SodViolation {
                                            executor_domain: executor_domain.clone(),
                                            author_domain: author_domain.clone(),
                                        },
                                        timestamp_ns,
                                    )
                                    .await
                            })
                        });
                    }

                    return Ok(PrivilegedResponse::error(
                        PrivilegedErrorCode::SodViolation,
                        format!(
                            "custody domain overlap: executor domain '{executor_domain}' overlaps with author domain '{author_domain}'"
                        ),
                    ));
                }
            }
        }

        let adapter_profile_hash = match self.resolve_spawn_adapter_profile_hash(
            request.adapter_profile_hash.as_deref(),
            request_role,
            &claim,
        ) {
            Ok(hash) => hash,
            Err(e) => {
                warn!(
                    work_id = %request.work_id,
                    error = %e,
                    "SpawnEpisode rejected: adapter profile hash resolution failed"
                );
                return Ok(PrivilegedResponse::error(
                    PrivilegedErrorCode::CapabilityRequestRejected,
                    e,
                ));
            },
        };

        let role_spec_hash = Self::derive_role_spec_hash(&claim);

        info!(
            work_id = %request.work_id,
            policy_resolved_ref = %claim.policy_resolution.policy_resolved_ref,
            adapter_profile_hash = %hex::encode(adapter_profile_hash),
            role_spec_hash_present = role_spec_hash.is_some(),
            "SpawnEpisode authorized with policy resolution"
        );

        // Generate session ID and ephemeral handle
        let session_id = format!("S-{}", uuid::Uuid::new_v4());
        let ephemeral_handle = EphemeralHandle::generate();

        // TCK-00384 security fix: Transactional session + telemetry registration
        // with guaranteed rollback on any failure path.
        //
        // Registration order:
        //   1. Register session (may evict oldest entries for capacity).
        //   2. Clean up telemetry for any evicted sessions (policy convergence).
        //   3. Register telemetry -- on failure, rollback session.
        //   4. Mint token -- on failure, rollback both session and telemetry.
        //   5. Serialize token -- on failure, rollback both.
        //
        // Session-first ordering is necessary because the session registry
        // uses LRU eviction at capacity while the telemetry store uses
        // fail-closed rejection.  By registering the session first, eviction
        // frees capacity in the telemetry store before we attempt telemetry
        // registration.  Any failure after step 1 rolls back the session via
        // `remove_session` (added to the `SessionRegistry` trait for this
        // purpose).
        //
        // This makes the three stores (session registry, telemetry, token)
        // atomically consistent: either ALL succeed or NONE are committed.

        // Step 1: Persist session state (may evict oldest for capacity).
        // TCK-00256: The episode_runtime can create/start episodes
        // asynchronously when needed.
        let session_state = SessionState {
            session_id: session_id.clone(),
            work_id: request.work_id.clone(),
            role: request_role.into(),
            ephemeral_handle: ephemeral_handle.to_string(),
            lease_id: claim.lease_id.clone(),
            policy_resolved_ref: claim.policy_resolution.policy_resolved_ref.clone(),
            capability_manifest_hash: claim.policy_resolution.capability_manifest_hash.to_vec(),
            episode_id: None, // Will be set when episode starts in async context
        };

        let evicted_sessions = match self.session_registry.register_session(session_state) {
            Ok(evicted) => evicted,
            Err(e) => {
                warn!(error = %e, "Session registration failed");
                return Ok(PrivilegedResponse::error(
                    PrivilegedErrorCode::CapabilityRequestRejected,
                    format!("session registration failed: {e}"),
                ));
            },
        };

        // Step 2: Clean up telemetry for evicted sessions to prevent
        // orphaned entries and free capacity (policy convergence fix).
        //
        // TCK-00384 security BLOCKER 1: Use `remove_and_return` to capture
        // the evicted telemetry entries.  If a later spawn step fails, we
        // restore them alongside the session registry entries so that
        // rollback is complete (telemetry + session + manifest).
        let evicted_telemetry: Vec<(String, std::sync::Arc<crate::session::SessionTelemetry>)> =
            self.telemetry_store
                .as_ref()
                .map_or_else(Vec::new, |store| {
                    evicted_sessions
                        .iter()
                        .filter_map(|s| {
                            store
                                .remove_and_return(&s.session_id)
                                .map(|t| (s.session_id.clone(), t))
                        })
                        .collect()
                });

        // Step 2b: Clean up manifest entries for evicted sessions to prevent
        // unbounded manifest store growth.  Captured via `remove_and_return`
        // so they can be restored during rollback.
        let evicted_manifests: Vec<(String, std::sync::Arc<crate::episode::CapabilityManifest>)> =
            evicted_sessions
                .iter()
                .filter_map(|s| {
                    self.manifest_store
                        .remove_and_return(&s.session_id)
                        .map(|m| (s.session_id.clone(), m))
                })
                .collect();

        // Step 2c: Capture and remove stop conditions for evicted sessions.
        // These are restored if a later spawn step fails (atomic rollback).
        let evicted_stop_conditions: Vec<(String, crate::episode::envelope::StopConditions)> = self
            .stop_conditions_store
            .as_ref()
            .map_or_else(Vec::new, |store| {
                evicted_sessions
                    .iter()
                    .filter_map(|s| {
                        store
                            .remove_and_return(&s.session_id)
                            .map(|c| (s.session_id.clone(), c))
                    })
                    .collect()
            });

        // Step 3: Register telemetry with started_at_ns.
        // The wall-clock timestamp is stored as audit metadata only;
        // elapsed duration is computed from a monotonic Instant inside the
        // store (security review: no wall-clock dependency for duration_ms).
        if let Some(ref store) = self.telemetry_store {
            let started_at_ns = SystemTime::now()
                .duration_since(SystemTime::UNIX_EPOCH)
                .map(|d| {
                    #[allow(clippy::cast_possible_truncation)]
                    let ns = d.as_nanos() as u64;
                    ns
                })
                .unwrap_or(0);
            if let Err(e) = store.register(&session_id, started_at_ns) {
                // Rollback session on telemetry failure and restore evicted
                // sessions + telemetry so capacity is not permanently lost.
                let rollback_warn = self.rollback_spawn(
                    &session_id,
                    &evicted_sessions,
                    &evicted_telemetry,
                    &evicted_manifests,
                    &evicted_stop_conditions,
                    false,
                );
                if let Some(ref rw) = rollback_warn {
                    warn!(rollback_errors = %rw, "Partial rollback failure during telemetry error recovery");
                }
                warn!(error = %e, "Telemetry registration rejected (store at capacity)");
                let msg = rollback_warn.map_or_else(
                    || format!("telemetry store at capacity: {e}"),
                    |rw| {
                        format!("telemetry store at capacity: {e} (rollback partial failure: {rw})")
                    },
                );
                return Ok(PrivilegedResponse::error(
                    PrivilegedErrorCode::CapabilityRequestRejected,
                    msg,
                ));
            }
        }

        // Step 3b: Register stop conditions for the session (TCK-00351 v4).
        // The pre-actuation gate reads from this store to enforce
        // max_episodes / escalation_predicate limits.
        //
        // TCK-00351 BLOCKER 1+2 FIX: Persist policy-validated stop
        // conditions derived from authoritative request fields.
        if let Some(ref store) = self.stop_conditions_store {
            if let Err(e) = store.register(&session_id, resolved_stop_conditions) {
                // Rollback session, telemetry, and restore evicted entries.
                let rollback_warn = self.rollback_spawn(
                    &session_id,
                    &evicted_sessions,
                    &evicted_telemetry,
                    &evicted_manifests,
                    &evicted_stop_conditions,
                    false,
                );
                if let Some(ref rw) = rollback_warn {
                    warn!(rollback_errors = %rw, "Partial rollback failure during stop conditions error recovery");
                }
                warn!(error = %e, "Stop conditions registration rejected (store at capacity)");
                let msg = rollback_warn.map_or_else(
                    || format!("stop conditions store at capacity: {e}"),
                    |rw| {
                        format!("stop conditions store at capacity: {e} (rollback partial failure: {rw})")
                    },
                );
                return Ok(PrivilegedResponse::error(
                    PrivilegedErrorCode::CapabilityRequestRejected,
                    msg,
                ));
            }
        }

        debug!(
            session_id = %session_id,
            ephemeral_handle = %ephemeral_handle,
            "Session persisted"
        );

        // Step 4: Generate session token for client authentication.
        // TCK-00287 BLOCKER 2: The token is HMAC-signed and bound to this
        // session's lease_id.
        //
        // NOTE: TokenMinter uses SystemTime for TTL calculation, which is
        // acceptable since token expiry is not a protocol-authoritative event.
        // The HTF clock is used for ledger events that require causal ordering.
        let spawn_time = SystemTime::now();
        let ttl = Duration::from_secs(DEFAULT_SESSION_TOKEN_TTL_SECS);
        let session_token = match self.token_minter.mint(
            &session_id,
            &claim.lease_id,
            spawn_time,
            ttl,
        ) {
            Ok(token) => token,
            Err(e) => {
                // Rollback session, telemetry, and restore evicted
                // sessions + telemetry so capacity is not lost.
                let rollback_warn = self.rollback_spawn(
                    &session_id,
                    &evicted_sessions,
                    &evicted_telemetry,
                    &evicted_manifests,
                    &evicted_stop_conditions,
                    false,
                );
                if let Some(ref rw) = rollback_warn {
                    warn!(rollback_errors = %rw, "Partial rollback failure during token minting error recovery");
                }
                warn!(error = %e, "Session token minting failed");
                let msg = rollback_warn.map_or_else(
                    || format!("session token generation failed: {e}"),
                    |rw| {
                        format!(
                            "session token generation failed: {e} (rollback partial failure: {rw})"
                        )
                    },
                );
                return Ok(PrivilegedResponse::error(
                    PrivilegedErrorCode::CapabilityRequestRejected,
                    msg,
                ));
            },
        };

        // Step 5: Serialize the token to JSON for inclusion in the response.
        let session_token_json = match serde_json::to_string(&session_token) {
            Ok(json) => json,
            Err(e) => {
                // Rollback session, telemetry, and restore evicted
                // sessions + telemetry so capacity is not lost.
                let rollback_warn = self.rollback_spawn(
                    &session_id,
                    &evicted_sessions,
                    &evicted_telemetry,
                    &evicted_manifests,
                    &evicted_stop_conditions,
                    false,
                );
                if let Some(ref rw) = rollback_warn {
                    warn!(rollback_errors = %rw, "Partial rollback failure during token serialization error recovery");
                }
                warn!(error = %e, "Session token serialization failed");
                let msg = rollback_warn.map_or_else(
                    || format!("session token serialization failed: {e}"),
                    |rw| format!("session token serialization failed: {e} (rollback partial failure: {rw})"),
                );
                return Ok(PrivilegedResponse::error(
                    PrivilegedErrorCode::CapabilityRequestRejected,
                    msg,
                ));
            },
        };

        // TCK-00317: Load capability manifest from CAS using hash from
        // PolicyResolution.
        //
        // Per DOD item 1 (CAS Storage & Hash Loading):
        // - Manifests are stored in CAS and referenced by hash
        // - SpawnEpisode loads the manifest using the hash from PolicyResolution
        // - For Reviewer role: Missing manifests result in fail-closed rejection
        // - For other roles: Fall back to minimal manifest (until their manifests are
        //   defined in CAS)
        //
        // Per DOD item 2 (Policy Resolution Bypass fix):
        // - The manifest is NOT selected by role name; it's loaded by hash
        // - StubPolicyResolver returns reviewer_v0_manifest_hash() for Reviewer
        // - This ensures the policy resolution controls which manifest is used
        let manifest_hash: [u8; 32] = claim.policy_resolution.capability_manifest_hash;

        let manifest = match self.manifest_loader.load_manifest(&manifest_hash) {
            Ok(m) => m,
            Err(e) => {
                // TCK-00317: For Reviewer role, fail-closed if manifest not found
                // For other roles, fall back to minimal manifest until their
                // manifests are stored in CAS.
                if request_role == WorkRole::Reviewer {
                    // TCK-00384 security fix: rollback session + telemetry on
                    // manifest-load failure so bounded capacity is not leaked.
                    // Restore evicted sessions + telemetry so capacity is not
                    // lost.
                    let rollback_warn = self.rollback_spawn(
                        &session_id,
                        &evicted_sessions,
                        &evicted_telemetry,
                        &evicted_manifests,
                        &evicted_stop_conditions,
                        false,
                    );
                    if let Some(ref rw) = rollback_warn {
                        warn!(rollback_errors = %rw, "Partial rollback failure during manifest load error recovery");
                    }
                    warn!(
                        work_id = %request.work_id,
                        manifest_hash = %hex::encode(manifest_hash),
                        error = %e,
                        "SpawnEpisode rejected: reviewer manifest not found in CAS"
                    );
                    let msg = rollback_warn.map_or_else(
                        || format!("reviewer capability manifest not found in CAS: {e}"),
                        |rw| format!("reviewer capability manifest not found in CAS: {e} (rollback partial failure: {rw})"),
                    );
                    return Ok(PrivilegedResponse::error(
                        PrivilegedErrorCode::CapabilityRequestRejected,
                        msg,
                    ));
                }

                // Non-Reviewer roles: fall back to minimal manifest
                // This maintains backward compatibility until all role manifests
                // are defined and stored in CAS.
                debug!(
                    work_id = %request.work_id,
                    role = ?request_role,
                    manifest_hash = %hex::encode(manifest_hash),
                    "Manifest not in CAS, using minimal fallback manifest for non-reviewer role"
                );
                CapabilityManifest::from_hash_with_default_allowlist(&manifest_hash)
            },
        };

        self.manifest_store.register(&session_id, manifest.clone());

        // TCK-00352 Security Review MAJOR 2: Mint and register V1 manifest
        // in the shared store so that SessionDispatcher can enforce V1 scope
        // checks (risk tier ceiling, host restrictions, envelope binding)
        // during handle_request_tool.
        if let Some(ref v1_store) = self.v1_manifest_store {
            // TCK-00352 MAJOR 1 v3 fix: The scope baseline MUST come from
            // an authoritative policy source (claim.policy_resolution), NOT
            // from the candidate manifest. Building the baseline from the
            // manifest itself makes the subset check tautological (the
            // manifest is always a subset of itself). Fail closed when no
            // independent baseline is available.
            let scope_baseline = if let Some(ref baseline) =
                claim.policy_resolution.resolved_scope_baseline
            {
                baseline.clone()
            } else {
                // No independent scope baseline available -- fail closed.
                let rollback_warn = self.rollback_spawn(
                    &session_id,
                    &evicted_sessions,
                    &evicted_telemetry,
                    &evicted_manifests,
                    &evicted_stop_conditions,
                    true,
                );
                if let Some(ref rw) = rollback_warn {
                    warn!(
                        rollback_errors = %rw,
                        "Partial rollback failure during scope baseline check"
                    );
                }
                warn!(
                    session_id = %session_id,
                    "SpawnEpisode rejected: no policy-resolved scope baseline (fail-closed)"
                );
                let msg = rollback_warn.map_or_else(
                        || "no policy-resolved scope baseline available; V1 minting denied (fail-closed)".to_string(),
                        |rw| format!("no policy-resolved scope baseline; V1 denied (rollback partial: {rw})"),
                    );
                return Ok(PrivilegedResponse::error(
                    PrivilegedErrorCode::CapabilityRequestRejected,
                    msg,
                ));
            };
            if let Err(e) = crate::episode::capability::validate_manifest_scope_subset(
                &manifest,
                &scope_baseline,
            ) {
                let rollback_warn = self.rollback_spawn(
                    &session_id,
                    &evicted_sessions,
                    &evicted_telemetry,
                    &evicted_manifests,
                    &evicted_stop_conditions,
                    true,
                );
                if let Some(ref rw) = rollback_warn {
                    warn!(
                        rollback_errors = %rw,
                        "Partial rollback failure during scope subset validation"
                    );
                }
                warn!(
                    session_id = %session_id,
                    error = %e,
                    "SpawnEpisode rejected: manifest scope exceeds policy baseline (fail-closed)"
                );
                let msg = rollback_warn.map_or_else(
                    || format!("manifest scope validation failed: {e}"),
                    |rw| {
                        format!(
                            "manifest scope validation failed: {e} (rollback partial failure: {rw})"
                        )
                    },
                );
                return Ok(PrivilegedResponse::error(
                    PrivilegedErrorCode::CapabilityRequestRejected,
                    msg,
                ));
            }

            // TCK-00352 MAJOR 3 fix: Obtain mint token through the
            // governance-resolver authority path, not direct construction.
            // This centralizes minting authority to the policy resolver and
            // ensures the token is obtained through the approved channel.
            let governance = crate::governance::GovernancePolicyResolver::new();
            let mint_token = governance.mint_token();

            // BLOCKER 2 v3 fix: Derive risk tier ceiling from the
            // policy-resolved risk tier (claim.policy_resolution), NOT
            // from the manifest's own capabilities. Using the manifest's
            // max_capability_tier() makes the check tautological because
            // the very manifest being validated determines its own ceiling.
            //
            // SECURITY: Fail closed (Tier4, most restrictive) when the
            // resolved_risk_tier cannot be parsed to a valid RiskTier.
            let risk_tier_ceiling = crate::episode::envelope::RiskTier::from_u8(
                claim.policy_resolution.resolved_risk_tier,
            )
            .unwrap_or(crate::episode::envelope::RiskTier::Tier4);

            // TCK-00352 BLOCKER 2 fix: V1 minting failure is now a HARD
            // DENY (fail-closed). When V1 controls are configured (v1_store
            // is Some), every session MUST have an active V1 manifest.
            // Continuing without one would leave the session without V1
            // scope enforcement, which is fail-open.
            match crate::episode::CapabilityManifestV1::mint(
                mint_token,
                manifest.clone(),
                risk_tier_ceiling,
                Vec::new(), // Host restrictions from policy (empty = fail-closed for network)
            ) {
                Ok(v1_manifest) => {
                    v1_store.register(&session_id, v1_manifest);
                    debug!(
                        session_id = %session_id,
                        risk_tier_ceiling = ?risk_tier_ceiling,
                        "V1 capability manifest minted and registered"
                    );
                },
                Err(e) => {
                    // HARD DENY: Roll back all state and reject the spawn.
                    let rollback_warn = self.rollback_spawn(
                        &session_id,
                        &evicted_sessions,
                        &evicted_telemetry,
                        &evicted_manifests,
                        &evicted_stop_conditions,
                        true,
                    );
                    if let Some(ref rw) = rollback_warn {
                        warn!(
                            rollback_errors = %rw,
                            "Partial rollback failure during V1 mint failure recovery"
                        );
                    }
                    warn!(
                        session_id = %session_id,
                        error = %e,
                        "SpawnEpisode rejected: V1 manifest minting failed (fail-closed)"
                    );
                    let msg = rollback_warn.map_or_else(
                        || format!("V1 manifest minting failed: {e}"),
                        |rw| {
                            format!(
                                "V1 manifest minting failed: {e} (rollback partial failure: {rw})"
                            )
                        },
                    );
                    return Ok(PrivilegedResponse::error(
                        PrivilegedErrorCode::CapabilityRequestRejected,
                        msg,
                    ));
                },
            }
        }

        debug!(
            session_id = %session_id,
            role = ?request_role,
            manifest_id = %manifest.manifest_id,
            tool_allowlist_len = manifest.tool_allowlist.len(),
            "Capability manifest registered in shared store"
        );

        // TCK-00268: Emit session_spawned metric
        if let Some(ref metrics) = self.metrics {
            let role_str = match request_role {
                WorkRole::Implementer => "implementer",
                WorkRole::Reviewer => "reviewer",
                WorkRole::GateExecutor => "gate_executor",
                WorkRole::Coordinator => "coordinator",
                WorkRole::Unspecified => "unspecified",
            };
            metrics.daemon_metrics().session_spawned(role_str);
        }

        // TCK-00289: Emit SessionStarted ledger event for audit trail.
        // Per DOD: "ClaimWork/SpawnEpisode persist state and emit ledger events"
        let timestamp_ns = match self.get_htf_timestamp_ns() {
            Ok(ts) => ts,
            Err(e) => {
                // TCK-00384 security fix: rollback session, telemetry, and
                // manifest on timestamp failure.  Also restore evicted
                // sessions + telemetry so capacity is not permanently lost.
                let rollback_warn = self.rollback_spawn(
                    &session_id,
                    &evicted_sessions,
                    &evicted_telemetry,
                    &evicted_manifests,
                    &evicted_stop_conditions,
                    true,
                );
                if let Some(ref rw) = rollback_warn {
                    warn!(rollback_errors = %rw, "Partial rollback failure during timestamp error recovery");
                }
                // TCK-00289: Fail-closed - do not proceed without valid timestamp
                warn!(error = %e, "HTF timestamp generation failed for SessionStarted - failing closed");
                let msg = rollback_warn.map_or_else(
                    || format!("HTF timestamp error: {e}"),
                    |rw| format!("HTF timestamp error: {e} (rollback partial failure: {rw})"),
                );
                return Ok(PrivilegedResponse::error(
                    PrivilegedErrorCode::CapabilityRequestRejected,
                    msg,
                ));
            },
        };

        // TCK-00319: Create and start episode with workspace root.
        // This ensures that all file/execute operations are confined to the
        // workspace directory. The episode must be started BEFORE returning
        // to the client so that tool handlers are properly initialized.
        //
        // Generate envelope hash from session_id + work_id + lease_id for uniqueness.
        // Null-byte delimiters prevent collision between fields (MAJOR-2 security fix).
        // adapter_profile_hash and role_spec_hash are included in the envelope
        // hash to ensure the envelope identity reflects the adapter binding.
        // TODO(TCK-00397): migrate to EpisodeEnvelopeBuilder for structured
        // envelope construction once the full EpisodeEnvelope lifecycle is wired.
        let role_spec_hash_component =
            role_spec_hash.map_or_else(|| "WVR-0002".to_string(), hex::encode);
        let envelope_data = format!(
            "{}\0{}\0{}\0{}\0{}",
            session_id,
            request.work_id,
            claim.lease_id,
            hex::encode(adapter_profile_hash),
            role_spec_hash_component
        );
        let envelope_hash: [u8; 32] = blake3::hash(envelope_data.as_bytes()).into();

        // Try to create and start the episode. This requires a Tokio runtime.
        // In unit tests without a runtime, we skip episode creation but still
        // return a valid session (for backward compatibility with existing tests).
        let episode_id_opt = if let Ok(handle) = tokio::runtime::Handle::try_current() {
            match tokio::task::block_in_place(|| {
                handle.block_on(async {
                    // Create the episode with envelope hash and timestamp
                    let episode_id = self
                        .episode_runtime
                        .create(envelope_hash, timestamp_ns)
                        .await?;

                    // Start with workspace - this initializes rooted handlers
                    let _session_handle = self
                        .episode_runtime
                        .start_with_workspace(
                            &episode_id,
                            &claim.lease_id,
                            timestamp_ns,
                            workspace_path,
                        )
                        .await?;

                    Ok::<_, crate::episode::EpisodeError>(episode_id)
                })
            }) {
                Ok(id) => Some(id),
                Err(e) => {
                    // TCK-00384 security fix: rollback session, telemetry,
                    // and manifest on episode creation failure.  Restore
                    // evicted sessions + telemetry so capacity is not lost.
                    let rollback_warn = self.rollback_spawn(
                        &session_id,
                        &evicted_sessions,
                        &evicted_telemetry,
                        &evicted_manifests,
                        &evicted_stop_conditions,
                        true,
                    );
                    if let Some(ref rw) = rollback_warn {
                        warn!(rollback_errors = %rw, "Partial rollback failure during episode creation error recovery");
                    }
                    warn!(
                        work_id = %request.work_id,
                        error = %e,
                        "SpawnEpisode failed: episode creation/start failed"
                    );
                    let msg = rollback_warn.map_or_else(
                        || format!("episode creation failed: {e}"),
                        |rw| {
                            format!("episode creation failed: {e} (rollback partial failure: {rw})")
                        },
                    );
                    return Ok(PrivilegedResponse::error(
                        PrivilegedErrorCode::CapabilityRequestRejected,
                        msg,
                    ));
                },
            }
        } else {
            // No Tokio runtime available (e.g., in sync unit tests).
            // In production, this should never happen.
            // For testing backward compatibility, we allow session creation
            // without episode creation.
            #[cfg(test)]
            {
                debug!(
                    session_id = %session_id,
                    "No Tokio runtime - skipping episode creation (test mode)"
                );
                None
            }
            #[cfg(not(test))]
            {
                // TCK-00384 security fix: rollback session, telemetry,
                // and manifest when no runtime is available.  Restore
                // evicted sessions + telemetry so capacity is not lost.
                let rollback_warn = self.rollback_spawn(
                    &session_id,
                    &evicted_sessions,
                    &evicted_telemetry,
                    &evicted_manifests,
                    &evicted_stop_conditions,
                    true,
                );
                if let Some(ref rw) = rollback_warn {
                    warn!(rollback_errors = %rw, "Partial rollback failure during no-runtime error recovery");
                }
                warn!("No Tokio runtime available for episode creation");
                let msg = rollback_warn.map_or_else(
                    || "episode creation failed: no async runtime available".to_string(),
                    |rw| format!("episode creation failed: no async runtime available (rollback partial failure: {rw})"),
                );
                return Ok(PrivilegedResponse::error(
                    PrivilegedErrorCode::CapabilityRequestRejected,
                    msg,
                ));
            }
        };

        // TCK-00395 Security BLOCKER 1: Write the episode_id back to the
        // session in the registry. Without this write-back, EndSession cannot
        // resolve the episode binding and will skip runtime stop, allowing
        // the episode to continue running after session termination.
        if let Some(ref episode_id) = episode_id_opt {
            if let Err(e) = self
                .session_registry
                .update_episode_id(&session_id, episode_id.to_string())
            {
                warn!(
                    error = %e,
                    session_id = %session_id,
                    episode_id = %episode_id,
                    "Failed to write episode_id back to session registry - failing closed"
                );

                // TCK-00384 review fix: unified post-start rollback stops
                // the episode and cleans up session/telemetry/manifest.
                let rollback_warn = self.rollback_spawn_with_episode_stop(
                    episode_id_opt.as_ref(),
                    &session_id,
                    &evicted_sessions,
                    &evicted_telemetry,
                    &evicted_manifests,
                    &evicted_stop_conditions,
                    timestamp_ns,
                    "update_episode_id failure",
                );
                let msg = rollback_warn.map_or_else(
                    || format!("session episode_id update failed: {e}"),
                    |rw| {
                        format!(
                            "session episode_id update failed: {e} (rollback partial failure: {rw})"
                        )
                    },
                );
                return Ok(PrivilegedResponse::error(
                    PrivilegedErrorCode::CapabilityRequestRejected,
                    msg,
                ));
            }
            debug!(
                session_id = %session_id,
                episode_id = %episode_id,
                workspace_root = %request.workspace_root,
                "Episode created, started, and bound to session"
            );
        }

        // TCK-00395 Security MAJOR 2: Fail closed when peer credentials are
        // missing. Same pattern as ClaimWork. SpawnEpisode emits authoritative
        // ledger events (SessionStarted + WorkTransitioned), and recording
        // "unknown" as the actor identity would break the accountability chain.
        //
        // SECURITY: Validate peer credentials BEFORE any side-effectful
        // operations (adapter process spawn). Unauthorized requests must be
        // rejected before triggering subprocess spawn.
        //
        // TCK-00384 review fix: Stop the running episode before returning on
        // this failure path.  The original `?` operator would exit without
        // stopping the episode, leaking a running runtime episode.
        let Some(peer_creds) = ctx.peer_credentials() else {
            // TCK-00384 review fix: unified post-start rollback stops the
            // episode and cleans up session/telemetry/manifest.
            let rollback_warn = self.rollback_spawn_with_episode_stop(
                episode_id_opt.as_ref(),
                &session_id,
                &evicted_sessions,
                &evicted_telemetry,
                &evicted_manifests,
                &evicted_stop_conditions,
                timestamp_ns,
                "peer credentials failure",
            );
            let msg = rollback_warn.map_or_else(
                || "peer credentials required for episode spawn".to_string(),
                |rw| format!("peer credentials required for episode spawn (rollback partial failure: {rw})"),
            );
            return Ok(PrivilegedResponse::error(
                PrivilegedErrorCode::CapabilityRequestRejected,
                msg,
            ));
        };
        let actor_id = derive_actor_id(peer_creds);

        // TCK-00399: Spawn agent CLI process via adapter registry.
        //
        // After the episode is created and Running, load the adapter profile
        // from CAS, build a HarnessConfig with template expansion, and spawn
        // the agent process. Fail-closed: if the adapter registry is not
        // configured, SpawnEpisode must return an error -- a "successful"
        // response without a spawned agent process is a silent failure.
        if let Some(episode_id) = &episode_id_opt {
            let Some(registry) = &self.adapter_registry else {
                error!(
                    episode_id = %episode_id,
                    "adapter registry not configured; cannot spawn adapter process"
                );
                let rollback_warn = self.rollback_spawn_with_episode_stop(
                    episode_id_opt.as_ref(),
                    &session_id,
                    &evicted_sessions,
                    &evicted_telemetry,
                    &evicted_manifests,
                    &evicted_stop_conditions,
                    timestamp_ns,
                    "missing adapter registry",
                );
                let msg = rollback_warn.map_or_else(
                    || "adapter registry not configured: cannot spawn adapter process".to_string(),
                    |rw| {
                        format!(
                            "adapter registry not configured: cannot spawn adapter process \
                         (rollback partial failure: {rw})"
                        )
                    },
                );
                return Ok(PrivilegedResponse::error(
                    PrivilegedErrorCode::CapabilityRequestRejected,
                    msg,
                ));
            };
            let spawn_result: Result<(), String> = (|| {
                let cas = self
                    .cas
                    .as_ref()
                    .ok_or_else(|| "adapter spawn requires CAS configuration".to_string())?;

                // SECURITY: adapter_profile_hash was resolved by
                // resolve_spawn_adapter_profile_hash which documents the
                // authorization trust chain: the caller was authenticated via
                // ClaimWork + lease_id before the hash was accepted. Loading
                // from CAS here is safe because only authorized callers can
                // reach this point.
                let profile = apm2_core::fac::AgentAdapterProfileV1::load_from_cas(
                    cas.as_ref(),
                    &adapter_profile_hash,
                )
                .map_err(|e| format!("adapter profile load failed: {e}"))?;

                // SECURITY: Fail-closed adapter mode mapping.  Unknown or
                // unsupported modes MUST be denied, not silently downgraded
                // to Raw (MAJOR: fail-open fix).
                let adapter_type = match profile.adapter_mode {
                    apm2_core::fac::AdapterMode::StructuredOutput => {
                        crate::episode::AdapterType::ClaudeCode
                    },
                    apm2_core::fac::AdapterMode::BlackBox => crate::episode::AdapterType::Raw,
                    unsupported => {
                        return Err(format!(
                            "unsupported adapter mode '{unsupported}': \
                             only BlackBox and StructuredOutput are supported"
                        ));
                    },
                };

                let adapter = registry.get(adapter_type).ok_or_else(|| {
                    format!("adapter type {adapter_type} not registered in registry")
                })?;

                // Build HarnessConfig. Prompt is empty -- actual prompt
                // delivery is via stdin/PTY, not in argv.
                let session_token_secret = secrecy::SecretString::from(session_token_json.clone());
                let config = Self::build_harness_config(
                    &profile,
                    episode_id.as_str(),
                    &request.workspace_root,
                    "",
                    &profile.profile_id,
                    &session_token_secret,
                )?;

                let rt_handle = tokio::runtime::Handle::try_current()
                    .map_err(|_| "adapter spawn requires async runtime".to_string())?;
                tokio::task::block_in_place(|| {
                    rt_handle.block_on(async {
                        self.episode_runtime
                            .spawn_adapter(episode_id, config, adapter)
                            .await
                            .map_err(|e| format!("adapter spawn failed: {e}"))
                    })
                })?;

                Ok(())
            })();

            if let Err(e) = spawn_result {
                // MAJOR fix: Fail-closed on spawn errors.  A successful
                // SpawnEpisode response with no agent process is a silent
                // failure.  Roll back the episode and return an error.
                error!(
                    episode_id = %episode_id,
                    error = %e,
                    "adapter process spawn failed; rolling back episode"
                );
                let rollback_warn = self.rollback_spawn_with_episode_stop(
                    Some(episode_id),
                    &session_id,
                    &evicted_sessions,
                    &evicted_telemetry,
                    &evicted_manifests,
                    &evicted_stop_conditions,
                    timestamp_ns,
                    "adapter spawn failure",
                );
                let msg = rollback_warn.map_or_else(
                    || format!("adapter spawn failed: {e}"),
                    |rw| format!("adapter spawn failed: {e} (rollback partial failure: {rw})"),
                );
                return Ok(PrivilegedResponse::error(
                    PrivilegedErrorCode::CapabilityRequestRejected,
                    msg,
                ));
            }
        }

        // TCK-00358: Resolve identity proof profile hash for SessionStarted.
        // Primary path: the session-open handler in main.rs sets the profile
        // hash on the ConnectionContext during identity materialization.
        // Defensive fallback: if no context-bound value is set (e.g. direct
        // test usage without session-open wiring), fall back to the baseline
        // SMT-256 10^12 profile hash and log a warning. This ensures audit
        // trail always includes the field per REQ-0012 while alerting
        // operators to a missing production wiring.
        let identity_proof_profile_hash: Option<[u8; 32]> =
            ctx.identity_proof_profile_hash().copied().map_or_else(
                || {
                    warn!(
                        "identity_proof_profile_hash not set on ConnectionContext at spawn time; \
                         falling back to baseline profile hash (session-open wiring may be missing)"
                    );
                    crate::identity::IdentityProofProfileV1::baseline_smt_10e12()
                        .content_hash()
                        .ok()
                },
                Some,
            );

        // TCK-00395: Emit SessionStarted + WorkTransitioned(Claimed->InProgress)
        // atomically via emit_spawn_lifecycle. Both events are persisted as a
        // single atomic operation to prevent partial state commits.
        // TCK-00348: Thread contract binding into SessionStarted.
        if let Err(e) = self.event_emitter.emit_spawn_lifecycle(
            &session_id,
            &request.work_id,
            &claim.lease_id,
            &actor_id,
            &adapter_profile_hash,
            role_spec_hash.as_ref(),
            timestamp_ns,
            ctx.contract_binding(),
            identity_proof_profile_hash.as_ref(),
        ) {
            // TCK-00384 review fix: unified post-start rollback stops the
            // episode and cleans up session/telemetry/manifest.
            let rollback_warn = self.rollback_spawn_with_episode_stop(
                episode_id_opt.as_ref(),
                &session_id,
                &evicted_sessions,
                &evicted_telemetry,
                &evicted_manifests,
                &evicted_stop_conditions,
                timestamp_ns,
                "event emission failure",
            );
            warn!(error = %e, "SessionStarted event emission failed");
            let msg = rollback_warn.map_or_else(
                || format!("event emission failed: {e}"),
                |rw| format!("event emission failed: {e} (rollback partial failure: {rw})"),
            );
            return Ok(PrivilegedResponse::error(
                PrivilegedErrorCode::CapabilityRequestRejected,
                msg,
            ));
        }

        debug!(
            session_id = %session_id,
            work_id = %request.work_id,
            "Spawn lifecycle events emitted (session_started + work_transitioned)"
        );

        // Successful spawn demonstrates governance-backed policy state is
        // accessible; record fresh governance health.
        self.record_governance_probe_success();

        Ok(PrivilegedResponse::SpawnEpisode(SpawnEpisodeResponse {
            session_id,
            ephemeral_handle: ephemeral_handle.to_string(),
            capability_manifest_hash: claim.policy_resolution.capability_manifest_hash.to_vec(),
            context_pack_sealed: true,
            session_token: session_token_json,
        }))
    }

    /// Handles `IssueCapability` requests (IPC-PRIV-003).
    ///
    /// # TCK-00289 Implementation
    ///
    /// This handler implements capability issuance with:
    /// 1. Session validation (must exist)
    /// 2. Lease validation (session's lease must be valid for its work)
    /// 3. HTF-compliant timestamps via `HolonicClock`
    fn handle_issue_capability(
        &self,
        payload: &[u8],
        ctx: &ConnectionContext,
    ) -> ProtocolResult<PrivilegedResponse> {
        let request = IssueCapabilityRequest::decode_bounded(payload, &self.decode_config)
            .map_err(|e| ProtocolError::Serialization {
                reason: format!("invalid IssueCapabilityRequest: {e}"),
            })?;

        info!(
            session_id = %request.session_id,
            has_capability_request = request.capability_request.is_some(),
            peer_pid = ?ctx.peer_credentials().map(|c| c.pid),
            "IssueCapability request received"
        );

        // Validate required fields
        if request.session_id.is_empty() {
            return Ok(PrivilegedResponse::error(
                PrivilegedErrorCode::CapabilityRequestRejected,
                "session_id is required",
            ));
        }

        if request.capability_request.is_none() {
            return Ok(PrivilegedResponse::error(
                PrivilegedErrorCode::CapabilityRequestRejected,
                "capability_request is required",
            ));
        }

        // 1. Retrieve session state
        let Some(session) = self.session_registry.get_session(&request.session_id) else {
            warn!(session_id = %request.session_id, "IssueCapability rejected: session not found");
            return Ok(PrivilegedResponse::error(
                PrivilegedErrorCode::SessionNotFound,
                format!("session not found: {}", request.session_id),
            ));
        };

        // 2. Validate lease (TCK-00289: "Implement IssueCapability with lease
        //    validation")
        // Ensure the session's lease matches the authoritative work claim.
        // This confirms the session corresponds to a valid, active work item.
        if let Some(claim) = self.work_registry.get_claim(&session.work_id) {
            // Verify lease_id matches
            // Constant-time comparison is good practice for IDs
            let lease_matches = session.lease_id.len() == claim.lease_id.len()
                && bool::from(session.lease_id.as_bytes().ct_eq(claim.lease_id.as_bytes()));

            if !lease_matches {
                warn!(
                    session_id = %request.session_id,
                    expected_lease = "[REDACTED]",
                    actual_lease = "[REDACTED]",
                    "IssueCapability rejected: lease mismatch against work claim"
                );
                return Ok(PrivilegedResponse::error(
                    PrivilegedErrorCode::CapabilityRequestRejected,
                    "lease validation failed: session lease does not match work claim",
                ));
            }
        } else {
            // Local state-precondition failure (missing in-process work claim),
            // not a governance transport/communication failure.
            warn!(
                session_id = %request.session_id,
                work_id = %session.work_id,
                "IssueCapability rejected: work claim not found"
            );
            return Ok(PrivilegedResponse::error(
                PrivilegedErrorCode::CapabilityRequestRejected,
                "lease validation failed: work claim not found",
            ));
        }

        // 3. Generate HTF-compliant timestamps
        let Ok(mono_tick) = self.holonic_clock.now_mono_tick() else {
            warn!("Clock error during IssueCapability");
            return Ok(PrivilegedResponse::error(
                PrivilegedErrorCode::PolicyResolutionFailed,
                "clock error",
            ));
        };
        let _mono_tick = mono_tick.value();

        // For grant/expire times, use HLC Wall Time per RFC-0016.
        // TCK-00289: Fail-closed - do not fall back to SystemTime if HLC disabled.
        let now_wall = match self.holonic_clock.now_hlc() {
            Ok(hlc) => hlc.wall_ns,
            Err(e) => {
                // TCK-00289: Fail-closed - do not use SystemTime fallback
                warn!(error = %e, "HLC clock error during IssueCapability - failing closed");
                return Ok(PrivilegedResponse::error(
                    PrivilegedErrorCode::PolicyResolutionFailed,
                    format!("HTF timestamp error: {e}"),
                ));
            },
        };

        // Duration is in seconds, convert to nanoseconds
        let duration_ns =
            request.capability_request.as_ref().unwrap().duration_secs * 1_000_000_000;
        let expires_at_ns = now_wall + duration_ns;

        // Convert to seconds for response (proto uses u64 seconds)
        let granted_at = now_wall / 1_000_000_000;
        let expires_at = expires_at_ns / 1_000_000_000;

        let capability_id = format!("C-{}", uuid::Uuid::new_v4());

        // TCK-00268: Emit capability_granted metric
        if let Some(ref metrics) = self.metrics {
            let role_str = match WorkRole::try_from(session.role).unwrap_or(WorkRole::Unspecified) {
                WorkRole::Implementer => "implementer",
                WorkRole::Reviewer => "reviewer",
                WorkRole::GateExecutor => "gate_executor",
                WorkRole::Coordinator => "coordinator",
                WorkRole::Unspecified => "unspecified",
            };

            let capability_type = request
                .capability_request
                .as_ref()
                .map_or("unknown", |c| c.tool_class.as_str());

            metrics
                .daemon_metrics()
                .capability_granted(role_str, capability_type);
        }

        info!(
            session_id = %request.session_id,
            capability_id = %capability_id,
            "Capability issued"
        );

        Ok(PrivilegedResponse::IssueCapability(
            IssueCapabilityResponse {
                capability_id,
                granted_at,
                expires_at,
            },
        ))
    }

    /// Handles Shutdown requests (IPC-PRIV-004, TCK-00392).
    ///
    /// Triggers graceful daemon shutdown by setting the atomic shutdown flag
    /// on `SharedState`. The main event loop detects this flag and initiates
    /// the shutdown sequence: stop all processes, clean up sockets, remove
    /// the PID file.
    ///
    /// The shutdown flag is set first, then the response is constructed and
    /// returned. Because the main event loop runs on a separate task, the
    /// caller still receives acknowledgment before the daemon acts on the
    /// flag.
    ///
    /// If `daemon_state` is `None` (test/stub mode), logs a warning and
    /// returns a stub response without triggering shutdown.
    #[allow(clippy::option_if_let_else)] // Both branches have logging side effects; if-let is clearer
    fn handle_shutdown(
        &self,
        payload: &[u8],
        ctx: &ConnectionContext,
    ) -> ProtocolResult<PrivilegedResponse> {
        let request =
            ShutdownRequest::decode_bounded(payload, &self.decode_config).map_err(|e| {
                ProtocolError::Serialization {
                    reason: format!("invalid ShutdownRequest: {e}"),
                }
            })?;

        let reason_display = request.reason.as_deref().unwrap_or("no reason provided");

        if let Some(state) = &self.daemon_state {
            info!(
                reason = %reason_display,
                peer_pid = ?ctx.peer_credentials().map(|c| c.pid),
                "Shutdown request received via IPC, initiating graceful shutdown"
            );

            // Set the atomic shutdown flag. The main event loop polls
            // `is_shutdown_requested()` and will trigger the graceful
            // shutdown sequence (stop processes, cleanup sockets, remove
            // PID file).
            state.request_shutdown();

            Ok(PrivilegedResponse::Shutdown(ShutdownResponse {
                message: format!("Shutdown initiated (reason: {reason_display})"),
            }))
        } else {
            warn!(
                reason = %reason_display,
                peer_pid = ?ctx.peer_credentials().map(|c| c.pid),
                "Shutdown request received but daemon state not configured (test mode)"
            );

            Ok(PrivilegedResponse::Shutdown(ShutdownResponse {
                message: "Shutdown acknowledged (stub — daemon state not configured)".to_string(),
            }))
        }
    }

    /// Handles `UpdateStopFlags` requests (IPC-PRIV-018, TCK-00351).
    ///
    /// Mutates the shared runtime stop flags used by pre-actuation gating.
    ///
    /// # Audit Trail
    ///
    /// After mutation, this emits a `stop_flags_mutated` ledger event on a
    /// best-effort basis. Emission failures are logged but do NOT roll back
    /// the stop mutation. The stop mutation is the safety-critical path and
    /// must take effect even when ledger infrastructure is unavailable.
    fn handle_update_stop_flags(
        &self,
        payload: &[u8],
        ctx: &ConnectionContext,
    ) -> ProtocolResult<PrivilegedResponse> {
        let request = UpdateStopFlagsRequest::decode_bounded(payload, &self.decode_config)
            .map_err(|e| ProtocolError::Serialization {
                reason: format!("invalid UpdateStopFlagsRequest: {e}"),
            })?;

        if request.emergency_stop_active.is_none() && request.governance_stop_active.is_none() {
            return Ok(PrivilegedResponse::error(
                PrivilegedErrorCode::CapabilityRequestRejected,
                "at least one stop flag must be provided",
            ));
        }

        let peer = ctx
            .peer_credentials()
            .ok_or_else(|| ProtocolError::Serialization {
                reason: "peer credentials required for UpdateStopFlags".to_string(),
            })?;
        let actor_id = derive_actor_id(peer);

        let Some(authority) = self.stop_authority.as_ref() else {
            return Ok(PrivilegedResponse::error(
                PrivilegedErrorCode::PermissionDenied,
                "stop authority is not configured",
            ));
        };

        let prev_emergency = authority.emergency_stop_active();
        let prev_governance = authority.governance_stop_active();

        if let Some(active) = request.emergency_stop_active {
            authority.set_emergency_stop(active);
        }
        if let Some(active) = request.governance_stop_active {
            authority.set_governance_stop(active);
        }

        let emergency_stop_active = authority.emergency_stop_active();
        let governance_stop_active = authority.governance_stop_active();

        info!(
            actor_id = %actor_id,
            emergency_stop_previous = prev_emergency,
            emergency_stop_current = emergency_stop_active,
            governance_stop_previous = prev_governance,
            governance_stop_current = governance_stop_active,
            "UpdateStopFlags applied"
        );

        // Best-effort audit evidence emission. Never roll back the stop
        // mutation based on ledger availability.
        if !self.event_emitter.has_durable_storage() {
            warn!(
                actor_id = %actor_id,
                "UpdateStopFlags audit trail is in-memory only; event durability unavailable"
            );
        }

        match self.get_htf_timestamp_ns() {
            Ok(timestamp_ns) => {
                let request_context = serde_json::json!({
                    "endpoint": "UpdateStopFlags",
                    "connection_id": ctx.connection_id(),
                    "connection_phase": format!("{:?}", ctx.phase()),
                    "peer_uid": peer.uid,
                    "peer_gid": peer.gid,
                    "peer_pid": peer.pid,
                    "requested_updates": {
                        "emergency_stop_active": request.emergency_stop_active,
                        "governance_stop_active": request.governance_stop_active,
                    },
                });

                let mutation = StopFlagsMutation {
                    actor_id: actor_id.as_str(),
                    emergency_stop_previous: prev_emergency,
                    emergency_stop_current: emergency_stop_active,
                    governance_stop_previous: prev_governance,
                    governance_stop_current: governance_stop_active,
                    timestamp_ns,
                    request_context: &request_context,
                };

                if let Err(error) = self.event_emitter.emit_stop_flags_mutated(&mutation) {
                    warn!(
                        actor_id = %actor_id,
                        error = %error,
                        emergency_stop_current = emergency_stop_active,
                        governance_stop_current = governance_stop_active,
                        "UpdateStopFlags mutation applied but ledger audit emission failed"
                    );
                }
            },
            Err(error) => {
                warn!(
                    actor_id = %actor_id,
                    error = %error,
                    emergency_stop_current = emergency_stop_active,
                    governance_stop_current = governance_stop_active,
                    "UpdateStopFlags mutation applied but audit timestamp acquisition failed"
                );
            },
        }

        Ok(PrivilegedResponse::UpdateStopFlags(
            UpdateStopFlagsResponse {
                emergency_stop_active,
                governance_stop_active,
            },
        ))
    }

    /// Handles `WorkStatus` requests (IPC-PRIV-005, TCK-00344).
    ///
    /// Queries the status of a work item from the session registry.
    ///
    /// # Returns
    ///
    /// - Work status if found in session registry
    /// - `WORK_NOT_FOUND` error if work ID is not found
    fn handle_work_status(
        &self,
        payload: &[u8],
        _ctx: &ConnectionContext,
    ) -> ProtocolResult<PrivilegedResponse> {
        let request =
            WorkStatusRequest::decode_bounded(payload, &self.decode_config).map_err(|e| {
                ProtocolError::Serialization {
                    reason: format!("invalid WorkStatusRequest: {e}"),
                }
            })?;

        // CTR-1603: Validate work_id length to prevent DoS
        if request.work_id.len() > MAX_ID_LENGTH {
            return Ok(PrivilegedResponse::error(
                PrivilegedErrorCode::CapabilityRequestRejected,
                format!("work_id exceeds maximum length of {MAX_ID_LENGTH} bytes"),
            ));
        }

        if request.work_id.is_empty() {
            return Ok(PrivilegedResponse::error(
                PrivilegedErrorCode::CapabilityRequestRejected,
                "work_id cannot be empty",
            ));
        }

        debug!(work_id = %request.work_id, "Processing WorkStatus request");

        // Query session registry for work status
        // Note: We search through sessions to find work associated with this work_id
        // This is a basic implementation; a dedicated work registry would be more
        // efficient
        let session_state = self.find_session_by_work_id(&request.work_id);

        match session_state {
            Some(session) => {
                // Work found via session
                let response = WorkStatusResponse {
                    work_id: request.work_id,
                    status: "SPAWNED".to_string(),
                    actor_id: None, // Not tracked in session
                    role: Some(session.role),
                    session_id: Some(session.session_id),
                    lease_id: None,   // Lease is redacted in SessionState
                    created_at_ns: 0, // Not tracked
                    claimed_at_ns: None,
                };
                Ok(PrivilegedResponse::WorkStatus(response))
            },
            None => {
                // Check work claims for claimed but not yet spawned work
                if let Some(claim) = self.work_registry.get_claim(&request.work_id) {
                    let response = WorkStatusResponse {
                        work_id: request.work_id,
                        status: "CLAIMED".to_string(),
                        actor_id: Some(claim.actor_id.clone()),
                        role: Some(claim.role.into()),
                        session_id: None,
                        lease_id: Some(claim.lease_id),
                        created_at_ns: 0,
                        claimed_at_ns: None, // WorkClaim doesn't track timestamp
                    };
                    Ok(PrivilegedResponse::WorkStatus(response))
                } else {
                    Ok(PrivilegedResponse::error(
                        PrivilegedErrorCode::WorkNotFound,
                        format!("work item not found: {}", request.work_id),
                    ))
                }
            },
        }
    }

    /// Finds a session by `work_id`.
    ///
    /// Delegates to `SessionRegistry::get_session_by_work_id` which performs
    /// an O(n) scan. This is acceptable for status queries which are not
    /// performance-critical.
    fn find_session_by_work_id(&self, work_id: &str) -> Option<SessionState> {
        self.session_registry.get_session_by_work_id(work_id)
    }

    // ========================================================================
    // Session Termination Handler (TCK-00395)
    // ========================================================================

    /// Handles `EndSession` requests (IPC-PRIV-016, TCK-00395).
    ///
    /// Terminates an active session and emits a `SessionTerminated` ledger
    /// event. This is the authoritative path for session termination that
    /// ensures the ledger records all session lifecycle events.
    ///
    /// # Security Contract
    ///
    /// - Session must exist in the session registry
    /// - Actor ID is derived from peer credentials (not user input)
    /// - `SessionTerminated` event is signed and persisted atomically
    /// - Fail-closed: returns error if timestamp generation, runtime stop, or
    ///   event emission fails
    /// - Session is removed from registry POST-COMMIT (after runtime stop +
    ///   ledger succeed), ensuring the session is re-findable for retry on
    ///   failure
    /// - `WorkTransitioned` emission failure is propagated (fail-closed)
    /// - Exit code derived from typed `TerminationOutcome` enum (not string
    ///   matching)
    /// - Reason field bounded by `MAX_REASON_LENGTH` to prevent OOM/bloated
    ///   ledger entries
    ///
    /// # Ordering (Security BLOCKER 2 / Quality BLOCKER 1)
    ///
    /// 1. Read session state (no removal)
    /// 2. Stop runtime via `stop_with_session_context` (fail-closed)
    /// 3. Derive exit code from typed `TerminationOutcome` enum
    /// 4. Emit ledger events (`SessionTerminated`, `WorkTransitioned`)
    /// 5. Remove session from registry (POST-COMMIT)
    fn handle_end_session(
        &self,
        payload: &[u8],
        ctx: &ConnectionContext,
    ) -> ProtocolResult<PrivilegedResponse> {
        let request =
            EndSessionRequest::decode_bounded(payload, &self.decode_config).map_err(|e| {
                ProtocolError::Serialization {
                    reason: format!("invalid EndSessionRequest: {e}"),
                }
            })?;

        info!(
            session_id = %request.session_id,
            reason = %request.reason,
            outcome = %request.outcome,
            peer_pid = ?ctx.peer_credentials().map(|c| c.pid),
            "EndSession request received"
        );

        // Validate required fields
        if request.session_id.is_empty() {
            return Ok(PrivilegedResponse::error(
                PrivilegedErrorCode::CapabilityRequestRejected,
                "session_id is required",
            ));
        }

        // SEC-SCP-FAC-0020: Enforce maximum length on session_id
        if request.session_id.len() > MAX_ID_LENGTH {
            return Ok(PrivilegedResponse::error(
                PrivilegedErrorCode::CapabilityRequestRejected,
                format!("session_id exceeds maximum length of {MAX_ID_LENGTH} bytes"),
            ));
        }

        // SEC-SCP-FAC-0020 / Security MAJOR 1: Enforce maximum length on
        // reason to prevent OOM and bloated signed ledger payloads.
        if request.reason.len() > MAX_REASON_LENGTH {
            return Ok(PrivilegedResponse::error(
                PrivilegedErrorCode::CapabilityRequestRejected,
                format!(
                    "reason exceeds maximum length of {MAX_REASON_LENGTH} bytes (got {})",
                    request.reason.len()
                ),
            ));
        }

        // ---- Phase 1: Read session state (no removal) ----
        // Look up session WITHOUT removing it. The session is only removed
        // after all fallible operations succeed (POST-COMMIT), ensuring the
        // session is re-findable for retry if any step fails.
        let Some(session) = self.session_registry.get_session(&request.session_id) else {
            return Ok(PrivilegedResponse::error(
                PrivilegedErrorCode::CapabilityRequestRejected,
                format!("session not found: {}", request.session_id),
            ));
        };

        // TCK-00395 Security MAJOR 2: Fail closed when peer credentials are
        // missing. Same pattern as ClaimWork (line 3631). EndSession emits
        // authoritative ledger events (SessionTerminated), and recording
        // "unknown" as the actor identity would break the accountability
        // chain for a privileged termination action.
        let peer_creds = ctx
            .peer_credentials()
            .ok_or_else(|| ProtocolError::Serialization {
                reason: "peer credentials required for session termination".to_string(),
            })?;
        let actor_id = derive_actor_id(peer_creds);

        // Get HTF-compliant timestamp
        let timestamp_ns = match self.get_htf_timestamp_ns() {
            Ok(ts) => ts,
            Err(e) => {
                warn!(error = %e, "HTF timestamp generation failed - failing closed");
                return Ok(PrivilegedResponse::error(
                    PrivilegedErrorCode::CapabilityRequestRejected,
                    format!("HTF timestamp error: {e}"),
                ));
            },
        };

        // TCK-00395 Quality v3 MAJOR: Use typed TerminationOutcome enum to
        // determine exit code instead of free-form string matching.
        // When `outcome` is set (non-zero), it takes precedence over the
        // legacy `reason` string. Unspecified (0) falls back to string
        // matching for backward compatibility.
        let exit_code = match TerminationOutcome::try_from(request.outcome) {
            Ok(TerminationOutcome::Success) => 0,
            Ok(
                TerminationOutcome::Failure
                | TerminationOutcome::Cancelled
                | TerminationOutcome::Timeout,
            ) => 1,
            // Unspecified or unknown: fall back to legacy string matching
            _ => {
                let is_failure =
                    request.reason != "completed_normally" && request.reason != "success";
                i32::from(is_failure)
            },
        };

        // ---- Phase 2: Stop runtime BEFORE emitting ledger events ----
        // Security BLOCKER 1: Fail closed if the session has no resolvable
        // episode binding. SpawnEpisode now writes back the episode_id
        // (see update_episode_id call above). If the session still lacks
        // an episode_id, it means the episode was never created or the
        // write-back failed, so we must not emit termination facts.
        //
        // In test mode (no tokio runtime), episode_id may legitimately be
        // None. Production sessions always have an episode_id after
        // SpawnEpisode succeeds.
        let episode_id_str = match session.episode_id {
            Some(ref id) => id.clone(),
            None => {
                // In cfg(test), allow sessions without episode_id for
                // backward compatibility with sync unit tests.
                #[cfg(test)]
                {
                    debug!(
                        session_id = %request.session_id,
                        "EndSession: session lacks episode_id (test mode) - skipping runtime stop"
                    );
                    String::new()
                }
                #[cfg(not(test))]
                {
                    warn!(
                        session_id = %request.session_id,
                        "EndSession: session has no episode_id binding - failing closed"
                    );
                    return Ok(PrivilegedResponse::error(
                        PrivilegedErrorCode::CapabilityRequestRejected,
                        "session has no episode binding: cannot stop runtime",
                    ));
                }
            },
        };

        // Stop the runtime episode if we have a valid episode_id.
        // TCK-00395 Security MAJOR 1: Fail closed on EpisodeId parse failure.
        // Previously, a malformed/corrupted episode_id would silently skip
        // the runtime stop but still emit termination facts and clean up
        // registry state, leaving the runtime running without accountability.
        if !episode_id_str.is_empty() {
            let episode_id_parsed = match EpisodeId::new(episode_id_str.clone()) {
                Ok(id) => id,
                Err(e) => {
                    warn!(
                        error = %e,
                        episode_id = %episode_id_str,
                        session_id = %request.session_id,
                        "EndSession: EpisodeId parse failed - failing closed before ledger emission"
                    );
                    return Ok(PrivilegedResponse::error(
                        PrivilegedErrorCode::CapabilityRequestRejected,
                        format!("malformed episode_id '{episode_id_str}': {e}"),
                    ));
                },
            };

            let termination_class = if exit_code == 0 {
                TerminationClass::Success
            } else {
                TerminationClass::Failure
            };
            let stop_result = if let Ok(handle) = tokio::runtime::Handle::try_current() {
                let rt = &self.episode_runtime;
                tokio::task::block_in_place(|| {
                    handle.block_on(rt.stop_with_session_context(
                        &episode_id_parsed,
                        termination_class,
                        timestamp_ns,
                        &request.session_id,
                        &session.work_id,
                        &actor_id,
                    ))
                })
            } else {
                // No tokio runtime available; fail closed rather than
                // skipping the runtime stop.
                warn!("No tokio runtime handle available for runtime stop - failing closed");
                return Ok(PrivilegedResponse::error(
                    PrivilegedErrorCode::CapabilityRequestRejected,
                    "runtime stop failed: no async runtime available",
                ));
            };

            if let Err(e) = stop_result {
                warn!(
                    error = %e,
                    episode_id = %episode_id_str,
                    "Runtime stop failed - failing closed without writing success facts"
                );
                return Ok(PrivilegedResponse::error(
                    PrivilegedErrorCode::CapabilityRequestRejected,
                    format!("runtime stop failed: {e}"),
                ));
            }
        }

        // ---- Phase 3: Emit ledger events (after successful runtime stop) ----
        let termination_reason = if request.reason.is_empty() {
            "session_ended_via_ipc"
        } else {
            &request.reason
        };

        // TCK-00395: Emit SessionTerminated event to ledger.
        // This is the authoritative path for recording session termination.
        // Only emitted AFTER runtime stop succeeds (Security BLOCKER 2).
        if let Err(e) = self.event_emitter.emit_session_terminated(
            &request.session_id,
            &session.work_id,
            exit_code,
            termination_reason,
            &actor_id,
            timestamp_ns,
        ) {
            warn!(error = %e, "SessionTerminated event emission failed");
            return Ok(PrivilegedResponse::error(
                PrivilegedErrorCode::CapabilityRequestRejected,
                format!("session terminated event emission failed: {e}"),
            ));
        }

        // TCK-00395 Quality BLOCKER 2: DO NOT emit
        // WorkTransitioned(InProgress -> Completed) on EndSession.
        // The InProgress -> Completed transition violates core work-state
        // transition rules (state.rs only allows InProgress -> Review,
        // CiPending, NeedsInput, NeedsAdjudication, or Aborted).
        // Work state completion belongs to a separate workflow path
        // (gate orchestration), not EndSession.

        // ---- Phase 4: POST-COMMIT session + manifest + telemetry removal ----
        // Remove session from registry only after ALL fallible operations
        // (runtime stop + ledger emission) have succeeded. This prevents
        // the session from being orphaned if a downstream step fails.
        //
        // TCK-00395 Security BLOCKER 2: Also remove the manifest entry
        // to prevent retained session tokens from passing manifest lookup
        // in RequestTool after EndSession.
        self.manifest_store.remove(&request.session_id);

        // TCK-00352 MAJOR 2 fix: Remove V1 manifest on EndSession.
        // Without this, a terminated session's V1 manifest remains in the
        // store. If a new session reuses the same ID (or a retained token
        // is replayed), the stale V1 manifest could incorrectly match,
        // either granting or denying based on an expired session's policy.
        if let Some(ref v1_store) = self.v1_manifest_store {
            v1_store.remove(&request.session_id);
        }

        // TCK-00351 BLOCKER-2: `episode_count` is defined as completed
        // episodes. Increment on termination (not spawn) to avoid
        // off-by-one self-deny on the first RequestTool.
        if let Some(ref store) = self.telemetry_store {
            if let Some(telemetry) = store.get(&request.session_id) {
                telemetry.increment_episode_count();
            }
        }

        // TCK-00384 review fix: Clean up telemetry on EndSession to free
        // capacity in the bounded store. Without this, repeated
        // spawn/end cycles exhaust MAX_TELEMETRY_SESSIONS and block new
        // spawns (DoS). Mirrors the cleanup in session_dispatch.rs:1421.
        if let Some(ref store) = self.telemetry_store {
            store.remove(&request.session_id);
        }

        // TCK-00351 BLOCKER 3 FIX: Clean up stop conditions on EndSession
        // to free capacity in the bounded store.  Without this, stop
        // conditions entries accumulate on every spawn/end cycle and are
        // never reclaimed, causing the store to reach capacity and reject
        // new registrations (DoS).  Same lifecycle as telemetry cleanup
        // above.
        if let Some(ref store) = self.stop_conditions_store {
            store.remove(&request.session_id);
        }

        if let Err(e) = self.session_registry.remove_session(&request.session_id) {
            warn!(
                error = %e,
                session_id = %request.session_id,
                "Session removal persistence failed after successful ledger commit"
            );
            return Ok(PrivilegedResponse::error(
                PrivilegedErrorCode::CapabilityRequestRejected,
                format!("session removal persistence failed: {e}"),
            ));
        }

        debug!(
            session_id = %request.session_id,
            work_id = %session.work_id,
            "EndSession completed - SessionTerminated event emitted"
        );

        Ok(PrivilegedResponse::EndSession(EndSessionResponse {
            session_id: request.session_id,
            message: "session terminated".to_string(),
        }))
    }

    // ========================================================================
    // TCK-00389: IngestReviewReceipt Handler
    // ========================================================================

    /// Handles `IngestReviewReceipt` requests (IPC-PRIV-017, TCK-00389).
    ///
    /// Ingests a review receipt from an external reviewer into the FAC ledger.
    /// Validates reviewer identity against the gate lease, verifies request
    /// integrity, and emits either `ReviewReceiptRecorded` or
    /// `ReviewBlockedRecorded` event.
    ///
    /// # Security Invariants
    ///
    /// - **Reviewer identity validation**: The `reviewer_actor_id` MUST match
    ///   the `executor_actor_id` bound in the gate lease (constant-time
    ///   comparison to prevent timing side-channels).
    /// - **Lease existence validation**: The `lease_id` MUST reference a valid
    ///   gate lease (fail-closed on missing lease).
    /// - **Idempotency**: Duplicate `receipt_id` values do not create duplicate
    ///   events (checked via ledger query).
    /// - **Fail-closed**: Any validation failure rejects the request with a
    ///   clear error.
    fn handle_ingest_review_receipt(
        &self,
        payload: &[u8],
        ctx: &ConnectionContext,
    ) -> ProtocolResult<PrivilegedResponse> {
        let request = IngestReviewReceiptRequest::decode_bounded(payload, &self.decode_config)
            .map_err(|e| ProtocolError::Serialization {
                reason: format!("invalid IngestReviewReceiptRequest: {e}"),
            })?;

        // ---- Phase -1 (v6 Finding 1): Bind reviewer identity to authenticated caller
        // ----
        //
        // SECURITY: The reviewer identity MUST be derived from peer credentials,
        // not trusted from the caller-supplied `reviewer_actor_id`. Without this
        // binding, any operator-socket process could submit receipts as any
        // reviewer. The authenticated identity is used for:
        //   - Reviewer-vs-executor comparison (Phase 1)
        //   - Attestation signer identity (Phase 1b)
        //   - Event actor attribution (Phase 4)
        let Some(peer_creds) = ctx.peer_credentials() else {
            return Ok(PrivilegedResponse::error(
                PrivilegedErrorCode::PermissionDenied,
                "peer credentials required for review receipt ingestion",
            ));
        };
        let authenticated_reviewer_id = derive_actor_id(peer_creds);

        info!(
            lease_id = %request.lease_id,
            receipt_id = %request.receipt_id,
            claimed_reviewer_actor_id = %request.reviewer_actor_id,
            authenticated_reviewer_id = %authenticated_reviewer_id,
            verdict = %request.verdict,
            peer_pid = ?ctx.peer_credentials().map(|c| c.pid),
            "IngestReviewReceipt request received"
        );

        // ---- Phase 0: Validate required fields (admission checks) ----

        if request.lease_id.is_empty() {
            return Ok(PrivilegedResponse::error(
                PrivilegedErrorCode::CapabilityRequestRejected,
                "lease_id is required",
            ));
        }
        if request.lease_id.len() > MAX_ID_LENGTH {
            return Ok(PrivilegedResponse::error(
                PrivilegedErrorCode::CapabilityRequestRejected,
                format!("lease_id exceeds maximum length of {MAX_ID_LENGTH} bytes"),
            ));
        }

        if request.receipt_id.is_empty() {
            return Ok(PrivilegedResponse::error(
                PrivilegedErrorCode::CapabilityRequestRejected,
                "receipt_id is required",
            ));
        }
        if request.receipt_id.len() > MAX_ID_LENGTH {
            return Ok(PrivilegedResponse::error(
                PrivilegedErrorCode::CapabilityRequestRejected,
                format!("receipt_id exceeds maximum length of {MAX_ID_LENGTH} bytes"),
            ));
        }

        if request.reviewer_actor_id.is_empty() {
            return Ok(PrivilegedResponse::error(
                PrivilegedErrorCode::CapabilityRequestRejected,
                "reviewer_actor_id is required",
            ));
        }
        if request.reviewer_actor_id.len() > MAX_ID_LENGTH {
            return Ok(PrivilegedResponse::error(
                PrivilegedErrorCode::CapabilityRequestRejected,
                format!("reviewer_actor_id exceeds maximum length of {MAX_ID_LENGTH} bytes"),
            ));
        }

        // REQ-0010: Identity-bearing authoritative requests MUST carry
        // proof-carrying pointers.
        //
        // SECURITY (TCK-00356 Fix 1): Validate the identity proof hash using
        // the centralized validator which enforces non-zero commitment and
        // correct length. Phase 1 (pre-CAS transport) validates the hash as
        // a binding commitment; full proof dereference requires CAS
        // integration (TCK-00359).
        if let Err(e) = crate::identity::validate_identity_proof_hash(&request.identity_proof_hash)
        {
            return Ok(PrivilegedResponse::error(
                PrivilegedErrorCode::CapabilityRequestRejected,
                format!("identity_proof_hash validation failed: {e}"),
            ));
        }

        // WVR-0103: Log once that identity proof hash is validated as
        // shape-only commitment (Phase 1 / pre-CAS transport).
        {
            static PROOF_WAIVER_WARN: std::sync::Once = std::sync::Once::new();
            PROOF_WAIVER_WARN.call_once(|| {
                warn!(
                    waiver = "WVR-0103",
                    "identity proof hash validated as shape-only commitment; \
                     full CAS dereference + IdentityProofV1::verify() deferred (WVR-0103)"
                );
            });
        }

        // Validate changeset_digest is exactly 32 bytes
        if request.changeset_digest.len() != 32 {
            return Ok(PrivilegedResponse::error(
                PrivilegedErrorCode::CapabilityRequestRejected,
                format!(
                    "changeset_digest must be exactly 32 bytes, got {}",
                    request.changeset_digest.len()
                ),
            ));
        }

        // Validate artifact_bundle_hash is exactly 32 bytes
        if request.artifact_bundle_hash.len() != 32 {
            return Ok(PrivilegedResponse::error(
                PrivilegedErrorCode::CapabilityRequestRejected,
                format!(
                    "artifact_bundle_hash must be exactly 32 bytes, got {}",
                    request.artifact_bundle_hash.len()
                ),
            ));
        }

        // Validate verdict is not unspecified (fail-closed)
        let verdict = ReviewReceiptVerdict::try_from(request.verdict)
            .unwrap_or(ReviewReceiptVerdict::Unspecified);
        if verdict == ReviewReceiptVerdict::Unspecified {
            return Ok(PrivilegedResponse::error(
                PrivilegedErrorCode::CapabilityRequestRejected,
                "verdict must be APPROVE or BLOCKED, not UNSPECIFIED",
            ));
        }

        // For BLOCKED verdict, validate blocked_log_hash is present
        if verdict == ReviewReceiptVerdict::Blocked {
            if request.blocked_log_hash.len() != 32 {
                return Ok(PrivilegedResponse::error(
                    PrivilegedErrorCode::CapabilityRequestRejected,
                    format!(
                        "blocked_log_hash must be exactly 32 bytes for BLOCKED verdict, got {}",
                        request.blocked_log_hash.len()
                    ),
                ));
            }
            if request.blocked_reason_code == 0 {
                return Ok(PrivilegedResponse::error(
                    PrivilegedErrorCode::CapabilityRequestRejected,
                    "blocked_reason_code must be non-zero for BLOCKED verdict",
                ));
            }
        }

        // ---- Phase 1: Lease existence + reviewer identity validation ----
        // Security-critical: reviewer identity MUST match lease executor.
        // This prevents unauthorized actors from submitting review results.

        let expected_actor_id = self
            .lease_validator
            .get_lease_executor_actor_id(&request.lease_id);

        let Some(expected_actor_id) = expected_actor_id else {
            return Ok(PrivilegedResponse::error(
                PrivilegedErrorCode::GateLeaseMissing,
                format!("gate lease not found: {}", request.lease_id),
            ));
        };

        // SEC-TIMING-001: Constant-time comparison to prevent timing
        // side-channel attacks on reviewer identity.
        //
        // SECURITY (v6 Finding 1): Compare the AUTHENTICATED reviewer identity
        // (derived from peer credentials) against the lease executor — NOT the
        // caller-supplied `request.reviewer_actor_id`. This prevents any
        // operator-socket process from submitting receipts as an arbitrary
        // reviewer.
        let identity_matches = expected_actor_id.len() == authenticated_reviewer_id.len()
            && bool::from(
                expected_actor_id
                    .as_bytes()
                    .ct_eq(authenticated_reviewer_id.as_bytes()),
            );

        if !identity_matches {
            warn!(
                lease_id = %request.lease_id,
                authenticated_reviewer = %authenticated_reviewer_id,
                claimed_reviewer = %request.reviewer_actor_id,
                "Authenticated reviewer identity mismatch - rejecting review receipt"
            );
            return Ok(PrivilegedResponse::error(
                PrivilegedErrorCode::PermissionDenied,
                "authenticated caller identity does not match gate lease executor",
            ));
        }

        // ---- Phase 1a (TCK-00408): CAS existence validation ----
        // CAS is a hard requirement for review receipt ingestion (fail-closed).
        // Without CAS, artifact_bundle_hash cannot be verified, so we reject
        // the request rather than silently skipping validation.
        let Some(cas) = &self.cas else {
            warn!(
                receipt_id = %request.receipt_id,
                "CAS not configured — rejecting review receipt ingestion (fail-closed). \
                 CAS is required to verify artifact_bundle_hash."
            );
            return Ok(PrivilegedResponse::error(
                PrivilegedErrorCode::CapabilityRequestRejected,
                "CAS is not configured; review receipt ingestion requires CAS \
                 for artifact_bundle_hash verification (fail-closed)",
            ));
        };
        {
            let bundle_hash: [u8; 32] = request
                .artifact_bundle_hash
                .as_slice()
                .try_into()
                .expect("validated to be 32 bytes above");
            match cas.exists(&bundle_hash) {
                Ok(true) => {
                    // Bundle exists in CAS, proceed.
                },
                Ok(false) => {
                    warn!(
                        receipt_id = %request.receipt_id,
                        artifact_bundle_hash = %hex::encode(bundle_hash),
                        "artifact_bundle_hash not found in CAS — rejecting review receipt (fail-closed)"
                    );
                    return Ok(PrivilegedResponse::error(
                        PrivilegedErrorCode::CapabilityRequestRejected,
                        format!(
                            "artifact_bundle_hash {} not found in CAS; \
                             the referenced bundle must be published before review receipt ingestion",
                            hex::encode(bundle_hash)
                        ),
                    ));
                },
                Err(e) => {
                    warn!(
                        receipt_id = %request.receipt_id,
                        error = %e,
                        "CAS existence check failed — rejecting review receipt (fail-closed)"
                    );
                    return Ok(PrivilegedResponse::error(
                        PrivilegedErrorCode::CapabilityRequestRejected,
                        format!("CAS existence check failed for artifact_bundle_hash: {e}"),
                    ));
                },
            }
        }

        // ---- Phase 1b (TCK-00340): Attestation ratchet validation ----
        //
        // SECURITY: Resolve the real risk tier from the work item's policy
        // resolution state. Higher tiers require stronger attestation
        // (CounterSigned / ThresholdSigned). Fail-closed: if risk tier
        // cannot be resolved, default to Tier4 (most restrictive).
        {
            let changeset_digest: [u8; 32] = request
                .changeset_digest
                .as_slice()
                .try_into()
                .expect("validated to be 32 bytes above");

            // Resolve risk tier via: lease_id -> work_id -> work_claim -> policy_resolution
            let (risk_tier, resolved_policy_hash) =
                if let Some(wid) = self.lease_validator.get_lease_work_id(&request.lease_id) {
                    if let Some(claim) = self.work_registry.get_claim(&wid) {
                        let tier_u8 = claim.policy_resolution.resolved_risk_tier;
                        let tier = RiskTier::try_from(tier_u8).unwrap_or_else(|_| {
                            // FAIL-CLOSED: invalid tier value -> Tier4 (most restrictive)
                            warn!(
                                lease_id = %request.lease_id,
                                work_id = %wid,
                                tier_value = tier_u8,
                                "Invalid risk tier value in policy resolution — \
                                 defaulting to Tier4 (fail-closed)"
                            );
                            RiskTier::Tier4
                        });
                        (tier, claim.policy_resolution.resolved_policy_hash)
                    } else {
                        // FAIL-CLOSED: no work claim -> Tier4
                        warn!(
                            lease_id = %request.lease_id,
                            work_id = %wid,
                            "Work claim not found for lease — \
                             defaulting to Tier4 (fail-closed)"
                        );
                        (RiskTier::Tier4, changeset_digest)
                    }
                } else {
                    // FAIL-CLOSED: no work_id from lease -> Tier4
                    warn!(
                        lease_id = %request.lease_id,
                        "Could not resolve work_id from lease — \
                         defaulting to Tier4 (fail-closed)"
                    );
                    (RiskTier::Tier4, changeset_digest)
                };

            // SECURITY (v5 Finding 4): Instead of hard-rejecting all non-Tier0
            // requests, consult the ratchet table first. The ratchet table
            // explicitly allows Tier1 with SelfSigned attestation. Only reject
            // if the tier's required attestation level exceeds what this
            // endpoint can provide (SelfSigned). This restores Tier1 throughput
            // while maintaining fail-closed semantics for higher tiers.
            let requirements = AttestationRequirements::new();
            let required_level = requirements.required_level(ReceiptKind::Review, risk_tier);
            if !AttestationLevel::SelfSigned.satisfies(required_level) {
                warn!(
                    receipt_id = %request.receipt_id,
                    risk_tier = ?risk_tier,
                    required_level = %required_level,
                    "Risk tier requires attestation stronger than SelfSigned — \
                     rejecting (fail-closed)"
                );
                return Ok(PrivilegedResponse::error(
                    PrivilegedErrorCode::CapabilityRequestRejected,
                    format!(
                        "review receipt endpoint provides SelfSigned attestation; \
                         {risk_tier:?} requires {required_level} which this endpoint cannot produce"
                    ),
                ));
            }

            // SECURITY (v6 Finding 1): Use authenticated reviewer identity
            // for attestation signer, not the caller-supplied value.
            let attestation = ReceiptAttestation {
                kind: ReceiptKind::Review,
                level: AttestationLevel::SelfSigned,
                policy_hash: resolved_policy_hash,
                signer_identity: authenticated_reviewer_id.clone(),
                counter_signer_identity: None,
                threshold_signer_count: None,
            };

            // Validate attestation against requirements using the resolved
            // policy hash from the work item's policy resolution (not the
            // raw changeset_digest). This binds the attestation to the
            // governance-resolved policy state.
            if let Err(e) = validate_receipt_attestation(
                &attestation,
                risk_tier,
                &resolved_policy_hash,
                &requirements,
            ) {
                warn!(
                    receipt_id = %request.receipt_id,
                    error = %e,
                    "Receipt attestation validation failed - rejecting (fail-closed)"
                );
                return Ok(PrivilegedResponse::error(
                    PrivilegedErrorCode::CapabilityRequestRejected,
                    format!("attestation validation failed: {e}"),
                ));
            }
        }

        let request_changeset_digest_arr: [u8; 32] = request
            .changeset_digest
            .as_slice()
            .try_into()
            .expect("validated to be 32 bytes above");
        let request_identity_proof_hash_arr: [u8; 32] = request
            .identity_proof_hash
            .as_slice()
            .try_into()
            .expect("validated to be 32 bytes by validate_identity_proof_hash above");
        let request_artifact_bundle_hash_arr: [u8; 32] = request
            .artifact_bundle_hash
            .as_slice()
            .try_into()
            .expect("validated to be 32 bytes above");
        let request_verdict = match verdict {
            ReviewReceiptVerdict::Approve => "APPROVE",
            ReviewReceiptVerdict::Blocked => "BLOCKED",
            ReviewReceiptVerdict::Unspecified => {
                return Ok(PrivilegedResponse::error(
                    PrivilegedErrorCode::CapabilityRequestRejected,
                    "verdict must be APPROVE or BLOCKED",
                ));
            },
        };
        let request_blocked_reason_code = if verdict == ReviewReceiptVerdict::Blocked {
            Some(request.blocked_reason_code)
        } else {
            None
        };
        let request_blocked_log_hash_arr = if verdict == ReviewReceiptVerdict::Blocked {
            Some(
                request
                    .blocked_log_hash
                    .as_slice()
                    .try_into()
                    .expect("validated to be 32 bytes above"),
            )
        } else {
            None
        };

        let to_response_event_type = |event_type: &str| -> String {
            match event_type {
                "review_receipt_recorded" => "ReviewReceiptRecorded".to_string(),
                "review_blocked_recorded" => "ReviewBlockedRecorded".to_string(),
                other => other.to_string(),
            }
        };

        let validate_receipt_replay = |existing: &SignedLedgerEvent| -> Result<(), String> {
            let original = extract_receipt_replay_bindings(&existing.payload).map_err(|e| {
                format!("receipt exists but replay bindings could not be extracted: {e}")
            })?;

            if original.lease_id != request.lease_id {
                warn!(
                    receipt_id = %request.receipt_id,
                    existing_event_id = %existing.event_id,
                    original_lease_id = %original.lease_id,
                    requested_lease_id = %request.lease_id,
                    "Idempotent review receipt replay rejected: lease_id mismatch"
                );
                return Err(format!(
                    "receipt_id '{}' was originally submitted for lease '{}', not '{}'",
                    request.receipt_id, original.lease_id, request.lease_id
                ));
            }

            if original.identity_proof_hash != request_identity_proof_hash_arr {
                warn!(
                    receipt_id = %request.receipt_id,
                    existing_event_id = %existing.event_id,
                    original_identity_proof_hash = %hex::encode(original.identity_proof_hash),
                    requested_identity_proof_hash = %hex::encode(request_identity_proof_hash_arr),
                    "Idempotent review receipt replay rejected: identity_proof_hash mismatch"
                );
                return Err(format!(
                    "receipt_id '{}' was originally submitted with identity_proof_hash '{}', not '{}'",
                    request.receipt_id,
                    hex::encode(original.identity_proof_hash),
                    hex::encode(request_identity_proof_hash_arr),
                ));
            }

            if original.changeset_digest != request_changeset_digest_arr {
                warn!(
                    receipt_id = %request.receipt_id,
                    existing_event_id = %existing.event_id,
                    original_changeset_digest = %hex::encode(original.changeset_digest),
                    requested_changeset_digest = %hex::encode(request_changeset_digest_arr),
                    "Idempotent review receipt replay rejected: changeset_digest mismatch"
                );
                return Err(format!(
                    "receipt_id '{}' was originally submitted with changeset_digest '{}', not '{}'",
                    request.receipt_id,
                    hex::encode(original.changeset_digest),
                    hex::encode(request_changeset_digest_arr),
                ));
            }

            if original.verdict != request_verdict {
                warn!(
                    receipt_id = %request.receipt_id,
                    existing_event_id = %existing.event_id,
                    original_verdict = %original.verdict,
                    requested_verdict = %request_verdict,
                    "Idempotent review receipt replay rejected: verdict mismatch"
                );
                return Err(format!(
                    "receipt_id '{}' was originally submitted with verdict '{}', not '{}'",
                    request.receipt_id, original.verdict, request_verdict
                ));
            }

            if original.artifact_bundle_hash != request_artifact_bundle_hash_arr {
                warn!(
                    receipt_id = %request.receipt_id,
                    existing_event_id = %existing.event_id,
                    original_artifact_bundle_hash = %hex::encode(original.artifact_bundle_hash),
                    requested_artifact_bundle_hash = %hex::encode(request_artifact_bundle_hash_arr),
                    "Idempotent review receipt replay rejected: artifact_bundle_hash mismatch"
                );
                return Err(format!(
                    "receipt_id '{}' was originally submitted with artifact_bundle_hash '{}', not '{}'",
                    request.receipt_id,
                    hex::encode(original.artifact_bundle_hash),
                    hex::encode(request_artifact_bundle_hash_arr),
                ));
            }

            if let Some(original_blocked_reason_code) = original.blocked_reason_code {
                let requested_blocked_reason_code = request_blocked_reason_code
                    .map_or_else(|| "<missing>".to_string(), |code| code.to_string());
                if Some(original_blocked_reason_code) != request_blocked_reason_code {
                    warn!(
                        receipt_id = %request.receipt_id,
                        existing_event_id = %existing.event_id,
                        original_blocked_reason_code = original_blocked_reason_code,
                        requested_blocked_reason_code = %requested_blocked_reason_code,
                        "Idempotent review receipt replay rejected: blocked_reason_code mismatch"
                    );
                    return Err(format!(
                        "receipt_id '{}' was originally submitted with blocked_reason_code '{}', not '{}'",
                        request.receipt_id,
                        original_blocked_reason_code,
                        requested_blocked_reason_code,
                    ));
                }
            }

            if let Some(original_blocked_log_hash) = original.blocked_log_hash {
                let requested_blocked_log_hash = request_blocked_log_hash_arr
                    .map_or_else(|| "<missing>".to_string(), hex::encode);
                if Some(original_blocked_log_hash) != request_blocked_log_hash_arr {
                    warn!(
                        receipt_id = %request.receipt_id,
                        existing_event_id = %existing.event_id,
                        original_blocked_log_hash = %hex::encode(original_blocked_log_hash),
                        requested_blocked_log_hash = %requested_blocked_log_hash,
                        "Idempotent review receipt replay rejected: blocked_log_hash mismatch"
                    );
                    return Err(format!(
                        "receipt_id '{}' was originally submitted with blocked_log_hash '{}', not '{}'",
                        request.receipt_id,
                        hex::encode(original_blocked_log_hash),
                        requested_blocked_log_hash,
                    ));
                }
            }

            Ok(())
        };

        // ---- Phase 2: Idempotency check ----
        //
        // SECURITY (v10 MAJOR 1 -- receipt_id idempotency fix):
        //
        // Use `get_event_by_receipt_id` to look up existing events by the
        // caller-supplied `receipt_id` embedded in the event payload.
        if let Some(existing) = self
            .event_emitter
            .get_event_by_receipt_id(&request.receipt_id)
        {
            if let Err(message) = validate_receipt_replay(&existing) {
                return Ok(PrivilegedResponse::error(
                    PrivilegedErrorCode::CapabilityRequestRejected,
                    message,
                ));
            }

            info!(
                receipt_id = %request.receipt_id,
                existing_event_id = %existing.event_id,
                "Duplicate receipt_id detected with matching replay bindings - returning existing event"
            );

            return Ok(PrivilegedResponse::IngestReviewReceipt(
                IngestReviewReceiptResponse {
                    receipt_id: request.receipt_id.clone(),
                    event_type: to_response_event_type(&existing.event_type),
                    event_id: existing.event_id,
                },
            ));
        }

        // ---- Phase 3: Get HTF timestamp ----
        let timestamp_ns = match self.get_htf_timestamp_ns() {
            Ok(ts) => ts,
            Err(e) => {
                warn!(error = %e, "HTF timestamp generation failed - failing closed");
                return Ok(PrivilegedResponse::error(
                    PrivilegedErrorCode::CapabilityRequestRejected,
                    format!("HTF timestamp error: {e}"),
                ));
            },
        };

        // ---- Phase 4: Emit event based on verdict ----
        let (event_type, emit_result, emit_error_prefix) = match verdict {
            ReviewReceiptVerdict::Approve => (
                "ReviewReceiptRecorded".to_string(),
                self.event_emitter.emit_review_receipt(
                    &request.lease_id,
                    &request.receipt_id,
                    &request_changeset_digest_arr,
                    &request_artifact_bundle_hash_arr,
                    &authenticated_reviewer_id,
                    timestamp_ns,
                    &request_identity_proof_hash_arr,
                ),
                "review receipt emission failed",
            ),
            ReviewReceiptVerdict::Blocked => {
                let blocked_log_hash_arr = request_blocked_log_hash_arr
                    .expect("validated to be present for BLOCKED verdict");
                (
                    "ReviewBlockedRecorded".to_string(),
                    self.event_emitter.emit_review_blocked_receipt(
                        &request.lease_id,
                        &request.receipt_id,
                        &request_changeset_digest_arr,
                        &request_artifact_bundle_hash_arr,
                        request.blocked_reason_code,
                        &blocked_log_hash_arr,
                        &authenticated_reviewer_id,
                        timestamp_ns,
                        &request_identity_proof_hash_arr,
                    ),
                    "review blocked emission failed",
                )
            },
            ReviewReceiptVerdict::Unspecified => unreachable!("validated above"),
        };

        let signed_event = match emit_result {
            Ok(event) => event,
            Err(e) if e.to_string().contains("UNIQUE constraint") => {
                warn!(
                    receipt_id = %request.receipt_id,
                    verdict = %request_verdict,
                    error = %e,
                    "Concurrent duplicate receipt_id detected by UNIQUE constraint"
                );

                let Some(existing) = self
                    .event_emitter
                    .get_event_by_receipt_id(&request.receipt_id)
                else {
                    return Ok(PrivilegedResponse::error(
                        PrivilegedErrorCode::CapabilityRequestRejected,
                        format!(
                            "receipt_id '{}' hit a uniqueness race, but the existing event \
                             could not be resolved",
                            request.receipt_id
                        ),
                    ));
                };

                if let Err(message) = validate_receipt_replay(&existing) {
                    return Ok(PrivilegedResponse::error(
                        PrivilegedErrorCode::CapabilityRequestRejected,
                        message,
                    ));
                }

                info!(
                    receipt_id = %request.receipt_id,
                    existing_event_id = %existing.event_id,
                    "Concurrent duplicate receipt_id resolved as idempotent replay"
                );

                return Ok(PrivilegedResponse::IngestReviewReceipt(
                    IngestReviewReceiptResponse {
                        receipt_id: request.receipt_id.clone(),
                        event_type: to_response_event_type(&existing.event_type),
                        event_id: existing.event_id,
                    },
                ));
            },
            Err(e) => {
                return Err(ProtocolError::Serialization {
                    reason: format!("{emit_error_prefix}: {e}"),
                });
            },
        };

        info!(
            receipt_id = %request.receipt_id,
            event_type = %event_type,
            event_id = %signed_event.event_id,
            reviewer = %authenticated_reviewer_id,
            "Review receipt ingested successfully"
        );

        Ok(PrivilegedResponse::IngestReviewReceipt(
            IngestReviewReceiptResponse {
                receipt_id: request.receipt_id,
                event_type,
                event_id: signed_event.event_id,
            },
        ))
    }

    // ========================================================================
    // Process Management Handlers (TCK-00342)
    // ========================================================================

    /// Converts a `ProcessState` to the corresponding proto `ProcessStateEnum`
    /// i32 value.
    fn process_state_to_proto(state: &ProcessState) -> i32 {
        match state {
            ProcessState::Starting => ProcessStateEnum::ProcessStateStarting.into(),
            ProcessState::Running => ProcessStateEnum::ProcessStateRunning.into(),
            ProcessState::Unhealthy => ProcessStateEnum::ProcessStateUnhealthy.into(),
            ProcessState::Stopping => ProcessStateEnum::ProcessStateStopping.into(),
            ProcessState::Stopped { .. } => ProcessStateEnum::ProcessStateStopped.into(),
            ProcessState::Crashed { .. } => ProcessStateEnum::ProcessStateCrashed.into(),
            ProcessState::Terminated => ProcessStateEnum::ProcessStateTerminated.into(),
        }
    }

    /// Builds a `ProcessInfo` proto message from supervisor data.
    ///
    /// Collects state information across all instances of a process,
    /// using the first running instance's PID and uptime.
    #[allow(clippy::cast_possible_truncation)] // Instance count bounded by ProcessSpec.instances (u32)
    fn build_process_info(
        name: &str,
        spec_instances: u32,
        handles: &[&apm2_core::process::ProcessHandle],
    ) -> ProcessInfo {
        let running_instances = handles.iter().filter(|h| h.state.is_running()).count() as u32;

        // Use first running instance's PID
        let pid = handles.iter().find(|h| h.pid.is_some()).and_then(|h| h.pid);

        // Use first running instance's uptime
        #[allow(clippy::cast_sign_loss)] // .max(0) guarantees non-negative
        let uptime_secs = handles.iter().find_map(|h| {
            h.started_at.map(|started| {
                let elapsed = chrono::Utc::now().signed_duration_since(started);
                elapsed.num_seconds().max(0) as u64
            })
        });

        // Determine aggregate state: if any running, report first running
        // instance's state; otherwise use first handle's state.
        let state = handles
            .iter()
            .find(|h| h.state.is_running())
            .or_else(|| handles.first())
            .map_or_else(
                || ProcessStateEnum::ProcessStateUnspecified.into(),
                |h| Self::process_state_to_proto(&h.state),
            );

        // Collect exit code from first stopped/crashed handle.
        let exit_code = handles.iter().find_map(|h| match &h.state {
            ProcessState::Stopped { exit_code } | ProcessState::Crashed { exit_code } => *exit_code,
            _ => None,
        });

        ProcessInfo {
            name: name.to_string(),
            state,
            instances: spec_instances,
            running_instances,
            pid,
            uptime_secs,
            exit_code,
        }
    }

    /// Tries to acquire a read lock on daemon state.
    ///
    /// Returns an error response if daemon state is not configured (test mode)
    /// or if the lock is currently held for writing.
    #[allow(clippy::result_large_err)] // PrivilegedResponse is large by design; boxing would be a breaking change
    fn try_read_daemon_state(
        &self,
    ) -> Result<tokio::sync::RwLockReadGuard<'_, crate::state::DaemonState>, PrivilegedResponse>
    {
        let state = self.daemon_state.as_ref().ok_or_else(|| {
            PrivilegedResponse::error(
                PrivilegedErrorCode::PrivilegedErrorUnspecified,
                "process management not available (daemon state not configured)",
            )
        })?;

        state.try_read().ok_or_else(|| {
            PrivilegedResponse::error(
                PrivilegedErrorCode::PrivilegedErrorUnspecified,
                "daemon state temporarily unavailable (write lock held)",
            )
        })
    }

    /// Tries to acquire a write lock on daemon state.
    ///
    /// Returns an error response if daemon state is not configured (test mode)
    /// or if any lock is currently held.
    #[allow(clippy::result_large_err)] // PrivilegedResponse is large by design; boxing would be a breaking change
    fn try_write_daemon_state(
        &self,
    ) -> Result<tokio::sync::RwLockWriteGuard<'_, crate::state::DaemonState>, PrivilegedResponse>
    {
        let state = self.daemon_state.as_ref().ok_or_else(|| {
            PrivilegedResponse::error(
                PrivilegedErrorCode::PrivilegedErrorUnspecified,
                "process management not available (daemon state not configured)",
            )
        })?;

        state.try_write().ok_or_else(|| {
            PrivilegedResponse::error(
                PrivilegedErrorCode::PrivilegedErrorUnspecified,
                "daemon state temporarily unavailable (lock held)",
            )
        })
    }

    /// Handles `ListProcesses` requests (IPC-PRIV-005).
    ///
    /// Returns a list of all configured processes with their current state
    /// by querying the `Supervisor` in `DaemonState`.
    fn handle_list_processes(&self, payload: &[u8]) -> ProtocolResult<PrivilegedResponse> {
        // Decode request (empty, but validate format)
        let _request =
            ListProcessesRequest::decode_bounded(payload, &self.decode_config).map_err(|e| {
                ProtocolError::Serialization {
                    reason: format!("invalid ListProcessesRequest: {e}"),
                }
            })?;

        debug!("ListProcesses request received");

        let daemon_state = match self.try_read_daemon_state() {
            Ok(state) => state,
            Err(err_resp) => return Ok(err_resp),
        };

        let supervisor = daemon_state.supervisor();
        let names = supervisor.list_names();

        let mut processes = Vec::with_capacity(names.len());
        for name in &names {
            if let Some(spec) = supervisor.get_spec(name) {
                let handles = supervisor.get_handles(name);
                processes.push(Self::build_process_info(name, spec.instances, &handles));
            }
        }

        Ok(PrivilegedResponse::ListProcesses(ListProcessesResponse {
            processes,
        }))
    }

    /// Handles `ProcessStatus` requests (IPC-PRIV-006).
    ///
    /// Returns detailed status for a specific process by name, including
    /// restart count, CPU usage, memory usage, and command information.
    fn handle_process_status(&self, payload: &[u8]) -> ProtocolResult<PrivilegedResponse> {
        let request =
            ProcessStatusRequest::decode_bounded(payload, &self.decode_config).map_err(|e| {
                ProtocolError::Serialization {
                    reason: format!("invalid ProcessStatusRequest: {e}"),
                }
            })?;

        // Validate process name length (CTR-1303: bounded inputs)
        if request.name.len() > MAX_ID_LENGTH {
            return Ok(PrivilegedResponse::error(
                PrivilegedErrorCode::CapabilityRequestRejected,
                format!(
                    "process name too long: {} > {}",
                    request.name.len(),
                    MAX_ID_LENGTH
                ),
            ));
        }

        debug!(name = %request.name, "ProcessStatus request received");

        let daemon_state = match self.try_read_daemon_state() {
            Ok(state) => state,
            Err(err_resp) => return Ok(err_resp),
        };

        let supervisor = daemon_state.supervisor();
        let Some(spec) = supervisor.get_spec(&request.name) else {
            return Ok(PrivilegedResponse::error(
                PrivilegedErrorCode::CapabilityRequestRejected,
                format!("process not found: {}", request.name),
            ));
        };

        let handles = supervisor.get_handles(&request.name);
        let info = Self::build_process_info(&request.name, spec.instances, &handles);

        // Aggregate restart count across all instances
        let restart_count: u32 = handles.iter().map(|h| h.restart_count).sum();

        Ok(PrivilegedResponse::ProcessStatus(ProcessStatusResponse {
            info: Some(info),
            restart_count,
            cpu_percent: None,
            memory_bytes: None,
            command: spec.command.clone(),
            cwd: spec.cwd.as_ref().map(|p| p.display().to_string()),
        }))
    }

    /// Handles `StartProcess` requests (IPC-PRIV-007).
    ///
    /// Marks all stopped/crashed instances of a configured process as
    /// starting. Transitions instance states via the `Supervisor` so the
    /// daemon's run loop can pick them up for actual OS process spawning.
    fn handle_start_process(&self, payload: &[u8]) -> ProtocolResult<PrivilegedResponse> {
        let request =
            StartProcessRequest::decode_bounded(payload, &self.decode_config).map_err(|e| {
                ProtocolError::Serialization {
                    reason: format!("invalid StartProcessRequest: {e}"),
                }
            })?;

        // Validate process name length
        if request.name.len() > MAX_ID_LENGTH {
            return Ok(PrivilegedResponse::error(
                PrivilegedErrorCode::CapabilityRequestRejected,
                format!(
                    "process name too long: {} > {}",
                    request.name.len(),
                    MAX_ID_LENGTH
                ),
            ));
        }

        info!(name = %request.name, "StartProcess request received");

        let mut daemon_state = match self.try_write_daemon_state() {
            Ok(state) => state,
            Err(err_resp) => return Ok(err_resp),
        };

        let supervisor = daemon_state.supervisor();
        let Some(spec) = supervisor.get_spec(&request.name) else {
            return Ok(PrivilegedResponse::error(
                PrivilegedErrorCode::CapabilityRequestRejected,
                format!("process not found: {}", request.name),
            ));
        };
        let spec_instances = spec.instances;

        // Collect instances that are not already running
        let handles = supervisor.get_handles(&request.name);
        let startable_indices: Vec<u32> = handles
            .iter()
            .filter(|h| !h.state.is_running())
            .map(|h| h.instance)
            .collect();
        #[allow(clippy::cast_possible_truncation)] // bounded by spec.instances (u32)
        let startable_count = startable_indices.len() as u32;

        // Transition each startable instance to Starting state
        let supervisor = daemon_state.supervisor_mut();
        for idx in &startable_indices {
            supervisor.update_state(&request.name, *idx, ProcessState::Starting);
        }

        Ok(PrivilegedResponse::StartProcess(StartProcessResponse {
            name: request.name.clone(),
            instances_started: startable_count,
            message: format!(
                "scheduled {} instance(s) of '{}' for start (total configured: {})",
                startable_count, request.name, spec_instances
            ),
        }))
    }

    /// Handles `StopProcess` requests (IPC-PRIV-008).
    ///
    /// Marks all running instances of a process as stopping. Transitions
    /// instance states via the `Supervisor` for the daemon's run loop to
    /// perform actual shutdown.
    fn handle_stop_process(&self, payload: &[u8]) -> ProtocolResult<PrivilegedResponse> {
        let request =
            StopProcessRequest::decode_bounded(payload, &self.decode_config).map_err(|e| {
                ProtocolError::Serialization {
                    reason: format!("invalid StopProcessRequest: {e}"),
                }
            })?;

        // Validate process name length
        if request.name.len() > MAX_ID_LENGTH {
            return Ok(PrivilegedResponse::error(
                PrivilegedErrorCode::CapabilityRequestRejected,
                format!(
                    "process name too long: {} > {}",
                    request.name.len(),
                    MAX_ID_LENGTH
                ),
            ));
        }

        info!(name = %request.name, "StopProcess request received");

        let mut daemon_state = match self.try_write_daemon_state() {
            Ok(state) => state,
            Err(err_resp) => return Ok(err_resp),
        };

        let supervisor = daemon_state.supervisor();
        if supervisor.get_spec(&request.name).is_none() {
            return Ok(PrivilegedResponse::error(
                PrivilegedErrorCode::CapabilityRequestRejected,
                format!("process not found: {}", request.name),
            ));
        }

        // Collect running instance indices
        let handles = supervisor.get_handles(&request.name);
        let running_indices: Vec<u32> = handles
            .iter()
            .filter(|h| h.state.is_running())
            .map(|h| h.instance)
            .collect();
        #[allow(clippy::cast_possible_truncation)] // bounded by spec.instances (u32)
        let running_count = running_indices.len() as u32;

        // Transition each running instance to Stopping state
        let supervisor = daemon_state.supervisor_mut();
        for idx in &running_indices {
            supervisor.update_state(&request.name, *idx, ProcessState::Stopping);
        }

        Ok(PrivilegedResponse::StopProcess(StopProcessResponse {
            name: request.name.clone(),
            instances_stopped: running_count,
            message: format!(
                "scheduled {} running instance(s) of '{}' for stop",
                running_count, request.name
            ),
        }))
    }

    /// Handles `RestartProcess` requests (IPC-PRIV-009).
    ///
    /// Transitions all instances through a stop-then-start cycle. Running
    /// instances are marked as stopping first; stopped/crashed instances
    /// are marked as starting directly.
    fn handle_restart_process(&self, payload: &[u8]) -> ProtocolResult<PrivilegedResponse> {
        let request =
            RestartProcessRequest::decode_bounded(payload, &self.decode_config).map_err(|e| {
                ProtocolError::Serialization {
                    reason: format!("invalid RestartProcessRequest: {e}"),
                }
            })?;

        // Validate process name length
        if request.name.len() > MAX_ID_LENGTH {
            return Ok(PrivilegedResponse::error(
                PrivilegedErrorCode::CapabilityRequestRejected,
                format!(
                    "process name too long: {} > {}",
                    request.name.len(),
                    MAX_ID_LENGTH
                ),
            ));
        }

        info!(name = %request.name, "RestartProcess request received");

        let mut daemon_state = match self.try_write_daemon_state() {
            Ok(state) => state,
            Err(err_resp) => return Ok(err_resp),
        };

        let supervisor = daemon_state.supervisor();
        let Some(spec) = supervisor.get_spec(&request.name) else {
            return Ok(PrivilegedResponse::error(
                PrivilegedErrorCode::CapabilityRequestRejected,
                format!("process not found: {}", request.name),
            ));
        };
        let spec_instances = spec.instances;

        // Collect all instance indices and their current running status
        let handles = supervisor.get_handles(&request.name);
        let instance_transitions: Vec<(u32, bool)> = handles
            .iter()
            .map(|h| (h.instance, h.state.is_running()))
            .collect();

        // Transition: running -> Stopping, stopped/crashed -> Starting
        let supervisor = daemon_state.supervisor_mut();
        for (idx, is_running) in &instance_transitions {
            if *is_running {
                supervisor.update_state(&request.name, *idx, ProcessState::Stopping);
            } else {
                supervisor.update_state(&request.name, *idx, ProcessState::Starting);
            }
        }

        Ok(PrivilegedResponse::RestartProcess(RestartProcessResponse {
            name: request.name.clone(),
            instances_restarted: spec_instances,
            message: format!(
                "scheduled {} instance(s) of '{}' for restart",
                spec_instances, request.name
            ),
        }))
    }

    /// Handles `ReloadProcess` requests (IPC-PRIV-010).
    ///
    /// Performs a rolling restart (graceful reload) by marking the first
    /// running instance as stopping. The daemon's run loop handles the
    /// sequential restart of remaining instances to maintain availability.
    fn handle_reload_process(&self, payload: &[u8]) -> ProtocolResult<PrivilegedResponse> {
        let request =
            ReloadProcessRequest::decode_bounded(payload, &self.decode_config).map_err(|e| {
                ProtocolError::Serialization {
                    reason: format!("invalid ReloadProcessRequest: {e}"),
                }
            })?;

        // Validate process name length
        if request.name.len() > MAX_ID_LENGTH {
            return Ok(PrivilegedResponse::error(
                PrivilegedErrorCode::CapabilityRequestRejected,
                format!(
                    "process name too long: {} > {}",
                    request.name.len(),
                    MAX_ID_LENGTH
                ),
            ));
        }

        info!(name = %request.name, "ReloadProcess request received");

        let mut daemon_state = match self.try_write_daemon_state() {
            Ok(state) => state,
            Err(err_resp) => return Ok(err_resp),
        };

        let supervisor = daemon_state.supervisor();
        if supervisor.get_spec(&request.name).is_none() {
            return Ok(PrivilegedResponse::error(
                PrivilegedErrorCode::CapabilityRequestRejected,
                format!("process not found: {}", request.name),
            ));
        }

        // For rolling restart, mark the first running instance as stopping.
        // The daemon run loop will restart it and proceed to the next instance
        // sequentially.
        let handles = supervisor.get_handles(&request.name);
        let first_running = handles
            .iter()
            .find(|h| h.state.is_running())
            .map(|h| h.instance);

        if let Some(idx) = first_running {
            let supervisor = daemon_state.supervisor_mut();
            supervisor.update_state(&request.name, idx, ProcessState::Stopping);
        }

        Ok(PrivilegedResponse::ReloadProcess(ReloadProcessResponse {
            name: request.name.clone(),
            success: true,
            message: format!("rolling restart scheduled for '{}'", request.name),
        }))
    }

    // =========================================================================
    // Credential Management Handlers (CTR-PROTO-011, TCK-00343)
    // =========================================================================

    /// Converts a protobuf `CredentialProvider` enum to a core `Provider`.
    fn proto_provider_to_core(proto: i32) -> Provider {
        match ProtoProvider::try_from(proto) {
            Ok(ProtoProvider::Anthropic) => Provider::Claude,
            Ok(ProtoProvider::Openai) => Provider::OpenAI,
            // Github, ApiKey, Unspecified, and unknown values all map to Custom
            _ => Provider::Custom,
        }
    }

    /// Converts a core `Provider` to a protobuf `CredentialProvider` enum.
    const fn core_provider_to_proto(provider: Provider) -> i32 {
        match provider {
            Provider::Claude => ProtoProvider::Anthropic as i32,
            Provider::OpenAI => ProtoProvider::Openai as i32,
            // Gemini and Custom both map to the generic ApiKey provider
            Provider::Gemini | Provider::Custom => ProtoProvider::ApiKey as i32,
        }
    }

    /// Converts a protobuf auth method enum value and secret bytes into a core
    /// `AuthMethod`.
    ///
    /// # Errors
    ///
    /// Returns `ProtocolError::Serialization` if `secret` is not valid UTF-8.
    fn proto_auth_to_core(auth_method: i32, secret: &[u8]) -> Result<AuthMethod, ProtocolError> {
        let secret_str =
            String::from_utf8(secret.to_vec()).map_err(|_| ProtocolError::Serialization {
                reason: "credential_secret is not valid UTF-8".to_string(),
            })?;
        let auth = match ProtoAuthMethod::try_from(auth_method) {
            Ok(ProtoAuthMethod::Oauth) => AuthMethod::OAuth {
                access_token: SecretString::from(secret_str),
                refresh_token: None,
                expires_at: None,
                scopes: vec![],
            },
            Ok(ProtoAuthMethod::Ssh) => AuthMethod::SessionToken {
                token: SecretString::from(secret_str),
                cookie_jar: None,
                expires_at: None,
            },
            // Pat, ApiKey, Unspecified, and unknown values all map to ApiKey
            _ => AuthMethod::ApiKey {
                key: SecretString::from(secret_str),
            },
        };
        Ok(auth)
    }

    /// Converts a core `AuthMethod` to a protobuf auth method enum value.
    const fn core_auth_method_to_proto(auth: &AuthMethod) -> i32 {
        match auth {
            AuthMethod::OAuth { .. } => ProtoAuthMethod::Oauth as i32,
            AuthMethod::ApiKey { .. } => ProtoAuthMethod::ApiKey as i32,
            AuthMethod::SessionToken { .. } => ProtoAuthMethod::Pat as i32,
        }
    }

    /// Converts a core `CredentialProfile` to a protobuf `CredentialProfile`
    /// message (without secrets).
    fn core_profile_to_proto(
        profile: &CoreCredentialProfile,
        display_name: &str,
    ) -> super::messages::CredentialProfile {
        let expires_at = match &profile.auth {
            AuthMethod::OAuth { expires_at, .. } | AuthMethod::SessionToken { expires_at, .. } => {
                expires_at
                    .map(|dt| dt.timestamp().try_into().unwrap_or(0u64))
                    .unwrap_or(0)
            },
            AuthMethod::ApiKey { .. } => 0,
        };

        super::messages::CredentialProfile {
            profile_id: profile.id.as_str().to_string(),
            provider: Self::core_provider_to_proto(profile.provider),
            auth_method: Self::core_auth_method_to_proto(&profile.auth),
            created_at: profile.created_at.timestamp().try_into().unwrap_or(0u64),
            expires_at,
            is_active: !profile.is_expired(),
            display_name: display_name.to_string(),
        }
    }

    // =========================================================================
    // TCK-00394: ChangeSet Publishing (RFC-0018)
    // =========================================================================

    /// Handles `PublishChangeSet` requests (IPC-PRIV-017).
    ///
    /// Accepts a `ChangeSetBundleV1` payload, validates it, stores it in CAS,
    /// emits a `ChangeSetPublished` ledger event, and returns the
    /// `changeset_digest` and `cas_hash` for subsequent gate lease binding.
    ///
    /// # Security
    ///
    /// - `work_id` is validated against the work registry (fail-closed)
    /// - Bundle bytes are validated before CAS storage
    /// - Idempotent: re-publishing the same bundle returns the same digest
    ///   without duplicate ledger events
    /// - Domain-separated signing prevents cross-context replay
    /// - Actor ID is derived from peer credentials, not user input
    fn handle_publish_changeset(
        &self,
        payload: &[u8],
        ctx: &ConnectionContext,
    ) -> ProtocolResult<PrivilegedResponse> {
        let request = PublishChangeSetRequest::decode_bounded(payload, &self.decode_config)
            .map_err(|e| ProtocolError::Serialization {
                reason: format!("invalid PublishChangeSetRequest: {e}"),
            })?;

        info!(
            work_id = %request.work_id,
            bundle_size = request.bundle_bytes.len(),
            peer_pid = ?ctx.peer_credentials().map(|c| c.pid),
            "PublishChangeSet request received"
        );

        // --- Validation Phase (all checks BEFORE any state mutation) ---

        // Validate required fields
        if request.work_id.is_empty() {
            return Ok(PrivilegedResponse::error(
                PrivilegedErrorCode::CapabilityRequestRejected,
                "work_id is required",
            ));
        }

        if request.work_id.len() > MAX_ID_LENGTH {
            return Ok(PrivilegedResponse::error(
                PrivilegedErrorCode::CapabilityRequestRejected,
                format!("work_id exceeds maximum length of {MAX_ID_LENGTH} bytes"),
            ));
        }

        if request.bundle_bytes.is_empty() {
            return Ok(PrivilegedResponse::error(
                PrivilegedErrorCode::CapabilityRequestRejected,
                "bundle_bytes is required (empty payload)",
            ));
        }

        // Derive actor_id from peer credentials (not user input)
        let peer_creds = ctx
            .peer_credentials()
            .ok_or_else(|| ProtocolError::Serialization {
                reason: "peer credentials required for changeset publishing".to_string(),
            })?;
        let actor_id = derive_actor_id(peer_creds);

        // Validate work_id exists in registry (fail-closed: reject orphaned changesets)
        let Some(claim) = self.work_registry.get_claim(&request.work_id) else {
            return Ok(PrivilegedResponse::error(
                PrivilegedErrorCode::CapabilityRequestRejected,
                format!("work_id not found in registry: {}", request.work_id),
            ));
        };

        // TCK-00408: Actor ownership check — the authenticated caller must be
        // the actor that owns the work claim. This prevents unauthorized actors
        // from publishing changesets against work items they don't own.
        if claim.actor_id != actor_id {
            warn!(
                work_id = %request.work_id,
                claim_actor = %claim.actor_id,
                caller_actor = %actor_id,
                "PublishChangeSet rejected: caller does not own work claim"
            );
            return Ok(PrivilegedResponse::error(
                PrivilegedErrorCode::PermissionDenied,
                format!(
                    "authenticated caller '{}' does not own work claim for '{}' (owned by '{}')",
                    actor_id, request.work_id, claim.actor_id
                ),
            ));
        }

        let bundle: apm2_core::fac::ChangeSetBundleV1 =
            match serde_json::from_slice(&request.bundle_bytes) {
                Ok(bundle) => bundle,
                Err(e) => {
                    return Ok(PrivilegedResponse::error(
                        PrivilegedErrorCode::CapabilityRequestRejected,
                        format!("invalid ChangeSetBundleV1 JSON: {e}"),
                    ));
                },
            };

        // Recompute digest from canonical bundle fields and reject caller-provided
        // digest mismatches before any side effects.
        let computed_changeset_digest = bundle.compute_digest();
        let provided_changeset_digest = bundle.changeset_digest();
        if computed_changeset_digest != provided_changeset_digest {
            return Ok(PrivilegedResponse::error(
                PrivilegedErrorCode::CapabilityRequestRejected,
                format!(
                    "changeset_digest mismatch: expected {}, got {}",
                    hex::encode(computed_changeset_digest),
                    hex::encode(provided_changeset_digest)
                ),
            ));
        }

        // Full bundle validation is mandatory and fail-closed.
        if let Err(e) = bundle.validate() {
            return Ok(PrivilegedResponse::error(
                PrivilegedErrorCode::CapabilityRequestRejected,
                format!("invalid ChangeSetBundleV1: {e}"),
            ));
        }

        // Require CAS to be configured (fail-closed)
        let Some(cas) = &self.cas else {
            return Ok(PrivilegedResponse::error(
                PrivilegedErrorCode::CapabilityRequestRejected,
                "content-addressed store not configured on daemon",
            ));
        };

        let changeset_digest_hex = hex::encode(computed_changeset_digest);
        if let Some((event_id, persisted_cas_hash)) =
            self.find_changeset_published_replay(&request.work_id, &changeset_digest_hex)
        {
            debug!(
                work_id = %request.work_id,
                changeset_digest = %changeset_digest_hex,
                event_id = %event_id,
                "Idempotent: returning existing ChangeSetPublished event"
            );
            return Ok(PrivilegedResponse::PublishChangeSet(
                PublishChangeSetResponse {
                    changeset_digest: changeset_digest_hex,
                    cas_hash: persisted_cas_hash,
                    work_id: request.work_id,
                    event_id,
                },
            ));
        }

        // --- State Mutation Phase ---

        // Store bundle bytes in CAS
        let store_result = match cas.store(&request.bundle_bytes) {
            Ok(result) => result,
            Err(e) => {
                warn!(error = %e, "CAS store failed for changeset bundle");
                return Ok(PrivilegedResponse::error(
                    PrivilegedErrorCode::CapabilityRequestRejected,
                    format!("CAS storage failed: {e}"),
                ));
            },
        };

        let cas_hash = store_result.hash;
        let cas_hash_hex = hex::encode(cas_hash);

        // Get HTF-compliant timestamp
        let timestamp_ns = match self.get_htf_timestamp_ns() {
            Ok(ts) => ts,
            Err(e) => {
                warn!(error = %e, "HTF timestamp generation failed - failing closed");
                return Ok(PrivilegedResponse::error(
                    PrivilegedErrorCode::CapabilityRequestRejected,
                    format!("HTF timestamp error: {e}"),
                ));
            },
        };

        // Emit ChangeSetPublished ledger event
        let signed_event = match self.event_emitter.emit_changeset_published(
            &request.work_id,
            &computed_changeset_digest,
            &cas_hash,
            &actor_id,
            timestamp_ns,
        ) {
            Ok(event) => event,
            Err(e) => {
                // Race-safe idempotency fallback: if another writer persisted the
                // same semantic event concurrently, replay the persisted binding.
                if let Some((event_id, persisted_cas_hash)) =
                    self.find_changeset_published_replay(&request.work_id, &changeset_digest_hex)
                {
                    return Ok(PrivilegedResponse::PublishChangeSet(
                        PublishChangeSetResponse {
                            changeset_digest: changeset_digest_hex,
                            cas_hash: persisted_cas_hash,
                            work_id: request.work_id,
                            event_id,
                        },
                    ));
                }
                warn!(error = %e, "ChangeSetPublished event emission failed");
                return Ok(PrivilegedResponse::error(
                    PrivilegedErrorCode::CapabilityRequestRejected,
                    format!("changeset published event emission failed: {e}"),
                ));
            },
        };

        info!(
            event_id = %signed_event.event_id,
            work_id = %request.work_id,
            changeset_digest = %changeset_digest_hex,
            cas_hash = %cas_hash_hex,
            "ChangeSetPublished: bundle stored in CAS and event emitted to ledger"
        );

        Ok(PrivilegedResponse::PublishChangeSet(
            PublishChangeSetResponse {
                changeset_digest: changeset_digest_hex,
                cas_hash: cas_hash_hex,
                work_id: request.work_id,
                event_id: signed_event.event_id,
            },
        ))
    }

    /// Returns persisted replay bindings for a semantically matching
    /// `changeset_published` event.
    ///
    /// Matching key: `(work_id, changeset_digest)`. Response values are the
    /// authoritative persisted `event_id` and `cas_hash`.
    fn find_changeset_published_replay(
        &self,
        work_id: &str,
        changeset_digest_hex: &str,
    ) -> Option<(String, String)> {
        self.event_emitter
            .get_events_by_work_id(work_id)
            .into_iter()
            .filter(|event| event.event_type == "changeset_published")
            .find_map(|event| {
                let payload = serde_json::from_slice::<serde_json::Value>(&event.payload).ok()?;
                let persisted_digest = payload.get("changeset_digest")?.as_str()?;
                if persisted_digest != changeset_digest_hex {
                    return None;
                }
                let persisted_cas_hash = payload.get("cas_hash")?.as_str()?.to_string();
                Some((event.event_id, persisted_cas_hash))
            })
    }

    // =========================================================================
    // TCK-00340: DelegateSublease Handler
    // =========================================================================

    /// Handles `DelegateSublease` requests (IPC-PRIV-072, TCK-00340).
    ///
    /// Delegates a sublease from a parent gate lease to a child holon,
    /// validating strict-subset constraints before signing the sublease.
    ///
    /// # Security Invariants
    ///
    /// - **Admission before mutation**: Parent lease existence and validity are
    ///   checked before any sublease issuance.
    /// - **Fail-closed**: Any validation failure rejects the request.
    /// - **Gate-scope enforcement**: Sublease inherits the parent's `gate_id`.
    /// - **Strict-subset**: Sublease constraints cannot exceed parent bounds.
    fn handle_delegate_sublease(
        &self,
        payload: &[u8],
        ctx: &ConnectionContext,
    ) -> ProtocolResult<PrivilegedResponse> {
        let request = DelegateSubleaseRequest::decode_bounded(payload, &self.decode_config)
            .map_err(|e| ProtocolError::Serialization {
                reason: format!("invalid DelegateSubleaseRequest: {e}"),
            })?;

        info!(
            parent_lease_id = %request.parent_lease_id,
            delegatee_actor_id = %request.delegatee_actor_id,
            sublease_id = %request.sublease_id,
            requested_expiry_ns = request.requested_expiry_ns,
            "DelegateSublease request received"
        );

        // ---- Phase 0a: Caller authorization (fail-closed) ----
        //
        // SECURITY: Verify the caller is authorized to delegate from the
        // parent lease. The caller's identity (derived from peer credentials)
        // must match the parent lease's `executor_actor_id` — only the
        // lease holder can delegate subleases from their lease.
        let Some(peer_creds) = ctx.peer_credentials() else {
            return Ok(PrivilegedResponse::error(
                PrivilegedErrorCode::PermissionDenied,
                "peer credentials required for sublease delegation",
            ));
        };
        let caller_actor_id = derive_actor_id(peer_creds);

        // ---- Phase 0b: Validate required fields (admission checks) ----

        if request.parent_lease_id.is_empty() {
            return Ok(PrivilegedResponse::error(
                PrivilegedErrorCode::CapabilityRequestRejected,
                "parent_lease_id is required",
            ));
        }
        if request.parent_lease_id.len() > MAX_ID_LENGTH {
            return Ok(PrivilegedResponse::error(
                PrivilegedErrorCode::CapabilityRequestRejected,
                format!("parent_lease_id exceeds maximum length of {MAX_ID_LENGTH} bytes"),
            ));
        }

        if request.delegatee_actor_id.is_empty() {
            return Ok(PrivilegedResponse::error(
                PrivilegedErrorCode::CapabilityRequestRejected,
                "delegatee_actor_id is required",
            ));
        }
        if request.delegatee_actor_id.len() > MAX_ID_LENGTH {
            return Ok(PrivilegedResponse::error(
                PrivilegedErrorCode::CapabilityRequestRejected,
                format!("delegatee_actor_id exceeds maximum length of {MAX_ID_LENGTH} bytes"),
            ));
        }

        // REQ-0010: Identity-bearing authoritative requests MUST carry
        // proof-carrying pointers.
        //
        // SECURITY (TCK-00356 Fix 1): Validate the identity proof hash using
        // the centralized validator which enforces non-zero commitment and
        // correct length. Phase 1 (pre-CAS transport) validates the hash as
        // a binding commitment; full proof dereference requires CAS
        // integration (TCK-00359).
        if let Err(e) = crate::identity::validate_identity_proof_hash(&request.identity_proof_hash)
        {
            return Ok(PrivilegedResponse::error(
                PrivilegedErrorCode::CapabilityRequestRejected,
                format!("identity_proof_hash validation failed: {e}"),
            ));
        }
        let request_identity_proof_hash: [u8; 32] = request
            .identity_proof_hash
            .as_slice()
            .try_into()
            .expect("validated to be 32 bytes by validate_identity_proof_hash above");

        // WVR-0103: Log once that identity proof hash is validated as
        // shape-only commitment (Phase 1 / pre-CAS transport).
        {
            static SUBLEASE_PROOF_WAIVER_WARN: std::sync::Once = std::sync::Once::new();
            SUBLEASE_PROOF_WAIVER_WARN.call_once(|| {
                warn!(
                    waiver = "WVR-0103",
                    "identity proof hash validated as shape-only commitment; \
                     full CAS dereference + IdentityProofV1::verify() deferred (WVR-0103)"
                );
            });
        }

        if request.sublease_id.is_empty() {
            return Ok(PrivilegedResponse::error(
                PrivilegedErrorCode::CapabilityRequestRejected,
                "sublease_id is required",
            ));
        }
        if request.sublease_id.len() > MAX_ID_LENGTH {
            return Ok(PrivilegedResponse::error(
                PrivilegedErrorCode::CapabilityRequestRejected,
                format!("sublease_id exceeds maximum length of {MAX_ID_LENGTH} bytes"),
            ));
        }

        if request.requested_expiry_ns == 0 {
            return Ok(PrivilegedResponse::error(
                PrivilegedErrorCode::CapabilityRequestRejected,
                "requested_expiry_ns must be non-zero",
            ));
        }

        // ---- Phase 1: Gate orchestrator availability ----
        let Some(gate_orchestrator) = &self.gate_orchestrator else {
            return Ok(PrivilegedResponse::error(
                PrivilegedErrorCode::CapabilityRequestRejected,
                "gate orchestrator not configured",
            ));
        };

        // ---- Phase 2: Retrieve parent lease (admission check) ----
        //
        // SECURITY (v7 Finding 1 — Trust Model for Persisted Leases):
        //
        // The parent lease is deserialized from the daemon-owned SQLite ledger.
        // We do NOT call `GateLease::validate_signature()` here because the
        // ledger is a **same-process trusted store**: only the daemon writes
        // to it (via authenticated IPC handlers that enforce admission checks
        // before persisting). The SQLite file is not externally writable.
        //
        // The GateLease's `issuer_signature` (Ed25519 over canonical bytes)
        // IS preserved in the serialized `full_lease` field and can be
        // verified by downstream consumers that need cryptographic proof of
        // issuance (e.g., cross-node replication). Within the daemon process,
        // the ledger's integrity is guaranteed by the process trust boundary.
        //
        // See `SqliteLeaseValidator::register_full_lease()` in ledger.rs for
        // the full trust model documentation (v6 Finding 2).
        //
        // TODO(RFC-0019): When cross-node lease federation is implemented,
        // add signature verification here for leases received from external
        // nodes (where the process trust boundary does not apply).
        let Some(parent_lease) = self
            .lease_validator
            .get_gate_lease(&request.parent_lease_id)
        else {
            return Ok(PrivilegedResponse::error(
                PrivilegedErrorCode::GateLeaseMissing,
                format!("parent gate lease not found: {}", request.parent_lease_id),
            ));
        };

        // ---- Phase 2a: Caller authorization against parent lease ----
        //
        // SECURITY: The caller must be the executor authorized by the parent
        // lease, OR the issuer of the parent lease. Only the lease holder
        // (executor) or the lease issuer should be able to delegate subleases.
        // This prevents confused-deputy / capability laundering attacks where
        // an unauthorized caller delegates from another entity's lease.
        if parent_lease.executor_actor_id != caller_actor_id
            && parent_lease.issuer_actor_id != caller_actor_id
        {
            warn!(
                parent_lease_id = %request.parent_lease_id,
                caller = %caller_actor_id,
                executor = %parent_lease.executor_actor_id,
                issuer = %parent_lease.issuer_actor_id,
                "Caller not authorized to delegate from parent lease"
            );
            return Ok(PrivilegedResponse::error(
                PrivilegedErrorCode::PermissionDenied,
                format!(
                    "caller is not authorized to delegate from lease '{}'",
                    request.parent_lease_id
                ),
            ));
        }

        // ---- Phase 2b: Sublease ID uniqueness check (admission before mutation) ----
        //
        // SECURITY (v10 — Defense-in-Depth Uniqueness):
        //
        // This application-level check provides the idempotent fast-path for
        // duplicate sublease requests. However, since `dispatch()` takes
        // `&self` (not `&mut self`), concurrent dispatch of two requests
        // with the same `sublease_id` could theoretically race past this
        // check. Two database-level constraints enforce authoritative
        // uniqueness (see `SqliteLedgerEventEmitter::init_schema`):
        //
        // 1. `idx_unique_full_lease_id`: Partial unique index on `json_extract(payload,
        //    '$.lease_id')` for `gate_lease_issued` events with `full_lease`. This
        //    prevents duplicate lease persistence via `register_full_lease`.
        //
        // 2. `idx_unique_sublease_issued`: Partial unique index on `(event_type,
        //    work_id)` for `SubleaseIssued` events. For these events, `work_id` =
        //    `sublease_id`, so the index enforces at-most-once event emission per
        //    sublease.
        //
        // If a duplicate races past the application check, either
        // `register_full_lease` or `emit_session_event` will fail with a
        // UNIQUE constraint violation, and the handler will fail-closed or
        // resolve via the idempotent duplicate path.
        if let Some(existing) = self.lease_validator.get_gate_lease(&request.sublease_id) {
            // Idempotent return: if the existing sublease has identical
            // parameters, return it. Otherwise reject as a conflict.
            // Compare executor_actor_id of existing sublease against the
            // requested delegatee — these are intentionally different field
            // names (the sublease's executor IS the delegatee).
            //
            // SECURITY (v6 Finding 3): The idempotent equality check MUST
            // cover the FULL request tuple, not just (work_id, gate_id,
            // delegatee). Omitting parent_lease_id or requested_expiry_ns
            // would allow a request with a different parent lineage or
            // different expiry to silently receive the cached sublease.
            //
            // Since GateLease does not store parent_lease_id directly, we
            // verify parent lineage by comparing the sublease's inherited
            // changeset_digest and policy_hash against the current parent
            // lease's values. A different parent_lease_id will have different
            // policy/changeset hashes (or fail the v5 revalidation below).
            //
            // For requested_expiry_ns, convert to ms and compare against
            // the existing sublease's expires_at (which is stored in ms).
            let delegatee_matches = existing.executor_actor_id == request.delegatee_actor_id;
            let expiry_ms = request.requested_expiry_ns / 1_000_000;
            let expiry_matches = existing.expires_at == expiry_ms;
            let lineage_matches = existing.changeset_digest == parent_lease.changeset_digest
                && existing.policy_hash == parent_lease.policy_hash;
            if existing.work_id == parent_lease.work_id
                && existing.gate_id == parent_lease.gate_id
                && delegatee_matches
                && expiry_matches
                && lineage_matches
            {
                // SECURITY (v5 Finding 1): Revalidate the existing sublease
                // against the PROVIDED parent lease. A stale sublease issued
                // under a previous parent boundary must not be returned if it
                // violates the current parent's constraints (time bounds,
                // policy hash, changeset digest, AAT extension).
                if let Err(e) = crate::gate::GateOrchestrator::validate_sublease_delegation(
                    &parent_lease,
                    &existing,
                ) {
                    warn!(
                        sublease_id = %request.sublease_id,
                        parent_lease_id = %request.parent_lease_id,
                        error = %e,
                        "Idempotent sublease failed revalidation against current parent lease"
                    );
                    return Ok(PrivilegedResponse::error(
                        PrivilegedErrorCode::CapabilityRequestRejected,
                        format!(
                            "existing sublease '{}' is incompatible with parent lease '{}': {e}",
                            request.sublease_id, request.parent_lease_id
                        ),
                    ));
                }

                // Look up the original SubleaseIssued event from the
                // ledger. The emit_session_event call stored the event with
                // work_id = sublease.lease_id, so we query by that key and
                // filter for event_type == "SubleaseIssued".
                //
                // SECURITY (v7 Finding 1 — Fail-Closed Idempotent Replay):
                //
                // The response contract defines event_id as the ledger event
                // identity for SubleaseIssued. If the original event cannot be
                // found, we MUST fail-closed rather than returning an empty
                // event_id. An empty event_id would violate the response
                // contract and prevent callers from verifying the ledger trail
                // of the sublease. This can happen if the ledger was truncated
                // or if there's a storage inconsistency.
                let Some(original_event) = self
                    .event_emitter
                    .get_events_by_work_id(&existing.lease_id)
                    .into_iter()
                    .find(|e| e.event_type == "SubleaseIssued")
                else {
                    warn!(
                        sublease_id = %request.sublease_id,
                        "Idempotent sublease replay failed: original SubleaseIssued event not found in ledger"
                    );
                    return Ok(PrivilegedResponse::error(
                        PrivilegedErrorCode::CapabilityRequestRejected,
                        format!(
                            "sublease '{}' exists but its original SubleaseIssued ledger event \
                             is missing — cannot verify ledger trail",
                            request.sublease_id
                        ),
                    ));
                };

                // SECURITY (v12): Idempotent replay must verify both lineage
                // (`parent_lease_id`) and proof pointer (`identity_proof_hash`)
                // extracted from the persisted `SubleaseIssued` payload.
                let (original_parent_id, original_identity_proof_hash) =
                    match extract_sublease_replay_bindings(&original_event.payload) {
                        Ok(values) => values,
                        Err(e) => {
                            warn!(
                                sublease_id = %request.sublease_id,
                                error = %e,
                                "Idempotent sublease replay rejected: cannot extract replay bindings"
                            );
                            return Ok(PrivilegedResponse::error(
                                PrivilegedErrorCode::CapabilityRequestRejected,
                                format!(
                                    "sublease '{}' exists but replay bindings could not be extracted \
                                     from its event payload: {e}",
                                    request.sublease_id
                                ),
                            ));
                        },
                    };

                if original_parent_id != request.parent_lease_id {
                    warn!(
                        sublease_id = %request.sublease_id,
                        original_parent = %original_parent_id,
                        requested_parent = %request.parent_lease_id,
                        "Idempotent sublease replay rejected: parent_lease_id mismatch"
                    );
                    return Ok(PrivilegedResponse::error(
                        PrivilegedErrorCode::CapabilityRequestRejected,
                        format!(
                            "sublease '{}' was originally delegated from parent '{}', \
                             not '{}'",
                            request.sublease_id, original_parent_id, request.parent_lease_id
                        ),
                    ));
                }

                if original_identity_proof_hash != request_identity_proof_hash {
                    warn!(
                        sublease_id = %request.sublease_id,
                        original_identity_proof_hash = %hex::encode(original_identity_proof_hash),
                        requested_identity_proof_hash = %hex::encode(request_identity_proof_hash),
                        "Idempotent sublease replay rejected: identity_proof_hash mismatch"
                    );
                    return Ok(PrivilegedResponse::error(
                        PrivilegedErrorCode::CapabilityRequestRejected,
                        format!(
                            "sublease '{}' was originally delegated with identity_proof_hash '{}', \
                             not '{}'",
                            request.sublease_id,
                            hex::encode(original_identity_proof_hash),
                            hex::encode(request_identity_proof_hash)
                        ),
                    ));
                }

                info!(
                    sublease_id = %request.sublease_id,
                    event_id = %original_event.event_id,
                    "Sublease already exists with identical parameters - idempotent return"
                );
                // Convert ms -> ns for response (existing.expires_at is in ms)
                return Ok(PrivilegedResponse::DelegateSublease(
                    DelegateSubleaseResponse {
                        sublease_id: existing.lease_id.clone(),
                        parent_lease_id: request.parent_lease_id,
                        delegatee_actor_id: request.delegatee_actor_id,
                        gate_id: existing.gate_id.clone(),
                        expires_at_ns: existing.expires_at.saturating_mul(1_000_000),
                        event_id: original_event.event_id,
                    },
                ));
            }
            warn!(
                sublease_id = %request.sublease_id,
                "Sublease ID conflict: existing sublease has different parameters"
            );
            return Ok(PrivilegedResponse::error(
                PrivilegedErrorCode::CapabilityRequestRejected,
                format!(
                    "sublease_id '{}' already exists with different parameters",
                    request.sublease_id
                ),
            ));
        }
        // Also check if a lease (non-sublease) with this ID already exists
        // via the executor lookup (covers leases registered through
        // register_lease_with_executor).
        if self
            .lease_validator
            .get_lease_executor_actor_id(&request.sublease_id)
            .is_some()
        {
            return Ok(PrivilegedResponse::error(
                PrivilegedErrorCode::CapabilityRequestRejected,
                format!(
                    "sublease_id '{}' conflicts with an existing lease",
                    request.sublease_id
                ),
            ));
        }

        // ---- Phase 3: Get HTF timestamp ----
        let timestamp_ns = match self.get_htf_timestamp_ns() {
            Ok(ts) => ts,
            Err(e) => {
                warn!(error = %e, "HTF timestamp generation failed - failing closed");
                return Ok(PrivilegedResponse::error(
                    PrivilegedErrorCode::CapabilityRequestRejected,
                    format!("HTF timestamp error: {e}"),
                ));
            },
        };

        // ---- Phase 4: Issue sublease via orchestrator (validates strict-subset) ----
        //
        // SECURITY (v5 Finding 2): Convert nanosecond values from the proto
        // wire format to milliseconds at the handler boundary. GateLease uses
        // milliseconds internally (issued_at, expires_at), but the proto
        // fields are `requested_expiry_ns` / `expires_at_ns`. Passing raw
        // nanosecond values through would cause temporal comparison mismatches
        // against parent lease bounds (which are in milliseconds).
        let expiry_millis = request.requested_expiry_ns / 1_000_000;
        let issued_at_millis = timestamp_ns / 1_000_000;
        let sublease = match gate_orchestrator.issue_delegated_sublease(
            &parent_lease,
            &request.sublease_id,
            &request.delegatee_actor_id,
            issued_at_millis,
            expiry_millis,
        ) {
            Ok(lease) => lease,
            Err(e) => {
                warn!(
                    parent_lease_id = %request.parent_lease_id,
                    error = %e,
                    "Sublease delegation rejected"
                );
                return Ok(PrivilegedResponse::error(
                    PrivilegedErrorCode::CapabilityRequestRejected,
                    format!("sublease delegation failed: {e}"),
                ));
            },
        };

        // ---- Phase 5: Persist sublease BEFORE event emission ----
        //
        // SECURITY (v10 BLOCKER 1 -- Non-atomic DelegateSublease fix):
        //
        // Lease persistence MUST happen BEFORE event emission. If we emitted
        // the SubleaseIssued event first and register_full_lease then failed,
        // we'd have an orphan event in the ledger with no corresponding lease
        // anchor. Retries would emit duplicate events. By persisting the
        // lease first:
        //   - If register_full_lease fails, no event is emitted (clean).
        //   - If emit_session_event fails after successful persistence, the lease
        //     exists (authoritative state) and we log a warning but return success --
        //     the event is an audit trail that follows.
        //
        // SECURITY (v10 BLOCKER 2 -- Sublease ID uniqueness via DB):
        //
        // register_full_lease is the authoritative uniqueness gate. The
        // pre-check in Phase 2b (get_gate_lease) is a fast-path optimization
        // only. If two concurrent requests pass the pre-check, the first to
        // call register_full_lease succeeds and the second gets a duplicate
        // error, which we handle as idempotent (look up existing lease and
        // return it) or reject as conflict.
        if let Err(e) = self.lease_validator.register_full_lease(&sublease) {
            // Check if this is a duplicate: the lease_id already exists.
            // If it does, treat as idempotent if the existing lease matches
            // ALL fields including parent lineage. Otherwise reject as conflict.
            if let Some(existing) = self.lease_validator.get_gate_lease(&sublease.lease_id) {
                // SECURITY (v11 BLOCKER 2 -- Full-Field Duplicate Validation):
                //
                // The duplicate resolution after register_full_lease failure
                // MUST verify the SAME fields as the Phase 2b pre-check path:
                // work_id, gate_id, executor_actor_id, expires_at, AND parent
                // lineage (changeset_digest, policy_hash, parent_lease_id).
                //
                // Without parent lineage verification, a concurrent request
                // with a different parent_lease_id could silently receive an
                // idempotent response for a sublease delegated from a different
                // parent, enabling confused-deputy / capability laundering.
                let fields_match = existing.work_id == sublease.work_id
                    && existing.gate_id == sublease.gate_id
                    && existing.executor_actor_id == sublease.executor_actor_id
                    && existing.expires_at == sublease.expires_at
                    && existing.changeset_digest == sublease.changeset_digest
                    && existing.policy_hash == sublease.policy_hash
                    && existing.issuer_actor_id == sublease.issuer_actor_id;

                if !fields_match {
                    warn!(
                        sublease_id = %sublease.lease_id,
                        error = %e,
                        "Lease persistence duplicate with field mismatch -- conflict"
                    );
                    return Ok(PrivilegedResponse::error(
                        PrivilegedErrorCode::CapabilityRequestRejected,
                        format!(
                            "sublease_id '{}' already exists with different parameters",
                            sublease.lease_id
                        ),
                    ));
                }

                // SECURITY (v11 BLOCKER 2 -- Parent Lineage Binding):
                //
                // Look up the original SubleaseIssued event and verify its
                // parent_lease_id matches the current request. This mirrors
                // the Phase 2b lineage verification logic: even if
                // changeset_digest/policy_hash match, two different parent
                // leases could share those inherited cryptographic fields.
                let Some(original_event) = self
                    .event_emitter
                    .get_events_by_work_id(&existing.lease_id)
                    .into_iter()
                    .find(|ev| ev.event_type == "SubleaseIssued")
                else {
                    // SECURITY (v11 MAJOR 1 -- Fail-Closed Event Lookup):
                    //
                    // If the original SubleaseIssued event cannot be found,
                    // we MUST fail-closed. Returning an empty event_id would
                    // violate the response contract and prevent callers from
                    // verifying the ledger trail.
                    warn!(
                        sublease_id = %sublease.lease_id,
                        "Concurrent duplicate sublease: original SubleaseIssued \
                         event not found in ledger -- failing closed"
                    );
                    return Ok(PrivilegedResponse::error(
                        PrivilegedErrorCode::CapabilityRequestRejected,
                        format!(
                            "sublease '{}' exists but its original SubleaseIssued \
                             ledger event is missing -- cannot verify ledger trail",
                            sublease.lease_id
                        ),
                    ));
                };

                // SECURITY (v12): Re-check persisted replay bindings
                // (`parent_lease_id` + `identity_proof_hash`) for the
                // duplicate detected at persistence time.
                let (original_parent_id, original_identity_proof_hash) =
                    match extract_sublease_replay_bindings(&original_event.payload) {
                        Ok(values) => values,
                        Err(extract_err) => {
                            warn!(
                                sublease_id = %sublease.lease_id,
                                error = %extract_err,
                                "Concurrent duplicate sublease: cannot extract replay bindings -- failing closed"
                            );
                            return Ok(PrivilegedResponse::error(
                                PrivilegedErrorCode::CapabilityRequestRejected,
                                format!(
                                    "sublease '{}' exists but replay bindings could not be \
                                     extracted from its event payload: {extract_err}",
                                    sublease.lease_id
                                ),
                            ));
                        },
                    };

                if original_parent_id != request.parent_lease_id {
                    warn!(
                        sublease_id = %sublease.lease_id,
                        original_parent = %original_parent_id,
                        requested_parent = %request.parent_lease_id,
                        "Concurrent duplicate sublease: parent_lease_id mismatch"
                    );
                    return Ok(PrivilegedResponse::error(
                        PrivilegedErrorCode::CapabilityRequestRejected,
                        format!(
                            "sublease '{}' was originally delegated from parent \
                             '{}', not '{}'",
                            sublease.lease_id, original_parent_id, request.parent_lease_id
                        ),
                    ));
                }

                if original_identity_proof_hash != request_identity_proof_hash {
                    warn!(
                        sublease_id = %sublease.lease_id,
                        original_identity_proof_hash = %hex::encode(original_identity_proof_hash),
                        requested_identity_proof_hash = %hex::encode(request_identity_proof_hash),
                        "Concurrent duplicate sublease: identity_proof_hash mismatch"
                    );
                    return Ok(PrivilegedResponse::error(
                        PrivilegedErrorCode::CapabilityRequestRejected,
                        format!(
                            "sublease '{}' was originally delegated with identity_proof_hash \
                             '{}', not '{}'",
                            sublease.lease_id,
                            hex::encode(original_identity_proof_hash),
                            hex::encode(request_identity_proof_hash)
                        ),
                    ));
                }

                info!(
                    sublease_id = %sublease.lease_id,
                    event_id = %original_event.event_id,
                    "Concurrent duplicate sublease persisted -- idempotent return"
                );
                return Ok(PrivilegedResponse::DelegateSublease(
                    DelegateSubleaseResponse {
                        sublease_id: existing.lease_id,
                        parent_lease_id: request.parent_lease_id,
                        delegatee_actor_id: request.delegatee_actor_id,
                        gate_id: existing.gate_id,
                        expires_at_ns: existing.expires_at.saturating_mul(1_000_000),
                        event_id: original_event.event_id,
                    },
                ));
            }
            warn!(
                sublease_id = %sublease.lease_id,
                error = %e,
                "Lease persistence failed -- failing closed without emitting event"
            );
            return Ok(PrivilegedResponse::error(
                PrivilegedErrorCode::CapabilityRequestRejected,
                format!(
                    "sublease '{}' lease persistence failed: {e} \
                     -- no event emitted (fail-closed)",
                    sublease.lease_id
                ),
            ));
        }

        // ---- Phase 6: Emit SubleaseIssued event (after successful persistence) ----
        //
        // SECURITY (v5 Finding 3): The event's actor_id MUST be the
        // authenticated caller (from peer credentials), NOT the
        // caller-controlled `delegatee_actor_id`. The delegatee is included
        // as a separate field in the event payload so the audit trail records
        // BOTH who performed the delegation and who received the sublease.
        //
        // SECURITY (TCK-00356 Fix 1): identity_proof_hash is included in
        // the signed event payload so it is audit-bound and cannot be
        // stripped post-signing.
        let event_payload = serde_json::json!({
            "parent_lease_id": request.parent_lease_id,
            "sublease_id": sublease.lease_id,
            "delegator_actor_id": caller_actor_id,
            "delegatee_actor_id": request.delegatee_actor_id,
            "gate_id": sublease.gate_id,
            "work_id": sublease.work_id,
            "expires_at": sublease.expires_at,
            "issued_at": sublease.issued_at,
            "identity_proof_hash": hex::encode(&request.identity_proof_hash),
        });
        // SECURITY (v10 MAJOR — Fail-closed serialization):
        //
        // Event payload serialization MUST NOT silently produce empty bytes.
        // An empty payload would persist a signed event without
        // lineage-critical fields (parent_lease_id, delegatee, gate_id),
        // making audit verification impossible. Fail closed instead.
        let event_payload_bytes = match serde_json::to_vec(&event_payload) {
            Ok(bytes) => bytes,
            Err(e) => {
                warn!(
                    sublease_id = %sublease.lease_id,
                    error = %e,
                    "SubleaseIssued event payload serialization failed -- failing closed"
                );
                return Ok(PrivilegedResponse::error(
                    PrivilegedErrorCode::CapabilityRequestRejected,
                    format!(
                        "sublease '{}' lease persisted but event payload serialization \
                         failed: {e} -- failing closed",
                        sublease.lease_id
                    ),
                ));
            },
        };

        let signed_event = match self.event_emitter.emit_session_event(
            &sublease.lease_id,
            "SubleaseIssued",
            &event_payload_bytes,
            &caller_actor_id,
            timestamp_ns,
        ) {
            Ok(evt) => evt,
            Err(e) => {
                // SECURITY (v10 BLOCKER 2 — Fail-closed on emission failure):
                //
                // The response contract requires `event_id` to be the ledger
                // identity of the `SubleaseIssued` event. Returning success
                // with an empty `event_id` violates proof-carrying effect
                // guarantees and breaks audit verification.
                //
                // Even though the lease is persisted (authoritative state),
                // we MUST fail-closed here because:
                // 1. The proto contract defines `event_id` as a required ledger event identity
                //    for `SubleaseIssued`.
                // 2. A retry will hit the idempotent `register_full_lease` duplicate path,
                //    which will find the existing lease and attempt to look up the event. If
                //    the event still cannot be emitted, the retry will also fail-closed.
                // 3. The persisted lease anchor ensures no authority is lost; the caller simply
                //    needs to retry when the event emitter recovers.
                warn!(
                    sublease_id = %sublease.lease_id,
                    error = %e,
                    "SubleaseIssued event emission failed after lease persistence \
                     -- failing closed (lease persisted, event missing)"
                );
                return Ok(PrivilegedResponse::error(
                    PrivilegedErrorCode::CapabilityRequestRejected,
                    format!(
                        "sublease '{}' lease persisted but SubleaseIssued event \
                         emission failed: {e} -- retry to complete",
                        sublease.lease_id
                    ),
                ));
            },
        };

        info!(
            parent_lease_id = %request.parent_lease_id,
            sublease_id = %sublease.lease_id,
            gate_id = %sublease.gate_id,
            event_id = %signed_event.event_id,
            "Sublease delegation completed successfully"
        );

        // Convert ms -> ns for the proto response wire format.
        Ok(PrivilegedResponse::DelegateSublease(
            DelegateSubleaseResponse {
                sublease_id: sublease.lease_id,
                parent_lease_id: request.parent_lease_id,
                delegatee_actor_id: request.delegatee_actor_id,
                gate_id: sublease.gate_id,
                expires_at_ns: sublease.expires_at.saturating_mul(1_000_000),
                event_id: signed_event.event_id,
            },
        ))
    }

    #[allow(clippy::result_large_err)] // Error variant is PrivilegedResponse, matching dispatch pattern
    fn require_credential_store(&self) -> Result<&CredentialStore, PrivilegedResponse> {
        self.credential_store.as_deref().ok_or_else(|| {
            PrivilegedResponse::error(
                PrivilegedErrorCode::CredentialInvalidConfig,
                "credential store not configured on daemon",
            )
        })
    }

    /// Handles `ListCredentials` requests (IPC-PRIV-021).
    ///
    /// Lists all credential profiles. Secrets are never included in responses.
    fn handle_list_credentials(
        &self,
        payload: &[u8],
        _ctx: &ConnectionContext,
    ) -> ProtocolResult<PrivilegedResponse> {
        let request = ListCredentialsRequest::decode_bounded(payload, &self.decode_config)
            .map_err(|e| ProtocolError::Serialization {
                reason: format!("invalid ListCredentialsRequest: {e}"),
            })?;

        debug!("ListCredentials handler invoked");

        let store = match self.require_credential_store() {
            Ok(s) => s,
            Err(resp) => return Ok(resp),
        };

        let profile_ids = match store.list() {
            Ok(ids) => ids,
            Err(e) => {
                warn!(error = %e, "failed to list credential profiles");
                return Ok(PrivilegedResponse::error(
                    PrivilegedErrorCode::CredentialInvalidConfig,
                    format!("failed to list credentials: {e}"),
                ));
            },
        };

        let mut profiles = Vec::new();
        for pid in &profile_ids {
            match store.get(pid) {
                Ok(profile) => {
                    // Apply provider filter if specified
                    if let Some(filter) = request.provider_filter {
                        let profile_provider = Self::core_provider_to_proto(profile.provider);
                        if profile_provider != filter {
                            continue;
                        }
                    }
                    let display_name = profile.label.clone().unwrap_or_default();
                    profiles.push(Self::core_profile_to_proto(&profile, &display_name));
                },
                Err(e) => {
                    debug!(
                        profile_id = %pid,
                        error = %e,
                        "skipping profile that could not be loaded"
                    );
                },
            }
        }

        let total_count: u32 = profiles.len().try_into().unwrap_or(u32::MAX);

        Ok(PrivilegedResponse::ListCredentials(
            ListCredentialsResponse {
                profiles,
                total_count,
            },
        ))
    }

    /// Handles `AddCredential` requests (IPC-PRIV-022).
    ///
    /// Adds a new credential profile. The secret is stored securely and never
    /// logged.
    fn handle_add_credential(
        &self,
        payload: &[u8],
        _ctx: &ConnectionContext,
    ) -> ProtocolResult<PrivilegedResponse> {
        let request =
            AddCredentialRequest::decode_bounded(payload, &self.decode_config).map_err(|e| {
                ProtocolError::Serialization {
                    reason: format!("invalid AddCredentialRequest: {e}"),
                }
            })?;

        // Validate profile_id length (MAX_ID_LENGTH)
        if request.profile_id.len() > MAX_ID_LENGTH {
            return Ok(PrivilegedResponse::error(
                PrivilegedErrorCode::CredentialInvalidConfig,
                format!(
                    "profile_id too long: {} bytes (max {})",
                    request.profile_id.len(),
                    MAX_ID_LENGTH
                ),
            ));
        }

        // Security: Never log credential_secret
        debug!(
            profile_id = %request.profile_id,
            provider = request.provider,
            auth_method = request.auth_method,
            "AddCredential handler invoked"
        );

        let store = match self.require_credential_store() {
            Ok(s) => s,
            Err(resp) => return Ok(resp),
        };

        // Convert protobuf types to core types
        let provider = Self::proto_provider_to_core(request.provider);
        let Ok(auth) = Self::proto_auth_to_core(request.auth_method, &request.credential_secret)
        else {
            return Ok(PrivilegedResponse::error(
                PrivilegedErrorCode::CredentialInvalidConfig,
                "credential_secret is not valid UTF-8".to_string(),
            ));
        };
        let profile_id = ProfileId::new(&request.profile_id);

        // Build the core credential profile
        let mut core_profile = CoreCredentialProfile::new(profile_id, provider, auth);
        if !request.display_name.is_empty() {
            core_profile = core_profile.with_label(&request.display_name);
        }

        // Store the credential
        if let Err(e) = store.store(core_profile.clone()) {
            warn!(
                profile_id = %request.profile_id,
                error = %e,
                "failed to store credential"
            );
            return Ok(PrivilegedResponse::error(
                PrivilegedErrorCode::CredentialInvalidConfig,
                format!("failed to store credential: {e}"),
            ));
        }

        info!(
            profile_id = %request.profile_id,
            "credential profile stored successfully"
        );

        let proto_profile = Self::core_profile_to_proto(&core_profile, &request.display_name);

        Ok(PrivilegedResponse::AddCredential(AddCredentialResponse {
            profile: Some(proto_profile),
        }))
    }

    /// Handles `RemoveCredential` requests (IPC-PRIV-023).
    ///
    /// Removes a credential profile from storage.
    fn handle_remove_credential(
        &self,
        payload: &[u8],
        _ctx: &ConnectionContext,
    ) -> ProtocolResult<PrivilegedResponse> {
        let request = RemoveCredentialRequest::decode_bounded(payload, &self.decode_config)
            .map_err(|e| ProtocolError::Serialization {
                reason: format!("invalid RemoveCredentialRequest: {e}"),
            })?;

        debug!(
            profile_id = %request.profile_id,
            "RemoveCredential handler invoked"
        );

        let store = match self.require_credential_store() {
            Ok(s) => s,
            Err(resp) => return Ok(resp),
        };

        let profile_id = ProfileId::new(&request.profile_id);

        // Check if profile exists before removal
        let exists = match store.exists(&profile_id) {
            Ok(e) => e,
            Err(e) => {
                warn!(
                    profile_id = %request.profile_id,
                    error = %e,
                    "failed to check credential existence"
                );
                return Ok(PrivilegedResponse::RemoveCredential(
                    RemoveCredentialResponse { removed: false },
                ));
            },
        };

        if !exists {
            return Ok(PrivilegedResponse::RemoveCredential(
                RemoveCredentialResponse { removed: false },
            ));
        }

        match store.remove(&profile_id) {
            Ok(()) => {
                info!(
                    profile_id = %request.profile_id,
                    "credential profile removed successfully"
                );
                Ok(PrivilegedResponse::RemoveCredential(
                    RemoveCredentialResponse { removed: true },
                ))
            },
            Err(e) => {
                warn!(
                    profile_id = %request.profile_id,
                    error = %e,
                    "failed to remove credential"
                );
                Ok(PrivilegedResponse::RemoveCredential(
                    RemoveCredentialResponse { removed: false },
                ))
            },
        }
    }

    /// Handles `RefreshCredential` requests (IPC-PRIV-024).
    ///
    /// Refreshes an OAuth credential by requesting a new token.
    /// Note: Automated OAuth refresh requires an external token endpoint,
    /// which is out of scope for TCK-00343. This handler verifies the
    /// profile exists in the store and returns an appropriate error.
    fn handle_refresh_credential(
        &self,
        payload: &[u8],
        _ctx: &ConnectionContext,
    ) -> ProtocolResult<PrivilegedResponse> {
        let request = RefreshCredentialRequest::decode_bounded(payload, &self.decode_config)
            .map_err(|e| ProtocolError::Serialization {
                reason: format!("invalid RefreshCredentialRequest: {e}"),
            })?;

        debug!(
            profile_id = %request.profile_id,
            "RefreshCredential handler invoked"
        );

        let store = match self.require_credential_store() {
            Ok(s) => s,
            Err(resp) => return Ok(resp),
        };

        let profile_id = ProfileId::new(&request.profile_id);

        // Verify the profile exists before attempting refresh
        match store.get(&profile_id) {
            Ok(profile) => {
                // Check that the credential is an OAuth type (only OAuth supports
                // refresh)
                if !matches!(profile.auth, AuthMethod::OAuth { .. }) {
                    return Ok(PrivilegedResponse::error(
                        PrivilegedErrorCode::CredentialRefreshNotSupported,
                        format!(
                            "profile '{}' uses {} auth, which does not support refresh",
                            request.profile_id,
                            profile.auth.method_type()
                        ),
                    ));
                }

                // OAuth token refresh requires an external token endpoint, which
                // is out of scope for TCK-00343. Return an informative error.
                Ok(PrivilegedResponse::error(
                    PrivilegedErrorCode::CredentialRefreshNotSupported,
                    "OAuth token refresh requires an external token endpoint (not yet implemented)",
                ))
            },
            Err(e) => Ok(PrivilegedResponse::error(
                PrivilegedErrorCode::CredentialInvalidConfig,
                format!("credential profile not found: {e}"),
            )),
        }
    }

    /// Handles `SwitchCredential` requests (IPC-PRIV-025).
    ///
    /// Switches the active credential for a process. Validates that the
    /// target profile exists in the credential store before reporting
    /// success.
    fn handle_switch_credential(
        &self,
        payload: &[u8],
        _ctx: &ConnectionContext,
    ) -> ProtocolResult<PrivilegedResponse> {
        let request = SwitchCredentialRequest::decode_bounded(payload, &self.decode_config)
            .map_err(|e| ProtocolError::Serialization {
                reason: format!("invalid SwitchCredentialRequest: {e}"),
            })?;

        debug!(
            process_name = %request.process_name,
            profile_id = %request.profile_id,
            "SwitchCredential handler invoked"
        );

        let store = match self.require_credential_store() {
            Ok(s) => s,
            Err(resp) => return Ok(resp),
        };

        let profile_id = ProfileId::new(&request.profile_id);

        // Verify the target profile exists before switching
        match store.exists(&profile_id) {
            Ok(true) => {
                info!(
                    process_name = %request.process_name,
                    profile_id = %request.profile_id,
                    "credential switch validated"
                );

                // Note: Actual process credential binding is managed at the
                // process supervision layer. This handler validates that the
                // target profile exists. The previous_profile_id is empty
                // because per-process credential binding tracking is managed
                // by the supervisor (TCK-FUTURE).
                Ok(PrivilegedResponse::SwitchCredential(
                    SwitchCredentialResponse {
                        previous_profile_id: String::new(),
                        success: true,
                    },
                ))
            },
            Ok(false) => Ok(PrivilegedResponse::error(
                PrivilegedErrorCode::CredentialInvalidConfig,
                format!("credential profile not found: {}", request.profile_id),
            )),
            Err(e) => {
                warn!(
                    profile_id = %request.profile_id,
                    error = %e,
                    "failed to check credential existence"
                );
                Ok(PrivilegedResponse::SwitchCredential(
                    SwitchCredentialResponse {
                        previous_profile_id: String::new(),
                        success: false,
                    },
                ))
            },
        }
    }

    /// Handles `LoginCredential` requests (IPC-PRIV-026).
    ///
    /// Initiates an interactive login for a provider. For OAuth flows, this
    /// would generate an authorization URL. For API key flows, the key is
    /// provided directly in a subsequent `AddCredential` call.
    ///
    /// Note: Full interactive OAuth flow is out of scope for TCK-00343.
    /// This handler returns a stub response indicating the flow type.
    fn handle_login_credential(
        &self,
        payload: &[u8],
        _ctx: &ConnectionContext,
    ) -> ProtocolResult<PrivilegedResponse> {
        let request = LoginCredentialRequest::decode_bounded(payload, &self.decode_config)
            .map_err(|e| ProtocolError::Serialization {
                reason: format!("invalid LoginCredentialRequest: {e}"),
            })?;

        debug!(
            provider = request.provider,
            profile_id = ?request.profile_id,
            "LoginCredential handler invoked"
        );

        // LoginCredential is an interactive flow that requires browser-based
        // OAuth or terminal prompts. For TCK-00343, the credential store is
        // wired but interactive login flows remain as future work.
        let profile_id = request
            .profile_id
            .unwrap_or_else(|| format!("auto-{}", uuid::Uuid::new_v4()));

        let profile = super::messages::CredentialProfile {
            profile_id,
            provider: request.provider,
            auth_method: ProtoAuthMethod::Oauth as i32,
            created_at: std::time::SystemTime::now()
                .duration_since(std::time::UNIX_EPOCH)
                .map(|d| d.as_secs())
                .unwrap_or(0),
            expires_at: 0,
            is_active: false, // Not active until login completes
            display_name: request.display_name,
        };

        Ok(PrivilegedResponse::LoginCredential(
            LoginCredentialResponse {
                profile: Some(profile),
                login_url: String::new(), // Would contain OAuth URL for browser-based login
                completed: false,         // Login not yet complete
            },
        ))
    }

    // ========================================================================
    // Consensus Query Handlers (TCK-00345)
    // ========================================================================

    /// Maximum number of Byzantine evidence entries to return.
    /// Per `consensus.rs::limits::MAX_BYZANTINE_EVIDENCE_ENTRIES`.
    const MAX_BYZANTINE_EVIDENCE_ENTRIES: u32 = 1000;

    /// Handles `ConsensusStatus` requests (IPC-PRIV-011).
    ///
    /// # TCK-00345: Consensus Status Query
    ///
    /// Returns current consensus cluster status. If the consensus subsystem
    /// is not configured (single-node mode), returns `CONSENSUS_NOT_CONFIGURED`
    /// error instead of mock data.
    fn handle_consensus_status(&self, payload: &[u8]) -> ProtocolResult<PrivilegedResponse> {
        let request = ConsensusStatusRequest::decode_bounded(payload, &self.decode_config)
            .map_err(|e| ProtocolError::Serialization {
                reason: format!("invalid ConsensusStatusRequest: {e}"),
            })?;

        debug!(
            verbose = request.verbose,
            "ConsensusStatus request received"
        );

        // Check if consensus subsystem is configured
        // For now, return "not configured" since consensus state integration
        // requires additional daemon wiring (future work)
        if self.consensus_state.is_none() {
            return Ok(PrivilegedResponse::Error(PrivilegedError {
                code: ConsensusErrorCode::ConsensusNotConfigured.into(),
                message: "consensus subsystem is not configured".to_string(),
            }));
        }

        // TODO: Wire to actual consensus state when available
        // For now, return a response indicating the subsystem exists but has no data
        let response = ConsensusStatusResponse {
            node_id: self.node_id.clone(),
            epoch: 0,
            round: 0,
            leader_id: String::new(),
            is_leader: false,
            validator_count: 0,
            active_validators: 0,
            quorum_threshold: 0,
            quorum_met: false,
            health: "unknown".to_string(),
            high_qc_round: if request.verbose { Some(0) } else { None },
            locked_qc_round: None,
            committed_blocks: if request.verbose { Some(0) } else { None },
            last_committed_hash: None,
        };

        Ok(PrivilegedResponse::ConsensusStatus(response))
    }

    /// Handles `ConsensusValidators` requests (IPC-PRIV-012).
    ///
    /// # TCK-00345: Validator List Query
    ///
    /// Returns list of validators in the consensus cluster. If the consensus
    /// subsystem is not configured, returns `CONSENSUS_NOT_CONFIGURED` error.
    fn handle_consensus_validators(&self, payload: &[u8]) -> ProtocolResult<PrivilegedResponse> {
        let request = ConsensusValidatorsRequest::decode_bounded(payload, &self.decode_config)
            .map_err(|e| ProtocolError::Serialization {
                reason: format!("invalid ConsensusValidatorsRequest: {e}"),
            })?;

        debug!(
            active_only = request.active_only,
            "ConsensusValidators request received"
        );

        // Check if consensus subsystem is configured
        if self.consensus_state.is_none() {
            return Ok(PrivilegedResponse::Error(PrivilegedError {
                code: ConsensusErrorCode::ConsensusNotConfigured.into(),
                message: "consensus subsystem is not configured".to_string(),
            }));
        }

        // TODO: Wire to actual consensus state when available
        let response = ConsensusValidatorsResponse {
            validators: Vec::new(),
            total: 0,
            active: 0,
        };

        Ok(PrivilegedResponse::ConsensusValidators(response))
    }

    /// Handles `ConsensusByzantineEvidence` requests (IPC-PRIV-013).
    ///
    /// # TCK-00345: Byzantine Evidence Query
    ///
    /// Returns list of detected Byzantine fault evidence. If the consensus
    /// subsystem is not configured, returns `CONSENSUS_NOT_CONFIGURED` error.
    fn handle_consensus_byzantine_evidence(
        &self,
        payload: &[u8],
    ) -> ProtocolResult<PrivilegedResponse> {
        let request =
            ConsensusByzantineEvidenceRequest::decode_bounded(payload, &self.decode_config)
                .map_err(|e| ProtocolError::Serialization {
                    reason: format!("invalid ConsensusByzantineEvidenceRequest: {e}"),
                })?;

        // Cap limit to prevent DoS
        let effective_limit = request.limit.min(Self::MAX_BYZANTINE_EVIDENCE_ENTRIES);

        debug!(
            fault_type = ?request.fault_type,
            limit = effective_limit,
            "ConsensusByzantineEvidence request received"
        );

        // Check if consensus subsystem is configured
        if self.consensus_state.is_none() {
            return Ok(PrivilegedResponse::Error(PrivilegedError {
                code: ConsensusErrorCode::ConsensusNotConfigured.into(),
                message: "consensus subsystem is not configured".to_string(),
            }));
        }

        // TODO: Wire to actual consensus state when available
        let response = ConsensusByzantineEvidenceResponse {
            evidence: Vec::new(),
            total: 0,
        };

        Ok(PrivilegedResponse::ConsensusByzantineEvidence(response))
    }

    /// Handles `ConsensusMetrics` requests (IPC-PRIV-014).
    ///
    /// # TCK-00345: Consensus Metrics Query
    ///
    /// Returns consensus metrics summary. If the consensus subsystem
    /// is not configured, returns `CONSENSUS_NOT_CONFIGURED` error.
    fn handle_consensus_metrics(&self, payload: &[u8]) -> ProtocolResult<PrivilegedResponse> {
        let request = ConsensusMetricsRequest::decode_bounded(payload, &self.decode_config)
            .map_err(|e| ProtocolError::Serialization {
                reason: format!("invalid ConsensusMetricsRequest: {e}"),
            })?;

        debug!(
            period_secs = request.period_secs,
            "ConsensusMetrics request received"
        );

        // Check if consensus subsystem is configured
        if self.consensus_state.is_none() {
            return Ok(PrivilegedResponse::Error(PrivilegedError {
                code: ConsensusErrorCode::ConsensusNotConfigured.into(),
                message: "consensus subsystem is not configured".to_string(),
            }));
        }

        // TODO: Wire to actual consensus state when available
        let response = ConsensusMetricsResponse {
            node_id: self.node_id.clone(),
            proposals_committed: 0,
            proposals_rejected: 0,
            proposals_timeout: 0,
            leader_elections: 0,
            sync_events: 0,
            conflicts: 0,
            byzantine_evidence: 0,
            latency_p50_ms: 0.0,
            latency_p99_ms: 0.0,
        };

        Ok(PrivilegedResponse::ConsensusMetrics(response))
    }

    // ========================================================================
    // HEF Pulse Plane Handlers (TCK-00302)
    // ========================================================================

    /// Handles `SubscribePulse` requests from operator sockets (IPC-HEF-001).
    ///
    /// # TCK-00302: Operator Full Taxonomy Access
    ///
    /// Per DD-HEF-0004: "Operator connections may subscribe broadly."
    /// Operators can subscribe to any valid pattern including wildcards.
    ///
    /// This handler:
    /// 1. Validates request structure and field lengths
    /// 2. Validates topic patterns using `pulse_topic` grammar
    /// 3. Allows all valid patterns (no ACL restrictions for operators)
    /// 4. Returns accepted patterns and any rejected invalid patterns
    ///
    /// # Note: Subscription Registry
    ///
    /// Actual subscription registration and pulse delivery are handled by
    /// TCK-00303 (resource governance) and TCK-00304 (outbox + publisher).
    fn handle_subscribe_pulse(
        &self,
        payload: &[u8],
        ctx: &ConnectionContext,
    ) -> ProtocolResult<PrivilegedResponse> {
        // Max patterns per request per RFC-0018 (must be declared before statements)
        const MAX_PATTERNS_PER_REQUEST: usize = 16;

        let request =
            SubscribePulseRequest::decode_bounded(payload, &self.decode_config).map_err(|e| {
                ProtocolError::Serialization {
                    reason: format!("invalid SubscribePulseRequest: {e}"),
                }
            })?;

        info!(
            client_sub_id = %request.client_sub_id,
            pattern_count = request.topic_patterns.len(),
            since_cursor = request.since_ledger_cursor,
            peer_pid = ?ctx.peer_credentials().map(|c| c.pid),
            "SubscribePulse (operator) request received"
        );

        // Validate client_sub_id length
        if let Err(e) = validate_client_sub_id(&request.client_sub_id) {
            warn!(error = %e, "Invalid client_sub_id");
            return Ok(PrivilegedResponse::error(
                PrivilegedErrorCode::PermissionDenied,
                e.to_string(),
            ));
        }

        // Validate topic_patterns count
        if request.topic_patterns.len() > MAX_PATTERNS_PER_REQUEST {
            warn!(
                pattern_count = request.topic_patterns.len(),
                "Too many patterns in subscribe request"
            );
            return Ok(PrivilegedResponse::error(
                PrivilegedErrorCode::PermissionDenied,
                format!(
                    "too many patterns: {} exceeds maximum {}",
                    request.topic_patterns.len(),
                    MAX_PATTERNS_PER_REQUEST
                ),
            ));
        }

        // Create ACL evaluator for operator subscriptions
        // Per DD-HEF-0004: "Operator connections may subscribe broadly"
        let evaluator = PulseAclEvaluator::for_operator();

        // Evaluate each pattern
        let mut accepted_patterns = Vec::new();
        let mut rejected_patterns = Vec::new();

        for pattern in &request.topic_patterns {
            match evaluator.check_subscribe(pattern) {
                AclDecision::Allow => {
                    accepted_patterns.push(pattern.clone());
                },
                AclDecision::Deny(err) => {
                    let reason_code = Self::acl_error_to_reason_code(&err);
                    rejected_patterns.push(PatternRejection {
                        pattern: pattern.clone(),
                        reason_code,
                    });
                    debug!(
                        pattern = %pattern,
                        reason = %err,
                        "Pattern rejected (invalid syntax)"
                    );
                },
            }
        }

        // Generate subscription ID; use connection ID from context (TCK-00303)
        let subscription_id = format!("SUB-{}", uuid::Uuid::new_v4());
        // TCK-00303: Use connection_id from context for consistent tracking
        // across the connection lifecycle. The connection handler will call
        // unregister_connection with this ID when the connection closes.
        let connection_id = ctx.connection_id();

        // TCK-00303: Wire resource governance - register connection if not exists
        // and add subscription with limit checks
        if !accepted_patterns.is_empty() {
            // Parse accepted patterns into TopicPattern
            let mut parsed_patterns = Vec::new();
            for pattern_str in &accepted_patterns {
                match super::pulse_topic::TopicPattern::parse(pattern_str) {
                    Ok(pattern) => parsed_patterns.push(pattern),
                    Err(e) => {
                        // Should not happen since ACL already validated, but be defensive
                        warn!(
                            pattern = %pattern_str,
                            error = %e,
                            "Pattern parse failed after ACL validation"
                        );
                        rejected_patterns.push(PatternRejection {
                            pattern: pattern_str.clone(),
                            reason_code: "INVALID_PATTERN".to_string(),
                        });
                    },
                }
            }

            // Register connection if it doesn't exist (idempotent)
            if let Err(e) = self
                .subscription_registry
                .register_connection(connection_id)
            {
                // Only TooManyConnections is a real error; ignore if connection already exists
                if matches!(
                    e,
                    super::resource_governance::ResourceError::TooManyConnections { .. }
                ) {
                    warn!(
                        connection_id = %connection_id,
                        error = %e,
                        "Connection registration failed: resource limit exceeded"
                    );
                    return Ok(PrivilegedResponse::error(
                        PrivilegedErrorCode::PermissionDenied,
                        format!("resource limit exceeded: {e}"),
                    ));
                }
                // Connection already exists - this is fine
            }

            // Create subscription state and add to registry
            let subscription = SubscriptionState::new(
                &subscription_id,
                &request.client_sub_id,
                parsed_patterns,
                request.since_ledger_cursor,
            );

            if let Err(e) = self
                .subscription_registry
                .add_subscription(connection_id, subscription)
            {
                warn!(
                    connection_id = %connection_id,
                    subscription_id = %subscription_id,
                    error = %e,
                    "Subscription registration failed: resource limit exceeded"
                );
                return Ok(PrivilegedResponse::error(
                    PrivilegedErrorCode::PermissionDenied,
                    format!("resource limit exceeded: {e}"),
                ));
            }
        }

        // Log outcome
        if rejected_patterns.is_empty() {
            info!(
                subscription_id = %subscription_id,
                connection_id = %connection_id,
                accepted_count = accepted_patterns.len(),
                "All patterns accepted (operator)"
            );
        } else {
            warn!(
                subscription_id = %subscription_id,
                connection_id = %connection_id,
                accepted_count = accepted_patterns.len(),
                rejected_count = rejected_patterns.len(),
                "Some patterns rejected (operator)"
            );
        }

        Ok(PrivilegedResponse::SubscribePulse(SubscribePulseResponse {
            subscription_id,
            effective_since_cursor: request.since_ledger_cursor,
            accepted_patterns,
            rejected_patterns,
        }))
    }

    /// Converts an `AclError` to a reason code string for `PatternRejection`.
    fn acl_error_to_reason_code(err: &AclError) -> String {
        match err {
            AclError::TopicNotAllowed { .. } => "ACL_DENY".to_string(),
            AclError::WildcardNotAllowed { .. } => "WILDCARD_NOT_ALLOWED".to_string(),
            AclError::PublishNotAllowed => "PUBLISH_DENIED".to_string(),
            AclError::InvalidPattern { .. } | AclError::InvalidTopic { .. } => {
                "INVALID_PATTERN".to_string()
            },
            AclError::AllowlistTooLarge { .. } | AclError::SubscriptionIdTooLong { .. } => {
                "LIMIT_EXCEEDED".to_string()
            },
        }
    }

    /// Handles `UnsubscribePulse` requests from operator sockets (IPC-HEF-002).
    ///
    /// # TCK-00302: Unsubscribe Handling
    ///
    /// This handler validates the unsubscribe request and returns success.
    ///
    /// # Note: Subscription Registry
    ///
    /// Actual subscription removal is handled by TCK-00303 (resource
    /// governance).
    fn handle_unsubscribe_pulse(
        &self,
        payload: &[u8],
        ctx: &ConnectionContext,
    ) -> ProtocolResult<PrivilegedResponse> {
        let request = UnsubscribePulseRequest::decode_bounded(payload, &self.decode_config)
            .map_err(|e| ProtocolError::Serialization {
                reason: format!("invalid UnsubscribePulseRequest: {e}"),
            })?;

        info!(
            subscription_id = %request.subscription_id,
            peer_pid = ?ctx.peer_credentials().map(|c| c.pid),
            "UnsubscribePulse (operator) request received"
        );

        // Validate subscription_id length
        if let Err(e) = validate_subscription_id(&request.subscription_id) {
            warn!(error = %e, "Invalid subscription_id");
            return Ok(PrivilegedResponse::error(
                PrivilegedErrorCode::PermissionDenied,
                e.to_string(),
            ));
        }

        // TCK-00303: Wire resource governance - remove subscription from registry
        // Use connection_id from context for consistent tracking
        let connection_id = ctx.connection_id();

        let removed = match self
            .subscription_registry
            .remove_subscription(connection_id, &request.subscription_id)
        {
            Ok(_) => {
                info!(
                    subscription_id = %request.subscription_id,
                    connection_id = %connection_id,
                    "Unsubscribe (operator) processed successfully"
                );
                true
            },
            Err(e) => {
                // Log but don't fail - subscription may already be removed or never existed
                debug!(
                    subscription_id = %request.subscription_id,
                    connection_id = %connection_id,
                    error = %e,
                    "Unsubscribe (operator) - subscription not found (may already be removed)"
                );
                false
            },
        };

        Ok(PrivilegedResponse::UnsubscribePulse(
            UnsubscribePulseResponse { removed },
        ))
    }
}

// ============================================================================
// Request Encoding Helpers
// ============================================================================

/// Encodes a `ClaimWork` request to bytes for sending.
///
/// The format is: `[tag: u8][payload: protobuf]`
#[must_use]
pub fn encode_claim_work_request(request: &ClaimWorkRequest) -> Bytes {
    let mut buf = vec![PrivilegedMessageType::ClaimWork.tag()];
    request.encode(&mut buf).expect("encode cannot fail");
    Bytes::from(buf)
}

/// Encodes a `SpawnEpisode` request to bytes for sending.
#[must_use]
pub fn encode_spawn_episode_request(request: &SpawnEpisodeRequest) -> Bytes {
    let mut buf = vec![PrivilegedMessageType::SpawnEpisode.tag()];
    request.encode(&mut buf).expect("encode cannot fail");
    Bytes::from(buf)
}

/// Encodes an `IssueCapability` request to bytes for sending.
#[must_use]
pub fn encode_issue_capability_request(request: &IssueCapabilityRequest) -> Bytes {
    let mut buf = vec![PrivilegedMessageType::IssueCapability.tag()];
    request.encode(&mut buf).expect("encode cannot fail");
    Bytes::from(buf)
}

/// Encodes a `Shutdown` request to bytes for sending.
#[must_use]
pub fn encode_shutdown_request(request: &ShutdownRequest) -> Bytes {
    let mut buf = vec![PrivilegedMessageType::Shutdown.tag()];
    request.encode(&mut buf).expect("encode cannot fail");
    Bytes::from(buf)
}

/// Encodes an `UpdateStopFlags` request to bytes for sending (TCK-00351).
#[must_use]
pub fn encode_update_stop_flags_request(request: &UpdateStopFlagsRequest) -> Bytes {
    let mut buf = vec![PrivilegedMessageType::UpdateStopFlags.tag()];
    request.encode(&mut buf).expect("encode cannot fail");
    Bytes::from(buf)
}

/// Encodes an `EndSession` request to bytes for sending (TCK-00395).
#[must_use]
pub fn encode_end_session_request(request: &EndSessionRequest) -> Bytes {
    let mut buf = vec![PrivilegedMessageType::EndSession.tag()];
    request.encode(&mut buf).expect("encode cannot fail");
    Bytes::from(buf)
}

/// Encodes an `IngestReviewReceipt` request to bytes for sending (TCK-00389).
#[must_use]
pub fn encode_ingest_review_receipt_request(request: &IngestReviewReceiptRequest) -> Bytes {
    let mut buf = vec![PrivilegedMessageType::IngestReviewReceipt.tag()];
    request.encode(&mut buf).expect("encode cannot fail");
    Bytes::from(buf)
}

/// Encodes a `DelegateSublease` request to bytes for sending (TCK-00340).
#[must_use]
pub fn encode_delegate_sublease_request(request: &DelegateSubleaseRequest) -> Bytes {
    let mut buf = vec![PrivilegedMessageType::DelegateSublease.tag()];
    request.encode(&mut buf).expect("encode cannot fail");
    Bytes::from(buf)
}

// ============================================================================
// Process Management Request Encoding (TCK-00342)
// ============================================================================

/// Encodes a `ListProcesses` request to bytes for sending.
#[must_use]
pub fn encode_list_processes_request(request: &ListProcessesRequest) -> Bytes {
    let mut buf = vec![PrivilegedMessageType::ListProcesses.tag()];
    request.encode(&mut buf).expect("encode cannot fail");
    Bytes::from(buf)
}

/// Encodes a `ProcessStatus` request to bytes for sending.
#[must_use]
pub fn encode_process_status_request(request: &ProcessStatusRequest) -> Bytes {
    let mut buf = vec![PrivilegedMessageType::ProcessStatus.tag()];
    request.encode(&mut buf).expect("encode cannot fail");
    Bytes::from(buf)
}

/// Encodes a `StartProcess` request to bytes for sending.
#[must_use]
pub fn encode_start_process_request(request: &StartProcessRequest) -> Bytes {
    let mut buf = vec![PrivilegedMessageType::StartProcess.tag()];
    request.encode(&mut buf).expect("encode cannot fail");
    Bytes::from(buf)
}

/// Encodes a `StopProcess` request to bytes for sending.
#[must_use]
pub fn encode_stop_process_request(request: &StopProcessRequest) -> Bytes {
    let mut buf = vec![PrivilegedMessageType::StopProcess.tag()];
    request.encode(&mut buf).expect("encode cannot fail");
    Bytes::from(buf)
}

/// Encodes a `RestartProcess` request to bytes for sending.
#[must_use]
pub fn encode_restart_process_request(request: &RestartProcessRequest) -> Bytes {
    let mut buf = vec![PrivilegedMessageType::RestartProcess.tag()];
    request.encode(&mut buf).expect("encode cannot fail");
    Bytes::from(buf)
}

/// Encodes a `ReloadProcess` request to bytes for sending.
#[must_use]
pub fn encode_reload_process_request(request: &ReloadProcessRequest) -> Bytes {
    let mut buf = vec![PrivilegedMessageType::ReloadProcess.tag()];
    request.encode(&mut buf).expect("encode cannot fail");
    Bytes::from(buf)
}

// ============================================================================
// CTR-PROTO-011: Consensus Query Request Encoding (RFC-0014, TCK-00345)
// ============================================================================

/// Encodes a `ConsensusStatus` request to bytes for sending.
///
/// # Wire Format
/// ```text
/// +------+---------------------------+
/// | 0x0B | ConsensusStatusRequest    |
/// +------+---------------------------+
/// ```
#[must_use]
pub fn encode_consensus_status_request(request: &ConsensusStatusRequest) -> Bytes {
    let mut buf = vec![PrivilegedMessageType::ConsensusStatus.tag()];
    request.encode(&mut buf).expect("encode cannot fail");
    Bytes::from(buf)
}

/// Encodes a `ConsensusValidators` request to bytes for sending.
///
/// # Wire Format
/// ```text
/// +------+------------------------------+
/// | 0x0C | ConsensusValidatorsRequest   |
/// +------+------------------------------+
/// ```
#[must_use]
pub fn encode_consensus_validators_request(request: &ConsensusValidatorsRequest) -> Bytes {
    let mut buf = vec![PrivilegedMessageType::ConsensusValidators.tag()];
    request.encode(&mut buf).expect("encode cannot fail");
    Bytes::from(buf)
}

/// Encodes a `ConsensusByzantineEvidence` request to bytes for sending.
///
/// # Wire Format
/// ```text
/// +------+------------------------------------+
/// | 0x0D | ConsensusByzantineEvidenceRequest  |
/// +------+------------------------------------+
/// ```
#[must_use]
pub fn encode_consensus_byzantine_evidence_request(
    request: &ConsensusByzantineEvidenceRequest,
) -> Bytes {
    let mut buf = vec![PrivilegedMessageType::ConsensusByzantineEvidence.tag()];
    request.encode(&mut buf).expect("encode cannot fail");
    Bytes::from(buf)
}

/// Encodes a `ConsensusMetrics` request to bytes for sending.
///
/// # Wire Format
/// ```text
/// +------+---------------------------+
/// | 0x0E | ConsensusMetricsRequest   |
/// +------+---------------------------+
/// ```
#[must_use]
pub fn encode_consensus_metrics_request(request: &ConsensusMetricsRequest) -> Bytes {
    let mut buf = vec![PrivilegedMessageType::ConsensusMetrics.tag()];
    request.encode(&mut buf).expect("encode cannot fail");
    Bytes::from(buf)
}

/// Encodes a `WorkStatus` request to bytes for sending (TCK-00344).
#[must_use]
pub fn encode_work_status_request(request: &WorkStatusRequest) -> Bytes {
    let mut buf = vec![PrivilegedMessageType::WorkStatus.tag()];
    request.encode(&mut buf).expect("encode cannot fail");
    Bytes::from(buf)
}

// =============================================================================
// TCK-00394: ChangeSet Publishing Encoding (RFC-0018)
// =============================================================================

/// Encodes a `PublishChangeSet` request to bytes for sending (TCK-00394).
#[must_use]
pub fn encode_publish_changeset_request(request: &PublishChangeSetRequest) -> Bytes {
    let mut buf = vec![PrivilegedMessageType::PublishChangeSet.tag()];
    request.encode(&mut buf).expect("encode cannot fail");
    Bytes::from(buf)
}

// =============================================================================
// CTR-PROTO-012: Credential Management Encoding (RFC-0018, TCK-00343)
// =============================================================================

/// Encodes a `ListCredentials` request to bytes for sending.
#[must_use]
pub fn encode_list_credentials_request(request: &ListCredentialsRequest) -> Bytes {
    let mut buf = vec![PrivilegedMessageType::ListCredentials.tag()];
    request.encode(&mut buf).expect("encode cannot fail");
    Bytes::from(buf)
}

/// Encodes an `AddCredential` request to bytes for sending.
#[must_use]
pub fn encode_add_credential_request(request: &AddCredentialRequest) -> Bytes {
    let mut buf = vec![PrivilegedMessageType::AddCredential.tag()];
    request.encode(&mut buf).expect("encode cannot fail");
    Bytes::from(buf)
}

/// Encodes a `RemoveCredential` request to bytes for sending.
#[must_use]
pub fn encode_remove_credential_request(request: &RemoveCredentialRequest) -> Bytes {
    let mut buf = vec![PrivilegedMessageType::RemoveCredential.tag()];
    request.encode(&mut buf).expect("encode cannot fail");
    Bytes::from(buf)
}

/// Encodes a `RefreshCredential` request to bytes for sending.
#[must_use]
pub fn encode_refresh_credential_request(request: &RefreshCredentialRequest) -> Bytes {
    let mut buf = vec![PrivilegedMessageType::RefreshCredential.tag()];
    request.encode(&mut buf).expect("encode cannot fail");
    Bytes::from(buf)
}

/// Encodes a `SwitchCredential` request to bytes for sending.
#[must_use]
pub fn encode_switch_credential_request(request: &SwitchCredentialRequest) -> Bytes {
    let mut buf = vec![PrivilegedMessageType::SwitchCredential.tag()];
    request.encode(&mut buf).expect("encode cannot fail");
    Bytes::from(buf)
}

/// Encodes a `LoginCredential` request to bytes for sending.
#[must_use]
pub fn encode_login_credential_request(request: &LoginCredentialRequest) -> Bytes {
    let mut buf = vec![PrivilegedMessageType::LoginCredential.tag()];
    request.encode(&mut buf).expect("encode cannot fail");
    Bytes::from(buf)
}

#[cfg(test)]
mod tests {
    use super::*;

    /// TCK-00319: Helper function to get a test workspace root.
    /// Uses /tmp which exists on all Unix systems.
    fn test_workspace_root() -> String {
        "/tmp".to_string()
    }

    mod governance_probe_failure_classification {
        use super::*;
        use crate::episode::preactuation::StopAuthority;
        use crate::governance::GovernanceFreshnessConfig;

        #[test]
        fn spawn_missing_claim_does_not_record_governance_failure() {
            let authority = Arc::new(StopAuthority::new());
            let monitor = Arc::new(GovernanceFreshnessMonitor::new(
                Arc::clone(&authority),
                GovernanceFreshnessConfig::default(),
                false,
            ));
            monitor.record_success();

            let dispatcher =
                PrivilegedDispatcher::new().with_governance_freshness_monitor(Arc::clone(&monitor));
            let ctx = ConnectionContext::privileged_session_open(Some(PeerCredentials {
                uid: 1000,
                gid: 1000,
                pid: Some(12345),
            }));

            let request = SpawnEpisodeRequest {
                workspace_root: test_workspace_root(),
                work_id: "W-NO-CLAIM".to_string(),
                role: WorkRole::Implementer.into(),
                lease_id: Some("L-NO-CLAIM".to_string()),
                adapter_profile_hash: None,
                max_episodes: None,
                escalation_predicate: None,
            };
            let frame = encode_spawn_episode_request(&request);

            let response = dispatcher.dispatch(&frame, &ctx).unwrap();
            match response {
                PrivilegedResponse::Error(err) => {
                    assert_eq!(
                        err.code,
                        PrivilegedErrorCode::PolicyResolutionMissing as i32
                    );
                },
                other => panic!("expected PolicyResolutionMissing error, got: {other:?}"),
            }

            assert!(
                !authority.governance_uncertain(),
                "local missing-claim path must not set governance uncertainty"
            );
        }

        #[test]
        fn issue_capability_missing_claim_does_not_record_governance_failure() {
            let authority = Arc::new(StopAuthority::new());
            let monitor = Arc::new(GovernanceFreshnessMonitor::new(
                Arc::clone(&authority),
                GovernanceFreshnessConfig::default(),
                false,
            ));
            monitor.record_success();

            let dispatcher =
                PrivilegedDispatcher::new().with_governance_freshness_monitor(Arc::clone(&monitor));
            let ctx = ConnectionContext::privileged_session_open(Some(PeerCredentials {
                uid: 1000,
                gid: 1000,
                pid: Some(12345),
            }));

            dispatcher
                .session_registry
                .register_session(crate::session::SessionState {
                    session_id: "S-NO-CLAIM".to_string(),
                    work_id: "W-NO-CLAIM".to_string(),
                    role: WorkRole::Implementer.into(),
                    lease_id: "L-NO-CLAIM".to_string(),
                    ephemeral_handle: "EH-NO-CLAIM".to_string(),
                    policy_resolved_ref: String::new(),
                    capability_manifest_hash: vec![],
                    episode_id: None,
                })
                .expect("session registration should succeed");

            let request = IssueCapabilityRequest {
                session_id: "S-NO-CLAIM".to_string(),
                capability_request: Some(super::super::super::messages::CapabilityRequest {
                    tool_class: "read".to_string(),
                    read_patterns: vec!["**/*".to_string()],
                    write_patterns: vec![],
                    duration_secs: 60,
                }),
            };
            let frame = encode_issue_capability_request(&request);

            let response = dispatcher.dispatch(&frame, &ctx).unwrap();
            match response {
                PrivilegedResponse::Error(err) => {
                    assert_eq!(
                        err.code,
                        PrivilegedErrorCode::CapabilityRequestRejected as i32
                    );
                },
                other => panic!("expected CapabilityRequestRejected error, got: {other:?}"),
            }

            assert!(
                !authority.governance_uncertain(),
                "local missing-claim path must not set governance uncertainty"
            );
        }
    }

    // ========================================================================
    // INT-001: Privileged endpoint routing (TCK-00251)
    // Test name matches verification command: cargo test -p apm2-daemon
    // privileged_routing
    // ========================================================================
    mod privileged_routing {
        use super::*;
        use crate::episode::preactuation::StopAuthority;

        #[test]
        fn test_claim_work_routing() {
            let dispatcher = PrivilegedDispatcher::new();
            let ctx = ConnectionContext::privileged_session_open(Some(PeerCredentials {
                uid: 1000,
                gid: 1000,
                pid: Some(12345),
            }));

            let request = ClaimWorkRequest {
                actor_id: "test-actor".to_string(),
                role: WorkRole::Implementer.into(),
                credential_signature: vec![1, 2, 3],
                nonce: vec![4, 5, 6],
            };
            let frame = encode_claim_work_request(&request);

            let response = dispatcher.dispatch(&frame, &ctx).unwrap();
            assert!(matches!(response, PrivilegedResponse::ClaimWork(_)));
        }

        #[test]
        fn test_update_stop_flags_tag_and_routing() {
            let authority = Arc::new(StopAuthority::new());
            let dispatcher =
                PrivilegedDispatcher::new().with_stop_authority(Arc::clone(&authority));
            let ctx = ConnectionContext::privileged_session_open(Some(PeerCredentials {
                uid: 1000,
                gid: 1000,
                pid: Some(12345),
            }));

            assert_eq!(PrivilegedMessageType::UpdateStopFlags.tag(), 18);
            assert_eq!(
                PrivilegedMessageType::from_tag(18),
                Some(PrivilegedMessageType::UpdateStopFlags)
            );

            let request = UpdateStopFlagsRequest {
                emergency_stop_active: Some(true),
                governance_stop_active: None,
            };
            let frame = encode_update_stop_flags_request(&request);
            let response = dispatcher.dispatch(&frame, &ctx).unwrap();

            match response {
                PrivilegedResponse::UpdateStopFlags(resp) => {
                    assert!(resp.emergency_stop_active);
                    assert!(!resp.governance_stop_active);
                },
                other => panic!("expected UpdateStopFlags response, got {other:?}"),
            }
            assert!(authority.emergency_stop_active());

            let events = dispatcher
                .event_emitter()
                .get_events_by_work_id(STOP_FLAGS_MUTATED_WORK_ID);
            assert_eq!(events.len(), 1, "expected one stop-flags audit event");
            let event = &events[0];
            assert_eq!(event.event_type, "stop_flags_mutated");

            let payload: serde_json::Value =
                serde_json::from_slice(&event.payload).expect("payload should be valid JSON");
            assert_eq!(
                payload["actor_id"],
                derive_actor_id(&PeerCredentials {
                    uid: 1000,
                    gid: 1000,
                    pid: Some(12345),
                })
            );
            assert_eq!(payload["emergency_stop_previous"], false);
            assert_eq!(payload["emergency_stop_current"], true);
            assert_eq!(payload["governance_stop_previous"], false);
            assert_eq!(payload["governance_stop_current"], false);
            assert_eq!(
                payload["request_context"]["endpoint"], "UpdateStopFlags",
                "request context must include endpoint"
            );
            assert_eq!(
                payload["request_context"]["requested_updates"]["emergency_stop_active"],
                true
            );
        }

        #[test]
        fn test_spawn_episode_routing() {
            let dispatcher = PrivilegedDispatcher::new();
            let ctx = ConnectionContext::privileged_session_open(Some(PeerCredentials {
                uid: 1000,
                gid: 1000,
                pid: Some(12345),
            }));

            // TCK-00256: First claim work to establish policy resolution
            let claim_request = ClaimWorkRequest {
                actor_id: "test-actor".to_string(),
                role: WorkRole::Implementer.into(),
                credential_signature: vec![1, 2, 3],
                nonce: vec![4, 5, 6],
            };
            let claim_frame = encode_claim_work_request(&claim_request);
            let claim_response = dispatcher.dispatch(&claim_frame, &ctx).unwrap();

            let (work_id, lease_id) = match claim_response {
                PrivilegedResponse::ClaimWork(resp) => (resp.work_id, resp.lease_id),
                _ => panic!("Expected ClaimWork response"),
            };

            // Now spawn with the claimed work_id
            let request = SpawnEpisodeRequest {
                workspace_root: test_workspace_root(),
                work_id,
                role: WorkRole::Implementer.into(),
                lease_id: Some(lease_id),
                adapter_profile_hash: None,
                max_episodes: None,
                escalation_predicate: None,
            };
            let frame = encode_spawn_episode_request(&request);

            let response = dispatcher.dispatch(&frame, &ctx).unwrap();
            assert!(matches!(response, PrivilegedResponse::SpawnEpisode(_)));
        }

        #[test]
        fn test_issue_capability_routing() {
            use crate::session::SessionState;

            let dispatcher = PrivilegedDispatcher::new();
            let ctx = ConnectionContext::privileged_session_open(Some(PeerCredentials {
                uid: 1000,
                gid: 1000,
                pid: Some(12345),
            }));

            // TCK-00289: Register a session and work claim for IssueCapability validation
            let work_id = "W-TEST-001";
            let lease_id = "L-TEST-001";
            let session_id = "S-001";

            // Register work claim
            let claim = WorkClaim {
                work_id: work_id.to_string(),
                lease_id: lease_id.to_string(),
                actor_id: "test-actor".to_string(),
                role: WorkRole::Implementer,
                policy_resolution: PolicyResolution {
                    policy_resolved_ref: format!("resolved-for-{work_id}"),
                    resolved_policy_hash: [0u8; 32],
                    capability_manifest_hash: [0u8; 32],
                    context_pack_hash: [0u8; 32],
                    resolved_risk_tier: 0,
                    resolved_scope_baseline: None,
                    expected_adapter_profile_hash: None,
                },
                author_custody_domains: vec![],
                executor_custody_domains: vec![],
            };
            dispatcher.work_registry.register_claim(claim).unwrap();

            // Register session
            let session_state = SessionState {
                session_id: session_id.to_string(),
                work_id: work_id.to_string(),
                role: WorkRole::Implementer.into(),
                lease_id: lease_id.to_string(),
                ephemeral_handle: String::new(),
                policy_resolved_ref: String::new(),
                capability_manifest_hash: vec![],
                episode_id: None,
            };
            dispatcher
                .session_registry
                .register_session(session_state)
                .unwrap();

            let request = IssueCapabilityRequest {
                session_id: session_id.to_string(),
                capability_request: Some(super::super::super::messages::CapabilityRequest {
                    tool_class: "file_read".to_string(),
                    read_patterns: vec!["**/*.rs".to_string()],
                    write_patterns: vec![],
                    duration_secs: 3600,
                }),
            };
            let frame = encode_issue_capability_request(&request);

            let response = dispatcher.dispatch(&frame, &ctx).unwrap();
            assert!(matches!(response, PrivilegedResponse::IssueCapability(_)));
        }

        #[test]
        fn test_shutdown_routing() {
            let dispatcher = PrivilegedDispatcher::new();
            let ctx = ConnectionContext::privileged_session_open(Some(PeerCredentials {
                uid: 1000,
                gid: 1000,
                pid: Some(12345),
            }));

            let request = ShutdownRequest {
                reason: Some("test".to_string()),
            };
            let frame = encode_shutdown_request(&request);

            let response = dispatcher.dispatch(&frame, &ctx).unwrap();
            assert!(matches!(response, PrivilegedResponse::Shutdown(_)));
        }

        #[test]
        fn test_session_socket_returns_permission_denied() {
            let dispatcher = PrivilegedDispatcher::new();
            let ctx = ConnectionContext::session_open(
                Some(PeerCredentials {
                    uid: 1000,
                    gid: 1000,
                    pid: Some(12346),
                }),
                Some("test-session".to_string()),
            );

            // All 4 endpoints should return PERMISSION_DENIED for session connections
            let requests = vec![
                encode_claim_work_request(&ClaimWorkRequest {
                    actor_id: "test".to_string(),
                    role: WorkRole::Implementer.into(),
                    credential_signature: vec![],
                    nonce: vec![],
                }),
                encode_spawn_episode_request(&SpawnEpisodeRequest {
                    workspace_root: test_workspace_root(),
                    work_id: "W-001".to_string(),
                    role: WorkRole::Implementer.into(),
                    lease_id: None,
                    adapter_profile_hash: None,
                    max_episodes: None,
                    escalation_predicate: None,
                }),
                encode_issue_capability_request(&IssueCapabilityRequest {
                    session_id: "S-001".to_string(),
                    capability_request: None,
                }),
                encode_shutdown_request(&ShutdownRequest {
                    reason: Some("test".to_string()),
                }),
            ];

            for frame in requests {
                let response = dispatcher.dispatch(&frame, &ctx).unwrap();
                match response {
                    PrivilegedResponse::Error(err) => {
                        assert_eq!(err.code, PrivilegedErrorCode::PermissionDenied as i32);
                    },
                    _ => panic!("Expected PERMISSION_DENIED for session socket"),
                }
            }
        }
    }

    fn make_privileged_ctx() -> ConnectionContext {
        ConnectionContext::privileged_session_open(Some(PeerCredentials {
            uid: 1000,
            gid: 1000,
            pid: Some(12345),
        }))
    }

    fn make_session_ctx() -> ConnectionContext {
        ConnectionContext::session_open(
            Some(PeerCredentials {
                uid: 1000,
                gid: 1000,
                pid: Some(12346),
            }),
            Some("test-session".to_string()),
        )
    }

    // ========================================================================
    // ADV-001: Agent calls ClaimWork → PERMISSION_DENIED
    // ========================================================================
    #[test]
    fn test_adv_001_session_cannot_claim_work() {
        let dispatcher = PrivilegedDispatcher::new();
        let ctx = make_session_ctx();

        let request = ClaimWorkRequest {
            actor_id: "test-actor".to_string(),
            role: WorkRole::Implementer.into(),
            credential_signature: vec![],
            nonce: vec![],
        };
        let frame = encode_claim_work_request(&request);

        let response = dispatcher.dispatch(&frame, &ctx).unwrap();

        match response {
            PrivilegedResponse::Error(err) => {
                assert_eq!(err.code, PrivilegedErrorCode::PermissionDenied as i32);
                assert_eq!(err.message, "permission denied");
            },
            _ => panic!("Expected PERMISSION_DENIED error"),
        }
    }

    // ========================================================================
    // ADV-002: Agent calls SpawnEpisode → PERMISSION_DENIED
    // ========================================================================
    #[test]
    fn test_adv_002_session_cannot_spawn_episode() {
        let dispatcher = PrivilegedDispatcher::new();
        let ctx = make_session_ctx();

        let request = SpawnEpisodeRequest {
            workspace_root: test_workspace_root(),
            work_id: "W-001".to_string(),
            role: WorkRole::Implementer.into(),
            lease_id: None,
            adapter_profile_hash: None,
            max_episodes: None,
            escalation_predicate: None,
        };
        let frame = encode_spawn_episode_request(&request);

        let response = dispatcher.dispatch(&frame, &ctx).unwrap();

        match response {
            PrivilegedResponse::Error(err) => {
                assert_eq!(err.code, PrivilegedErrorCode::PermissionDenied as i32);
            },
            _ => panic!("Expected PERMISSION_DENIED error"),
        }
    }

    #[test]
    fn test_session_cannot_issue_capability() {
        let dispatcher = PrivilegedDispatcher::new();
        let ctx = make_session_ctx();

        let request = IssueCapabilityRequest {
            session_id: "S-001".to_string(),
            capability_request: None,
        };
        let frame = encode_issue_capability_request(&request);

        let response = dispatcher.dispatch(&frame, &ctx).unwrap();

        match response {
            PrivilegedResponse::Error(err) => {
                assert_eq!(err.code, PrivilegedErrorCode::PermissionDenied as i32);
            },
            _ => panic!("Expected PERMISSION_DENIED error"),
        }
    }

    #[test]
    fn test_session_cannot_shutdown() {
        let dispatcher = PrivilegedDispatcher::new();
        let ctx = make_session_ctx();

        let request = ShutdownRequest {
            reason: Some("test".to_string()),
        };
        let frame = encode_shutdown_request(&request);

        let response = dispatcher.dispatch(&frame, &ctx).unwrap();

        match response {
            PrivilegedResponse::Error(err) => {
                assert_eq!(err.code, PrivilegedErrorCode::PermissionDenied as i32);
            },
            _ => panic!("Expected PERMISSION_DENIED error"),
        }
    }

    // ========================================================================
    // Privileged Connection Tests (Success Path)
    // ========================================================================
    #[test]
    fn test_privileged_claim_work_stub() {
        let dispatcher = PrivilegedDispatcher::new();
        let ctx = make_privileged_ctx();

        let request = ClaimWorkRequest {
            actor_id: "test-actor".to_string(),
            role: WorkRole::Implementer.into(),
            credential_signature: vec![1, 2, 3],
            nonce: vec![4, 5, 6],
        };
        let frame = encode_claim_work_request(&request);

        let response = dispatcher.dispatch(&frame, &ctx).unwrap();

        match response {
            PrivilegedResponse::ClaimWork(resp) => {
                assert!(!resp.work_id.is_empty());
                assert!(!resp.lease_id.is_empty());
            },
            PrivilegedResponse::Error(err) => {
                panic!("Unexpected error: {err:?}");
            },
            _ => panic!("Expected ClaimWork response"),
        }
    }

    #[test]
    fn test_privileged_spawn_episode_stub() {
        let dispatcher = PrivilegedDispatcher::new();
        let ctx = make_privileged_ctx();

        // TCK-00256: First claim work to establish policy resolution
        let claim_request = ClaimWorkRequest {
            actor_id: "test-actor".to_string(),
            role: WorkRole::Implementer.into(),
            credential_signature: vec![1, 2, 3],
            nonce: vec![4, 5, 6],
        };
        let claim_frame = encode_claim_work_request(&claim_request);
        let claim_response = dispatcher.dispatch(&claim_frame, &ctx).unwrap();

        let (work_id, lease_id) = match claim_response {
            PrivilegedResponse::ClaimWork(resp) => (resp.work_id, resp.lease_id),
            _ => panic!("Expected ClaimWork response"),
        };

        // Now spawn with the claimed work_id
        let request = SpawnEpisodeRequest {
            workspace_root: test_workspace_root(),
            work_id,
            role: WorkRole::Implementer.into(),
            lease_id: Some(lease_id),
            adapter_profile_hash: None,
            max_episodes: None,
            escalation_predicate: None,
        };
        let frame = encode_spawn_episode_request(&request);

        let response = dispatcher.dispatch(&frame, &ctx).unwrap();

        match response {
            PrivilegedResponse::SpawnEpisode(resp) => {
                assert!(!resp.session_id.is_empty());
                assert!(!resp.ephemeral_handle.is_empty());
            },
            PrivilegedResponse::Error(err) => {
                panic!("Unexpected error: {err:?}");
            },
            _ => panic!("Expected SpawnEpisode response"),
        }
    }

    #[test]
    fn test_privileged_issue_capability_stub() {
        use crate::session::SessionState;

        let dispatcher = PrivilegedDispatcher::new();
        let ctx = make_privileged_ctx();

        // TCK-00289: Register a session and work claim for IssueCapability validation
        let work_id = "W-TEST-001";
        let lease_id = "L-TEST-001";
        let session_id = "S-001";

        // Register work claim
        let claim = WorkClaim {
            work_id: work_id.to_string(),
            lease_id: lease_id.to_string(),
            actor_id: "test-actor".to_string(),
            role: WorkRole::Implementer,
            policy_resolution: PolicyResolution {
                policy_resolved_ref: format!("resolved-for-{work_id}"),
                resolved_policy_hash: [0u8; 32],
                capability_manifest_hash: [0u8; 32],
                context_pack_hash: [0u8; 32],
                resolved_risk_tier: 0,
                resolved_scope_baseline: None,
                expected_adapter_profile_hash: None,
            },
            author_custody_domains: vec![],
            executor_custody_domains: vec![],
        };
        dispatcher.work_registry.register_claim(claim).unwrap();

        // Register session
        let session_state = SessionState {
            session_id: session_id.to_string(),
            work_id: work_id.to_string(),
            role: WorkRole::Implementer.into(),
            lease_id: lease_id.to_string(),
            ephemeral_handle: String::new(),
            policy_resolved_ref: String::new(),
            capability_manifest_hash: vec![],
            episode_id: None,
        };
        dispatcher
            .session_registry
            .register_session(session_state)
            .unwrap();

        let request = IssueCapabilityRequest {
            session_id: session_id.to_string(),
            capability_request: Some(super::super::messages::CapabilityRequest {
                tool_class: "file_read".to_string(),
                read_patterns: vec!["**/*.rs".to_string()],
                write_patterns: vec![],
                duration_secs: 3600,
            }),
        };
        let frame = encode_issue_capability_request(&request);

        let response = dispatcher.dispatch(&frame, &ctx).unwrap();

        match response {
            PrivilegedResponse::IssueCapability(resp) => {
                assert!(!resp.capability_id.is_empty());
                assert!(resp.capability_id.starts_with("C-")); // UUID-based ID
                // TCK-00289: HTF-compliant timestamps from HolonicClock
                // Per Definition of Done: "IssueCapability returns non-zero HTF-compliant
                // timestamps"
                assert!(
                    resp.granted_at > 0,
                    "granted_at should be non-zero HTF timestamp"
                );
                assert!(
                    resp.expires_at > resp.granted_at,
                    "expires_at should be after granted_at"
                );
                // Verify expires_at is granted_at + 1 hour (duration_secs in seconds)
                let expected_ttl_secs = 3600u64;
                assert_eq!(
                    resp.expires_at - resp.granted_at,
                    expected_ttl_secs,
                    "TTL should be 1 hour in seconds"
                );
            },
            PrivilegedResponse::Error(err) => {
                panic!("Unexpected error: {err:?}");
            },
            _ => panic!("Expected IssueCapability response"),
        }
    }

    /// IT-00392-01: Shutdown without daemon state returns stub response.
    #[test]
    fn test_privileged_shutdown_stub() {
        let dispatcher = PrivilegedDispatcher::new();
        let ctx = make_privileged_ctx();

        let request = ShutdownRequest {
            reason: Some("test shutdown".to_string()),
        };
        let frame = encode_shutdown_request(&request);

        let response = dispatcher.dispatch(&frame, &ctx).unwrap();

        match response {
            PrivilegedResponse::Shutdown(resp) => {
                assert!(!resp.message.is_empty());
                assert!(
                    resp.message.contains("stub"),
                    "stub response should indicate daemon state not configured: {}",
                    resp.message
                );
            },
            PrivilegedResponse::Error(err) => {
                panic!("Unexpected error: {err:?}");
            },
            _ => panic!("Expected Shutdown response"),
        }
    }

    /// IT-00392-02: Shutdown with daemon state sets shutdown flag.
    #[test]
    fn test_shutdown_with_daemon_state_sets_flag() {
        let (dispatcher, shared_state) = create_dispatcher_with_processes();
        let ctx = make_privileged_ctx();

        // Verify shutdown is not yet requested
        assert!(
            !shared_state.is_shutdown_requested(),
            "shutdown should not be requested before sending Shutdown"
        );

        let request = ShutdownRequest {
            reason: Some("operator requested stop".to_string()),
        };
        let frame = encode_shutdown_request(&request);

        let response = dispatcher.dispatch(&frame, &ctx).unwrap();

        // Verify response is success (not error)
        match &response {
            PrivilegedResponse::Shutdown(resp) => {
                assert!(
                    resp.message.contains("Shutdown initiated"),
                    "response should confirm shutdown initiation: {}",
                    resp.message
                );
                assert!(
                    resp.message.contains("operator requested stop"),
                    "response should echo the reason: {}",
                    resp.message
                );
            },
            PrivilegedResponse::Error(err) => {
                panic!("Unexpected error: {err:?}");
            },
            other => panic!("Expected Shutdown response, got {other:?}"),
        }

        // Verify the atomic shutdown flag was set
        assert!(
            shared_state.is_shutdown_requested(),
            "shutdown flag should be set after Shutdown command"
        );
    }

    /// IT-00392-03: Shutdown with no reason uses default display.
    #[test]
    fn test_shutdown_with_no_reason() {
        let (dispatcher, shared_state) = create_dispatcher_with_processes();
        let ctx = make_privileged_ctx();

        let request = ShutdownRequest { reason: None };
        let frame = encode_shutdown_request(&request);

        let response = dispatcher.dispatch(&frame, &ctx).unwrap();

        match &response {
            PrivilegedResponse::Shutdown(resp) => {
                assert!(
                    resp.message.contains("no reason provided"),
                    "response should indicate no reason: {}",
                    resp.message
                );
            },
            PrivilegedResponse::Error(err) => {
                panic!("Unexpected error: {err:?}");
            },
            other => panic!("Expected Shutdown response, got {other:?}"),
        }

        assert!(
            shared_state.is_shutdown_requested(),
            "shutdown flag should be set even without reason"
        );
    }

    // ========================================================================
    // ADV-005: ClaimWork role validation
    // ========================================================================
    #[test]
    fn test_adv_005_claim_work_validation() {
        let dispatcher = PrivilegedDispatcher::new();
        let ctx = make_privileged_ctx();

        // TCK-00253: Empty actor_id in request is OK (it's just a display hint)
        // The authoritative actor_id is derived from credential
        let request = ClaimWorkRequest {
            actor_id: String::new(), // Empty is OK - we derive from credential
            role: WorkRole::Implementer.into(),
            credential_signature: vec![],
            nonce: vec![1, 2, 3, 4], // Nonce for actor_id derivation
        };
        let frame = encode_claim_work_request(&request);
        let response = dispatcher.dispatch(&frame, &ctx).unwrap();

        // Should succeed now - actor_id is derived, not validated
        match response {
            PrivilegedResponse::ClaimWork(resp) => {
                assert!(!resp.work_id.is_empty());
                assert!(!resp.lease_id.is_empty());
            },
            PrivilegedResponse::Error(err) => {
                panic!("Unexpected error: {err:?}");
            },
            _ => panic!("Expected ClaimWork response"),
        }

        // Test missing role - still required
        let request = ClaimWorkRequest {
            actor_id: "test-actor".to_string(),
            role: WorkRole::Unspecified.into(),
            credential_signature: vec![],
            nonce: vec![],
        };
        let frame = encode_claim_work_request(&request);
        let response = dispatcher.dispatch(&frame, &ctx).unwrap();

        match response {
            PrivilegedResponse::Error(err) => {
                assert_eq!(
                    err.code,
                    PrivilegedErrorCode::CapabilityRequestRejected as i32
                );
                assert!(err.message.contains("role"));
            },
            _ => panic!("Expected validation error for unspecified role"),
        }
    }

    // ========================================================================
    // TCK-00253: Actor ID derived from credential tests
    // ========================================================================
    mod tck_00253 {
        use super::*;

        /// ADV-005: Actor ID must be derived from credential, not user input.
        ///
        /// This test verifies that:
        /// 1. Different user-provided `actor_ids` with the same credential
        ///    produce the same derived `actor_id`
        /// 2. Different nonces do NOT affect the derived `actor_id` (stable
        ///    identity)
        #[test]
        fn test_actor_id_derived_from_credential_not_user_input() {
            let dispatcher = PrivilegedDispatcher::new();
            let ctx = make_privileged_ctx();

            // Request 1: User provides "alice" with nonce A
            let request1 = ClaimWorkRequest {
                actor_id: "alice".to_string(),
                role: WorkRole::Implementer.into(),
                credential_signature: vec![],
                nonce: vec![1, 2, 3, 4, 5, 6, 7, 8],
            };
            let frame1 = encode_claim_work_request(&request1);
            let response1 = dispatcher.dispatch(&frame1, &ctx).unwrap();

            // Request 2: User provides "bob" with different nonce B
            // Per stable actor_id design: same credential = same actor_id regardless of
            // nonce
            let request2 = ClaimWorkRequest {
                actor_id: "bob".to_string(),
                role: WorkRole::Implementer.into(),
                credential_signature: vec![],
                nonce: vec![9, 9, 9, 9], // Different nonce - should NOT change actor_id
            };
            let frame2 = encode_claim_work_request(&request2);
            let response2 = dispatcher.dispatch(&frame2, &ctx).unwrap();

            // Both should succeed
            let PrivilegedResponse::ClaimWork(resp1) = response1 else {
                panic!("Expected ClaimWork response")
            };
            let PrivilegedResponse::ClaimWork(resp2) = response2 else {
                panic!("Expected ClaimWork response")
            };

            // Work IDs should be different (unique per claim)
            assert_ne!(resp1.work_id, resp2.work_id);

            // But the derived actor_id should be the same since credentials are the same
            // This is the key ADV-005 invariant: user input (actor_id, nonce) does NOT
            // affect the derived actor_id - only the Unix credential (UID, GID)
            // matters.
            let claim1 = dispatcher.work_registry.get_claim(&resp1.work_id);
            let claim2 = dispatcher.work_registry.get_claim(&resp2.work_id);

            assert!(claim1.is_some(), "Work claim 1 should be registered");
            assert!(claim2.is_some(), "Work claim 2 should be registered");

            // Same credential = same derived actor_id (stable identity)
            assert_eq!(
                claim1.unwrap().actor_id,
                claim2.unwrap().actor_id,
                "Derived actor_id should be the same for same credential (nonce is ignored)"
            );
        }

        /// Same credential always produces the same `actor_id` (stable
        /// identity).
        ///
        /// This is the inverse test of what was previously tested - we now
        /// verify that nonces do NOT produce different `actor_ids` (which
        /// was the bug).
        #[test]
        fn test_same_credential_produces_same_actor_id_regardless_of_nonce() {
            let dispatcher = PrivilegedDispatcher::new();
            let ctx = make_privileged_ctx();

            let request1 = ClaimWorkRequest {
                actor_id: "test".to_string(),
                role: WorkRole::Implementer.into(),
                credential_signature: vec![],
                nonce: vec![1, 1, 1, 1],
            };
            let frame1 = encode_claim_work_request(&request1);
            let response1 = dispatcher.dispatch(&frame1, &ctx).unwrap();

            let request2 = ClaimWorkRequest {
                actor_id: "test".to_string(),
                role: WorkRole::Implementer.into(),
                credential_signature: vec![],
                nonce: vec![2, 2, 2, 2], // Different nonce - should NOT change actor_id
            };
            let frame2 = encode_claim_work_request(&request2);
            let response2 = dispatcher.dispatch(&frame2, &ctx).unwrap();

            let PrivilegedResponse::ClaimWork(resp1) = response1 else {
                panic!("Expected ClaimWork response")
            };
            let PrivilegedResponse::ClaimWork(resp2) = response2 else {
                panic!("Expected ClaimWork response")
            };

            let claim1 = dispatcher.work_registry.get_claim(&resp1.work_id).unwrap();
            let claim2 = dispatcher.work_registry.get_claim(&resp2.work_id).unwrap();

            // Same credential = same actor_id (stable identity per code quality fix)
            assert_eq!(
                claim1.actor_id, claim2.actor_id,
                "Same credential should produce same actor_id regardless of nonce"
            );
        }

        /// Policy resolution is required for work claim.
        #[test]
        fn test_policy_resolution_required_for_claim() {
            let dispatcher = PrivilegedDispatcher::new();
            let ctx = make_privileged_ctx();

            let request = ClaimWorkRequest {
                actor_id: "test-actor".to_string(),
                role: WorkRole::Implementer.into(),
                credential_signature: vec![],
                nonce: vec![1, 2, 3, 4],
            };
            let frame = encode_claim_work_request(&request);
            let response = dispatcher.dispatch(&frame, &ctx).unwrap();

            // Should succeed with policy resolution reference
            match response {
                PrivilegedResponse::ClaimWork(resp) => {
                    assert!(
                        !resp.policy_resolved_ref.is_empty(),
                        "PolicyResolvedForChangeSet reference should be present"
                    );
                    assert!(
                        resp.policy_resolved_ref
                            .contains("PolicyResolvedForChangeSet"),
                        "Reference should indicate PolicyResolvedForChangeSet"
                    );
                    assert_eq!(
                        resp.capability_manifest_hash.len(),
                        32,
                        "Capability manifest hash should be 32 bytes"
                    );
                    assert_eq!(
                        resp.context_pack_hash.len(),
                        32,
                        "Context pack hash should be 32 bytes"
                    );
                },
                PrivilegedResponse::Error(err) => {
                    panic!("Unexpected error: {err:?}");
                },
                _ => panic!("Expected ClaimWork response"),
            }
        }

        /// Work claim is persisted in registry.
        #[test]
        fn test_work_claimed_event_persisted() {
            let dispatcher = PrivilegedDispatcher::new();
            let ctx = make_privileged_ctx();

            let request = ClaimWorkRequest {
                actor_id: "test-actor".to_string(),
                role: WorkRole::Reviewer.into(),
                credential_signature: vec![],
                nonce: vec![5, 6, 7, 8],
            };
            let frame = encode_claim_work_request(&request);
            let response = dispatcher.dispatch(&frame, &ctx).unwrap();

            let work_id = match response {
                PrivilegedResponse::ClaimWork(resp) => resp.work_id,
                _ => panic!("Expected ClaimWork response"),
            };

            // Verify the claim is queryable from the registry
            let claim = dispatcher
                .work_registry
                .get_claim(&work_id)
                .expect("Work claim should be persisted");

            assert_eq!(claim.work_id, work_id);
            assert_eq!(claim.role, WorkRole::Reviewer);
            assert!(claim.actor_id.starts_with("actor:"));
            assert!(!claim.policy_resolution.policy_resolved_ref.is_empty());
        }

        /// Missing credentials should fail.
        #[test]
        fn test_missing_credentials_fails() {
            let dispatcher = PrivilegedDispatcher::new();
            // Privileged connection but no credentials
            let ctx = ConnectionContext::privileged_session_open(None);

            let request = ClaimWorkRequest {
                actor_id: "test-actor".to_string(),
                role: WorkRole::Implementer.into(),
                credential_signature: vec![],
                nonce: vec![1, 2, 3, 4],
            };
            let frame = encode_claim_work_request(&request);
            let result = dispatcher.dispatch(&frame, &ctx);

            // Should fail because we can't derive actor_id without credentials
            assert!(result.is_err(), "Should fail when credentials are missing");
        }

        /// Test `derive_actor_id` function directly.
        ///
        /// Verifies that `actor_id` derivation is:
        /// 1. Deterministic (same credential = same output)
        /// 2. Independent of PID (different PIDs with same UID/GID = same
        ///    output)
        #[test]
        fn test_derive_actor_id_deterministic() {
            let creds = PeerCredentials {
                uid: 1000,
                gid: 1000,
                pid: Some(12345),
            };

            let actor1 = derive_actor_id(&creds);
            let actor2 = derive_actor_id(&creds);

            assert_eq!(
                actor1, actor2,
                "Same credential should produce same actor_id"
            );
            assert!(
                actor1.starts_with("actor:"),
                "Actor ID should have 'actor:' prefix"
            );

            // Different PID should NOT change actor_id (stable identity)
            let creds_different_pid = PeerCredentials {
                uid: 1000,
                gid: 1000,
                pid: Some(99999), // Different PID
            };

            let actor3 = derive_actor_id(&creds_different_pid);
            assert_eq!(
                actor1, actor3,
                "Different PID should NOT change actor_id (only UID/GID matter)"
            );

            // Different UID/GID SHOULD change actor_id
            let creds_different_user = PeerCredentials {
                uid: 2000, // Different UID
                gid: 1000,
                pid: Some(12345),
            };

            let actor4 = derive_actor_id(&creds_different_user);
            assert_ne!(
                actor1, actor4,
                "Different UID should produce different actor_id"
            );
        }

        /// Test work and lease ID generation.
        #[test]
        fn test_id_generation_unique() {
            let work_id1 = generate_work_id();
            let work_id2 = generate_work_id();
            assert_ne!(work_id1, work_id2, "Work IDs should be unique");
            assert!(work_id1.starts_with("W-"), "Work ID should start with 'W-'");

            let lease_id1 = generate_lease_id();
            let lease_id2 = generate_lease_id();
            assert_ne!(lease_id1, lease_id2, "Lease IDs should be unique");
            assert!(
                lease_id1.starts_with("L-"),
                "Lease ID should start with 'L-'"
            );
        }

        /// TCK-00253: `WorkClaimed` event is signed and persisted.
        ///
        /// Per acceptance criteria: "`WorkClaimed` event signed and persisted"
        /// This test verifies that:
        /// 1. A signed event is emitted when work is claimed
        /// 2. The event is queryable from the ledger
        /// 3. The signature is present and has the correct length
        #[test]
        fn test_work_claimed_event_signed_and_persisted() {
            let dispatcher = PrivilegedDispatcher::new();
            let ctx = make_privileged_ctx();

            let request = ClaimWorkRequest {
                actor_id: "test-actor".to_string(),
                role: WorkRole::Implementer.into(),
                credential_signature: vec![],
                nonce: vec![1, 2, 3, 4],
            };
            let frame = encode_claim_work_request(&request);
            let response = dispatcher.dispatch(&frame, &ctx).unwrap();

            let work_id = match response {
                PrivilegedResponse::ClaimWork(resp) => resp.work_id,
                _ => panic!("Expected ClaimWork response"),
            };

            // Query events by work_id from the ledger
            let events = dispatcher.event_emitter.get_events_by_work_id(&work_id);

            // TCK-00395: ClaimWork now emits both work_claimed and
            // work_transitioned(Open->Claimed)
            assert_eq!(
                events.len(),
                2,
                "ClaimWork should emit work_claimed + work_transitioned events"
            );

            let event = &events[0];
            assert_eq!(event.work_id, work_id);
            assert_eq!(event.event_type, "work_claimed");
            assert!(!event.signature.is_empty(), "Event should be signed");
            assert_eq!(
                event.signature.len(),
                64,
                "Ed25519 signature should be 64 bytes"
            );
            assert!(
                event.event_id.starts_with("EVT-"),
                "Event ID should have EVT- prefix"
            );
            // TCK-00289: HTF-compliant timestamps from HolonicClock
            // Per Definition of Done: timestamps must be non-zero HTF-compliant
            assert!(
                event.timestamp_ns > 0,
                "Timestamp should be non-zero HTF-compliant value"
            );

            // Verify payload contains expected fields
            let payload: serde_json::Value =
                serde_json::from_slice(&event.payload).expect("Payload should be valid JSON");
            assert_eq!(payload["event_type"], "work_claimed");
            assert_eq!(payload["work_id"], work_id);
            assert!(payload["actor_id"].as_str().unwrap().starts_with("actor:"));
            assert!(payload["policy_resolved_ref"].as_str().is_some());

            // Also verify the event is queryable by event_id
            let queried_event = dispatcher.event_emitter.get_event(&event.event_id);
            assert!(queried_event.is_some(), "Event should be queryable by ID");
            assert_eq!(queried_event.unwrap().event_id, event.event_id);
        }

        /// TCK-00253: Ledger query returns signed event.
        ///
        /// Per acceptance criteria: "Ledger query returns signed event"
        #[test]
        fn test_ledger_query_returns_signed_event() {
            let dispatcher = PrivilegedDispatcher::new();
            let ctx = make_privileged_ctx();

            // Claim work
            let request = ClaimWorkRequest {
                actor_id: "test-actor".to_string(),
                role: WorkRole::Reviewer.into(),
                credential_signature: vec![],
                nonce: vec![5, 6, 7, 8],
            };
            let frame = encode_claim_work_request(&request);
            let response = dispatcher.dispatch(&frame, &ctx).unwrap();

            let work_id = match response {
                PrivilegedResponse::ClaimWork(resp) => resp.work_id,
                _ => panic!("Expected ClaimWork response"),
            };

            // Query events by work_id
            let events = dispatcher.event_emitter.get_events_by_work_id(&work_id);

            // Verify at least one signed event is returned
            assert!(!events.is_empty(), "Should return at least one event");
            let event = &events[0];
            assert!(
                !event.signature.is_empty(),
                "Queried event should have signature"
            );
            assert_eq!(
                event.signature.len(),
                64,
                "Signature should be Ed25519 (64 bytes)"
            );
        }
    }

    #[test]
    fn test_gate_executor_requires_lease_id() {
        let dispatcher = PrivilegedDispatcher::new();
        let ctx = make_privileged_ctx();

        let request = SpawnEpisodeRequest {
            workspace_root: test_workspace_root(),
            work_id: "W-001".to_string(),
            role: WorkRole::GateExecutor.into(),
            lease_id: None, // Missing required lease_id
            adapter_profile_hash: None,
            max_episodes: None,
            escalation_predicate: None,
        };
        let frame = encode_spawn_episode_request(&request);

        let response = dispatcher.dispatch(&frame, &ctx).unwrap();

        match response {
            PrivilegedResponse::Error(err) => {
                assert_eq!(err.code, PrivilegedErrorCode::GateLeaseMissing as i32);
            },
            _ => panic!("Expected GATE_LEASE_MISSING error"),
        }
    }

    #[test]
    fn test_gate_executor_with_lease_id_succeeds() {
        let dispatcher = PrivilegedDispatcher::new();
        let ctx = make_privileged_ctx();

        // First, claim work with GateExecutor role to establish policy resolution
        let claim_request = ClaimWorkRequest {
            actor_id: "test-actor".to_string(),
            role: WorkRole::GateExecutor.into(),
            credential_signature: vec![1, 2, 3],
            nonce: vec![4, 5, 6],
        };
        let claim_frame = encode_claim_work_request(&claim_request);
        let claim_response = dispatcher.dispatch(&claim_frame, &ctx).unwrap();

        // SEC-SCP-FAC-0020: Get the correct lease_id from the claim response
        let (work_id, lease_id) = match claim_response {
            PrivilegedResponse::ClaimWork(resp) => (resp.work_id, resp.lease_id),
            _ => panic!("Expected ClaimWork response"),
        };

        // TCK-00257: Register the lease for validation
        dispatcher
            .lease_validator()
            .register_lease(&lease_id, &work_id, "gate-build");

        // Now spawn with the claimed work_id and correct lease_id
        let request = SpawnEpisodeRequest {
            workspace_root: test_workspace_root(),
            work_id,
            role: WorkRole::GateExecutor.into(),
            lease_id: Some(lease_id), // Use the correct lease_id from ClaimWork
            adapter_profile_hash: None,
            max_episodes: None,
            escalation_predicate: None,
        };
        let frame = encode_spawn_episode_request(&request);

        let response = dispatcher.dispatch(&frame, &ctx).unwrap();

        match response {
            PrivilegedResponse::SpawnEpisode(_) => {
                // Success
            },
            PrivilegedResponse::Error(err) => {
                panic!("Unexpected error: {err:?}");
            },
            _ => panic!("Expected SpawnEpisode response"),
        }
    }

    // ========================================================================
    // SEC-SCP-FAC-0020: Lease ID Validation Tests
    // ========================================================================

    /// SEC-SCP-FAC-0020: `SpawnEpisode` with wrong `lease_id` fails.
    ///
    /// Per security review: `lease_id` must be validated against the claim to
    /// prevent authorization bypass.
    #[test]
    fn tck_00256_spawn_with_wrong_lease_id_fails() {
        let dispatcher = PrivilegedDispatcher::new();
        let ctx = make_privileged_ctx();

        // Claim work with GateExecutor role
        let claim_request = ClaimWorkRequest {
            actor_id: "test-actor".to_string(),
            role: WorkRole::GateExecutor.into(),
            credential_signature: vec![1, 2, 3],
            nonce: vec![4, 5, 6],
        };
        let claim_frame = encode_claim_work_request(&claim_request);
        let claim_response = dispatcher.dispatch(&claim_frame, &ctx).unwrap();

        let work_id = match claim_response {
            PrivilegedResponse::ClaimWork(resp) => resp.work_id,
            _ => panic!("Expected ClaimWork response"),
        };

        // Try to spawn with a WRONG lease_id (arbitrary string)
        let request = SpawnEpisodeRequest {
            workspace_root: test_workspace_root(),
            work_id,
            role: WorkRole::GateExecutor.into(),
            lease_id: Some("L-WRONG-LEASE-ID".to_string()), // Wrong!
            adapter_profile_hash: None,
            max_episodes: None,
            escalation_predicate: None,
        };
        let frame = encode_spawn_episode_request(&request);

        let response = dispatcher.dispatch(&frame, &ctx).unwrap();

        match response {
            PrivilegedResponse::Error(err) => {
                // TCK-00257: With lease validation, wrong lease_id fails at
                // lease validation (GATE_LEASE_MISSING) before claim validation
                assert_eq!(
                    err.code,
                    PrivilegedErrorCode::GateLeaseMissing as i32,
                    "Should return GateLeaseMissing for unknown lease_id"
                );
                assert!(
                    err.message.contains("lease"),
                    "Error message should mention lease: {}",
                    err.message
                );
            },
            _ => panic!("Expected lease validation error, got: {response:?}"),
        }
    }

    /// SEC-SCP-FAC-0020: `SpawnEpisode` with MISSING `lease_id` fails.
    ///
    /// Security Fix Verification: Ensure that omitting `lease_id` (None) is NOT
    /// treated as a valid bypass. It must match the claimed `lease_id`.
    #[test]
    fn tck_00256_spawn_with_missing_lease_id_fails() {
        let dispatcher = PrivilegedDispatcher::new();
        let ctx = make_privileged_ctx();

        // Claim work with Implementer role
        let claim_request = ClaimWorkRequest {
            actor_id: "test-actor".to_string(),
            role: WorkRole::Implementer.into(),
            credential_signature: vec![1, 2, 3],
            nonce: vec![4, 5, 6],
        };
        let claim_frame = encode_claim_work_request(&claim_request);
        let claim_response = dispatcher.dispatch(&claim_frame, &ctx).unwrap();

        let work_id = match claim_response {
            PrivilegedResponse::ClaimWork(resp) => resp.work_id,
            _ => panic!("Expected ClaimWork response"),
        };

        // Try to spawn with NO lease_id (None)
        let request = SpawnEpisodeRequest {
            workspace_root: test_workspace_root(),
            work_id,
            role: WorkRole::Implementer.into(),
            lease_id: None, // Missing! Should fail because claim has a lease_id
            adapter_profile_hash: None,
            max_episodes: None,
            escalation_predicate: None,
        };
        let frame = encode_spawn_episode_request(&request);

        let response = dispatcher.dispatch(&frame, &ctx).unwrap();

        match response {
            PrivilegedResponse::Error(err) => {
                assert_eq!(
                    err.code,
                    PrivilegedErrorCode::CapabilityRequestRejected as i32,
                    "Should return CapabilityRequestRejected for missing lease_id"
                );
                assert!(
                    err.message.contains("lease_id"),
                    "Error message should mention lease_id: {}",
                    err.message
                );
            },
            _ => panic!("Expected lease_id mismatch error for Missing ID, got: {response:?}"),
        }
    }

    /// SEC-SCP-FAC-0020: `SpawnEpisode` with correct `lease_id` succeeds.
    #[test]
    fn tck_00256_spawn_with_correct_lease_id_succeeds() {
        let dispatcher = PrivilegedDispatcher::new();
        let ctx = make_privileged_ctx();

        // Claim work
        let claim_request = ClaimWorkRequest {
            actor_id: "test-actor".to_string(),
            role: WorkRole::Implementer.into(),
            credential_signature: vec![1, 2, 3],
            nonce: vec![4, 5, 6],
        };
        let claim_frame = encode_claim_work_request(&claim_request);
        let claim_response = dispatcher.dispatch(&claim_frame, &ctx).unwrap();

        let (work_id, lease_id) = match claim_response {
            PrivilegedResponse::ClaimWork(resp) => (resp.work_id, resp.lease_id),
            _ => panic!("Expected ClaimWork response"),
        };

        // Spawn with the correct lease_id (optional for non-GateExecutor)
        let request = SpawnEpisodeRequest {
            workspace_root: test_workspace_root(),
            work_id,
            role: WorkRole::Implementer.into(),
            lease_id: Some(lease_id), // Correct lease_id
            adapter_profile_hash: None,
            max_episodes: None,
            escalation_predicate: None,
        };
        let frame = encode_spawn_episode_request(&request);

        let response = dispatcher.dispatch(&frame, &ctx).unwrap();

        match response {
            PrivilegedResponse::SpawnEpisode(resp) => {
                assert!(!resp.session_id.is_empty());
            },
            PrivilegedResponse::Error(err) => {
                panic!("Unexpected error: {err:?}");
            },
            _ => panic!("Expected SpawnEpisode response"),
        }
    }

    /// SEC-SCP-FAC-0020: Session state is persisted after successful spawn.
    #[test]
    fn tck_00256_session_state_persisted() {
        let dispatcher = PrivilegedDispatcher::new();
        let ctx = make_privileged_ctx();

        // Claim work
        let claim_request = ClaimWorkRequest {
            actor_id: "test-actor".to_string(),
            role: WorkRole::Implementer.into(),
            credential_signature: vec![1, 2, 3],
            nonce: vec![4, 5, 6],
        };
        let claim_frame = encode_claim_work_request(&claim_request);
        let claim_response = dispatcher.dispatch(&claim_frame, &ctx).unwrap();

        let (work_id, lease_id) = match claim_response {
            PrivilegedResponse::ClaimWork(resp) => (resp.work_id, resp.lease_id),
            _ => panic!("Expected ClaimWork response"),
        };

        // Spawn episode
        let request = SpawnEpisodeRequest {
            workspace_root: test_workspace_root(),
            work_id: work_id.clone(),
            role: WorkRole::Implementer.into(),
            lease_id: Some(lease_id),
            adapter_profile_hash: None,
            max_episodes: None,
            escalation_predicate: None,
        };
        let frame = encode_spawn_episode_request(&request);
        let response = dispatcher.dispatch(&frame, &ctx).unwrap();

        let (session_id, ephemeral_handle) = match response {
            PrivilegedResponse::SpawnEpisode(resp) => (resp.session_id, resp.ephemeral_handle),
            _ => panic!("Expected SpawnEpisode response"),
        };

        // Verify session is persisted
        let session = dispatcher.session_registry.get_session(&session_id);
        assert!(session.is_some(), "Session should be persisted");

        let session = session.unwrap();
        assert_eq!(session.session_id, session_id);
        assert_eq!(session.work_id, work_id);
        assert_eq!(session.role, i32::from(WorkRole::Implementer));
        assert_eq!(session.ephemeral_handle, ephemeral_handle);

        // Also verify we can query by ephemeral handle
        let session_by_handle = dispatcher
            .session_registry
            .get_session_by_handle(&ephemeral_handle);
        assert!(
            session_by_handle.is_some(),
            "Session should be queryable by handle"
        );
        assert_eq!(session_by_handle.unwrap().session_id, session_id);
    }

    // ========================================================================
    // TCK-00257: ADV-004 Gate Lease Validation Tests
    // ========================================================================

    /// ADV-004: `GATE_EXECUTOR` spawn with unknown/unregistered `lease_id`
    /// fails.
    ///
    /// This test verifies that the ledger is queried for a valid
    /// `GateLeaseIssued` event. If the lease is not found, the spawn is
    /// rejected with `GATE_LEASE_MISSING`.
    #[test]
    fn test_adv_004_gate_executor_unknown_lease_fails() {
        let dispatcher = PrivilegedDispatcher::new();
        let ctx = make_privileged_ctx();

        // DO NOT register any lease - this simulates an unknown/invalid lease

        let request = SpawnEpisodeRequest {
            workspace_root: test_workspace_root(),
            work_id: "W-001".to_string(),
            role: WorkRole::GateExecutor.into(),
            lease_id: Some("L-UNKNOWN".to_string()), // Not registered
            adapter_profile_hash: None,
            max_episodes: None,
            escalation_predicate: None,
        };
        let frame = encode_spawn_episode_request(&request);

        let response = dispatcher.dispatch(&frame, &ctx).unwrap();

        match response {
            PrivilegedResponse::Error(err) => {
                assert_eq!(
                    err.code,
                    PrivilegedErrorCode::GateLeaseMissing as i32,
                    "Should fail with GATE_LEASE_MISSING for unknown lease"
                );
                assert!(
                    err.message.contains("lease not found"),
                    "Error message should indicate lease not found: {}",
                    err.message
                );
            },
            _ => panic!("Expected GATE_LEASE_MISSING error for unknown lease"),
        }
    }

    /// ADV-004: `GATE_EXECUTOR` spawn with mismatched `work_id` fails.
    ///
    /// This test verifies that the lease's `work_id` must match the request's
    /// `work_id`. A lease for work W-001 cannot be used for spawn on W-002.
    #[test]
    fn test_adv_004_gate_executor_work_id_mismatch_fails() {
        let dispatcher = PrivilegedDispatcher::new();
        let ctx = make_privileged_ctx();

        // Register lease for W-001
        dispatcher
            .lease_validator()
            .register_lease("L-001", "W-001", "gate-build");

        // Try to use that lease for W-002 (different work_id)
        let request = SpawnEpisodeRequest {
            workspace_root: test_workspace_root(),
            work_id: "W-002".to_string(), // Mismatched work_id
            role: WorkRole::GateExecutor.into(),
            lease_id: Some("L-001".to_string()),
            adapter_profile_hash: None,
            max_episodes: None,
            escalation_predicate: None,
        };
        let frame = encode_spawn_episode_request(&request);

        let response = dispatcher.dispatch(&frame, &ctx).unwrap();

        match response {
            PrivilegedResponse::Error(err) => {
                assert_eq!(
                    err.code,
                    PrivilegedErrorCode::GateLeaseMissing as i32,
                    "Should fail with GATE_LEASE_MISSING for work_id mismatch"
                );
                assert!(
                    err.message.contains("mismatch"),
                    "Error message should indicate work_id mismatch: {}",
                    err.message
                );
            },
            _ => panic!("Expected GATE_LEASE_MISSING error for work_id mismatch"),
        }
    }

    /// ADV-004: `GATE_EXECUTOR` spawn with valid registered lease succeeds
    /// (at the lease validation stage).
    ///
    /// This test verifies that a properly registered lease that matches
    /// the `work_id` passes the lease validation. Note: The spawn may still
    /// fail at the claim validation stage if `ClaimWork` wasn't called first.
    #[test]
    fn test_adv_004_gate_executor_valid_lease_passes_validation() {
        let dispatcher = PrivilegedDispatcher::new();
        let ctx = make_privileged_ctx();

        // Register the lease for the correct work_id
        dispatcher
            .lease_validator()
            .register_lease("L-VALID", "W-VALID", "gate-aat");

        let request = SpawnEpisodeRequest {
            workspace_root: test_workspace_root(),
            work_id: "W-VALID".to_string(),
            role: WorkRole::GateExecutor.into(),
            lease_id: Some("L-VALID".to_string()),
            adapter_profile_hash: None,
            max_episodes: None,
            escalation_predicate: None,
        };
        let frame = encode_spawn_episode_request(&request);

        let response = dispatcher.dispatch(&frame, &ctx).unwrap();

        // The response should NOT be GATE_LEASE_MISSING - the lease validation
        // passed. It may fail for other reasons (policy resolution missing),
        // but not lease validation.
        if let PrivilegedResponse::Error(err) = &response {
            assert_ne!(
                err.code,
                PrivilegedErrorCode::GateLeaseMissing as i32,
                "Should NOT fail with GATE_LEASE_MISSING - lease is valid"
            );
            // Expected to fail with PolicyResolutionMissing since we didn't
            // call ClaimWork
            assert_eq!(
                err.code,
                PrivilegedErrorCode::PolicyResolutionMissing as i32,
                "Should fail with PolicyResolutionMissing (no ClaimWork)"
            );
        }
        // If it somehow succeeded, that's also fine for this test
    }

    // ========================================================================
    // Protocol Error Tests
    // ========================================================================
    #[test]
    fn test_empty_frame_error() {
        let dispatcher = PrivilegedDispatcher::new();
        let ctx = make_privileged_ctx();

        let frame = Bytes::new();
        let result = dispatcher.dispatch(&frame, &ctx);

        assert!(result.is_err());
        assert!(matches!(result, Err(ProtocolError::Serialization { .. })));
    }

    #[test]
    fn test_unknown_message_type_error() {
        let dispatcher = PrivilegedDispatcher::new();
        let ctx = make_privileged_ctx();

        let frame = Bytes::from(vec![255u8, 0, 0, 0]); // Unknown tag
        let result = dispatcher.dispatch(&frame, &ctx);

        assert!(result.is_err());
        assert!(matches!(result, Err(ProtocolError::Serialization { .. })));
    }

    #[test]
    fn test_malformed_payload_error() {
        let dispatcher = PrivilegedDispatcher::new();
        let ctx = make_privileged_ctx();

        let frame = Bytes::from(vec![1u8, 0xFF, 0xFF, 0xFF]); // ClaimWork tag + garbage
        let result = dispatcher.dispatch(&frame, &ctx);

        assert!(result.is_err());
    }

    // ========================================================================
    // Connection Context Tests
    // ========================================================================
    #[test]
    fn test_connection_context_privileged() {
        let ctx = ConnectionContext::privileged(Some(PeerCredentials {
            uid: 1000,
            gid: 1000,
            pid: Some(123),
        }));

        assert!(ctx.is_privileged());
        assert!(ctx.peer_credentials().is_some());
        assert!(ctx.session_id().is_none());
    }

    #[test]
    fn test_connection_context_session() {
        let ctx = ConnectionContext::session(
            Some(PeerCredentials {
                uid: 1000,
                gid: 1000,
                pid: Some(456),
            }),
            Some("session-123".to_string()),
        );

        assert!(!ctx.is_privileged());
        assert!(ctx.peer_credentials().is_some());
        assert_eq!(ctx.session_id(), Some("session-123"));
    }

    // ========================================================================
    // Response Encoding Tests
    // ========================================================================
    #[test]
    fn test_response_encoding() {
        let error_resp = PrivilegedResponse::permission_denied();
        let encoded = error_resp.encode();
        assert!(!encoded.is_empty());
        assert_eq!(encoded[0], 0); // Error tag

        let claim_resp = PrivilegedResponse::ClaimWork(ClaimWorkResponse {
            work_id: "W-001".to_string(),
            lease_id: "L-001".to_string(),
            capability_manifest_hash: vec![],
            policy_resolved_ref: String::new(),
            context_pack_hash: vec![],
        });
        let encoded = claim_resp.encode();
        assert!(!encoded.is_empty());
        assert_eq!(encoded[0], PrivilegedMessageType::ClaimWork.tag());
    }

    // ========================================================================
    // TCK-00256: SpawnEpisode with PolicyResolvedForChangeSet check
    // ========================================================================

    /// TCK-00256: Spawn without policy resolution fails (fail-closed).
    ///
    /// Per acceptance criteria: "Spawn without policy resolution fails"
    /// This test verifies ADV-004 variant: attempting to spawn an episode
    /// without first calling `ClaimWork` to establish policy resolution.
    #[test]
    fn tck_00256_spawn_without_policy_resolution_fails() {
        let dispatcher = PrivilegedDispatcher::new();
        let ctx = make_privileged_ctx();

        // Attempt to spawn for a non-existent work_id (no ClaimWork was called)
        let request = SpawnEpisodeRequest {
            workspace_root: test_workspace_root(),
            work_id: "W-NONEXISTENT".to_string(),
            role: WorkRole::Implementer.into(),
            lease_id: None,
            adapter_profile_hash: None,
            max_episodes: None,
            escalation_predicate: None,
        };
        let frame = encode_spawn_episode_request(&request);

        let response = dispatcher.dispatch(&frame, &ctx).unwrap();

        match response {
            PrivilegedResponse::Error(err) => {
                assert_eq!(
                    err.code,
                    PrivilegedErrorCode::PolicyResolutionMissing as i32,
                    "Should return PolicyResolutionMissing error"
                );
                assert!(
                    err.message.contains("policy resolution not found"),
                    "Error message should indicate policy resolution is missing: {}",
                    err.message
                );
            },
            _ => panic!("Expected PolicyResolutionMissing error, got: {response:?}"),
        }
    }

    /// TCK-00256: Valid policy resolution allows spawn.
    ///
    /// Per acceptance criteria: "Valid policy resolution allows spawn"
    /// This test verifies the integration flow: `ClaimWork` followed by
    /// `SpawnEpisode`.
    #[test]
    fn tck_00256_spawn_with_policy_resolution_succeeds() {
        let dispatcher = PrivilegedDispatcher::new();
        let ctx = make_privileged_ctx();

        // 1. Claim Work (generates policy resolution and persists it)
        let claim_req = ClaimWorkRequest {
            actor_id: "test-actor".to_string(),
            role: WorkRole::Implementer.into(),
            credential_signature: vec![1, 2, 3],
            nonce: vec![4, 5, 6],
        };
        let claim_frame = encode_claim_work_request(&claim_req);
        let claim_response = dispatcher.dispatch(&claim_frame, &ctx).unwrap();

        let (work_id, expected_manifest_hash, lease_id) = match claim_response {
            PrivilegedResponse::ClaimWork(resp) => {
                (resp.work_id, resp.capability_manifest_hash, resp.lease_id)
            },
            _ => panic!("Expected ClaimWork response"),
        };

        // 2. Spawn Episode (should succeed because ClaimWork persisted the resolution)
        let spawn_req = SpawnEpisodeRequest {
            workspace_root: test_workspace_root(),
            work_id,
            role: WorkRole::Implementer.into(),
            lease_id: Some(lease_id),
            adapter_profile_hash: None,
            max_episodes: None,
            escalation_predicate: None,
        };
        let spawn_frame = encode_spawn_episode_request(&spawn_req);

        let response = dispatcher.dispatch(&spawn_frame, &ctx).unwrap();

        match response {
            PrivilegedResponse::SpawnEpisode(resp) => {
                assert!(
                    !resp.session_id.is_empty(),
                    "Session ID should not be empty"
                );
                assert!(
                    resp.session_id.starts_with("S-"),
                    "Session ID should start with S-"
                );
                assert!(
                    !resp.ephemeral_handle.is_empty(),
                    "Ephemeral handle should not be empty"
                );
                assert!(
                    resp.ephemeral_handle.starts_with("H-"),
                    "Ephemeral handle should start with H-"
                );
                assert_eq!(
                    resp.capability_manifest_hash, expected_manifest_hash,
                    "Capability manifest hash should match the one from ClaimWork"
                );
                assert!(resp.context_pack_sealed, "Context pack should be sealed");
            },
            PrivilegedResponse::Error(err) => {
                panic!("Unexpected error: {err:?}");
            },
            _ => panic!("Expected SpawnEpisode response"),
        }
    }

    /// TCK-00256: `SpawnEpisode` with mismatched role fails.
    ///
    /// Per DD-001, the role in the spawn request should match the claimed role.
    /// This test verifies that attempting to spawn with a different role fails.
    #[test]
    fn tck_00256_spawn_with_mismatched_role_fails() {
        let dispatcher = PrivilegedDispatcher::new();
        let ctx = make_privileged_ctx();

        // 1. Claim Work with Implementer role
        let claim_req = ClaimWorkRequest {
            actor_id: "test-actor".to_string(),
            role: WorkRole::Implementer.into(),
            credential_signature: vec![1, 2, 3],
            nonce: vec![4, 5, 6],
        };
        let claim_frame = encode_claim_work_request(&claim_req);
        let claim_response = dispatcher.dispatch(&claim_frame, &ctx).unwrap();

        let work_id = match claim_response {
            PrivilegedResponse::ClaimWork(resp) => resp.work_id,
            _ => panic!("Expected ClaimWork response"),
        };

        // 2. Try to spawn with Reviewer role (mismatched)
        let spawn_req = SpawnEpisodeRequest {
            workspace_root: test_workspace_root(),
            work_id,
            role: WorkRole::Reviewer.into(), // Different from claimed role
            lease_id: None,
            adapter_profile_hash: None,
            max_episodes: None,
            escalation_predicate: None,
        };
        let spawn_frame = encode_spawn_episode_request(&spawn_req);

        let response = dispatcher.dispatch(&spawn_frame, &ctx).unwrap();

        match response {
            PrivilegedResponse::Error(err) => {
                assert_eq!(
                    err.code,
                    PrivilegedErrorCode::CapabilityRequestRejected as i32,
                    "Should return CapabilityRequestRejected for role mismatch"
                );
                assert!(
                    err.message.contains("role mismatch"),
                    "Error message should indicate role mismatch: {}",
                    err.message
                );
            },
            _ => panic!("Expected role mismatch error, got: {response:?}"),
        }
    }

    /// TCK-00256: `SpawnEpisode` returns policy resolution data.
    ///
    /// Verifies that the spawn response includes the capability manifest hash
    /// from the original policy resolution.
    #[test]
    fn tck_00256_spawn_returns_policy_resolution_data() {
        let dispatcher = PrivilegedDispatcher::new();
        let ctx = make_privileged_ctx();

        // Claim work
        let claim_req = ClaimWorkRequest {
            actor_id: "test-actor".to_string(),
            role: WorkRole::Implementer.into(),
            credential_signature: vec![1, 2, 3],
            nonce: vec![4, 5, 6],
        };
        let claim_frame = encode_claim_work_request(&claim_req);
        let claim_response = dispatcher.dispatch(&claim_frame, &ctx).unwrap();

        let (work_id, claim_manifest_hash, _claim_context_hash, lease_id) = match claim_response {
            PrivilegedResponse::ClaimWork(resp) => (
                resp.work_id,
                resp.capability_manifest_hash,
                resp.context_pack_hash,
                resp.lease_id,
            ),
            _ => panic!("Expected ClaimWork response"),
        };

        // Spawn episode
        let spawn_req = SpawnEpisodeRequest {
            workspace_root: test_workspace_root(),
            work_id,
            role: WorkRole::Implementer.into(),
            lease_id: Some(lease_id),
            adapter_profile_hash: None,
            max_episodes: None,
            escalation_predicate: None,
        };
        let spawn_frame = encode_spawn_episode_request(&spawn_req);
        let spawn_response = dispatcher.dispatch(&spawn_frame, &ctx).unwrap();

        match spawn_response {
            PrivilegedResponse::SpawnEpisode(resp) => {
                // Verify the capability manifest hash matches
                assert_eq!(
                    resp.capability_manifest_hash, claim_manifest_hash,
                    "SpawnEpisode should return same capability_manifest_hash as ClaimWork"
                );
                // Verify context pack is marked as sealed
                assert!(resp.context_pack_sealed);
            },
            _ => panic!("Expected SpawnEpisode response"),
        }
    }

    // ========================================================================
    // CTR-1303: Bounded Store Tests (DoS Protection)
    // ========================================================================

    /// CTR-1303: `StubWorkRegistry` enforces capacity limits.
    ///
    /// Per CTR-1303: In-memory stores must have `max_entries` limit with O(1)
    /// eviction. This test verifies that the registry evicts oldest entries
    /// when at capacity.
    #[test]
    fn test_stub_work_registry_capacity_limit() {
        let registry = StubWorkRegistry::default();

        // Register claims up to capacity
        // Note: We test with a smaller number to keep the test fast
        let test_limit = 100; // Test with 100 instead of 10_000

        for i in 0..test_limit {
            let claim = WorkClaim {
                work_id: format!("W-{i:05}"),
                lease_id: format!("L-{i:05}"),
                actor_id: format!("actor:{i:016x}"),
                role: WorkRole::Implementer,
                policy_resolution: PolicyResolution {
                    policy_resolved_ref: format!("PolicyResolvedForChangeSet:{i}"),
                    resolved_policy_hash: [0u8; 32],
                    capability_manifest_hash: [0u8; 32],
                    context_pack_hash: [0u8; 32],
                    resolved_risk_tier: 0,
                    resolved_scope_baseline: None,
                    expected_adapter_profile_hash: None,
                },
                executor_custody_domains: vec![],
                author_custody_domains: vec![],
            };
            registry.register_claim(claim).unwrap();
        }

        // All claims should be present
        for i in 0..test_limit {
            let work_id = format!("W-{i:05}");
            assert!(
                registry.get_claim(&work_id).is_some(),
                "Claim {work_id} should exist"
            );
        }
    }

    /// CTR-1303: `StubWorkRegistry` rejects duplicate `work_ids`.
    #[test]
    fn test_stub_work_registry_rejects_duplicates() {
        let registry = StubWorkRegistry::default();

        let claim = WorkClaim {
            work_id: "W-DUPLICATE".to_string(),
            lease_id: "L-001".to_string(),
            actor_id: "actor:test".to_string(),
            role: WorkRole::Implementer,
            policy_resolution: PolicyResolution {
                policy_resolved_ref: "PolicyResolvedForChangeSet:test".to_string(),
                resolved_policy_hash: [0u8; 32],
                capability_manifest_hash: [0u8; 32],
                context_pack_hash: [0u8; 32],
                resolved_risk_tier: 0,
                resolved_scope_baseline: None,
                expected_adapter_profile_hash: None,
            },
            executor_custody_domains: vec![],
            author_custody_domains: vec![],
        };

        // First registration succeeds
        assert!(registry.register_claim(claim.clone()).is_ok());

        // Second registration with same work_id fails
        let result = registry.register_claim(claim);
        assert!(matches!(
            result,
            Err(WorkRegistryError::DuplicateWorkId { .. })
        ));
    }

    // ========================================================================
    // TCK-00258: SoD Enforcement Integration Tests
    //
    // These tests verify that Separation of Duties (SoD) is enforced when
    // spawning GATE_EXECUTOR episodes. The custody domain check prevents
    // actors from reviewing their own work (self-review attacks).
    // ========================================================================

    /// TCK-00258: `GATE_EXECUTOR` spawn with overlapping custody domains is
    /// denied.
    ///
    /// This tests the fail-closed `SoD` enforcement: when the executor's
    /// custody domains overlap with the changeset author's domains, the
    /// spawn must be rejected with `SOD_VIOLATION` error.
    #[test]
    fn test_sod_spawn_overlapping_domains_denied() {
        let dispatcher = PrivilegedDispatcher::new();
        let ctx = ConnectionContext::privileged_session_open(Some(PeerCredentials {
            uid: 1001,
            gid: 1001,
            pid: Some(12345),
        }));

        // First, claim work as GATE_EXECUTOR with overlapping domains
        // Actor ID: team-alpha:alice -> domain: team-alpha
        // Work ID: W-team-alpha-12345 -> author domain: team-alpha
        // These domains overlap, so SoD should be violated
        let claim_request = ClaimWorkRequest {
            actor_id: "team-alpha:alice".to_string(),
            role: WorkRole::GateExecutor.into(),
            credential_signature: vec![],
            nonce: vec![],
        };
        let claim_frame = encode_claim_work_request(&claim_request);
        let claim_response = dispatcher.dispatch(&claim_frame, &ctx).unwrap();

        // Extract work_id and lease_id from claim response
        let (_work_id, lease_id) = match claim_response {
            PrivilegedResponse::ClaimWork(resp) => (resp.work_id, resp.lease_id),
            _ => panic!("Expected ClaimWork response"),
        };

        // Create a new claim with a specific work_id format
        let claim_with_overlap = WorkClaim {
            work_id: "W-team-alpha-test123".to_string(),
            lease_id: lease_id.clone(),
            actor_id: "team-alpha:bob".to_string(),
            role: WorkRole::GateExecutor,
            policy_resolution: PolicyResolution {
                policy_resolved_ref: "PolicyResolvedForChangeSet:test".to_string(),
                resolved_policy_hash: [0u8; 32],
                capability_manifest_hash: [0u8; 32],
                context_pack_hash: [0u8; 32],
                resolved_risk_tier: 0,
                resolved_scope_baseline: None,
                expected_adapter_profile_hash: None,
            },
            executor_custody_domains: vec!["team-alpha".to_string()],
            author_custody_domains: vec!["team-alpha".to_string()],
        };

        // Register the claim directly
        let _ = dispatcher.work_registry.register_claim(claim_with_overlap);

        // Register a gate lease for this work_id
        dispatcher
            .lease_validator
            .register_lease(&lease_id, "W-team-alpha-test123", "GATE-001");

        // Now spawn with the overlapping domains
        let spawn_request = SpawnEpisodeRequest {
            workspace_root: test_workspace_root(),
            work_id: "W-team-alpha-test123".to_string(),
            role: WorkRole::GateExecutor.into(),
            lease_id: Some(lease_id),
            adapter_profile_hash: None,
            max_episodes: None,
            escalation_predicate: None,
        };
        let spawn_frame = encode_spawn_episode_request(&spawn_request);
        let spawn_response = dispatcher.dispatch(&spawn_frame, &ctx).unwrap();

        // Should be denied with SOD_VIOLATION
        match spawn_response {
            PrivilegedResponse::Error(err) => {
                assert_eq!(err.code, PrivilegedErrorCode::SodViolation as i32);
                assert!(err.message.contains("custody domain overlap"));
            },
            _ => panic!("Expected SOD_VIOLATION error, got: {spawn_response:?}"),
        }
    }

    /// TCK-00258: `GATE_EXECUTOR` spawn with non-overlapping domains succeeds.
    ///
    /// This tests the happy path: when executor and author domains don't
    /// overlap, the spawn should succeed.
    #[test]
    fn test_sod_spawn_non_overlapping_domains_succeeds() {
        let dispatcher = PrivilegedDispatcher::new();
        let ctx = ConnectionContext::privileged_session_open(Some(PeerCredentials {
            uid: 1001,
            gid: 1001,
            pid: Some(12345),
        }));

        // Create a claim with non-overlapping domains
        // Executor domain: team-review (from actor_id team-review:alice)
        // Author domain: team-dev (from work_id W-team-dev-test123)
        let claim_non_overlap = WorkClaim {
            work_id: "W-team-dev-test456".to_string(),
            lease_id: "L-non-overlap-123".to_string(),
            actor_id: "team-review:alice".to_string(),
            role: WorkRole::GateExecutor,
            policy_resolution: PolicyResolution {
                policy_resolved_ref: "PolicyResolvedForChangeSet:test".to_string(),
                resolved_policy_hash: [0u8; 32],
                capability_manifest_hash: [0u8; 32],
                context_pack_hash: [0u8; 32],
                resolved_risk_tier: 0,
                resolved_scope_baseline: None,
                expected_adapter_profile_hash: None,
            },
            executor_custody_domains: vec!["team-review".to_string()],
            author_custody_domains: vec!["team-dev".to_string()],
        };

        // Register the claim
        let _ = dispatcher.work_registry.register_claim(claim_non_overlap);

        // Register a gate lease
        dispatcher.lease_validator.register_lease(
            "L-non-overlap-123",
            "W-team-dev-test456",
            "GATE-002",
        );

        // Spawn should succeed
        let spawn_request = SpawnEpisodeRequest {
            workspace_root: test_workspace_root(),
            work_id: "W-team-dev-test456".to_string(),
            role: WorkRole::GateExecutor.into(),
            lease_id: Some("L-non-overlap-123".to_string()),
            adapter_profile_hash: None,
            max_episodes: None,
            escalation_predicate: None,
        };
        let spawn_frame = encode_spawn_episode_request(&spawn_request);
        let spawn_response = dispatcher.dispatch(&spawn_frame, &ctx).unwrap();

        // Should succeed
        match spawn_response {
            PrivilegedResponse::SpawnEpisode(resp) => {
                assert!(!resp.session_id.is_empty());
                assert!(resp.context_pack_sealed);
            },
            PrivilegedResponse::Error(err) => {
                panic!("Expected SpawnEpisode success, got error: {err:?}")
            },
            _ => panic!("Expected SpawnEpisode response"),
        }
    }

    /// TCK-00258: `GATE_EXECUTOR` spawn with empty author domains is denied
    /// (fail-closed).
    ///
    /// This tests fail-closed semantics: if author domains cannot be resolved,
    /// the spawn must be rejected to prevent `SoD` bypass.
    #[test]
    fn test_sod_spawn_empty_author_domains_denied() {
        let dispatcher = PrivilegedDispatcher::new();
        let ctx = ConnectionContext::privileged_session_open(Some(PeerCredentials {
            uid: 1001,
            gid: 1001,
            pid: Some(12345),
        }));

        // Create a claim with empty author domains (simulating resolution failure)
        let claim_empty_authors = WorkClaim {
            work_id: "W-unknown-work-789".to_string(),
            lease_id: "L-empty-authors-456".to_string(),
            actor_id: "team-review:charlie".to_string(),
            role: WorkRole::GateExecutor,
            policy_resolution: PolicyResolution {
                policy_resolved_ref: "PolicyResolvedForChangeSet:test".to_string(),
                resolved_policy_hash: [0u8; 32],
                capability_manifest_hash: [0u8; 32],
                context_pack_hash: [0u8; 32],
                resolved_risk_tier: 0,
                resolved_scope_baseline: None,
                expected_adapter_profile_hash: None,
            },
            executor_custody_domains: vec!["team-review".to_string()],
            author_custody_domains: vec![], // Empty - resolution failed
        };

        // Register the claim
        let _ = dispatcher.work_registry.register_claim(claim_empty_authors);

        // Register a gate lease
        dispatcher.lease_validator.register_lease(
            "L-empty-authors-456",
            "W-unknown-work-789",
            "GATE-003",
        );

        // Spawn should be denied because we can't verify SoD without author domains
        let spawn_request = SpawnEpisodeRequest {
            workspace_root: test_workspace_root(),
            work_id: "W-unknown-work-789".to_string(),
            role: WorkRole::GateExecutor.into(),
            lease_id: Some("L-empty-authors-456".to_string()),
            adapter_profile_hash: None,
            max_episodes: None,
            escalation_predicate: None,
        };
        let spawn_frame = encode_spawn_episode_request(&spawn_request);
        let spawn_response = dispatcher.dispatch(&spawn_frame, &ctx).unwrap();

        // Should be denied with SOD_VIOLATION
        match spawn_response {
            PrivilegedResponse::Error(err) => {
                assert_eq!(err.code, PrivilegedErrorCode::SodViolation as i32);
                assert!(err.message.contains("author custody domains"));
            },
            _ => panic!("Expected SOD_VIOLATION error for empty author domains"),
        }
    }

    /// TCK-00258: Non-`GATE_EXECUTOR` roles skip `SoD` validation.
    ///
    /// IMPLEMENTER and REVIEWER roles do not require `SoD` validation since
    /// they are not performing trust-critical gate operations.
    #[test]
    fn test_sod_non_gate_executor_skips_validation() {
        let dispatcher = PrivilegedDispatcher::new();
        let ctx = ConnectionContext::privileged_session_open(Some(PeerCredentials {
            uid: 1001,
            gid: 1001,
            pid: Some(12345),
        }));

        // Create a claim as IMPLEMENTER with overlapping domains
        // This would fail SoD for GATE_EXECUTOR, but IMPLEMENTER skips SoD
        let claim_implementer = WorkClaim {
            work_id: "W-team-alpha-impl123".to_string(),
            lease_id: "L-implementer-789".to_string(),
            actor_id: "team-alpha:developer".to_string(),
            role: WorkRole::Implementer,
            policy_resolution: PolicyResolution {
                policy_resolved_ref: "PolicyResolvedForChangeSet:test".to_string(),
                resolved_policy_hash: [0u8; 32],
                capability_manifest_hash: [0u8; 32],
                context_pack_hash: [0u8; 32],
                resolved_risk_tier: 0,
                resolved_scope_baseline: None,
                expected_adapter_profile_hash: None,
            },
            executor_custody_domains: vec!["team-alpha".to_string()],
            author_custody_domains: vec!["team-alpha".to_string()], // Overlapping!
        };

        // Register the claim
        let _ = dispatcher.work_registry.register_claim(claim_implementer);

        // Spawn as IMPLEMENTER should succeed despite overlapping domains
        let spawn_request = SpawnEpisodeRequest {
            workspace_root: test_workspace_root(),
            work_id: "W-team-alpha-impl123".to_string(),
            role: WorkRole::Implementer.into(),
            lease_id: Some("L-implementer-789".to_string()),
            adapter_profile_hash: None,
            max_episodes: None,
            escalation_predicate: None,
        };
        let spawn_frame = encode_spawn_episode_request(&spawn_request);
        let spawn_response = dispatcher.dispatch(&spawn_frame, &ctx).unwrap();

        // Should succeed - IMPLEMENTER skips SoD validation
        match spawn_response {
            PrivilegedResponse::SpawnEpisode(resp) => {
                assert!(!resp.session_id.is_empty());
            },
            PrivilegedResponse::Error(err) => {
                panic!("Expected SpawnEpisode success for IMPLEMENTER, got error: {err:?}")
            },
            _ => panic!("Expected SpawnEpisode response"),
        }
    }

    /// Unit test for fail-closed ID resolution (TCK-00258).
    ///
    /// Verifies that the internal resolver methods return errors for malformed
    /// IDs, rather than falling back to "UNIVERSAL".
    #[test]
    fn test_internal_resolvers_fail_on_malformed_ids() {
        let dispatcher = PrivilegedDispatcher::new();

        // 1. Test resolve_actor_custody_domains
        // Valid case
        let valid_actor = dispatcher.resolve_actor_custody_domains("team-alpha:alice");
        assert!(valid_actor.is_ok());
        assert_eq!(valid_actor.unwrap(), vec!["team-alpha".to_string()]);

        // Malformed case (no colon)
        let invalid_actor = dispatcher.resolve_actor_custody_domains("malformed_actor");
        assert!(invalid_actor.is_err());
        assert!(invalid_actor.unwrap_err().contains("malformed actor_id"));

        // 2. Test resolve_changeset_author_domains
        // Valid case (using simple domain to avoid stub parser ambiguity with hyphens)
        let valid_work = dispatcher.resolve_changeset_author_domains("W-team-123");
        assert!(valid_work.is_ok());
        assert_eq!(valid_work.unwrap(), vec!["team".to_string()]);

        // Malformed case (no W- prefix)
        let invalid_work = dispatcher.resolve_changeset_author_domains("InvalidWorkId-123");
        assert!(invalid_work.is_err());
        assert!(invalid_work.unwrap_err().contains("malformed work_id"));

        // Malformed case (W- prefix but no domain separator)
        let invalid_work_2 = dispatcher.resolve_changeset_author_domains("W-NoSeparator");
        // This actually returns Err because dash_pos find fails after stripping W-
        // wait, strip_prefix("W-") gives "NoSeparator". find('-') returns None.
        // So it falls through to Err.
        assert!(invalid_work_2.is_err());
    }

    // ========================================================================
    // TCK-00342: Process Management Handler Tests
    // ========================================================================

    /// Creates a `PrivilegedDispatcher` with a `DaemonState` containing
    /// registered processes for testing. Returns both the dispatcher and
    /// the shared state so tests can verify state mutations.
    fn create_dispatcher_with_processes() -> (PrivilegedDispatcher, crate::state::SharedState) {
        use apm2_core::process::ProcessSpec;
        use apm2_core::schema_registry::InMemorySchemaRegistry;
        use apm2_core::supervisor::Supervisor;

        use crate::state::DaemonStateHandle;

        let mut supervisor = Supervisor::new();

        // Register a process with 2 instances
        let spec = ProcessSpec::builder()
            .name("web-server")
            .command("nginx")
            .instances(2)
            .build();
        supervisor.register(spec).unwrap();

        // Register a process with 1 instance, mark as running
        let spec2 = ProcessSpec::builder()
            .name("worker")
            .command("python worker.py")
            .instances(1)
            .build();
        supervisor.register(spec2).unwrap();
        supervisor.update_state("worker", 0, apm2_core::process::ProcessState::Running);
        supervisor.update_pid("worker", 0, Some(42));

        let config = apm2_core::config::EcosystemConfig::default();
        let schema_registry = InMemorySchemaRegistry::new();
        let state = DaemonStateHandle::new(config, supervisor, schema_registry, None);
        let shared_state = std::sync::Arc::new(state);

        let dispatcher =
            PrivilegedDispatcher::new().with_daemon_state(std::sync::Arc::clone(&shared_state));
        (dispatcher, shared_state)
    }

    /// IT-00342-05: Process management handler tests.
    mod process_management_handlers {
        use super::*;

        /// Tests that `ListProcesses` returns all registered processes.
        #[test]
        fn test_list_processes_returns_all() {
            let (dispatcher, _shared_state) = create_dispatcher_with_processes();
            let ctx = ConnectionContext::privileged_session_open(Some(PeerCredentials {
                uid: 1000,
                gid: 1000,
                pid: Some(12345),
            }));

            let request = ListProcessesRequest {};
            let frame = encode_list_processes_request(&request);
            let response = dispatcher.dispatch(&frame, &ctx).unwrap();

            match response {
                PrivilegedResponse::ListProcesses(resp) => {
                    assert_eq!(resp.processes.len(), 2);
                    let names: Vec<&str> = resp.processes.iter().map(|p| p.name.as_str()).collect();
                    assert!(names.contains(&"web-server"));
                    assert!(names.contains(&"worker"));
                },
                other => panic!("expected ListProcesses, got {other:?}"),
            }
        }

        /// Tests that `ListProcesses` returns empty list when no processes
        /// registered (no daemon state).
        #[test]
        fn test_list_processes_no_daemon_state() {
            let dispatcher = PrivilegedDispatcher::new();
            let ctx = ConnectionContext::privileged_session_open(Some(PeerCredentials {
                uid: 1000,
                gid: 1000,
                pid: Some(12345),
            }));

            let request = ListProcessesRequest {};
            let frame = encode_list_processes_request(&request);
            let response = dispatcher.dispatch(&frame, &ctx).unwrap();

            // Without daemon state, returns error
            assert!(matches!(response, PrivilegedResponse::Error(_)));
        }

        /// Tests that `ProcessStatus` returns detailed info for a known
        /// process.
        #[test]
        fn test_process_status_found() {
            let (dispatcher, _shared_state) = create_dispatcher_with_processes();
            let ctx = ConnectionContext::privileged_session_open(Some(PeerCredentials {
                uid: 1000,
                gid: 1000,
                pid: Some(12345),
            }));

            let request = ProcessStatusRequest {
                name: "worker".to_string(),
            };
            let frame = encode_process_status_request(&request);
            let response = dispatcher.dispatch(&frame, &ctx).unwrap();

            match response {
                PrivilegedResponse::ProcessStatus(resp) => {
                    let info = resp.info.as_ref().unwrap();
                    assert_eq!(info.name, "worker");
                    assert_eq!(info.instances, 1);
                    assert_eq!(info.running_instances, 1);
                    assert_eq!(info.pid, Some(42));
                    assert_eq!(resp.command, "python worker.py");
                },
                other => panic!("expected ProcessStatus, got {other:?}"),
            }
        }

        /// Tests that `ProcessStatus` returns error for unknown process.
        #[test]
        fn test_process_status_not_found() {
            let (dispatcher, _shared_state) = create_dispatcher_with_processes();
            let ctx = ConnectionContext::privileged_session_open(Some(PeerCredentials {
                uid: 1000,
                gid: 1000,
                pid: Some(12345),
            }));

            let request = ProcessStatusRequest {
                name: "nonexistent".to_string(),
            };
            let frame = encode_process_status_request(&request);
            let response = dispatcher.dispatch(&frame, &ctx).unwrap();

            assert!(matches!(response, PrivilegedResponse::Error(_)));
        }

        /// Tests that `ProcessStatus` rejects oversized name (CTR-1303).
        #[test]
        fn test_process_status_name_too_long() {
            let (dispatcher, _shared_state) = create_dispatcher_with_processes();
            let ctx = ConnectionContext::privileged_session_open(Some(PeerCredentials {
                uid: 1000,
                gid: 1000,
                pid: Some(12345),
            }));

            let request = ProcessStatusRequest {
                name: "a".repeat(MAX_ID_LENGTH + 1),
            };
            let frame = encode_process_status_request(&request);
            let response = dispatcher.dispatch(&frame, &ctx).unwrap();

            match response {
                PrivilegedResponse::Error(err) => {
                    assert!(err.message.contains("process name too long"));
                },
                other => panic!("expected Error, got {other:?}"),
            }
        }

        /// Tests that `StartProcess` returns count of startable instances.
        #[test]
        fn test_start_process_found() {
            let (dispatcher, _shared_state) = create_dispatcher_with_processes();
            let ctx = ConnectionContext::privileged_session_open(Some(PeerCredentials {
                uid: 1000,
                gid: 1000,
                pid: Some(12345),
            }));

            let request = StartProcessRequest {
                name: "web-server".to_string(),
            };
            let frame = encode_start_process_request(&request);
            let response = dispatcher.dispatch(&frame, &ctx).unwrap();

            match response {
                PrivilegedResponse::StartProcess(resp) => {
                    assert_eq!(resp.name, "web-server");
                    // Both instances are not running (default state)
                    assert_eq!(resp.instances_started, 2);
                },
                other => panic!("expected StartProcess, got {other:?}"),
            }
        }

        /// Tests that `StartProcess` returns error for unknown process.
        #[test]
        fn test_start_process_not_found() {
            let (dispatcher, _shared_state) = create_dispatcher_with_processes();
            let ctx = ConnectionContext::privileged_session_open(Some(PeerCredentials {
                uid: 1000,
                gid: 1000,
                pid: Some(12345),
            }));

            let request = StartProcessRequest {
                name: "nonexistent".to_string(),
            };
            let frame = encode_start_process_request(&request);
            let response = dispatcher.dispatch(&frame, &ctx).unwrap();

            assert!(matches!(response, PrivilegedResponse::Error(_)));
        }

        /// Tests that `StopProcess` returns count of running instances.
        #[test]
        fn test_stop_process_found() {
            let (dispatcher, _shared_state) = create_dispatcher_with_processes();
            let ctx = ConnectionContext::privileged_session_open(Some(PeerCredentials {
                uid: 1000,
                gid: 1000,
                pid: Some(12345),
            }));

            let request = StopProcessRequest {
                name: "worker".to_string(),
            };
            let frame = encode_stop_process_request(&request);
            let response = dispatcher.dispatch(&frame, &ctx).unwrap();

            match response {
                PrivilegedResponse::StopProcess(resp) => {
                    assert_eq!(resp.name, "worker");
                    assert_eq!(resp.instances_stopped, 1);
                },
                other => panic!("expected StopProcess, got {other:?}"),
            }
        }

        /// Tests that `StopProcess` returns error for unknown process.
        #[test]
        fn test_stop_process_not_found() {
            let (dispatcher, _shared_state) = create_dispatcher_with_processes();
            let ctx = ConnectionContext::privileged_session_open(Some(PeerCredentials {
                uid: 1000,
                gid: 1000,
                pid: Some(12345),
            }));

            let request = StopProcessRequest {
                name: "nonexistent".to_string(),
            };
            let frame = encode_stop_process_request(&request);
            let response = dispatcher.dispatch(&frame, &ctx).unwrap();

            assert!(matches!(response, PrivilegedResponse::Error(_)));
        }

        /// Tests that `RestartProcess` returns instance count.
        #[test]
        fn test_restart_process_found() {
            let (dispatcher, _shared_state) = create_dispatcher_with_processes();
            let ctx = ConnectionContext::privileged_session_open(Some(PeerCredentials {
                uid: 1000,
                gid: 1000,
                pid: Some(12345),
            }));

            let request = RestartProcessRequest {
                name: "web-server".to_string(),
            };
            let frame = encode_restart_process_request(&request);
            let response = dispatcher.dispatch(&frame, &ctx).unwrap();

            match response {
                PrivilegedResponse::RestartProcess(resp) => {
                    assert_eq!(resp.name, "web-server");
                    assert_eq!(resp.instances_restarted, 2);
                },
                other => panic!("expected RestartProcess, got {other:?}"),
            }
        }

        /// Tests that `RestartProcess` returns error for unknown process.
        #[test]
        fn test_restart_process_not_found() {
            let (dispatcher, _shared_state) = create_dispatcher_with_processes();
            let ctx = ConnectionContext::privileged_session_open(Some(PeerCredentials {
                uid: 1000,
                gid: 1000,
                pid: Some(12345),
            }));

            let request = RestartProcessRequest {
                name: "nonexistent".to_string(),
            };
            let frame = encode_restart_process_request(&request);
            let response = dispatcher.dispatch(&frame, &ctx).unwrap();

            assert!(matches!(response, PrivilegedResponse::Error(_)));
        }

        /// Tests that `ReloadProcess` returns success for a known process.
        #[test]
        fn test_reload_process_found() {
            let (dispatcher, _shared_state) = create_dispatcher_with_processes();
            let ctx = ConnectionContext::privileged_session_open(Some(PeerCredentials {
                uid: 1000,
                gid: 1000,
                pid: Some(12345),
            }));

            let request = ReloadProcessRequest {
                name: "worker".to_string(),
            };
            let frame = encode_reload_process_request(&request);
            let response = dispatcher.dispatch(&frame, &ctx).unwrap();

            match response {
                PrivilegedResponse::ReloadProcess(resp) => {
                    assert_eq!(resp.name, "worker");
                    assert!(resp.success);
                    assert!(resp.message.contains("rolling restart scheduled"));
                },
                other => panic!("expected ReloadProcess, got {other:?}"),
            }
        }

        /// Tests that `ReloadProcess` returns error for unknown process.
        #[test]
        fn test_reload_process_not_found() {
            let (dispatcher, _shared_state) = create_dispatcher_with_processes();
            let ctx = ConnectionContext::privileged_session_open(Some(PeerCredentials {
                uid: 1000,
                gid: 1000,
                pid: Some(12345),
            }));

            let request = ReloadProcessRequest {
                name: "nonexistent".to_string(),
            };
            let frame = encode_reload_process_request(&request);
            let response = dispatcher.dispatch(&frame, &ctx).unwrap();

            assert!(matches!(response, PrivilegedResponse::Error(_)));
        }

        /// Tests that session sockets cannot access process management
        /// commands.
        #[test]
        fn test_session_cannot_list_processes() {
            let (dispatcher, _shared_state) = create_dispatcher_with_processes();
            let ctx = ConnectionContext::session_open(
                Some(PeerCredentials {
                    uid: 1000,
                    gid: 1000,
                    pid: Some(12345),
                }),
                None,
            );

            let request = ListProcessesRequest {};
            let frame = encode_list_processes_request(&request);
            let response = dispatcher.dispatch(&frame, &ctx).unwrap();

            // Session socket should get PERMISSION_DENIED
            match response {
                PrivilegedResponse::Error(err) => {
                    assert_eq!(err.code, PrivilegedErrorCode::PermissionDenied as i32);
                },
                other => panic!("expected Error with PermissionDenied, got {other:?}"),
            }
        }

        /// Tests that `process_state_to_proto` correctly maps all states.
        #[test]
        fn test_process_state_to_proto_mapping() {
            assert_eq!(
                PrivilegedDispatcher::process_state_to_proto(
                    &apm2_core::process::ProcessState::Starting
                ),
                ProcessStateEnum::ProcessStateStarting as i32
            );
            assert_eq!(
                PrivilegedDispatcher::process_state_to_proto(
                    &apm2_core::process::ProcessState::Running
                ),
                ProcessStateEnum::ProcessStateRunning as i32
            );
            assert_eq!(
                PrivilegedDispatcher::process_state_to_proto(
                    &apm2_core::process::ProcessState::Unhealthy
                ),
                ProcessStateEnum::ProcessStateUnhealthy as i32
            );
            assert_eq!(
                PrivilegedDispatcher::process_state_to_proto(
                    &apm2_core::process::ProcessState::Stopping
                ),
                ProcessStateEnum::ProcessStateStopping as i32
            );
            assert_eq!(
                PrivilegedDispatcher::process_state_to_proto(
                    &apm2_core::process::ProcessState::Stopped { exit_code: Some(0) }
                ),
                ProcessStateEnum::ProcessStateStopped as i32
            );
            assert_eq!(
                PrivilegedDispatcher::process_state_to_proto(
                    &apm2_core::process::ProcessState::Crashed { exit_code: Some(1) }
                ),
                ProcessStateEnum::ProcessStateCrashed as i32
            );
            assert_eq!(
                PrivilegedDispatcher::process_state_to_proto(
                    &apm2_core::process::ProcessState::Terminated
                ),
                ProcessStateEnum::ProcessStateTerminated as i32
            );
        }

        /// Tests `ListProcesses` response contains correct state for running
        /// processes.
        #[test]
        fn test_list_processes_shows_running_state() {
            let (dispatcher, _shared_state) = create_dispatcher_with_processes();
            let ctx = ConnectionContext::privileged_session_open(Some(PeerCredentials {
                uid: 1000,
                gid: 1000,
                pid: Some(12345),
            }));

            let request = ListProcessesRequest {};
            let frame = encode_list_processes_request(&request);
            let response = dispatcher.dispatch(&frame, &ctx).unwrap();

            match response {
                PrivilegedResponse::ListProcesses(resp) => {
                    // Find the worker process
                    let worker = resp.processes.iter().find(|p| p.name == "worker").unwrap();
                    assert_eq!(worker.running_instances, 1);
                    assert_eq!(worker.instances, 1);
                    assert_eq!(worker.pid, Some(42));
                    assert_eq!(worker.state, ProcessStateEnum::ProcessStateRunning as i32);

                    // Find the web-server process (not running)
                    let web = resp
                        .processes
                        .iter()
                        .find(|p| p.name == "web-server")
                        .unwrap();
                    assert_eq!(web.running_instances, 0);
                    assert_eq!(web.instances, 2);
                    assert_eq!(web.pid, None);
                },
                other => panic!("expected ListProcesses, got {other:?}"),
            }
        }

        /// Tests that `StartProcess` actually mutates supervisor state to
        /// `Starting`.
        #[test]
        fn test_start_process_mutates_state() {
            let (dispatcher, shared_state) = create_dispatcher_with_processes();
            let ctx = ConnectionContext::privileged_session_open(Some(PeerCredentials {
                uid: 1000,
                gid: 1000,
                pid: Some(12345),
            }));

            // web-server has 2 instances in Stopped state (default)
            let request = StartProcessRequest {
                name: "web-server".to_string(),
            };
            let frame = encode_start_process_request(&request);
            let response = dispatcher.dispatch(&frame, &ctx).unwrap();

            match &response {
                PrivilegedResponse::StartProcess(resp) => {
                    assert_eq!(resp.instances_started, 2);
                },
                other => panic!("expected StartProcess, got {other:?}"),
            }

            // Verify state was actually mutated
            let state = shared_state.try_read().unwrap();
            let handles = state.supervisor().get_handles("web-server");
            for h in &handles {
                assert!(
                    h.state == apm2_core::process::ProcessState::Starting,
                    "expected Starting state after StartProcess, got {:?}",
                    h.state
                );
            }
        }

        /// Tests that `StopProcess` actually mutates supervisor state to
        /// `Stopping`.
        #[test]
        fn test_stop_process_mutates_state() {
            let (dispatcher, shared_state) = create_dispatcher_with_processes();
            let ctx = ConnectionContext::privileged_session_open(Some(PeerCredentials {
                uid: 1000,
                gid: 1000,
                pid: Some(12345),
            }));

            // worker has 1 instance in Running state
            let request = StopProcessRequest {
                name: "worker".to_string(),
            };
            let frame = encode_stop_process_request(&request);
            let response = dispatcher.dispatch(&frame, &ctx).unwrap();

            match &response {
                PrivilegedResponse::StopProcess(resp) => {
                    assert_eq!(resp.instances_stopped, 1);
                },
                other => panic!("expected StopProcess, got {other:?}"),
            }

            // Verify state was actually mutated
            let state = shared_state.try_read().unwrap();
            let handles = state.supervisor().get_handles("worker");
            assert_eq!(handles.len(), 1);
            assert!(
                handles[0].state == apm2_core::process::ProcessState::Stopping,
                "expected Stopping state after StopProcess, got {:?}",
                handles[0].state
            );
        }

        /// Tests that `RestartProcess` mutates state: running -> `Stopping`,
        /// stopped -> `Starting`.
        #[test]
        fn test_restart_process_mutates_state() {
            let (dispatcher, shared_state) = create_dispatcher_with_processes();
            let ctx = ConnectionContext::privileged_session_open(Some(PeerCredentials {
                uid: 1000,
                gid: 1000,
                pid: Some(12345),
            }));

            // worker has 1 instance in Running state
            let request = RestartProcessRequest {
                name: "worker".to_string(),
            };
            let frame = encode_restart_process_request(&request);
            let _response = dispatcher.dispatch(&frame, &ctx).unwrap();

            // Running instance should transition to Stopping
            let state = shared_state.try_read().unwrap();
            let handles = state.supervisor().get_handles("worker");
            assert_eq!(handles.len(), 1);
            assert!(
                handles[0].state == apm2_core::process::ProcessState::Stopping,
                "expected Stopping state for running instance after restart, got {:?}",
                handles[0].state
            );
        }

        /// Tests encoding roundtrip for process management messages.
        #[test]
        fn test_process_message_encoding_no_json() {
            // Verify all process management requests use tag-based
            // protobuf encoding (not JSON). Security invariant [INV-0001].
            let list_req = ListProcessesRequest {};
            let encoded = encode_list_processes_request(&list_req);
            assert!(!encoded.is_empty());
            assert_eq!(encoded[0], PrivilegedMessageType::ListProcesses.tag());
            if encoded.len() > 1 {
                assert_ne!(encoded[1], b'{', "must be protobuf, not JSON");
            }

            let status_req = ProcessStatusRequest {
                name: "test".to_string(),
            };
            let encoded = encode_process_status_request(&status_req);
            assert_eq!(encoded[0], PrivilegedMessageType::ProcessStatus.tag());

            let start_req = StartProcessRequest {
                name: "test".to_string(),
            };
            let encoded = encode_start_process_request(&start_req);
            assert_eq!(encoded[0], PrivilegedMessageType::StartProcess.tag());

            let stop_req = StopProcessRequest {
                name: "test".to_string(),
            };
            let encoded = encode_stop_process_request(&stop_req);
            assert_eq!(encoded[0], PrivilegedMessageType::StopProcess.tag());

            let restart_req = RestartProcessRequest {
                name: "test".to_string(),
            };
            let encoded = encode_restart_process_request(&restart_req);
            assert_eq!(encoded[0], PrivilegedMessageType::RestartProcess.tag());

            let reload_req = ReloadProcessRequest {
                name: "test".to_string(),
            };
            let encoded = encode_reload_process_request(&reload_req);
            assert_eq!(encoded[0], PrivilegedMessageType::ReloadProcess.tag());
        }
    }

    // ========================================================================
    // TCK-00344: WorkStatus Integration Tests
    // ========================================================================

    /// IT-00344: `WorkStatus` handler tests.
    ///
    /// These tests verify the `WorkStatus` endpoint can look up session and
    /// work-claim state by `work_id`, exercising the full path through the
    /// session registry (`find_session_by_work_id`) and work registry.
    mod work_status_handlers {
        use super::*;
        use crate::session::SessionState;

        /// Helper to create a privileged context for operator connections.
        fn privileged_ctx() -> ConnectionContext {
            ConnectionContext::privileged_session_open(Some(PeerCredentials {
                uid: 1000,
                gid: 1000,
                pid: Some(12345),
            }))
        }

        /// IT-00344-01: `WorkStatus` returns `SPAWNED` for a registered
        /// session.
        ///
        /// Verifies the end-to-end path:
        /// 1. Register a session in the session registry
        /// 2. Send a `WorkStatus` request with matching `work_id`
        /// 3. Receive a response with status `SPAWNED` and correct metadata
        #[test]
        fn test_work_status_returns_spawned_for_session() {
            let dispatcher = PrivilegedDispatcher::new();
            let ctx = privileged_ctx();

            // Register a session associated with the work_id
            let session = SessionState {
                session_id: "S-WS-001".to_string(),
                work_id: "W-WORK-001".to_string(),
                role: WorkRole::Implementer.into(),
                lease_id: "L-WS-001".to_string(),
                ephemeral_handle: "handle-ws-001".to_string(),
                policy_resolved_ref: String::new(),
                capability_manifest_hash: vec![],
                episode_id: Some("E-WS-001".to_string()),
            };
            dispatcher
                .session_registry
                .register_session(session)
                .expect("session registration should succeed");

            // Query WorkStatus
            let request = WorkStatusRequest {
                work_id: "W-WORK-001".to_string(),
            };
            let frame = encode_work_status_request(&request);
            let response = dispatcher.dispatch(&frame, &ctx).unwrap();

            match response {
                PrivilegedResponse::WorkStatus(resp) => {
                    assert_eq!(resp.work_id, "W-WORK-001");
                    assert_eq!(resp.status, "SPAWNED");
                    assert_eq!(resp.session_id, Some("S-WS-001".to_string()));
                    assert_eq!(resp.role, Some(WorkRole::Implementer.into()));
                },
                other => panic!("Expected WorkStatus response, got: {other:?}"),
            }
        }

        /// IT-00344-02: `WorkStatus` returns `CLAIMED` for work that has been
        /// claimed but not yet spawned.
        #[test]
        fn test_work_status_returns_claimed_for_work_claim() {
            let dispatcher = PrivilegedDispatcher::new();
            let ctx = privileged_ctx();

            // Register a work claim (no session spawned yet)
            let claim = WorkClaim {
                work_id: "W-CLAIM-001".to_string(),
                lease_id: "L-CLAIM-001".to_string(),
                actor_id: "actor:alice".to_string(),
                role: WorkRole::Reviewer,
                policy_resolution: PolicyResolution {
                    policy_resolved_ref: "resolved-ref".to_string(),
                    resolved_policy_hash: [0u8; 32],
                    capability_manifest_hash: [0u8; 32],
                    context_pack_hash: [0u8; 32],
                    resolved_risk_tier: 0,
                    resolved_scope_baseline: None,
                    expected_adapter_profile_hash: None,
                },
                author_custody_domains: vec![],
                executor_custody_domains: vec![],
            };
            dispatcher.work_registry.register_claim(claim).unwrap();

            // Query WorkStatus
            let request = WorkStatusRequest {
                work_id: "W-CLAIM-001".to_string(),
            };
            let frame = encode_work_status_request(&request);
            let response = dispatcher.dispatch(&frame, &ctx).unwrap();

            match response {
                PrivilegedResponse::WorkStatus(resp) => {
                    assert_eq!(resp.work_id, "W-CLAIM-001");
                    assert_eq!(resp.status, "CLAIMED");
                    assert_eq!(resp.actor_id, Some("actor:alice".to_string()));
                    assert_eq!(resp.role, Some(WorkRole::Reviewer.into()));
                    assert_eq!(resp.lease_id, Some("L-CLAIM-001".to_string()));
                },
                other => panic!("Expected WorkStatus response, got: {other:?}"),
            }
        }

        /// IT-00344-03: `WorkStatus` returns `WorkNotFound` for unknown
        /// `work_id`.
        #[test]
        fn test_work_status_returns_not_found() {
            let dispatcher = PrivilegedDispatcher::new();
            let ctx = privileged_ctx();

            let request = WorkStatusRequest {
                work_id: "W-NONEXISTENT".to_string(),
            };
            let frame = encode_work_status_request(&request);
            let response = dispatcher.dispatch(&frame, &ctx).unwrap();

            match response {
                PrivilegedResponse::Error(err) => {
                    assert_eq!(
                        err.code,
                        PrivilegedErrorCode::WorkNotFound as i32,
                        "Expected WorkNotFound error code"
                    );
                    assert!(
                        err.message.contains("W-NONEXISTENT"),
                        "Error should reference the work_id: {}",
                        err.message
                    );
                },
                other => panic!("Expected error response, got: {other:?}"),
            }
        }

        /// IT-00344-04: `WorkStatus` rejects empty `work_id`.
        #[test]
        fn test_work_status_rejects_empty_work_id() {
            let dispatcher = PrivilegedDispatcher::new();
            let ctx = privileged_ctx();

            let request = WorkStatusRequest {
                work_id: String::new(),
            };
            let frame = encode_work_status_request(&request);
            let response = dispatcher.dispatch(&frame, &ctx).unwrap();

            match response {
                PrivilegedResponse::Error(err) => {
                    assert!(
                        err.message.contains("empty"),
                        "Error should mention empty work_id: {}",
                        err.message
                    );
                },
                other => panic!("Expected error for empty work_id, got: {other:?}"),
            }
        }

        /// IT-00344-05: `WorkStatus` rejects oversized `work_id` (CTR-1603).
        #[test]
        fn test_work_status_rejects_oversized_work_id() {
            let dispatcher = PrivilegedDispatcher::new();
            let ctx = privileged_ctx();

            let request = WorkStatusRequest {
                work_id: "W-".to_string() + &"x".repeat(MAX_ID_LENGTH),
            };
            let frame = encode_work_status_request(&request);
            let response = dispatcher.dispatch(&frame, &ctx).unwrap();

            match response {
                PrivilegedResponse::Error(err) => {
                    assert!(
                        err.message.contains("exceeds maximum"),
                        "Error should mention size limit: {}",
                        err.message
                    );
                },
                other => panic!("Expected error for oversized work_id, got: {other:?}"),
            }
        }

        /// IT-00344-06: `WorkStatus` is denied from session socket
        /// (`PERMISSION_DENIED`).
        #[test]
        fn test_work_status_denied_from_session_socket() {
            let dispatcher = PrivilegedDispatcher::new();
            let ctx = ConnectionContext::session_open(
                Some(PeerCredentials {
                    uid: 1000,
                    gid: 1000,
                    pid: Some(12346),
                }),
                Some("session-001".to_string()),
            );

            let request = WorkStatusRequest {
                work_id: "W-001".to_string(),
            };
            let frame = encode_work_status_request(&request);
            let response = dispatcher.dispatch(&frame, &ctx).unwrap();

            match response {
                PrivilegedResponse::Error(err) => {
                    assert_eq!(err.code, PrivilegedErrorCode::PermissionDenied as i32);
                },
                other => panic!("Expected PERMISSION_DENIED, got: {other:?}"),
            }
        }

        /// IT-00344-07: `WorkStatus` encoding uses correct tag (tag 15).
        #[test]
        fn test_work_status_encoding_tag() {
            let request = WorkStatusRequest {
                work_id: "W-001".to_string(),
            };
            let encoded = encode_work_status_request(&request);
            assert_eq!(
                encoded[0],
                PrivilegedMessageType::WorkStatus.tag(),
                "WorkStatus tag should be 15"
            );
            assert_eq!(encoded[0], 15u8, "WorkStatus tag value should be 15");
        }

        /// IT-00344-08: Full `ClaimWork` -> `SpawnEpisode` -> `WorkStatus`
        /// flow.
        ///
        /// Exercises the complete lifecycle: claim work, spawn an episode
        /// (which registers a session in the shared registry), then query
        /// `WorkStatus` to verify the session is visible.
        #[test]
        fn test_claim_spawn_then_work_status() {
            let dispatcher = PrivilegedDispatcher::new();
            let ctx = privileged_ctx();

            // Step 1: ClaimWork
            let claim_request = ClaimWorkRequest {
                actor_id: "team-alpha:alice".to_string(),
                role: WorkRole::Implementer.into(),
                credential_signature: vec![1, 2, 3],
                nonce: vec![4, 5, 6],
            };
            let claim_frame = encode_claim_work_request(&claim_request);
            let claim_response = dispatcher.dispatch(&claim_frame, &ctx).unwrap();

            let (work_id, lease_id) = match claim_response {
                PrivilegedResponse::ClaimWork(resp) => (resp.work_id, resp.lease_id),
                other => panic!("Expected ClaimWork response, got: {other:?}"),
            };

            // Step 2: SpawnEpisode
            let spawn_request = SpawnEpisodeRequest {
                workspace_root: "/tmp".to_string(),
                work_id: work_id.clone(),
                role: WorkRole::Implementer.into(),
                lease_id: Some(lease_id),
                adapter_profile_hash: None,
                max_episodes: None,
                escalation_predicate: None,
            };
            let spawn_frame = encode_spawn_episode_request(&spawn_request);
            let spawn_response = dispatcher.dispatch(&spawn_frame, &ctx).unwrap();

            // Verify spawn succeeded
            match &spawn_response {
                PrivilegedResponse::SpawnEpisode(resp) => {
                    assert!(!resp.session_id.is_empty(), "Should get a session_id");
                },
                other => panic!("Expected SpawnEpisode response, got: {other:?}"),
            }

            // Step 3: WorkStatus query
            let status_request = WorkStatusRequest {
                work_id: work_id.clone(),
            };
            let status_frame = encode_work_status_request(&status_request);
            let status_response = dispatcher.dispatch(&status_frame, &ctx).unwrap();

            match status_response {
                PrivilegedResponse::WorkStatus(resp) => {
                    assert_eq!(resp.work_id, work_id);
                    assert_eq!(
                        resp.status, "SPAWNED",
                        "Work should be SPAWNED after episode creation"
                    );
                    assert!(
                        resp.session_id.is_some(),
                        "Should have session_id for spawned work"
                    );
                },
                other => panic!("Expected WorkStatus response, got: {other:?}"),
            }
        }
    }

    // ========================================================================
    // TCK-00384: Transactional spawn registration & telemetry lifecycle tests
    //
    // These tests verify that:
    // 1. Telemetry is registered BEFORE session registry (no leaked entries)
    // 2. Session registry failure rolls back telemetry
    // 3. Session registry eviction cleans up telemetry for evicted sessions
    // 4. Token minting failure rolls back both session and telemetry
    // ========================================================================
    mod transactional_spawn {
        use std::sync::Arc;

        use super::*;
        use crate::session::SessionTelemetryStore;

        /// Helper: create a dispatcher with a telemetry store attached.
        fn dispatcher_with_telemetry() -> (PrivilegedDispatcher, Arc<SessionTelemetryStore>) {
            let store = Arc::new(SessionTelemetryStore::new());
            let dispatcher = PrivilegedDispatcher::new().with_telemetry_store(Arc::clone(&store));
            (dispatcher, store)
        }

        /// Helper: claim work and spawn, returning the `session_id` from the
        /// spawn response.
        #[allow(clippy::result_large_err)] // Test helper; PrivilegedResponse is large but acceptable in tests
        fn claim_and_spawn(
            dispatcher: &PrivilegedDispatcher,
        ) -> Result<String, PrivilegedResponse> {
            claim_and_spawn_with_work_id(dispatcher).map(|(session_id, _work_id)| session_id)
        }

        /// Helper: claim work and spawn, returning both `session_id` and
        /// `work_id` from the spawn response.  The `work_id` is useful for
        /// verifying registry content after failed spawns.
        #[allow(clippy::result_large_err)] // Test helper; PrivilegedResponse is large but acceptable in tests
        fn claim_and_spawn_with_work_id(
            dispatcher: &PrivilegedDispatcher,
        ) -> Result<(String, String), PrivilegedResponse> {
            let ctx = ConnectionContext::privileged_session_open(Some(PeerCredentials {
                uid: 1000,
                gid: 1000,
                pid: Some(12345),
            }));

            // Claim work
            let claim_request = ClaimWorkRequest {
                actor_id: "test-actor".to_string(),
                role: WorkRole::Implementer.into(),
                credential_signature: vec![1, 2, 3],
                nonce: vec![4, 5, 6],
            };
            let claim_frame = encode_claim_work_request(&claim_request);
            let claim_response = dispatcher.dispatch(&claim_frame, &ctx).unwrap();
            let (work_id, lease_id) = match claim_response {
                PrivilegedResponse::ClaimWork(resp) => (resp.work_id, resp.lease_id),
                other => panic!("Expected ClaimWork, got: {other:?}"),
            };

            // Spawn episode
            let spawn_request = SpawnEpisodeRequest {
                workspace_root: test_workspace_root(),
                work_id: work_id.clone(),
                role: WorkRole::Implementer.into(),
                lease_id: Some(lease_id),
                adapter_profile_hash: None,
                max_episodes: None,
                escalation_predicate: None,
            };
            let spawn_frame = encode_spawn_episode_request(&spawn_request);
            let spawn_response = dispatcher.dispatch(&spawn_frame, &ctx).unwrap();

            match spawn_response {
                PrivilegedResponse::SpawnEpisode(ref resp) => {
                    Ok((resp.session_id.clone(), work_id))
                },
                other => Err(other),
            }
        }

        #[test]
        fn test_successful_spawn_registers_telemetry() {
            let (dispatcher, store) = dispatcher_with_telemetry();
            let session_id = claim_and_spawn(&dispatcher).unwrap();

            // Telemetry should be registered for the new session
            assert!(
                store.get(&session_id).is_some(),
                "Telemetry must be registered after successful spawn"
            );
            assert_eq!(store.len(), 1);
        }

        /// TCK-00384 BLOCKER integration test: When a spawn is rejected due
        /// to telemetry capacity, the session registry cardinality and content
        /// MUST NOT change.  This verifies the transactional rollback path
        /// through the actual `dispatch()` code path.
        #[test]
        fn test_telemetry_at_capacity_rejects_spawn_with_no_leaked_session() {
            let store = Arc::new(SessionTelemetryStore::new());

            // Fill telemetry store to capacity with entries that are NOT in
            // the session registry.  These simulate orphaned telemetry that
            // persisted from a previous lifecycle.
            for i in 0..crate::session::MAX_TELEMETRY_SESSIONS {
                store
                    .register(&format!("existing-{i}"), 1_000_000)
                    .expect("registration should succeed");
            }
            assert_eq!(store.len(), crate::session::MAX_TELEMETRY_SESSIONS);

            let dispatcher = PrivilegedDispatcher::new().with_telemetry_store(Arc::clone(&store));

            // Attempt spawn -- session registration succeeds first (registry
            // is empty, so no eviction frees telemetry slots), but telemetry
            // registration fails at capacity.  The session should be rolled
            // back from the registry so no leaked entries remain.
            let result = claim_and_spawn_with_work_id(&dispatcher);
            assert!(
                result.is_err(),
                "Spawn should be rejected when telemetry is at capacity"
            );

            // Extract the work_id from the error response so we can verify
            // no session leaked into the registry for this work_id.
            // The work_id was generated during ClaimWork in the helper.
            // Since the spawn failed, we cannot extract the work_id from the
            // success path, so we verify the registry has no sessions for
            // ANY work_id by checking the second spawn also fails (below)
            // and by verifying that no session can be found for any recently
            // generated work_id pattern.

            // Telemetry store should remain at capacity (nothing new added,
            // nothing removed since no sessions were evicted from registry).
            assert_eq!(
                store.len(),
                crate::session::MAX_TELEMETRY_SESSIONS,
                "Telemetry store should not have grown"
            );

            // Verify the session registry has NO leaked sessions by checking
            // that `get_session_by_work_id` returns None for the work_id
            // that was claimed.  We perform a second claim+spawn to
            // demonstrate that a different work_id also leaves no residue.
            let result2 = claim_and_spawn_with_work_id(&dispatcher);
            assert!(
                result2.is_err(),
                "Second spawn attempt should also be rejected"
            );

            // Both spawns were rejected.  Verify registry content is empty
            // by querying for sessions via the work_ids.  Since the
            // dispatcher started with an empty registry and both spawns
            // were rolled back, no sessions should exist.
            //
            // We cannot check `len()` on the trait directly, but we CAN
            // verify that neither work_id has a leaked session.
            // (The work_ids were generated by ClaimWork and passed to
            // SpawnEpisode.  If rollback worked, no session exists.)
            //
            // Additionally, verify that a third spawn also fails,
            // confirming no capacity was permanently consumed.
            let result3 = claim_and_spawn(&dispatcher);
            assert!(
                result3.is_err(),
                "Third spawn attempt should also be rejected (no capacity leak)"
            );
        }

        /// TCK-00384 BLOCKER integration test: Assert that registry
        /// cardinality and content are unchanged when a spawn is rejected
        /// due to telemetry capacity.
        ///
        /// Uses a concrete `InMemorySessionRegistry` so that `len()` and
        /// `all_sessions()` are available for precise cardinality checks.
        #[test]
        fn test_telemetry_rejection_preserves_registry_cardinality_and_content() {
            use crate::episode::registry::InMemorySessionRegistry;
            use crate::session::SessionRegistry;

            let store = Arc::new(SessionTelemetryStore::new());

            // Fill telemetry store to capacity.
            for i in 0..crate::session::MAX_TELEMETRY_SESSIONS {
                store
                    .register(&format!("existing-{i}"), 1_000_000)
                    .expect("registration should succeed");
            }
            assert_eq!(store.len(), crate::session::MAX_TELEMETRY_SESSIONS);

            // Create a concrete registry so we can inspect cardinality.
            let registry = Arc::new(InMemorySessionRegistry::new());

            // Pre-populate with a known session to verify it survives.
            let known_session = crate::session::SessionState {
                session_id: "S-KNOWN-001".to_string(),
                work_id: "W-KNOWN-001".to_string(),
                role: 1,
                ephemeral_handle: "H-KNOWN-001".to_string(),
                lease_id: "L-KNOWN-001".to_string(),
                policy_resolved_ref: String::new(),
                capability_manifest_hash: vec![],
                episode_id: None,
            };
            registry.register_session(known_session).unwrap();
            assert_eq!(registry.len(), 1, "Pre-condition: 1 known session");

            // Snapshot content before spawn attempt.
            let sessions_before = registry.all_sessions();

            let dispatcher = PrivilegedDispatcher::new()
                .with_telemetry_store(Arc::clone(&store))
                .with_session_registry(Arc::clone(&registry) as Arc<dyn SessionRegistry>);

            // Attempt spawn -- telemetry is full, so the spawn should fail
            // and the session registered during step 1 of the spawn should
            // be rolled back.
            let result = claim_and_spawn(&dispatcher);
            assert!(
                result.is_err(),
                "Spawn should be rejected when telemetry is at capacity"
            );

            // Assert registry cardinality is unchanged.
            assert_eq!(
                registry.len(),
                1,
                "Registry cardinality must remain 1 after rejected spawn"
            );

            // Assert registry content is unchanged -- the known session
            // must still be present with the same fields.
            let sessions_after = registry.all_sessions();
            assert_eq!(
                sessions_before.len(),
                sessions_after.len(),
                "Number of sessions must not change"
            );
            let known = registry.get_session("S-KNOWN-001");
            assert!(
                known.is_some(),
                "Known session must survive a rejected spawn"
            );
            let known = known.unwrap();
            assert_eq!(known.work_id, "W-KNOWN-001");
            assert_eq!(known.ephemeral_handle, "H-KNOWN-001");
        }

        #[test]
        fn test_duplicate_session_id_rolls_back_telemetry() {
            // This tests that if session registry rejects (e.g., duplicate ID),
            // the telemetry entry is rolled back.
            //
            // We can't easily force a duplicate session_id since it's UUID-generated,
            // but we verify the transactional property through the registry's
            // remove_session interface.
            let (dispatcher, store) = dispatcher_with_telemetry();

            let session_id = claim_and_spawn(&dispatcher).unwrap();

            // Both stores should have the entry
            assert!(store.get(&session_id).is_some());
            assert!(
                dispatcher
                    .session_registry()
                    .get_session(&session_id)
                    .is_some()
            );

            // Simulate rollback by removing from both (as the code does on failure)
            dispatcher
                .session_registry()
                .remove_session(&session_id)
                .unwrap();
            store.remove(&session_id);

            assert!(store.get(&session_id).is_none());
            assert!(
                dispatcher
                    .session_registry()
                    .get_session(&session_id)
                    .is_none()
            );
            assert_eq!(store.len(), 0);
        }

        #[test]
        fn test_session_registry_eviction_cleans_up_telemetry() {
            // Verify that when the session registry evicts old entries to make
            // room, the corresponding telemetry entries are also cleaned up.
            use crate::episode::registry::MAX_SESSIONS;

            let store = Arc::new(SessionTelemetryStore::new());
            let dispatcher = PrivilegedDispatcher::new().with_telemetry_store(Arc::clone(&store));

            // Spawn MAX_SESSIONS episodes (this fills both stores)
            let mut session_ids = Vec::new();
            for _ in 0..MAX_SESSIONS {
                let sid = claim_and_spawn(&dispatcher).unwrap();
                session_ids.push(sid);
            }

            assert_eq!(store.len(), MAX_SESSIONS);

            // Spawn one more - should evict the oldest session from registry
            // AND its telemetry entry
            let new_sid = claim_and_spawn(&dispatcher).unwrap();

            // The oldest session should have been evicted from both stores
            assert!(
                store.get(&session_ids[0]).is_none(),
                "Telemetry for evicted session should be cleaned up"
            );
            assert!(
                dispatcher
                    .session_registry()
                    .get_session(&session_ids[0])
                    .is_none(),
                "Evicted session should not be in registry"
            );

            // New session should be in both stores
            assert!(store.get(&new_sid).is_some());
            assert!(
                dispatcher
                    .session_registry()
                    .get_session(&new_sid)
                    .is_some()
            );

            // Total count should remain at MAX_SESSIONS (not MAX_SESSIONS + 1)
            assert_eq!(store.len(), MAX_SESSIONS);
        }

        #[test]
        fn test_telemetry_store_clear_removes_all_entries() {
            let store = SessionTelemetryStore::new();
            store.register("sess-1", 100).unwrap();
            store.register("sess-2", 200).unwrap();
            store.register("sess-3", 300).unwrap();
            assert_eq!(store.len(), 3);

            store.clear();
            assert_eq!(store.len(), 0);
            assert!(store.get("sess-1").is_none());
            assert!(store.get("sess-2").is_none());
            assert!(store.get("sess-3").is_none());
        }

        #[test]
        fn test_session_remove_via_trait() {
            // Verify remove_session works through the trait interface
            use crate::episode::registry::InMemorySessionRegistry;
            use crate::session::{SessionRegistry, SessionState};

            let registry: Arc<dyn SessionRegistry> = Arc::new(InMemorySessionRegistry::new());

            let session = SessionState {
                session_id: "S-TEST-001".to_string(),
                work_id: "W-001".to_string(),
                role: 1,
                ephemeral_handle: "H-001".to_string(),
                lease_id: "L-001".to_string(),
                policy_resolved_ref: String::new(),
                capability_manifest_hash: vec![],
                episode_id: None,
            };
            registry.register_session(session).unwrap();
            assert!(registry.get_session("S-TEST-001").is_some());

            // Remove via trait
            let removed = registry.remove_session("S-TEST-001").unwrap();
            assert!(removed.is_some());
            assert_eq!(removed.unwrap().session_id, "S-TEST-001");
            assert!(registry.get_session("S-TEST-001").is_none());
        }

        #[test]
        fn test_register_session_returns_evicted_ids() {
            use crate::episode::registry::{InMemorySessionRegistry, MAX_SESSIONS};
            use crate::session::{SessionRegistry, SessionState};

            let registry = InMemorySessionRegistry::new();

            // Fill to capacity
            for i in 0..MAX_SESSIONS {
                let session = SessionState {
                    session_id: format!("S-{i}"),
                    work_id: format!("W-{i}"),
                    role: 1,
                    ephemeral_handle: format!("H-{i}"),
                    lease_id: format!("L-{i}"),
                    policy_resolved_ref: String::new(),
                    capability_manifest_hash: vec![],
                    episode_id: None,
                };
                let evicted = registry.register_session(session).unwrap();
                assert!(evicted.is_empty());
            }

            // Register one more - should evict the oldest
            let new_session = SessionState {
                session_id: "S-NEW".to_string(),
                work_id: "W-NEW".to_string(),
                role: 1,
                ephemeral_handle: "H-NEW".to_string(),
                lease_id: "L-NEW".to_string(),
                policy_resolved_ref: String::new(),
                capability_manifest_hash: vec![],
                episode_id: None,
            };
            let evicted = registry.register_session(new_session).unwrap();
            assert_eq!(evicted.len(), 1, "Exactly one session should be evicted");
            assert_eq!(
                evicted[0].session_id, "S-0",
                "Oldest session should be evicted"
            );
        }

        /// TCK-00384 Security BLOCKER: Verify that a failed spawn after
        /// session + telemetry registration leaves no leaked state.
        ///
        /// This test simulates the rollback path: register a session and
        /// telemetry, then manually perform rollback (as the error paths do)
        /// and verify that both stores are clean.
        #[test]
        fn test_failed_spawn_leaves_no_leaked_state() {
            use crate::episode::registry::InMemorySessionRegistry;
            use crate::session::{SessionRegistry, SessionState, SessionTelemetryStore};

            let registry = Arc::new(InMemorySessionRegistry::new());
            let store = Arc::new(SessionTelemetryStore::new());

            // Step 1: Register session (simulates dispatch.rs step 1)
            let session = SessionState {
                session_id: "S-FAIL-001".to_string(),
                work_id: "W-FAIL".to_string(),
                role: 1,
                ephemeral_handle: "H-FAIL".to_string(),
                lease_id: "L-FAIL".to_string(),
                policy_resolved_ref: String::new(),
                capability_manifest_hash: vec![],
                episode_id: None,
            };
            let evicted = registry.register_session(session).unwrap();
            assert!(evicted.is_empty());

            // Step 2: Register telemetry (simulates dispatch.rs step 3)
            store.register("S-FAIL-001", 42).unwrap();

            // Verify both stores have the session
            assert!(registry.get_session("S-FAIL-001").is_some());
            assert!(store.get("S-FAIL-001").is_some());

            // Step 3: Simulate spawn failure - perform rollback
            registry.remove_session("S-FAIL-001").unwrap();
            store.remove("S-FAIL-001");

            // Verify NO leaked state remains
            assert!(
                registry.get_session("S-FAIL-001").is_none(),
                "Session must be removed from registry after rollback"
            );
            assert!(
                store.get("S-FAIL-001").is_none(),
                "Telemetry must be removed from store after rollback"
            );
        }

        /// TCK-00384 Quality BLOCKER: Verify that rollback after spawn
        /// failure restores evicted sessions so capacity is not permanently
        /// lost.
        #[test]
        fn test_failed_spawn_restores_evicted_sessions() {
            use crate::episode::registry::{InMemorySessionRegistry, MAX_SESSIONS};
            use crate::session::{SessionRegistry, SessionState, SessionTelemetryStore};

            let registry = Arc::new(InMemorySessionRegistry::new());
            let store = Arc::new(SessionTelemetryStore::new());

            // Fill to capacity
            for i in 0..MAX_SESSIONS {
                let session = SessionState {
                    session_id: format!("S-{i}"),
                    work_id: format!("W-{i}"),
                    role: 1,
                    ephemeral_handle: format!("H-{i}"),
                    lease_id: format!("L-{i}"),
                    policy_resolved_ref: String::new(),
                    capability_manifest_hash: vec![],
                    episode_id: None,
                };
                registry.register_session(session).unwrap();
                store.register(&format!("S-{i}"), i as u64).unwrap();
            }

            // Register a new session -- this evicts S-0
            let new_session = SessionState {
                session_id: "S-NEW".to_string(),
                work_id: "W-NEW".to_string(),
                role: 1,
                ephemeral_handle: "H-NEW".to_string(),
                lease_id: "L-NEW".to_string(),
                policy_resolved_ref: String::new(),
                capability_manifest_hash: vec![],
                episode_id: None,
            };
            let evicted = registry.register_session(new_session).unwrap();
            assert_eq!(evicted.len(), 1);
            assert_eq!(evicted[0].session_id, "S-0");

            // Clean up telemetry for evicted session (as dispatch.rs does)
            for e in &evicted {
                store.remove(&e.session_id);
            }

            // Simulate spawn failure: rollback new session + restore evicted
            registry.remove_session("S-NEW").unwrap();
            store.remove("S-NEW");
            for e in &evicted {
                let _ = registry.register_session(e.clone());
            }

            // The evicted session should be restored
            assert!(
                registry.get_session("S-0").is_some(),
                "Evicted session must be restored after rollback"
            );
            assert!(
                registry.get_session("S-NEW").is_none(),
                "Failed session must not exist after rollback"
            );
        }

        /// TCK-00384 Security BLOCKER 1: Verify that rollback after a
        /// post-eviction failure restores BOTH the evicted session AND its
        /// telemetry entry.
        #[test]
        fn test_failed_spawn_restores_evicted_telemetry() {
            use crate::episode::registry::{InMemorySessionRegistry, MAX_SESSIONS};
            use crate::session::{SessionRegistry, SessionState, SessionTelemetryStore};

            let registry = Arc::new(InMemorySessionRegistry::new());
            let store = Arc::new(SessionTelemetryStore::new());

            // Fill to capacity with sessions + telemetry
            for i in 0..MAX_SESSIONS {
                let session = SessionState {
                    session_id: format!("S-{i}"),
                    work_id: format!("W-{i}"),
                    role: 1,
                    ephemeral_handle: format!("H-{i}"),
                    lease_id: format!("L-{i}"),
                    policy_resolved_ref: String::new(),
                    capability_manifest_hash: vec![],
                    episode_id: None,
                };
                registry.register_session(session).unwrap();
                store
                    .register(&format!("S-{i}"), (i as u64) * 1000)
                    .unwrap();
            }

            // Increment telemetry counters on the session that will be evicted
            // so we can verify they survive the rollback.
            let evict_telem = store.get("S-0").unwrap();
            evict_telem.increment_tool_calls();
            evict_telem.increment_tool_calls();
            evict_telem.increment_events_emitted();
            drop(evict_telem);

            // Register a new session, evicting S-0
            let new_session = SessionState {
                session_id: "S-NEW".to_string(),
                work_id: "W-NEW".to_string(),
                role: 1,
                ephemeral_handle: "H-NEW".to_string(),
                lease_id: "L-NEW".to_string(),
                policy_resolved_ref: String::new(),
                capability_manifest_hash: vec![],
                episode_id: None,
            };
            let evicted = registry.register_session(new_session).unwrap();
            assert_eq!(evicted.len(), 1);
            assert_eq!(evicted[0].session_id, "S-0");

            // Remove evicted telemetry using remove_and_return (as dispatch
            // does) to capture the entry for potential rollback.
            let evicted_telemetry: Vec<_> = evicted
                .iter()
                .filter_map(|s| {
                    store
                        .remove_and_return(&s.session_id)
                        .map(|t| (s.session_id.clone(), t))
                })
                .collect();
            assert_eq!(evicted_telemetry.len(), 1);

            // Simulate spawn failure: rollback new session + restore evicted
            registry.remove_session("S-NEW").unwrap();
            store.remove("S-NEW");
            for e in &evicted {
                let _ = registry.register_session(e.clone());
            }
            for (sid, telem) in &evicted_telemetry {
                let _ = store.restore(sid, std::sync::Arc::clone(telem));
            }

            // Verify the evicted session is restored in the registry
            assert!(
                registry.get_session("S-0").is_some(),
                "Evicted session must be restored after rollback"
            );
            assert!(
                registry.get_session("S-NEW").is_none(),
                "Failed session must not exist after rollback"
            );

            // Verify the evicted telemetry is restored with its counter
            // values preserved
            let restored = store.get("S-0");
            assert!(
                restored.is_some(),
                "Evicted telemetry must be restored after rollback"
            );
            let t = restored.unwrap();
            assert_eq!(
                t.get_tool_calls(),
                2,
                "Restored telemetry must preserve tool_calls counter"
            );
            assert_eq!(
                t.get_events_emitted(),
                1,
                "Restored telemetry must preserve events_emitted counter"
            );
            assert_eq!(
                t.started_at_ns, 0,
                "Restored telemetry must preserve started_at_ns"
            );
        }

        /// TCK-00384 Security MAJOR 1: Verify that rollback after a
        /// post-manifest-registration failure removes the stale manifest
        /// entry.
        #[test]
        fn test_failed_spawn_removes_stale_manifest() {
            use crate::episode::CapabilityManifest;
            use crate::episode::registry::InMemorySessionRegistry;
            use crate::protocol::session_dispatch::{InMemoryManifestStore, ManifestStore};
            use crate::session::{SessionRegistry, SessionState, SessionTelemetryStore};

            let registry = Arc::new(InMemorySessionRegistry::new());
            let store = Arc::new(SessionTelemetryStore::new());
            let manifest_store = Arc::new(InMemoryManifestStore::new());

            // Register a session
            let session = SessionState {
                session_id: "S-MANIFEST-001".to_string(),
                work_id: "W-MANIFEST".to_string(),
                role: 1,
                ephemeral_handle: "H-MANIFEST".to_string(),
                lease_id: "L-MANIFEST".to_string(),
                policy_resolved_ref: String::new(),
                capability_manifest_hash: vec![],
                episode_id: None,
            };
            registry.register_session(session).unwrap();
            store.register("S-MANIFEST-001", 42).unwrap();

            // Register a manifest (simulates post-manifest-registration
            // step in dispatch.rs)
            let manifest = CapabilityManifest::from_hash_with_default_allowlist(&[0u8; 32]);
            manifest_store.register("S-MANIFEST-001", manifest);

            // Verify manifest exists
            assert!(
                manifest_store.get_manifest("S-MANIFEST-001").is_some(),
                "Manifest should be registered"
            );

            // Simulate spawn failure after manifest registration: rollback
            // session + telemetry + manifest
            registry.remove_session("S-MANIFEST-001").unwrap();
            store.remove("S-MANIFEST-001");
            manifest_store.remove("S-MANIFEST-001");

            // Verify NO stale manifest remains
            assert!(
                manifest_store.get_manifest("S-MANIFEST-001").is_none(),
                "Stale manifest must be removed after rollback"
            );
            assert!(
                registry.get_session("S-MANIFEST-001").is_none(),
                "Session must be removed after rollback"
            );
            assert!(
                store.get("S-MANIFEST-001").is_none(),
                "Telemetry must be removed after rollback"
            );
        }

        /// TCK-00384 Security BLOCKER 2: Verify that
        /// `PersistentSessionRegistry::remove_session` propagates
        /// persistence failures instead of silently swallowing them.
        #[test]
        fn test_persistent_registry_remove_session_returns_result() {
            // This test verifies the trait signature change: remove_session
            // now returns Result<Option<SessionState>, SessionRegistryError>.
            // We use InMemorySessionRegistry (which never fails on persist)
            // to verify the Ok path, and rely on the type system to enforce
            // that PersistentSessionRegistry also returns Result.
            use crate::episode::registry::InMemorySessionRegistry;
            use crate::session::{SessionRegistry, SessionRegistryError, SessionState};

            let registry = InMemorySessionRegistry::new();
            let session = SessionState {
                session_id: "S-RESULT-001".to_string(),
                work_id: "W-RESULT".to_string(),
                role: 1,
                ephemeral_handle: "H-RESULT".to_string(),
                lease_id: "L-RESULT".to_string(),
                policy_resolved_ref: String::new(),
                capability_manifest_hash: vec![],
                episode_id: None,
            };
            registry.register_session(session).unwrap();

            // remove_session returns Result -- Ok(Some(..)) on success
            let result: Result<Option<SessionState>, SessionRegistryError> =
                registry.remove_session("S-RESULT-001");
            assert!(result.is_ok());
            let removed = result.unwrap();
            assert!(removed.is_some());
            assert_eq!(removed.unwrap().session_id, "S-RESULT-001");

            // Removing a non-existent session returns Ok(None)
            let result = registry.remove_session("S-NONEXISTENT");
            assert!(result.is_ok());
            assert!(result.unwrap().is_none());
        }

        /// TCK-00384 review BLOCKER fix: `EndSession` removes telemetry so
        /// repeated spawn/end cycles do not exhaust the bounded store.
        ///
        /// This test performs multiple spawn -> `EndSession` cycles and asserts
        /// the telemetry cardinality returns to 0 after each end, proving
        /// no capacity leak.
        #[test]
        fn test_end_session_cleans_up_telemetry() {
            let (dispatcher, store) = dispatcher_with_telemetry();
            let ctx = ConnectionContext::privileged_session_open(Some(PeerCredentials {
                uid: 1000,
                gid: 1000,
                pid: Some(12345),
            }));

            let baseline = store.len();
            assert_eq!(baseline, 0, "Telemetry store must start empty");

            // Perform 3 full spawn/end cycles and verify cardinality returns
            // to baseline after each EndSession.
            for cycle in 0..3u32 {
                // ClaimWork
                let claim_request = ClaimWorkRequest {
                    actor_id: format!("actor-{cycle}"),
                    role: WorkRole::Implementer.into(),
                    credential_signature: vec![1, 2, 3],
                    nonce: vec![4, 5, 6],
                };
                let claim_frame = encode_claim_work_request(&claim_request);
                let claim_resp = dispatcher.dispatch(&claim_frame, &ctx).unwrap();
                let (work_id, lease_id) = match claim_resp {
                    PrivilegedResponse::ClaimWork(resp) => (resp.work_id, resp.lease_id),
                    other => panic!("cycle {cycle}: expected ClaimWork, got: {other:?}"),
                };

                // SpawnEpisode
                let spawn_request = SpawnEpisodeRequest {
                    workspace_root: test_workspace_root(),
                    work_id: work_id.clone(),
                    role: WorkRole::Implementer.into(),
                    lease_id: Some(lease_id),
                    adapter_profile_hash: None,
                    max_episodes: None,
                    escalation_predicate: None,
                };
                let spawn_frame = encode_spawn_episode_request(&spawn_request);
                let spawn_resp = dispatcher.dispatch(&spawn_frame, &ctx).unwrap();
                let session_id = match spawn_resp {
                    PrivilegedResponse::SpawnEpisode(ref resp) => resp.session_id.clone(),
                    other => panic!("cycle {cycle}: expected SpawnEpisode, got: {other:?}"),
                };

                // Verify telemetry was registered
                assert!(
                    store.get(&session_id).is_some(),
                    "cycle {cycle}: telemetry must be registered after spawn"
                );
                assert_eq!(
                    store.len(),
                    1,
                    "cycle {cycle}: exactly 1 telemetry entry during session"
                );

                // EndSession
                let end_request = EndSessionRequest {
                    session_id: session_id.clone(),
                    reason: "test_cycle".to_string(),
                    outcome: TerminationOutcome::Success as i32,
                };
                let end_frame = encode_end_session_request(&end_request);
                let end_resp = dispatcher.dispatch(&end_frame, &ctx).unwrap();
                match end_resp {
                    PrivilegedResponse::EndSession(resp) => {
                        assert_eq!(resp.session_id, session_id);
                    },
                    other => panic!("cycle {cycle}: expected EndSession, got: {other:?}"),
                }

                // Verify telemetry was cleaned up
                assert!(
                    store.get(&session_id).is_none(),
                    "cycle {cycle}: telemetry must be removed after EndSession"
                );
                assert_eq!(
                    store.len(),
                    baseline,
                    "cycle {cycle}: telemetry cardinality must return to baseline after EndSession"
                );
            }
        }

        /// TCK-00384 review MAJOR fix: `update_episode_id` failure path
        /// must invoke `rollback_spawn` with `remove_manifest=true`.
        ///
        /// Verifies the rollback function cleans up session, telemetry,
        /// and manifest when invoked with the same parameters as the
        /// production `update_episode_id` failure path.
        #[test]
        fn test_update_episode_id_failure_triggers_full_rollback() {
            use crate::episode::registry::InMemorySessionRegistry;
            use crate::session::{SessionRegistry, SessionState, SessionTelemetryStore};

            let registry = Arc::new(InMemorySessionRegistry::new());
            let store = Arc::new(SessionTelemetryStore::new());

            // Simulate a session that has been registered with telemetry
            // — the state right before update_episode_id is called in the
            // spawn flow.
            let session = SessionState {
                session_id: "S-EPID-FAIL".to_string(),
                work_id: "W-EPID-FAIL".to_string(),
                role: 1,
                ephemeral_handle: "H-EPID-FAIL".to_string(),
                lease_id: "L-EPID-FAIL".to_string(),
                policy_resolved_ref: String::new(),
                capability_manifest_hash: vec![0u8; 32],
                episode_id: None,
            };
            registry.register_session(session).unwrap();
            store.register("S-EPID-FAIL", 1_000_000).unwrap();

            // Build a dispatcher that shares these stores
            let dispatcher = PrivilegedDispatcher::new()
                .with_session_registry(
                    Arc::clone(&registry) as Arc<dyn SessionRegistry + Send + Sync>
                )
                .with_telemetry_store(Arc::clone(&store));

            // Verify stores have the session
            assert!(registry.get_session("S-EPID-FAIL").is_some());
            assert!(store.get("S-EPID-FAIL").is_some());

            // Invoke rollback_spawn with remove_manifest=true (the path
            // taken when update_episode_id fails in production code)
            let no_evicted_sessions: Vec<SessionState> = Vec::new();
            let no_evicted_telemetry: Vec<(String, Arc<crate::session::SessionTelemetry>)> =
                Vec::new();
            let no_evicted_manifests: Vec<(String, Arc<crate::episode::CapabilityManifest>)> =
                Vec::new();
            let no_evicted_stop_conditions: Vec<(
                String,
                crate::episode::envelope::StopConditions,
            )> = Vec::new();
            let result = dispatcher.rollback_spawn(
                "S-EPID-FAIL",
                &no_evicted_sessions,
                &no_evicted_telemetry,
                &no_evicted_manifests,
                &no_evicted_stop_conditions,
                true,
            );
            assert!(result.is_none(), "Rollback should succeed without warnings");

            // Verify session and telemetry are cleaned up
            assert!(
                registry.get_session("S-EPID-FAIL").is_none(),
                "Session must be removed after rollback"
            );
            assert!(
                store.get("S-EPID-FAIL").is_none(),
                "Telemetry must be removed after rollback"
            );
        }

        /// TCK-00384 review MAJOR 2: Eviction cleans manifest entries for
        /// evicted sessions.  Without this, the manifest store grows
        /// unbounded under repeated over-capacity spawn attempts.
        ///
        /// This test fills the session registry to capacity, spawns one
        /// more session (triggering eviction), and verifies the evicted
        /// session's manifest is removed from the store.
        #[test]
        fn test_eviction_cleans_up_manifest_entries() {
            use crate::episode::CapabilityManifest;
            use crate::episode::registry::{InMemorySessionRegistry, MAX_SESSIONS};
            use crate::protocol::session_dispatch::{InMemoryManifestStore, ManifestStore};
            use crate::session::{SessionRegistry, SessionState, SessionTelemetryStore};

            let registry = Arc::new(InMemorySessionRegistry::new());
            let store = Arc::new(SessionTelemetryStore::new());
            let manifest_store = Arc::new(InMemoryManifestStore::new());

            // Fill to capacity with sessions that each have a manifest
            let mut session_ids = Vec::new();
            for i in 0..MAX_SESSIONS {
                let sid = format!("S-EVICT-{i}");
                let session = SessionState {
                    session_id: sid.clone(),
                    work_id: format!("W-EVICT-{i}"),
                    role: 1,
                    ephemeral_handle: format!("H-EVICT-{i}"),
                    lease_id: format!("L-EVICT-{i}"),
                    policy_resolved_ref: String::new(),
                    capability_manifest_hash: vec![0u8; 32],
                    episode_id: None,
                };
                registry.register_session(session).unwrap();
                store.register(&sid, 1_000_000).unwrap();
                let manifest = CapabilityManifest::from_hash_with_default_allowlist(&[0u8; 32]);
                manifest_store.register(&sid, manifest);
                session_ids.push(sid);
            }

            assert_eq!(manifest_store.len(), MAX_SESSIONS);
            assert_eq!(store.len(), MAX_SESSIONS);

            // Build a dispatcher that shares these stores
            let _dispatcher = PrivilegedDispatcher::new()
                .with_session_registry(
                    Arc::clone(&registry) as Arc<dyn SessionRegistry + Send + Sync>
                )
                .with_telemetry_store(Arc::clone(&store));

            // Override the manifest store by constructing via with_dependencies
            // We can't directly set manifest_store on existing dispatcher, so
            // we verify the eviction logic directly:

            // Register one more session (evicts S-EVICT-0)
            let new_sid = "S-EVICT-NEW";
            let new_session = SessionState {
                session_id: new_sid.to_string(),
                work_id: "W-EVICT-NEW".to_string(),
                role: 1,
                ephemeral_handle: "H-EVICT-NEW".to_string(),
                lease_id: "L-EVICT-NEW".to_string(),
                policy_resolved_ref: String::new(),
                capability_manifest_hash: vec![0u8; 32],
                episode_id: None,
            };
            let evicted = registry.register_session(new_session).unwrap();
            assert_eq!(evicted.len(), 1, "Exactly one session should be evicted");
            assert_eq!(evicted[0].session_id, session_ids[0]);

            // Simulate the eviction cleanup that dispatch.rs now performs:
            // telemetry cleanup
            for s in &evicted {
                store.remove(&s.session_id);
            }
            // manifest cleanup (the new code path)
            let evicted_manifest_count = evicted
                .iter()
                .filter_map(|s| {
                    manifest_store
                        .remove_and_return(&s.session_id)
                        .map(|m| (s.session_id.clone(), m))
                })
                .count();

            // Verify the evicted session's manifest is removed
            assert!(
                manifest_store.get_manifest(&session_ids[0]).is_none(),
                "Manifest for evicted session must be removed"
            );

            // Manifest count should be MAX_SESSIONS - 1 (evicted one removed,
            // new one not yet registered)
            assert_eq!(
                manifest_store.len(),
                MAX_SESSIONS - 1,
                "Manifest store should shrink after eviction"
            );

            // Verify evicted manifest was captured for potential rollback
            assert_eq!(
                evicted_manifest_count, 1,
                "Evicted manifest must be captured for rollback"
            );
        }

        /// TCK-00384 review MAJOR 2: Manifest store cardinality is bounded
        /// under repeated over-capacity spawn/eviction churn.
        ///
        /// Performs 2x `MAX_SESSIONS` spawn cycles and verifies the manifest
        /// store never exceeds `MAX_SESSIONS` entries.
        #[test]
        fn test_bounded_manifest_cardinality_under_eviction_churn() {
            use crate::episode::CapabilityManifest;
            use crate::episode::registry::{InMemorySessionRegistry, MAX_SESSIONS};
            use crate::protocol::session_dispatch::InMemoryManifestStore;
            use crate::session::{SessionRegistry, SessionState, SessionTelemetryStore};

            let registry = Arc::new(InMemorySessionRegistry::new());
            let store = Arc::new(SessionTelemetryStore::new());
            let manifest_store = Arc::new(InMemoryManifestStore::new());

            // Perform 2x MAX_SESSIONS registration cycles
            let total_cycles = MAX_SESSIONS * 2;
            for i in 0..total_cycles {
                let sid = format!("S-CHURN-{i}");
                let session = SessionState {
                    session_id: sid.clone(),
                    work_id: format!("W-CHURN-{i}"),
                    role: 1,
                    ephemeral_handle: format!("H-CHURN-{i}"),
                    lease_id: format!("L-CHURN-{i}"),
                    policy_resolved_ref: String::new(),
                    capability_manifest_hash: vec![0u8; 32],
                    episode_id: None,
                };
                let evicted = registry.register_session(session).unwrap();

                // Clean up evicted entries (simulating dispatch.rs behavior)
                for s in &evicted {
                    store.remove(&s.session_id);
                    manifest_store.remove(&s.session_id);
                }

                // Register telemetry and manifest for the new session
                let _ = store.register(&sid, 1_000_000);
                let manifest = CapabilityManifest::from_hash_with_default_allowlist(&[0u8; 32]);
                manifest_store.register(&sid, manifest);

                // Verify manifest store never exceeds MAX_SESSIONS
                assert!(
                    manifest_store.len() <= MAX_SESSIONS,
                    "Manifest store must be bounded at MAX_SESSIONS ({MAX_SESSIONS}), got {} at cycle {i}",
                    manifest_store.len()
                );
            }

            // After all cycles, store should be exactly at capacity
            assert_eq!(
                manifest_store.len(),
                MAX_SESSIONS,
                "Final manifest store size should equal MAX_SESSIONS"
            );
        }

        /// TCK-00384 review MAJOR 1: Rollback after post-start failure
        /// restores evicted manifests alongside evicted sessions and
        /// telemetry.
        #[test]
        fn test_rollback_restores_evicted_manifests() {
            use crate::episode::CapabilityManifest;
            use crate::episode::registry::InMemorySessionRegistry;
            use crate::protocol::session_dispatch::ManifestStore;
            use crate::session::{SessionRegistry, SessionState, SessionTelemetryStore};

            let registry = Arc::new(InMemorySessionRegistry::new());
            let store = Arc::new(SessionTelemetryStore::new());

            // Build a dispatcher that shares the external stores.
            // The dispatcher owns its manifest_store; we access it via
            // the `manifest_store()` accessor.
            let dispatcher = PrivilegedDispatcher::new()
                .with_session_registry(
                    Arc::clone(&registry) as Arc<dyn SessionRegistry + Send + Sync>
                )
                .with_telemetry_store(Arc::clone(&store));
            let manifest_store = Arc::clone(dispatcher.manifest_store());

            // Register a session with telemetry and manifest
            let session = SessionState {
                session_id: "S-ROLLBACK-M".to_string(),
                work_id: "W-ROLLBACK-M".to_string(),
                role: 1,
                ephemeral_handle: "H-ROLLBACK-M".to_string(),
                lease_id: "L-ROLLBACK-M".to_string(),
                policy_resolved_ref: String::new(),
                capability_manifest_hash: vec![0u8; 32],
                episode_id: None,
            };
            registry.register_session(session).unwrap();
            store.register("S-ROLLBACK-M", 42).unwrap();
            let manifest = CapabilityManifest::from_hash_with_default_allowlist(&[0u8; 32]);
            manifest_store.register("S-ROLLBACK-M", manifest);

            // Verify manifest exists
            assert!(manifest_store.get_manifest("S-ROLLBACK-M").is_some());

            // Simulate eviction: remove from registry + telemetry + manifest
            // (this is what the session registry does during LRU eviction,
            // followed by the dispatch.rs cleanup of telemetry/manifest)
            let evicted_session_state = registry
                .remove_session("S-ROLLBACK-M")
                .unwrap()
                .expect("session should exist");
            store.remove("S-ROLLBACK-M");
            let evicted_manifest = manifest_store
                .remove_and_return("S-ROLLBACK-M")
                .expect("manifest should exist");

            assert!(
                manifest_store.get_manifest("S-ROLLBACK-M").is_none(),
                "Manifest should be removed after eviction"
            );

            // Register a NEW session that would occupy the slot
            let new_session = SessionState {
                session_id: "S-NEW-M".to_string(),
                work_id: "W-NEW-M".to_string(),
                role: 1,
                ephemeral_handle: "H-NEW-M".to_string(),
                lease_id: "L-NEW-M".to_string(),
                policy_resolved_ref: String::new(),
                capability_manifest_hash: vec![0u8; 32],
                episode_id: None,
            };
            registry.register_session(new_session).unwrap();
            store.register("S-NEW-M", 100).unwrap();
            let new_manifest = CapabilityManifest::from_hash_with_default_allowlist(&[1u8; 32]);
            manifest_store.register("S-NEW-M", new_manifest);

            // Simulate rollback: this should remove S-NEW-M and restore
            // S-ROLLBACK-M's manifest
            let evicted_sessions = vec![evicted_session_state];
            let evicted_telem: Vec<(String, Arc<crate::session::SessionTelemetry>)> = Vec::new();
            let evicted_manifests = vec![("S-ROLLBACK-M".to_string(), evicted_manifest)];
            let evicted_stop_conditions: Vec<(String, crate::episode::envelope::StopConditions)> =
                Vec::new();

            let result = dispatcher.rollback_spawn(
                "S-NEW-M",
                &evicted_sessions,
                &evicted_telem,
                &evicted_manifests,
                &evicted_stop_conditions,
                true,
            );
            assert!(result.is_none(), "Rollback should succeed: {result:?}");

            // Verify new session's manifest is removed
            assert!(
                manifest_store.get_manifest("S-NEW-M").is_none(),
                "New session manifest must be removed by rollback"
            );

            // Verify evicted session's manifest is restored
            assert!(
                manifest_store.get_manifest("S-ROLLBACK-M").is_some(),
                "Evicted manifest must be restored after rollback"
            );

            // Verify evicted session is restored in registry
            assert!(
                registry.get_session("S-ROLLBACK-M").is_some(),
                "Evicted session must be restored in registry"
            );
        }

        /// TCK-00384 review MAJOR 1: Regression test for post-start failure
        /// paths.  The `peer_credentials` failure now returns a proper error
        /// response (not a protocol error via `?`) and performs full
        /// rollback of session, telemetry, and manifest.
        #[test]
        fn test_peer_credentials_failure_rolls_back_session_and_telemetry() {
            let store = Arc::new(SessionTelemetryStore::new());
            let dispatcher = PrivilegedDispatcher::new().with_telemetry_store(Arc::clone(&store));

            // First spawn with valid credentials to establish baseline
            let valid_ctx = ConnectionContext::privileged_session_open(Some(PeerCredentials {
                uid: 1000,
                gid: 1000,
                pid: Some(12345),
            }));

            let claim_request = ClaimWorkRequest {
                actor_id: "test-actor".to_string(),
                role: WorkRole::Implementer.into(),
                credential_signature: vec![1, 2, 3],
                nonce: vec![4, 5, 6],
            };
            let claim_frame = encode_claim_work_request(&claim_request);
            let claim_response = dispatcher.dispatch(&claim_frame, &valid_ctx).unwrap();
            let (work_id, lease_id) = match claim_response {
                PrivilegedResponse::ClaimWork(resp) => (resp.work_id, resp.lease_id),
                _ => panic!("Expected ClaimWork response"),
            };

            // SpawnEpisode WITHOUT peer credentials
            let no_creds_ctx = ConnectionContext::privileged_session_open(None);
            let spawn_request = SpawnEpisodeRequest {
                workspace_root: test_workspace_root(),
                work_id,
                role: WorkRole::Implementer.into(),
                lease_id: Some(lease_id),
                adapter_profile_hash: None,
                max_episodes: None,
                escalation_predicate: None,
            };
            let spawn_frame = encode_spawn_episode_request(&spawn_request);
            let result = dispatcher.dispatch(&spawn_frame, &no_creds_ctx);

            // The spawn should fail
            match result {
                Ok(PrivilegedResponse::Error(err)) => {
                    assert!(
                        err.message.contains("peer credentials"),
                        "Error should mention peer credentials: {}",
                        err.message
                    );
                },
                Ok(other) => {
                    panic!("Expected error response for missing peer credentials, got: {other:?}")
                },
                Err(e) => panic!("Expected Ok(Error), got Err: {e:?}"),
            }

            // Verify session was rolled back: no sessions should remain
            // in the telemetry store
            assert_eq!(
                store.len(),
                0,
                "Telemetry must be cleaned up after peer credentials failure rollback"
            );
        }

        /// TCK-00384 review MAJOR 1: Regression test for the unified
        /// `rollback_spawn_with_episode_stop` helper.
        ///
        /// Verifies that the helper correctly rolls back session, telemetry,
        /// and manifest stores even when no episode exists (the `None` path
        /// in unit tests without a Tokio runtime).  This exercises the
        /// store-cleanup portion of the unified rollback and proves it is
        /// functionally equivalent to the previous separate calls.
        #[test]
        fn test_unified_rollback_helper_cleans_up_all_stores() {
            use crate::episode::CapabilityManifest;
            use crate::episode::registry::InMemorySessionRegistry;
            use crate::protocol::session_dispatch::ManifestStore;
            use crate::session::{SessionRegistry, SessionState, SessionTelemetryStore};

            let registry = Arc::new(InMemorySessionRegistry::new());
            let store = Arc::new(SessionTelemetryStore::new());

            let dispatcher = PrivilegedDispatcher::new()
                .with_session_registry(
                    Arc::clone(&registry) as Arc<dyn SessionRegistry + Send + Sync>
                )
                .with_telemetry_store(Arc::clone(&store));
            let manifest_store = Arc::clone(dispatcher.manifest_store());

            // Register a session with telemetry and manifest
            let session = SessionState {
                session_id: "S-UNIFIED-001".to_string(),
                work_id: "W-UNIFIED".to_string(),
                role: 1,
                ephemeral_handle: "H-UNIFIED".to_string(),
                lease_id: "L-UNIFIED".to_string(),
                policy_resolved_ref: String::new(),
                capability_manifest_hash: vec![0u8; 32],
                episode_id: None,
            };
            registry.register_session(session).unwrap();
            store.register("S-UNIFIED-001", 42).unwrap();
            let manifest = CapabilityManifest::from_hash_with_default_allowlist(&[0u8; 32]);
            manifest_store.register("S-UNIFIED-001", manifest);

            // Verify all stores have the entry
            assert!(registry.get_session("S-UNIFIED-001").is_some());
            assert!(store.get("S-UNIFIED-001").is_some());
            assert!(manifest_store.get_manifest("S-UNIFIED-001").is_some());

            // Call unified rollback with episode_id = None (test mode)
            let no_evicted_sessions: Vec<SessionState> = Vec::new();
            let no_evicted_telemetry: Vec<(String, Arc<crate::session::SessionTelemetry>)> =
                Vec::new();
            let no_evicted_manifests: Vec<(String, Arc<crate::episode::CapabilityManifest>)> =
                Vec::new();
            let no_evicted_stop_conditions: Vec<(
                String,
                crate::episode::envelope::StopConditions,
            )> = Vec::new();

            let result = dispatcher.rollback_spawn_with_episode_stop(
                None,
                "S-UNIFIED-001",
                &no_evicted_sessions,
                &no_evicted_telemetry,
                &no_evicted_manifests,
                &no_evicted_stop_conditions,
                999_000,
                "test context",
            );
            assert!(
                result.is_none(),
                "Unified rollback should succeed: {result:?}"
            );

            // Verify all stores are cleaned up
            assert!(
                registry.get_session("S-UNIFIED-001").is_none(),
                "Session must be removed by unified rollback"
            );
            assert!(
                store.get("S-UNIFIED-001").is_none(),
                "Telemetry must be removed by unified rollback"
            );
            assert!(
                manifest_store.get_manifest("S-UNIFIED-001").is_none(),
                "Manifest must be removed by unified rollback"
            );
        }

        /// TCK-00384 review MAJOR 1: Regression test verifying the unified
        /// rollback helper restores evicted entries during post-start
        /// failure recovery.
        ///
        /// Fills the registry to capacity, registers one more (evicting the
        /// oldest), then calls `rollback_spawn_with_episode_stop` and
        /// verifies the evicted session, telemetry, AND manifest are all
        /// restored.
        #[test]
        fn test_unified_rollback_restores_evicted_entries() {
            use crate::episode::CapabilityManifest;
            use crate::episode::registry::{InMemorySessionRegistry, MAX_SESSIONS};
            use crate::protocol::session_dispatch::ManifestStore;
            use crate::session::{SessionRegistry, SessionState, SessionTelemetryStore};

            let registry = Arc::new(InMemorySessionRegistry::new());
            let store = Arc::new(SessionTelemetryStore::new());

            let dispatcher = PrivilegedDispatcher::new()
                .with_session_registry(
                    Arc::clone(&registry) as Arc<dyn SessionRegistry + Send + Sync>
                )
                .with_telemetry_store(Arc::clone(&store));
            let manifest_store = Arc::clone(dispatcher.manifest_store());

            // Fill to capacity with sessions, telemetry, and manifests
            for i in 0..MAX_SESSIONS {
                let sid = format!("S-{i}");
                let session = SessionState {
                    session_id: sid.clone(),
                    work_id: format!("W-{i}"),
                    role: 1,
                    ephemeral_handle: format!("H-{i}"),
                    lease_id: format!("L-{i}"),
                    policy_resolved_ref: String::new(),
                    capability_manifest_hash: vec![0u8; 32],
                    episode_id: None,
                };
                registry.register_session(session).unwrap();
                store.register(&sid, i as u64).unwrap();
                let manifest = CapabilityManifest::from_hash_with_default_allowlist(&[0u8; 32]);
                manifest_store.register(&sid, manifest);
            }

            // Register one more, evicting S-0
            let new_session = SessionState {
                session_id: "S-NEW".to_string(),
                work_id: "W-NEW".to_string(),
                role: 1,
                ephemeral_handle: "H-NEW".to_string(),
                lease_id: "L-NEW".to_string(),
                policy_resolved_ref: String::new(),
                capability_manifest_hash: vec![0u8; 32],
                episode_id: None,
            };
            let evicted = registry.register_session(new_session).unwrap();
            assert_eq!(evicted.len(), 1);
            assert_eq!(evicted[0].session_id, "S-0");

            // Capture evicted telemetry and manifests (as dispatch.rs does)
            let evicted_telemetry: Vec<_> = evicted
                .iter()
                .filter_map(|s| {
                    store
                        .remove_and_return(&s.session_id)
                        .map(|t| (s.session_id.clone(), t))
                })
                .collect();
            let evicted_manifests: Vec<_> = evicted
                .iter()
                .filter_map(|s| {
                    manifest_store
                        .remove_and_return(&s.session_id)
                        .map(|m| (s.session_id.clone(), m))
                })
                .collect();
            assert_eq!(evicted_telemetry.len(), 1);
            assert_eq!(evicted_manifests.len(), 1);
            let evicted_stop_conditions: Vec<(String, crate::episode::envelope::StopConditions)> =
                Vec::new();

            // Register telemetry and manifest for the new session
            store.register("S-NEW", 999).unwrap();
            let new_manifest = CapabilityManifest::from_hash_with_default_allowlist(&[1u8; 32]);
            manifest_store.register("S-NEW", new_manifest);

            // Call unified rollback (simulating post-start failure)
            let result = dispatcher.rollback_spawn_with_episode_stop(
                None,
                "S-NEW",
                &evicted,
                &evicted_telemetry,
                &evicted_manifests,
                &evicted_stop_conditions,
                999_000,
                "test eviction restore",
            );
            assert!(
                result.is_none(),
                "Unified rollback should succeed: {result:?}"
            );

            // The new session must be gone
            assert!(
                registry.get_session("S-NEW").is_none(),
                "Failed new session must not exist after rollback"
            );
            assert!(
                store.get("S-NEW").is_none(),
                "Failed new session telemetry must not exist after rollback"
            );
            assert!(
                manifest_store.get_manifest("S-NEW").is_none(),
                "Failed new session manifest must not exist after rollback"
            );

            // The evicted session must be restored
            assert!(
                registry.get_session("S-0").is_some(),
                "Evicted session must be restored by unified rollback"
            );
            assert!(
                store.get("S-0").is_some(),
                "Evicted telemetry must be restored by unified rollback"
            );
            assert!(
                manifest_store.get_manifest("S-0").is_some(),
                "Evicted manifest must be restored by unified rollback"
            );
        }

        /// TCK-00351 BLOCKER 1: Regression for non-atomic stop-condition
        /// rollback.
        ///
        /// Simulates: spawn -> eviction -> post-eviction failure -> rollback,
        /// and verifies the evicted session's original stop conditions are
        /// restored.
        #[test]
        fn test_failed_spawn_restores_evicted_stop_conditions() {
            use crate::episode::envelope::StopConditions;
            use crate::episode::registry::{InMemorySessionRegistry, MAX_SESSIONS};
            use crate::session::{
                SessionRegistry, SessionState, SessionStopConditionsStore, SessionTelemetryStore,
            };

            let registry = Arc::new(InMemorySessionRegistry::new());
            let telemetry = Arc::new(SessionTelemetryStore::new());
            let stop_store = Arc::new(SessionStopConditionsStore::new());

            let dispatcher = PrivilegedDispatcher::new()
                .with_session_registry(
                    Arc::clone(&registry) as Arc<dyn SessionRegistry + Send + Sync>
                )
                .with_telemetry_store(Arc::clone(&telemetry))
                .with_stop_conditions_store(Arc::clone(&stop_store));

            let original_conditions = StopConditions {
                max_episodes: 3,
                escalation_predicate: "severity>=high".to_string(),
                goal_predicate: String::new(),
                failure_predicate: String::new(),
            };

            // Fill to capacity with sessions, telemetry, and stop conditions.
            for i in 0..MAX_SESSIONS {
                let sid = format!("S-{i}");
                let session = SessionState {
                    session_id: sid.clone(),
                    work_id: format!("W-{i}"),
                    role: 1,
                    ephemeral_handle: format!("H-{i}"),
                    lease_id: format!("L-{i}"),
                    policy_resolved_ref: String::new(),
                    capability_manifest_hash: vec![0u8; 32],
                    episode_id: None,
                };
                registry.register_session(session).unwrap();
                telemetry.register(&sid, i as u64).unwrap();
                let conditions = if i == 0 {
                    original_conditions.clone()
                } else {
                    StopConditions::max_episodes(1)
                };
                stop_store.register(&sid, conditions).unwrap();
            }

            // Register one more session to evict S-0.
            let new_session = SessionState {
                session_id: "S-NEW-STOP".to_string(),
                work_id: "W-NEW-STOP".to_string(),
                role: 1,
                ephemeral_handle: "H-NEW-STOP".to_string(),
                lease_id: "L-NEW-STOP".to_string(),
                policy_resolved_ref: String::new(),
                capability_manifest_hash: vec![0u8; 32],
                episode_id: None,
            };
            let evicted = registry.register_session(new_session).unwrap();
            assert_eq!(evicted.len(), 1);
            assert_eq!(evicted[0].session_id, "S-0");

            // Capture evicted telemetry and stop conditions as spawn path does.
            let evicted_telemetry: Vec<_> = evicted
                .iter()
                .filter_map(|s| {
                    telemetry
                        .remove_and_return(&s.session_id)
                        .map(|t| (s.session_id.clone(), t))
                })
                .collect();
            let evicted_stop_conditions: Vec<_> = evicted
                .iter()
                .filter_map(|s| {
                    stop_store
                        .remove_and_return(&s.session_id)
                        .map(|c| (s.session_id.clone(), c))
                })
                .collect();
            assert_eq!(evicted_telemetry.len(), 1);
            assert_eq!(evicted_stop_conditions.len(), 1);

            // Register resources for the newly spawned session.
            telemetry.register("S-NEW-STOP", 999).unwrap();
            stop_store
                .register("S-NEW-STOP", StopConditions::max_episodes(1))
                .unwrap();

            // Simulate spawn failure rollback.
            let evicted_manifests: Vec<(String, Arc<crate::episode::CapabilityManifest>)> =
                Vec::new();
            let result = dispatcher.rollback_spawn(
                "S-NEW-STOP",
                &evicted,
                &evicted_telemetry,
                &evicted_manifests,
                &evicted_stop_conditions,
                false,
            );
            assert!(result.is_none(), "Rollback should succeed: {result:?}");

            // New session should be fully removed.
            assert!(registry.get_session("S-NEW-STOP").is_none());
            assert!(telemetry.get("S-NEW-STOP").is_none());
            assert!(stop_store.get("S-NEW-STOP").is_none());

            // Evicted session and its original stop conditions should be restored.
            assert!(registry.get_session("S-0").is_some());
            let restored = stop_store
                .get("S-0")
                .expect("evicted stop conditions must be restored");
            assert_eq!(restored.max_episodes, original_conditions.max_episodes);
            assert_eq!(
                restored.escalation_predicate,
                original_conditions.escalation_predicate
            );
        }

        /// TCK-00351 BLOCKER 2: caller tampering with permissive stop values
        /// (`max_episodes=0`) is rejected by policy-floor validation.
        #[test]
        fn test_spawn_rejects_untrusted_max_episodes_zero() {
            let dispatcher = PrivilegedDispatcher::new();
            let ctx = ConnectionContext::privileged_session_open(Some(PeerCredentials {
                uid: 1000,
                gid: 1000,
                pid: Some(12345),
            }));

            let claim_request = ClaimWorkRequest {
                actor_id: "tamper-actor".to_string(),
                role: WorkRole::Implementer.into(),
                credential_signature: vec![1, 2, 3],
                nonce: vec![4, 5, 6],
            };
            let claim_frame = encode_claim_work_request(&claim_request);
            let claim_response = dispatcher.dispatch(&claim_frame, &ctx).unwrap();
            let (work_id, lease_id) = match claim_response {
                PrivilegedResponse::ClaimWork(resp) => (resp.work_id, resp.lease_id),
                other => panic!("expected ClaimWork response, got {other:?}"),
            };

            let tampered_spawn = SpawnEpisodeRequest {
                workspace_root: test_workspace_root(),
                work_id: work_id.clone(),
                role: WorkRole::Implementer.into(),
                lease_id: Some(lease_id),
                adapter_profile_hash: None,
                max_episodes: Some(0),
                escalation_predicate: None,
            };
            let frame = encode_spawn_episode_request(&tampered_spawn);
            let response = dispatcher.dispatch(&frame, &ctx).unwrap();

            match response {
                PrivilegedResponse::Error(err) => {
                    assert_eq!(
                        err.code,
                        PrivilegedErrorCode::CapabilityRequestRejected as i32
                    );
                    assert!(
                        err.message.contains("max_episodes=0"),
                        "tamper rejection should mention max_episodes floor: {}",
                        err.message
                    );
                },
                other => panic!("expected tamper rejection, got {other:?}"),
            }

            assert!(
                dispatcher
                    .session_registry()
                    .get_session_by_work_id(&work_id)
                    .is_none(),
                "rejected tampered spawn must not register a session"
            );
        }

        /// SECURITY MAJOR-1: oversized escalation predicates are rejected
        /// before persistence.
        #[test]
        fn test_spawn_rejects_oversized_escalation_predicate() {
            let dispatcher = PrivilegedDispatcher::new();
            let ctx = ConnectionContext::privileged_session_open(Some(PeerCredentials {
                uid: 1000,
                gid: 1000,
                pid: Some(12345),
            }));

            let claim_request = ClaimWorkRequest {
                actor_id: "tamper-escalation".to_string(),
                role: WorkRole::Implementer.into(),
                credential_signature: vec![1, 2, 3],
                nonce: vec![4, 5, 6],
            };
            let claim_frame = encode_claim_work_request(&claim_request);
            let claim_response = dispatcher.dispatch(&claim_frame, &ctx).unwrap();
            let (work_id, lease_id) = match claim_response {
                PrivilegedResponse::ClaimWork(resp) => (resp.work_id, resp.lease_id),
                other => panic!("expected ClaimWork response, got {other:?}"),
            };

            let oversized_predicate = "x".repeat(MAX_ESCALATION_PREDICATE_LEN + 1);
            let spawn_request = SpawnEpisodeRequest {
                workspace_root: test_workspace_root(),
                work_id: work_id.clone(),
                role: WorkRole::Implementer.into(),
                lease_id: Some(lease_id),
                adapter_profile_hash: None,
                max_episodes: None,
                escalation_predicate: Some(oversized_predicate),
            };
            let frame = encode_spawn_episode_request(&spawn_request);
            let response = dispatcher.dispatch(&frame, &ctx).unwrap();

            match response {
                PrivilegedResponse::Error(err) => {
                    assert_eq!(
                        err.code,
                        PrivilegedErrorCode::CapabilityRequestRejected as i32
                    );
                    assert!(
                        err.message
                            .contains("escalation_predicate exceeds maximum length"),
                        "rejection should mention escalation_predicate bound: {}",
                        err.message
                    );
                },
                other => panic!("expected oversized escalation rejection, got {other:?}"),
            }

            assert!(
                dispatcher
                    .session_registry()
                    .get_session_by_work_id(&work_id)
                    .is_none(),
                "rejected oversized spawn must not register a session"
            );
        }
    }

    // ========================================================================
    // TCK-00395: Work lifecycle observability ledger events
    // ========================================================================
    mod tck_00395_work_lifecycle_observability {
        use super::*;

        /// TCK-00395: `ClaimWork` emits `WorkTransitioned`(Open -> Claimed)
        /// event.
        #[test]
        fn claim_work_emits_work_transitioned_open_to_claimed() {
            let dispatcher = PrivilegedDispatcher::new();
            let ctx = ConnectionContext::privileged_session_open(Some(PeerCredentials {
                uid: 1000,
                gid: 1000,
                pid: Some(12345),
            }));

            let request = ClaimWorkRequest {
                actor_id: "test-actor".to_string(),
                role: WorkRole::Implementer.into(),
                credential_signature: vec![1, 2, 3],
                nonce: vec![4, 5, 6],
            };
            let frame = encode_claim_work_request(&request);

            let response = dispatcher.dispatch(&frame, &ctx).unwrap();
            let work_id = match response {
                PrivilegedResponse::ClaimWork(ref resp) => resp.work_id.clone(),
                _ => panic!("Expected ClaimWork response"),
            };

            // Verify WorkTransitioned event was emitted to ledger
            let events = dispatcher.event_emitter.get_events_by_work_id(&work_id);
            let transition_events: Vec<_> = events
                .iter()
                .filter(|e| e.event_type == "work_transitioned")
                .collect();

            assert_eq!(
                transition_events.len(),
                1,
                "Expected exactly 1 WorkTransitioned event, got {}",
                transition_events.len()
            );

            let event = transition_events[0];
            assert_eq!(event.work_id, work_id);

            // Parse payload to verify transition details
            let payload: serde_json::Value =
                serde_json::from_slice(&event.payload).expect("valid JSON payload");
            assert_eq!(payload["event_type"], "work_transitioned");
            assert_eq!(payload["from_state"], "Open");
            assert_eq!(payload["to_state"], "Claimed");
            assert_eq!(payload["rationale_code"], "work_claimed_via_ipc");
            assert_eq!(payload["previous_transition_count"], 0);
            assert!(event.timestamp_ns > 0, "timestamp must be non-zero");
        }

        /// TCK-00395: `SpawnEpisode` emits `WorkTransitioned`(Claimed ->
        /// `InProgress`) event.
        #[test]
        fn spawn_episode_emits_work_transitioned_claimed_to_in_progress() {
            let dispatcher = PrivilegedDispatcher::new();
            let ctx = ConnectionContext::privileged_session_open(Some(PeerCredentials {
                uid: 1000,
                gid: 1000,
                pid: Some(12345),
            }));

            // Step 1: ClaimWork to establish policy resolution
            let claim_request = ClaimWorkRequest {
                actor_id: "test-actor".to_string(),
                role: WorkRole::Implementer.into(),
                credential_signature: vec![1, 2, 3],
                nonce: vec![4, 5, 6],
            };
            let claim_frame = encode_claim_work_request(&claim_request);
            let claim_response = dispatcher.dispatch(&claim_frame, &ctx).unwrap();

            let (work_id, lease_id) = match claim_response {
                PrivilegedResponse::ClaimWork(resp) => (resp.work_id, resp.lease_id),
                _ => panic!("Expected ClaimWork response"),
            };

            // Step 2: SpawnEpisode
            let spawn_request = SpawnEpisodeRequest {
                workspace_root: test_workspace_root(),
                work_id: work_id.clone(),
                role: WorkRole::Implementer.into(),
                lease_id: Some(lease_id),
                adapter_profile_hash: None,
                max_episodes: None,
                escalation_predicate: None,
            };
            let spawn_frame = encode_spawn_episode_request(&spawn_request);
            let spawn_response = dispatcher.dispatch(&spawn_frame, &ctx).unwrap();
            assert!(
                matches!(spawn_response, PrivilegedResponse::SpawnEpisode(_)),
                "Expected SpawnEpisode response"
            );

            // Verify WorkTransitioned events
            let events = dispatcher.event_emitter.get_events_by_work_id(&work_id);
            let transition_events: Vec<_> = events
                .iter()
                .filter(|e| e.event_type == "work_transitioned")
                .collect();

            assert_eq!(
                transition_events.len(),
                2,
                "Expected 2 WorkTransitioned events (Open->Claimed, Claimed->InProgress), got {}",
                transition_events.len()
            );

            // First transition: Open -> Claimed
            let first_payload: serde_json::Value =
                serde_json::from_slice(&transition_events[0].payload).expect("valid JSON");
            assert_eq!(first_payload["from_state"], "Open");
            assert_eq!(first_payload["to_state"], "Claimed");
            assert_eq!(first_payload["rationale_code"], "work_claimed_via_ipc");
            assert_eq!(first_payload["previous_transition_count"], 0);

            // Second transition: Claimed -> InProgress
            let second_payload: serde_json::Value =
                serde_json::from_slice(&transition_events[1].payload).expect("valid JSON");
            assert_eq!(second_payload["from_state"], "Claimed");
            assert_eq!(second_payload["to_state"], "InProgress");
            assert_eq!(second_payload["rationale_code"], "episode_spawned_via_ipc");
            assert_eq!(second_payload["previous_transition_count"], 1);

            let session_started_events: Vec<_> = events
                .iter()
                .filter(|e| e.event_type == "session_started")
                .collect();
            assert_eq!(
                session_started_events.len(),
                1,
                "Expected exactly 1 SessionStarted event"
            );
            let session_payload: serde_json::Value =
                serde_json::from_slice(&session_started_events[0].payload).expect("valid JSON");
            let expected_adapter_hash = apm2_core::fac::builtin_profiles::claude_code_profile()
                .compute_cas_hash()
                .expect("builtin adapter hash should compute");
            assert_eq!(
                session_payload["adapter_profile_hash"],
                hex::encode(expected_adapter_hash)
            );
            assert_eq!(session_payload["waiver_id"], "WVR-0002");
            assert_eq!(session_payload["role_spec_hash_absent"], true);
            assert!(
                session_payload.get("role_spec_hash").is_none(),
                "role_spec_hash should be absent during WVR-0002 rollout path"
            );
        }

        /// TCK-00395: `WorkTransitioned` events use domain-separated
        /// signatures.
        #[test]
        fn work_transitioned_uses_domain_separated_signatures() {
            let emitter = StubLedgerEventEmitter::new();
            let result = emitter.emit_work_transitioned(&WorkTransition {
                work_id: "W-TEST-001",
                from_state: "Open",
                to_state: "Claimed",
                rationale_code: "work_claimed_via_ipc",
                previous_transition_count: 0,
                actor_id: "uid:1000",
                timestamp_ns: 1_000_000_000,
            });

            assert!(result.is_ok(), "emit_work_transitioned should succeed");
            let event = result.unwrap();

            assert_eq!(event.event_type, "work_transitioned");
            assert_eq!(event.work_id, "W-TEST-001");
            assert_eq!(event.actor_id, "uid:1000");
            assert!(!event.signature.is_empty(), "signature must be non-empty");
            assert_eq!(
                event.signature.len(),
                64,
                "Ed25519 signature must be 64 bytes"
            );
        }

        /// TCK-00395: `SessionTerminated` event uses domain-separated
        /// signatures.
        #[test]
        fn session_terminated_uses_domain_separated_signatures() {
            let emitter = StubLedgerEventEmitter::new();
            let result = emitter.emit_session_terminated(
                "SESS-001",
                "W-TEST-001",
                0,
                "completed_normally",
                "uid:1000",
                1_000_000_000,
            );

            assert!(result.is_ok(), "emit_session_terminated should succeed");
            let event = result.unwrap();

            assert_eq!(event.event_type, "session_terminated");
            assert_eq!(event.work_id, "W-TEST-001");
            assert_eq!(event.actor_id, "uid:1000");
            assert!(!event.signature.is_empty(), "signature must be non-empty");
            assert_eq!(
                event.signature.len(),
                64,
                "Ed25519 signature must be 64 bytes"
            );

            // Parse payload to verify all fields present
            let payload: serde_json::Value =
                serde_json::from_slice(&event.payload).expect("valid JSON payload");
            assert_eq!(payload["event_type"], "session_terminated");
            assert_eq!(payload["session_id"], "SESS-001");
            assert_eq!(payload["work_id"], "W-TEST-001");
            assert_eq!(payload["exit_code"], 0);
            assert_eq!(payload["termination_reason"], "completed_normally");
            assert_eq!(payload["actor_id"], "uid:1000");
            assert_eq!(payload["timestamp_ns"], 1_000_000_000);
        }

        /// TCK-00395: Full lifecycle `ClaimWork` -> `SpawnEpisode` -> verify
        /// all event types in ledger.
        #[test]
        fn full_lifecycle_claim_spawn_verify_all_events() {
            let dispatcher = PrivilegedDispatcher::new();
            let ctx = ConnectionContext::privileged_session_open(Some(PeerCredentials {
                uid: 1000,
                gid: 1000,
                pid: Some(12345),
            }));

            // Step 1: ClaimWork
            let claim_request = ClaimWorkRequest {
                actor_id: "test-actor".to_string(),
                role: WorkRole::Implementer.into(),
                credential_signature: vec![1, 2, 3],
                nonce: vec![4, 5, 6],
            };
            let claim_frame = encode_claim_work_request(&claim_request);
            let claim_response = dispatcher.dispatch(&claim_frame, &ctx).unwrap();

            let (work_id, lease_id) = match claim_response {
                PrivilegedResponse::ClaimWork(resp) => (resp.work_id, resp.lease_id),
                _ => panic!("Expected ClaimWork response"),
            };

            // Step 2: SpawnEpisode
            let spawn_request = SpawnEpisodeRequest {
                workspace_root: test_workspace_root(),
                work_id: work_id.clone(),
                role: WorkRole::Implementer.into(),
                lease_id: Some(lease_id),
                adapter_profile_hash: None,
                max_episodes: None,
                escalation_predicate: None,
            };
            let spawn_frame = encode_spawn_episode_request(&spawn_request);
            let spawn_response = dispatcher.dispatch(&spawn_frame, &ctx).unwrap();
            assert!(
                matches!(spawn_response, PrivilegedResponse::SpawnEpisode(_)),
                "Expected SpawnEpisode response"
            );

            // Step 3: Verify full event chain in ledger
            let events = dispatcher.event_emitter.get_events_by_work_id(&work_id);

            // Should have: work_claimed, work_transitioned(Open->Claimed),
            //              session_started, work_transitioned(Claimed->InProgress)
            let event_types: Vec<&str> = events.iter().map(|e| e.event_type.as_str()).collect();
            assert!(
                event_types.contains(&"work_claimed"),
                "Expected work_claimed event in ledger, got: {event_types:?}"
            );
            assert!(
                event_types.contains(&"session_started"),
                "Expected session_started event in ledger, got: {event_types:?}"
            );
            assert!(
                event_types.contains(&"work_transitioned"),
                "Expected work_transitioned event in ledger, got: {event_types:?}"
            );

            // Verify we have exactly 2 work_transitioned events
            let transition_count = events
                .iter()
                .filter(|e| e.event_type == "work_transitioned")
                .count();
            assert_eq!(
                transition_count, 2,
                "Expected 2 work_transitioned events, got {transition_count}"
            );

            // Verify HTF-compliant timestamps (all non-zero)
            for event in &events {
                assert!(
                    event.timestamp_ns > 0,
                    "Event {} should have non-zero HTF timestamp",
                    event.event_type
                );
            }

            // Verify all events are signed (non-empty signatures)
            for event in &events {
                assert!(
                    !event.signature.is_empty(),
                    "Event {} should have non-empty signature",
                    event.event_type
                );
            }
        }

        /// TCK-00395: `WorkTransitioned` and `SessionTerminated` domain
        /// prefixes are unique.
        #[test]
        fn domain_prefixes_are_unique() {
            // All relevant domain prefixes must be unique
            let prefixes: Vec<&[u8]> = vec![
                WORK_CLAIMED_DOMAIN_PREFIX,
                WORK_TRANSITIONED_DOMAIN_PREFIX,
                DEFECT_RECORDED_DOMAIN_PREFIX,
                EPISODE_EVENT_DOMAIN_PREFIX,
                EPISODE_RUN_ATTRIBUTED_PREFIX,
                SESSION_TERMINATED_LEDGER_DOMAIN_PREFIX,
            ];

            for (i, prefix_a) in prefixes.iter().enumerate() {
                for (j, prefix_b) in prefixes.iter().enumerate() {
                    if i != j {
                        assert_ne!(
                            prefix_a, prefix_b,
                            "Domain prefixes must be unique to prevent replay attacks"
                        );
                    }
                }
            }

            // Verify prefixes have the correct format
            for prefix in &prefixes {
                let prefix_str = std::str::from_utf8(prefix).expect("Prefix must be valid UTF-8");
                assert!(
                    prefix_str.starts_with("apm2.event."),
                    "Prefix must start with 'apm2.event.': {prefix_str}"
                );
                assert!(
                    prefix_str.ends_with(':'),
                    "Prefix must end with ':': {prefix_str}"
                );
            }
        }

        /// TCK-00395: `WorkTransitioned` events are queryable by `work_id`.
        #[test]
        fn work_transitioned_events_queryable_by_work_id() {
            let emitter = StubLedgerEventEmitter::new();

            // Emit two transition events for the same work_id
            emitter
                .emit_work_transitioned(&WorkTransition {
                    work_id: "W-QUERY-001",
                    from_state: "Open",
                    to_state: "Claimed",
                    rationale_code: "work_claimed_via_ipc",
                    previous_transition_count: 0,
                    actor_id: "uid:1000",
                    timestamp_ns: 1_000_000_000,
                })
                .unwrap();

            emitter
                .emit_work_transitioned(&WorkTransition {
                    work_id: "W-QUERY-001",
                    from_state: "Claimed",
                    to_state: "InProgress",
                    rationale_code: "episode_spawned_via_ipc",
                    previous_transition_count: 1,
                    actor_id: "uid:1000",
                    timestamp_ns: 2_000_000_000,
                })
                .unwrap();

            // Emit a transition for a different work_id
            emitter
                .emit_work_transitioned(&WorkTransition {
                    work_id: "W-QUERY-002",
                    from_state: "Open",
                    to_state: "Claimed",
                    rationale_code: "work_claimed_via_ipc",
                    previous_transition_count: 0,
                    actor_id: "uid:2000",
                    timestamp_ns: 3_000_000_000,
                })
                .unwrap();

            // Query by first work_id - should get 2 events
            let events = emitter.get_events_by_work_id("W-QUERY-001");
            assert_eq!(
                events.len(),
                2,
                "Expected 2 events for W-QUERY-001, got {}",
                events.len()
            );

            // Query by second work_id - should get 1 event
            let events = emitter.get_events_by_work_id("W-QUERY-002");
            assert_eq!(
                events.len(),
                1,
                "Expected 1 event for W-QUERY-002, got {}",
                events.len()
            );
        }

        // ================================================================
        // TCK-00395 v2: Regression tests for review findings
        // ================================================================

        /// FIX-BLOCKER: `EndSession` handler emits `SessionTerminated` ledger
        /// event.
        ///
        /// Integration test: `ClaimWork` -> `SpawnEpisode` -> `EndSession` ->
        /// verify `session_terminated` is persisted.
        #[test]
        fn end_session_emits_session_terminated_event() {
            let dispatcher = PrivilegedDispatcher::new();
            let ctx = ConnectionContext::privileged_session_open(Some(PeerCredentials {
                uid: 1000,
                gid: 1000,
                pid: Some(12345),
            }));

            // Step 1: ClaimWork
            let claim_request = ClaimWorkRequest {
                actor_id: "test-actor".to_string(),
                role: WorkRole::Implementer.into(),
                credential_signature: vec![1, 2, 3],
                nonce: vec![4, 5, 6],
            };
            let claim_frame = encode_claim_work_request(&claim_request);
            let claim_response = dispatcher.dispatch(&claim_frame, &ctx).unwrap();
            let (work_id, lease_id) = match claim_response {
                PrivilegedResponse::ClaimWork(resp) => (resp.work_id, resp.lease_id),
                _ => panic!("Expected ClaimWork response"),
            };

            // Step 2: SpawnEpisode
            let spawn_request = SpawnEpisodeRequest {
                workspace_root: test_workspace_root(),
                work_id: work_id.clone(),
                role: WorkRole::Implementer.into(),
                lease_id: Some(lease_id),
                adapter_profile_hash: None,
                max_episodes: None,
                escalation_predicate: None,
            };
            let spawn_frame = encode_spawn_episode_request(&spawn_request);
            let spawn_response = dispatcher.dispatch(&spawn_frame, &ctx).unwrap();
            let session_id = match spawn_response {
                PrivilegedResponse::SpawnEpisode(ref resp) => resp.session_id.clone(),
                _ => panic!("Expected SpawnEpisode response"),
            };

            // Step 3: EndSession
            let end_request = EndSessionRequest {
                session_id: session_id.clone(),
                reason: "completed_normally".to_string(),
                outcome: TerminationOutcome::Success as i32,
            };
            let end_frame = encode_end_session_request(&end_request);
            let end_response = dispatcher.dispatch(&end_frame, &ctx).unwrap();
            match end_response {
                PrivilegedResponse::EndSession(resp) => {
                    assert_eq!(resp.session_id, session_id);
                },
                other => panic!("Expected EndSession response, got: {other:?}"),
            }

            // Step 4: Verify session_terminated event in ledger
            let events = dispatcher.event_emitter.get_events_by_work_id(&work_id);
            let terminated_events: Vec<_> = events
                .iter()
                .filter(|e| e.event_type == "session_terminated")
                .collect();
            assert_eq!(
                terminated_events.len(),
                1,
                "Expected exactly 1 session_terminated event, got {}",
                terminated_events.len()
            );

            let event = terminated_events[0];
            let payload: serde_json::Value =
                serde_json::from_slice(&event.payload).expect("valid JSON payload");
            assert_eq!(payload["event_type"], "session_terminated");
            assert_eq!(payload["session_id"], session_id);
            assert_eq!(payload["work_id"], work_id);
            assert_eq!(payload["exit_code"], 0);
            assert_eq!(payload["termination_reason"], "completed_normally");
        }

        /// Quality BLOCKER 2: `EndSession` does NOT emit
        /// `WorkTransitioned(InProgress -> Completed)`.
        ///
        /// The `InProgress` -> `Completed` transition violates core
        /// work-state rules (`state.rs`). Work completion belongs to
        /// gate orchestration, not `EndSession`. `EndSession` only emits
        /// `session_terminated`.
        #[test]
        fn end_session_success_does_not_emit_completed_transition() {
            let dispatcher = PrivilegedDispatcher::new();
            let ctx = ConnectionContext::privileged_session_open(Some(PeerCredentials {
                uid: 1000,
                gid: 1000,
                pid: Some(12345),
            }));

            // ClaimWork + SpawnEpisode
            let claim_request = ClaimWorkRequest {
                actor_id: "test-actor".to_string(),
                role: WorkRole::Implementer.into(),
                credential_signature: vec![1, 2, 3],
                nonce: vec![4, 5, 6],
            };
            let claim_frame = encode_claim_work_request(&claim_request);
            let claim_response = dispatcher.dispatch(&claim_frame, &ctx).unwrap();
            let (work_id, lease_id) = match claim_response {
                PrivilegedResponse::ClaimWork(resp) => (resp.work_id, resp.lease_id),
                _ => panic!("Expected ClaimWork response"),
            };

            let spawn_request = SpawnEpisodeRequest {
                workspace_root: test_workspace_root(),
                work_id: work_id.clone(),
                role: WorkRole::Implementer.into(),
                lease_id: Some(lease_id),
                adapter_profile_hash: None,
                max_episodes: None,
                escalation_predicate: None,
            };
            let spawn_frame = encode_spawn_episode_request(&spawn_request);
            let spawn_response = dispatcher.dispatch(&spawn_frame, &ctx).unwrap();
            let session_id = match spawn_response {
                PrivilegedResponse::SpawnEpisode(ref resp) => resp.session_id.clone(),
                _ => panic!("Expected SpawnEpisode response"),
            };

            // EndSession with success
            let end_request = EndSessionRequest {
                session_id,
                reason: "completed_normally".to_string(),
                outcome: TerminationOutcome::Success as i32,
            };
            let end_frame = encode_end_session_request(&end_request);
            dispatcher.dispatch(&end_frame, &ctx).unwrap();

            // Verify NO WorkTransitioned(InProgress -> Completed) is emitted.
            // Per Quality BLOCKER 2: work completion belongs to gate
            // orchestration, not EndSession.
            let events = dispatcher.event_emitter.get_events_by_work_id(&work_id);
            let completed_count = events
                .iter()
                .filter(|e| e.event_type == "work_transitioned")
                .filter(|e| {
                    let p: serde_json::Value = serde_json::from_slice(&e.payload).unwrap();
                    p["to_state"] == "Completed"
                })
                .count();

            assert_eq!(
                completed_count, 0,
                "EndSession must NOT emit WorkTransitioned(Completed) - work completion belongs to gate orchestration"
            );

            // Verify session_terminated was still emitted
            let terminated_count = events
                .iter()
                .filter(|e| e.event_type == "session_terminated")
                .count();
            assert_eq!(
                terminated_count, 1,
                "EndSession should emit session_terminated"
            );
        }

        /// FIX-BLOCKER: `EndSession` rejects unknown session ID.
        #[test]
        fn end_session_rejects_unknown_session() {
            let dispatcher = PrivilegedDispatcher::new();
            let ctx = ConnectionContext::privileged_session_open(Some(PeerCredentials {
                uid: 1000,
                gid: 1000,
                pid: Some(12345),
            }));

            let end_request = EndSessionRequest {
                session_id: "NONEXISTENT-SESSION".to_string(),
                reason: "test".to_string(),
                outcome: TerminationOutcome::Failure as i32,
            };
            let end_frame = encode_end_session_request(&end_request);
            let response = dispatcher.dispatch(&end_frame, &ctx).unwrap();

            match response {
                PrivilegedResponse::Error(err) => {
                    assert!(
                        err.message.contains("session not found"),
                        "Expected 'session not found' error, got: {}",
                        err.message
                    );
                },
                other => panic!("Expected error response, got: {other:?}"),
            }
        }

        /// FIX-SEC-MAJOR: `previous_transition_count` is derived from
        /// authoritative ledger state, not hardcoded.
        #[test]
        fn transition_count_derived_from_ledger_state() {
            let emitter = StubLedgerEventEmitter::new();

            // Initially, no transitions
            assert_eq!(
                emitter.get_work_transition_count("W-COUNT-001"),
                0,
                "No transitions yet"
            );

            // Emit first transition
            emitter
                .emit_work_transitioned(&WorkTransition {
                    work_id: "W-COUNT-001",
                    from_state: "Open",
                    to_state: "Claimed",
                    rationale_code: "work_claimed_via_ipc",
                    previous_transition_count: 0,
                    actor_id: "uid:1000",
                    timestamp_ns: 1_000_000_000,
                })
                .unwrap();

            assert_eq!(
                emitter.get_work_transition_count("W-COUNT-001"),
                1,
                "Should have 1 transition after first emit"
            );

            // Emit second transition
            emitter
                .emit_work_transitioned(&WorkTransition {
                    work_id: "W-COUNT-001",
                    from_state: "Claimed",
                    to_state: "InProgress",
                    rationale_code: "episode_spawned_via_ipc",
                    previous_transition_count: 1,
                    actor_id: "uid:1000",
                    timestamp_ns: 2_000_000_000,
                })
                .unwrap();

            assert_eq!(
                emitter.get_work_transition_count("W-COUNT-001"),
                2,
                "Should have 2 transitions after second emit"
            );

            // Different work_id should have 0 transitions
            assert_eq!(
                emitter.get_work_transition_count("W-COUNT-002"),
                0,
                "Different work_id should have 0 transitions"
            );
        }

        /// FIX-SEC-MAJOR: Duplicate/retry spawn transitions are detected
        /// via authoritative transition count.
        #[test]
        fn duplicate_spawn_detected_via_transition_count() {
            let dispatcher = PrivilegedDispatcher::new();
            let ctx = ConnectionContext::privileged_session_open(Some(PeerCredentials {
                uid: 1000,
                gid: 1000,
                pid: Some(12345),
            }));

            // ClaimWork
            let claim_request = ClaimWorkRequest {
                actor_id: "test-actor".to_string(),
                role: WorkRole::Implementer.into(),
                credential_signature: vec![1, 2, 3],
                nonce: vec![4, 5, 6],
            };
            let claim_frame = encode_claim_work_request(&claim_request);
            let claim_response = dispatcher.dispatch(&claim_frame, &ctx).unwrap();
            let (work_id, _lease_id) = match claim_response {
                PrivilegedResponse::ClaimWork(resp) => (resp.work_id, resp.lease_id),
                _ => panic!("Expected ClaimWork response"),
            };

            // After ClaimWork: should have 1 transition (Open->Claimed)
            let count_after_claim = dispatcher.event_emitter.get_work_transition_count(&work_id);
            assert_eq!(
                count_after_claim, 1,
                "After ClaimWork, transition count should be 1"
            );

            // Verify the transition event has previous_transition_count=0
            // (which is the count BEFORE the transition was emitted)
            let events = dispatcher.event_emitter.get_events_by_work_id(&work_id);
            let first_transition: Vec<_> = events
                .iter()
                .filter(|e| e.event_type == "work_transitioned")
                .collect();
            assert_eq!(first_transition.len(), 1);
            let payload: serde_json::Value =
                serde_json::from_slice(&first_transition[0].payload).unwrap();
            assert_eq!(
                payload["previous_transition_count"], 0,
                "First transition should have previous_transition_count=0"
            );
        }

        /// FIX-SEC-BLOCKER: Replay ordering is deterministic for
        /// equal-timestamp events.
        ///
        /// Events emitted with the same timestamp must maintain stable
        /// insertion order when queried via `get_events_by_work_id`.
        #[test]
        fn equal_timestamp_events_maintain_insertion_order() {
            let emitter = StubLedgerEventEmitter::new();
            let ts = 1_000_000_000u64;

            // Emit three events with the same timestamp
            emitter
                .emit_work_claimed(
                    &WorkClaim {
                        work_id: "W-ORDER-001".to_string(),
                        lease_id: "L-001".to_string(),
                        actor_id: "uid:1000".to_string(),
                        role: WorkRole::Implementer,
                        policy_resolution: PolicyResolution {
                            policy_resolved_ref: "test-resolved".to_string(),
                            resolved_policy_hash: [0u8; 32],
                            capability_manifest_hash: [0u8; 32],
                            context_pack_hash: [0u8; 32],
                            resolved_risk_tier: 0,
                            resolved_scope_baseline: None,
                            expected_adapter_profile_hash: None,
                        },
                        executor_custody_domains: vec![],
                        author_custody_domains: vec![],
                    },
                    ts,
                )
                .unwrap();

            emitter
                .emit_work_transitioned(&WorkTransition {
                    work_id: "W-ORDER-001",
                    from_state: "Open",
                    to_state: "Claimed",
                    rationale_code: "work_claimed_via_ipc",
                    previous_transition_count: 0,
                    actor_id: "uid:1000",
                    timestamp_ns: ts,
                })
                .unwrap();

            emitter
                .emit_session_started(
                    "SESS-001",
                    "W-ORDER-001",
                    "L-001",
                    "uid:1000",
                    &[0xAA; 32],
                    None,
                    ts,
                    None,
                    None,
                )
                .unwrap();

            // Query events - must maintain insertion order
            let events = emitter.get_events_by_work_id("W-ORDER-001");
            assert_eq!(events.len(), 3, "Expected 3 events");

            // Verify ordering: work_claimed, work_transitioned, session_started
            assert_eq!(
                events[0].event_type, "work_claimed",
                "First event should be work_claimed"
            );
            assert_eq!(
                events[1].event_type, "work_transitioned",
                "Second event should be work_transitioned"
            );
            assert_eq!(
                events[2].event_type, "session_started",
                "Third event should be session_started"
            );

            // All have the same timestamp
            for event in &events {
                assert_eq!(
                    event.timestamp_ns, ts,
                    "All events should have the same timestamp"
                );
            }
        }

        /// FIX-QUALITY-MAJOR: `emit_claim_lifecycle` is atomic.
        ///
        /// Tests that `emit_claim_lifecycle` emits both `work_claimed` and
        /// `work_transitioned` events.
        #[test]
        fn emit_claim_lifecycle_emits_both_events() {
            let emitter = StubLedgerEventEmitter::new();
            let claim = WorkClaim {
                work_id: "W-ATOMIC-001".to_string(),
                lease_id: "L-001".to_string(),
                actor_id: "uid:1000".to_string(),
                role: WorkRole::Implementer,
                policy_resolution: PolicyResolution {
                    policy_resolved_ref: "test-resolved".to_string(),
                    resolved_policy_hash: [0u8; 32],
                    capability_manifest_hash: [0u8; 32],
                    context_pack_hash: [0u8; 32],
                    resolved_risk_tier: 0,
                    resolved_scope_baseline: None,
                    expected_adapter_profile_hash: None,
                },
                executor_custody_domains: vec![],
                author_custody_domains: vec![],
            };

            let result = emitter.emit_claim_lifecycle(&claim, "uid:1000", 1_000_000_000);
            assert!(result.is_ok(), "emit_claim_lifecycle should succeed");

            let events = emitter.get_events_by_work_id("W-ATOMIC-001");
            assert_eq!(
                events.len(),
                2,
                "Expected 2 events (work_claimed + work_transitioned)"
            );
            assert_eq!(events[0].event_type, "work_claimed");
            assert_eq!(events[1].event_type, "work_transitioned");

            // Verify the transition has correct previous_transition_count
            let payload: serde_json::Value = serde_json::from_slice(&events[1].payload).unwrap();
            assert_eq!(payload["from_state"], "Open");
            assert_eq!(payload["to_state"], "Claimed");
            assert_eq!(payload["previous_transition_count"], 0);
        }

        /// FIX-QUALITY-MAJOR: `emit_spawn_lifecycle` is atomic.
        ///
        /// Tests that `emit_spawn_lifecycle` emits both `session_started` and
        /// `work_transitioned` events.
        #[test]
        fn emit_spawn_lifecycle_emits_both_events() {
            let emitter = StubLedgerEventEmitter::new();

            // First, emit the claim lifecycle so transition count is 1
            let claim = WorkClaim {
                work_id: "W-ATOMIC-002".to_string(),
                lease_id: "L-001".to_string(),
                actor_id: "uid:1000".to_string(),
                role: WorkRole::Implementer,
                policy_resolution: PolicyResolution {
                    policy_resolved_ref: "test-resolved".to_string(),
                    resolved_policy_hash: [0u8; 32],
                    capability_manifest_hash: [0u8; 32],
                    context_pack_hash: [0u8; 32],
                    resolved_risk_tier: 0,
                    resolved_scope_baseline: None,
                    expected_adapter_profile_hash: None,
                },
                executor_custody_domains: vec![],
                author_custody_domains: vec![],
            };
            emitter
                .emit_claim_lifecycle(&claim, "uid:1000", 1_000_000_000)
                .unwrap();

            // Now emit spawn lifecycle
            let result = emitter.emit_spawn_lifecycle(
                "SESS-002",
                "W-ATOMIC-002",
                "L-001",
                "uid:1000",
                &[0xAA; 32],
                None,
                2_000_000_000,
                None,
                None,
            );
            assert!(result.is_ok(), "emit_spawn_lifecycle should succeed");

            let events = emitter.get_events_by_work_id("W-ATOMIC-002");
            // Should have: work_claimed, work_transitioned(Open->Claimed),
            // session_started, work_transitioned(Claimed->InProgress)
            assert_eq!(events.len(), 4, "Expected 4 events total");
            assert_eq!(events[2].event_type, "session_started");
            assert_eq!(events[3].event_type, "work_transitioned");

            let payload: serde_json::Value = serde_json::from_slice(&events[3].payload).unwrap();
            assert_eq!(payload["from_state"], "Claimed");
            assert_eq!(payload["to_state"], "InProgress");
            // previous_transition_count should be 1 (derived from ledger)
            assert_eq!(payload["previous_transition_count"], 1);
        }

        // ================================================================
        // TCK-00395 v3: Quality review v2 fixes
        // ================================================================

        /// MAJOR 1: `EndSession` removes session from registry, preventing
        /// repeated termination.
        #[test]
        fn end_session_removes_session_from_registry() {
            let dispatcher = PrivilegedDispatcher::new();
            let ctx = ConnectionContext::privileged_session_open(Some(PeerCredentials {
                uid: 1000,
                gid: 1000,
                pid: Some(12345),
            }));

            // ClaimWork + SpawnEpisode
            let claim_request = ClaimWorkRequest {
                actor_id: "test-actor".to_string(),
                role: WorkRole::Implementer.into(),
                credential_signature: vec![1, 2, 3],
                nonce: vec![4, 5, 6],
            };
            let claim_frame = encode_claim_work_request(&claim_request);
            let claim_response = dispatcher.dispatch(&claim_frame, &ctx).unwrap();
            let (work_id, lease_id) = match claim_response {
                PrivilegedResponse::ClaimWork(resp) => (resp.work_id, resp.lease_id),
                _ => panic!("Expected ClaimWork response"),
            };

            let spawn_request = SpawnEpisodeRequest {
                workspace_root: test_workspace_root(),
                work_id,
                role: WorkRole::Implementer.into(),
                lease_id: Some(lease_id),
                adapter_profile_hash: None,
                max_episodes: None,
                escalation_predicate: None,
            };
            let spawn_frame = encode_spawn_episode_request(&spawn_request);
            let spawn_response = dispatcher.dispatch(&spawn_frame, &ctx).unwrap();
            let session_id = match spawn_response {
                PrivilegedResponse::SpawnEpisode(ref resp) => resp.session_id.clone(),
                _ => panic!("Expected SpawnEpisode response"),
            };

            // Verify session exists before EndSession
            assert!(
                dispatcher
                    .session_registry()
                    .get_session(&session_id)
                    .is_some(),
                "Session should exist before EndSession"
            );

            // First EndSession - should succeed
            let end_request = EndSessionRequest {
                session_id: session_id.clone(),
                reason: "completed_normally".to_string(),
                outcome: TerminationOutcome::Success as i32,
            };
            let end_frame = encode_end_session_request(&end_request);
            let response = dispatcher.dispatch(&end_frame, &ctx).unwrap();
            match response {
                PrivilegedResponse::EndSession(resp) => {
                    assert_eq!(resp.session_id, session_id);
                },
                other => panic!("Expected EndSession response, got: {other:?}"),
            }

            // Verify session is removed from registry
            assert!(
                dispatcher
                    .session_registry()
                    .get_session(&session_id)
                    .is_none(),
                "Session should be removed after EndSession"
            );

            // Second EndSession - should fail with "session not found"
            let end_frame2 = encode_end_session_request(&end_request);
            let response2 = dispatcher.dispatch(&end_frame2, &ctx).unwrap();
            match response2 {
                PrivilegedResponse::Error(err) => {
                    assert!(
                        err.message.contains("session not found"),
                        "Expected 'session not found' on repeated EndSession, got: {}",
                        err.message
                    );
                },
                other => panic!("Expected error on repeated EndSession, got: {other:?}"),
            }
        }

        /// Security MAJOR 1: `EndSession` rejects reason strings that exceed
        /// `MAX_REASON_LENGTH`. Prevents OOM and bloated signed ledger
        /// payloads.
        #[test]
        fn end_session_rejects_oversized_reason() {
            let dispatcher = PrivilegedDispatcher::new();
            let ctx = ConnectionContext::privileged_session_open(Some(PeerCredentials {
                uid: 1000,
                gid: 1000,
                pid: Some(12345),
            }));

            // ClaimWork + SpawnEpisode to get a valid session
            let claim_request = ClaimWorkRequest {
                actor_id: "test-actor".to_string(),
                role: WorkRole::Implementer.into(),
                credential_signature: vec![1, 2, 3],
                nonce: vec![4, 5, 6],
            };
            let claim_frame = encode_claim_work_request(&claim_request);
            let claim_response = dispatcher.dispatch(&claim_frame, &ctx).unwrap();
            let (work_id, lease_id) = match claim_response {
                PrivilegedResponse::ClaimWork(resp) => (resp.work_id, resp.lease_id),
                _ => panic!("Expected ClaimWork response"),
            };

            let spawn_request = SpawnEpisodeRequest {
                workspace_root: test_workspace_root(),
                work_id,
                role: WorkRole::Implementer.into(),
                lease_id: Some(lease_id),
                adapter_profile_hash: None,
                max_episodes: None,
                escalation_predicate: None,
            };
            let spawn_frame = encode_spawn_episode_request(&spawn_request);
            let spawn_response = dispatcher.dispatch(&spawn_frame, &ctx).unwrap();
            let session_id = match spawn_response {
                PrivilegedResponse::SpawnEpisode(ref resp) => resp.session_id.clone(),
                _ => panic!("Expected SpawnEpisode response"),
            };

            // EndSession with oversized reason (MAX_REASON_LENGTH + 1)
            let oversized_reason = "x".repeat(MAX_REASON_LENGTH + 1);
            let end_request = EndSessionRequest {
                session_id: session_id.clone(),
                reason: oversized_reason,
                outcome: TerminationOutcome::Success as i32,
            };
            let end_frame = encode_end_session_request(&end_request);
            let response = dispatcher.dispatch(&end_frame, &ctx).unwrap();
            match response {
                PrivilegedResponse::Error(err) => {
                    assert!(
                        err.message.contains("reason exceeds maximum length"),
                        "Expected reason length error, got: {}",
                        err.message
                    );
                },
                other => panic!("Expected error for oversized reason, got: {other:?}"),
            }

            // Verify session is NOT removed (request was rejected)
            assert!(
                dispatcher
                    .session_registry()
                    .get_session(&session_id)
                    .is_some(),
                "Session MUST be preserved when reason validation fails"
            );
        }

        /// Security MAJOR 1: `EndSession` accepts reason at exactly
        /// `MAX_REASON_LENGTH` bytes (boundary).
        #[test]
        fn end_session_accepts_reason_at_max_length() {
            let dispatcher = PrivilegedDispatcher::new();
            let ctx = ConnectionContext::privileged_session_open(Some(PeerCredentials {
                uid: 1000,
                gid: 1000,
                pid: Some(12345),
            }));

            let claim_request = ClaimWorkRequest {
                actor_id: "test-actor".to_string(),
                role: WorkRole::Implementer.into(),
                credential_signature: vec![1, 2, 3],
                nonce: vec![4, 5, 6],
            };
            let claim_frame = encode_claim_work_request(&claim_request);
            let claim_response = dispatcher.dispatch(&claim_frame, &ctx).unwrap();
            let (work_id, lease_id) = match claim_response {
                PrivilegedResponse::ClaimWork(resp) => (resp.work_id, resp.lease_id),
                _ => panic!("Expected ClaimWork response"),
            };

            let spawn_request = SpawnEpisodeRequest {
                workspace_root: test_workspace_root(),
                work_id,
                role: WorkRole::Implementer.into(),
                lease_id: Some(lease_id),
                adapter_profile_hash: None,
                max_episodes: None,
                escalation_predicate: None,
            };
            let spawn_frame = encode_spawn_episode_request(&spawn_request);
            let spawn_response = dispatcher.dispatch(&spawn_frame, &ctx).unwrap();
            let session_id = match spawn_response {
                PrivilegedResponse::SpawnEpisode(ref resp) => resp.session_id.clone(),
                _ => panic!("Expected SpawnEpisode response"),
            };

            // EndSession with reason at exactly MAX_REASON_LENGTH
            let max_reason = "r".repeat(MAX_REASON_LENGTH);
            let end_request = EndSessionRequest {
                session_id: session_id.clone(),
                reason: max_reason,
                outcome: TerminationOutcome::Failure as i32,
            };
            let end_frame = encode_end_session_request(&end_request);
            let response = dispatcher.dispatch(&end_frame, &ctx).unwrap();
            match response {
                PrivilegedResponse::EndSession(resp) => {
                    assert_eq!(resp.session_id, session_id);
                },
                other => {
                    panic!("Expected EndSession response for max-length reason, got: {other:?}")
                },
            }
        }

        /// Quality BLOCKER 2: `EndSession` succeeds regardless of
        /// `emit_work_transitioned` status because `EndSession` no longer
        /// emits work transitions. Work completion belongs to gate
        /// orchestration.
        #[test]
        fn end_session_succeeds_without_work_transition() {
            let dispatcher = PrivilegedDispatcher::new();
            let ctx = ConnectionContext::privileged_session_open(Some(PeerCredentials {
                uid: 1000,
                gid: 1000,
                pid: Some(12345),
            }));

            // ClaimWork
            let claim_request = ClaimWorkRequest {
                actor_id: "test-actor".to_string(),
                role: WorkRole::Implementer.into(),
                credential_signature: vec![1, 2, 3],
                nonce: vec![4, 5, 6],
            };
            let claim_frame = encode_claim_work_request(&claim_request);
            let claim_response = dispatcher.dispatch(&claim_frame, &ctx).unwrap();
            let (work_id, lease_id) = match claim_response {
                PrivilegedResponse::ClaimWork(resp) => (resp.work_id, resp.lease_id),
                _ => panic!("Expected ClaimWork response"),
            };

            // SpawnEpisode
            let spawn_request = SpawnEpisodeRequest {
                workspace_root: test_workspace_root(),
                work_id: work_id.clone(),
                role: WorkRole::Implementer.into(),
                lease_id: Some(lease_id),
                adapter_profile_hash: None,
                max_episodes: None,
                escalation_predicate: None,
            };
            let spawn_frame = encode_spawn_episode_request(&spawn_request);
            let spawn_response = dispatcher.dispatch(&spawn_frame, &ctx).unwrap();
            let session_id = match spawn_response {
                PrivilegedResponse::SpawnEpisode(ref resp) => resp.session_id.clone(),
                _ => panic!("Expected SpawnEpisode response"),
            };

            // EndSession with success
            let end_request = EndSessionRequest {
                session_id: session_id.clone(),
                reason: "completed_normally".to_string(),
                outcome: TerminationOutcome::Success as i32,
            };
            let end_frame = encode_end_session_request(&end_request);
            let response = dispatcher.dispatch(&end_frame, &ctx).unwrap();

            // EndSession should succeed (no work transition emitted)
            match response {
                PrivilegedResponse::EndSession(resp) => {
                    assert_eq!(resp.message, "session terminated");
                },
                other => panic!("Expected EndSession response, got: {other:?}"),
            }

            // Session should be removed from registry after success
            assert!(
                dispatcher
                    .session_registry()
                    .get_session(&session_id)
                    .is_none(),
                "Session MUST be removed from registry after successful EndSession"
            );

            // Verify no Completed transitions were emitted
            let events = dispatcher.event_emitter.get_events_by_work_id(&work_id);
            let completed_count = events
                .iter()
                .filter(|e| e.event_type == "work_transitioned")
                .filter(|e| {
                    let p: serde_json::Value = serde_json::from_slice(&e.payload).unwrap();
                    p["to_state"] == "Completed"
                })
                .count();
            assert_eq!(
                completed_count, 0,
                "EndSession must not emit WorkTransitioned(Completed)"
            );
        }

        /// Quality v3 MAJOR: Typed `TerminationOutcome` enum determines
        /// exit code instead of free-form reason string matching.
        #[test]
        fn end_session_typed_outcome_determines_exit_code() {
            let dispatcher = PrivilegedDispatcher::new();
            let ctx = ConnectionContext::privileged_session_open(Some(PeerCredentials {
                uid: 1000,
                gid: 1000,
                pid: Some(12345),
            }));

            // Helper: claim + spawn and return (work_id, session_id)
            let setup = |d: &PrivilegedDispatcher| {
                let claim_request = ClaimWorkRequest {
                    actor_id: "test-actor".to_string(),
                    role: WorkRole::Implementer.into(),
                    credential_signature: vec![1, 2, 3],
                    nonce: vec![4, 5, 6],
                };
                let claim_frame = encode_claim_work_request(&claim_request);
                let claim_response = d.dispatch(&claim_frame, &ctx).unwrap();
                let (work_id, lease_id) = match claim_response {
                    PrivilegedResponse::ClaimWork(resp) => (resp.work_id, resp.lease_id),
                    _ => panic!("Expected ClaimWork response"),
                };
                let spawn_request = SpawnEpisodeRequest {
                    workspace_root: test_workspace_root(),
                    work_id: work_id.clone(),
                    role: WorkRole::Implementer.into(),
                    lease_id: Some(lease_id),
                    adapter_profile_hash: None,
                    max_episodes: None,
                    escalation_predicate: None,
                };
                let spawn_frame = encode_spawn_episode_request(&spawn_request);
                let spawn_response = d.dispatch(&spawn_frame, &ctx).unwrap();
                let session_id = match spawn_response {
                    PrivilegedResponse::SpawnEpisode(ref resp) => resp.session_id.clone(),
                    _ => panic!("Expected SpawnEpisode response"),
                };
                (work_id, session_id)
            };

            // Test 1: TerminationOutcome::Success -> exit_code 0.
            // Quality BLOCKER 2: NO WorkTransitioned(Completed) emitted.
            let (work_id, session_id) = setup(&dispatcher);
            let end_request = EndSessionRequest {
                session_id,
                reason: "some_reason".to_string(), // reason is irrelevant when outcome is set
                outcome: TerminationOutcome::Success as i32,
            };
            let end_frame = encode_end_session_request(&end_request);
            dispatcher.dispatch(&end_frame, &ctx).unwrap();
            let events = dispatcher.event_emitter.get_events_by_work_id(&work_id);
            let terminated: Vec<_> = events
                .iter()
                .filter(|e| e.event_type == "session_terminated")
                .collect();
            assert_eq!(terminated.len(), 1);
            let payload: serde_json::Value =
                serde_json::from_slice(&terminated[0].payload).unwrap();
            assert_eq!(payload["exit_code"], 0, "Success outcome -> exit_code 0");
            // Quality BLOCKER 2: EndSession must NOT emit
            // WorkTransitioned(Completed). Work completion belongs to
            // gate orchestration.
            assert!(
                !events.iter().any(|e| {
                    e.event_type == "work_transitioned" && {
                        let p: serde_json::Value = serde_json::from_slice(&e.payload).unwrap();
                        p["to_state"] == "Completed"
                    }
                }),
                "Success outcome must NOT emit WorkTransitioned(Completed) per Quality BLOCKER 2"
            );

            // Test 2: TerminationOutcome::Failure -> exit_code 1,
            //         NO WorkTransitioned(Completed).
            let (work_id2, session_id2) = setup(&dispatcher);
            let end_request2 = EndSessionRequest {
                session_id: session_id2,
                reason: "completed_normally".to_string(), /* Even with "success" reason, outcome
                                                           * overrides */
                outcome: TerminationOutcome::Failure as i32,
            };
            let end_frame2 = encode_end_session_request(&end_request2);
            dispatcher.dispatch(&end_frame2, &ctx).unwrap();
            let events2 = dispatcher.event_emitter.get_events_by_work_id(&work_id2);
            let terminated2: Vec<_> = events2
                .iter()
                .filter(|e| e.event_type == "session_terminated")
                .collect();
            assert_eq!(terminated2.len(), 1);
            let payload2: serde_json::Value =
                serde_json::from_slice(&terminated2[0].payload).unwrap();
            assert_eq!(payload2["exit_code"], 1, "Failure outcome -> exit_code 1");
            // Should NOT have WorkTransitioned to Completed
            let completed_count = events2
                .iter()
                .filter(|e| {
                    e.event_type == "work_transitioned" && {
                        let p: serde_json::Value = serde_json::from_slice(&e.payload).unwrap();
                        p["to_state"] == "Completed"
                    }
                })
                .count();
            assert_eq!(
                completed_count, 0,
                "Failure outcome should NOT emit WorkTransitioned(Completed)"
            );

            // Test 3: TerminationOutcome::Unspecified (0) falls back to
            //         legacy string matching (backward compat).
            let (work_id3, session_id3) = setup(&dispatcher);
            let end_request3 = EndSessionRequest {
                session_id: session_id3,
                reason: "completed_normally".to_string(),
                outcome: TerminationOutcome::Unspecified as i32,
            };
            let end_frame3 = encode_end_session_request(&end_request3);
            dispatcher.dispatch(&end_frame3, &ctx).unwrap();
            let events3 = dispatcher.event_emitter.get_events_by_work_id(&work_id3);
            let terminated3: Vec<_> = events3
                .iter()
                .filter(|e| e.event_type == "session_terminated")
                .collect();
            assert_eq!(terminated3.len(), 1);
            let payload3: serde_json::Value =
                serde_json::from_slice(&terminated3[0].payload).unwrap();
            assert_eq!(
                payload3["exit_code"], 0,
                "Unspecified outcome with 'completed_normally' reason -> exit_code 0 (legacy)"
            );
        }

        /// BLOCKER 1: `stop_with_session_context` emits `SessionTerminated`
        /// event to the ledger when episode runtime stops.
        #[tokio::test]
        async fn stop_with_session_context_emits_session_terminated() {
            let emitter = Arc::new(StubLedgerEventEmitter::new());
            let config = EpisodeRuntimeConfig {
                emit_events: true,
                ..EpisodeRuntimeConfig::default()
            };

            let runtime = EpisodeRuntime::new(config).with_ledger_emitter(emitter.clone());

            // Create and start an episode
            let episode_id = runtime.create([0u8; 32], 1_000_000_000).await.unwrap();
            let _handle = runtime
                .start_with_workspace(
                    &episode_id,
                    "lease-001",
                    1_000_001_000,
                    std::path::Path::new("/tmp"),
                )
                .await
                .unwrap();

            // Stop via stop_with_session_context (simulating runtime-driven
            // termination with session context)
            let result = runtime
                .stop_with_session_context(
                    &episode_id,
                    crate::episode::TerminationClass::Success,
                    1_000_002_000,
                    "SESS-RUNTIME-001",
                    "W-RUNTIME-001",
                    "uid:1000",
                )
                .await;
            assert!(result.is_ok(), "stop_with_session_context should succeed");

            // Verify SessionTerminated event was emitted to the ledger
            let events = emitter.get_events_by_work_id("W-RUNTIME-001");
            let terminated: Vec<_> = events
                .iter()
                .filter(|e| e.event_type == "session_terminated")
                .collect();
            assert_eq!(
                terminated.len(),
                1,
                "Expected 1 session_terminated event from runtime stop"
            );

            let payload: serde_json::Value =
                serde_json::from_slice(&terminated[0].payload).unwrap();
            assert_eq!(payload["session_id"], "SESS-RUNTIME-001");
            assert_eq!(payload["work_id"], "W-RUNTIME-001");
            assert_eq!(payload["exit_code"], 0);
        }

        /// BLOCKER 1: `EndSession` integration test where runtime termination
        /// occurs through the handler (end-to-end with session removal).
        #[test]
        fn end_session_terminates_and_cleans_up_completely() {
            let dispatcher = PrivilegedDispatcher::new();
            let ctx = ConnectionContext::privileged_session_open(Some(PeerCredentials {
                uid: 1000,
                gid: 1000,
                pid: Some(12345),
            }));

            // ClaimWork + SpawnEpisode
            let claim_request = ClaimWorkRequest {
                actor_id: "test-actor".to_string(),
                role: WorkRole::Implementer.into(),
                credential_signature: vec![1, 2, 3],
                nonce: vec![4, 5, 6],
            };
            let claim_frame = encode_claim_work_request(&claim_request);
            let claim_response = dispatcher.dispatch(&claim_frame, &ctx).unwrap();
            let (work_id, lease_id) = match claim_response {
                PrivilegedResponse::ClaimWork(resp) => (resp.work_id, resp.lease_id),
                _ => panic!("Expected ClaimWork response"),
            };

            let spawn_request = SpawnEpisodeRequest {
                workspace_root: test_workspace_root(),
                work_id: work_id.clone(),
                role: WorkRole::Implementer.into(),
                lease_id: Some(lease_id),
                adapter_profile_hash: None,
                max_episodes: None,
                escalation_predicate: None,
            };
            let spawn_frame = encode_spawn_episode_request(&spawn_request);
            let spawn_response = dispatcher.dispatch(&spawn_frame, &ctx).unwrap();
            let session_id = match spawn_response {
                PrivilegedResponse::SpawnEpisode(ref resp) => resp.session_id.clone(),
                _ => panic!("Expected SpawnEpisode response"),
            };

            // EndSession
            let end_request = EndSessionRequest {
                session_id: session_id.clone(),
                reason: "completed_normally".to_string(),
                outcome: TerminationOutcome::Success as i32,
            };
            let end_frame = encode_end_session_request(&end_request);
            let response = dispatcher.dispatch(&end_frame, &ctx).unwrap();
            match &response {
                PrivilegedResponse::EndSession(resp) => {
                    assert_eq!(resp.session_id, session_id);
                },
                other => panic!("Expected EndSession response, got: {other:?}"),
            }

            // Verify: session removed from registry
            assert!(
                dispatcher
                    .session_registry()
                    .get_session(&session_id)
                    .is_none(),
                "Session should be removed from registry"
            );

            // Verify: SessionTerminated event in ledger
            let events = dispatcher.event_emitter.get_events_by_work_id(&work_id);
            assert!(
                events.iter().any(|e| e.event_type == "session_terminated"),
                "SessionTerminated event should be in ledger"
            );

            // Quality BLOCKER 2: EndSession must NOT emit
            // WorkTransitioned(InProgress->Completed). Work completion
            // belongs to gate orchestration.
            let completed_count = events
                .iter()
                .filter(|e| e.event_type == "work_transitioned")
                .filter(|e| {
                    let p: serde_json::Value = serde_json::from_slice(&e.payload).unwrap();
                    p["to_state"] == "Completed"
                })
                .count();
            assert_eq!(
                completed_count, 0,
                "EndSession must NOT emit WorkTransitioned(Completed)"
            );

            // Verify: repeated EndSession is rejected
            let end_frame2 = encode_end_session_request(&end_request);
            let response2 = dispatcher.dispatch(&end_frame2, &ctx).unwrap();
            match response2 {
                PrivilegedResponse::Error(err) => {
                    assert!(err.message.contains("session not found"));
                },
                other => panic!("Expected error on repeated EndSession, got: {other:?}"),
            }
        }

        // ================================================================
        // TCK-00395 Security review v4 regression tests
        // ================================================================

        /// Security MAJOR 2: `EndSession` fails closed when peer credentials
        /// are missing. The handler must not emit authoritative ledger events
        /// with placeholder "unknown" actor IDs.
        #[test]
        fn end_session_rejects_missing_peer_credentials() {
            let dispatcher = PrivilegedDispatcher::new();

            // Create a session first (with valid credentials)
            let valid_ctx = ConnectionContext::privileged_session_open(Some(PeerCredentials {
                uid: 1000,
                gid: 1000,
                pid: Some(12345),
            }));

            let claim_request = ClaimWorkRequest {
                actor_id: "test-actor".to_string(),
                role: WorkRole::Implementer.into(),
                credential_signature: vec![1, 2, 3],
                nonce: vec![4, 5, 6],
            };
            let claim_frame = encode_claim_work_request(&claim_request);
            let claim_response = dispatcher.dispatch(&claim_frame, &valid_ctx).unwrap();
            let (work_id, lease_id) = match claim_response {
                PrivilegedResponse::ClaimWork(resp) => (resp.work_id, resp.lease_id),
                _ => panic!("Expected ClaimWork response"),
            };

            let spawn_request = SpawnEpisodeRequest {
                workspace_root: test_workspace_root(),
                work_id: work_id.clone(),
                role: WorkRole::Implementer.into(),
                lease_id: Some(lease_id),
                adapter_profile_hash: None,
                max_episodes: None,
                escalation_predicate: None,
            };
            let spawn_frame = encode_spawn_episode_request(&spawn_request);
            let spawn_response = dispatcher.dispatch(&spawn_frame, &valid_ctx).unwrap();
            let session_id = match spawn_response {
                PrivilegedResponse::SpawnEpisode(ref resp) => resp.session_id.clone(),
                _ => panic!("Expected SpawnEpisode response"),
            };

            // Now try EndSession WITHOUT peer credentials
            let no_creds_ctx = ConnectionContext::privileged_session_open(None);
            let end_request = EndSessionRequest {
                session_id: session_id.clone(),
                reason: "completed_normally".to_string(),
                outcome: TerminationOutcome::Success as i32,
            };
            let end_frame = encode_end_session_request(&end_request);
            let result = dispatcher.dispatch(&end_frame, &no_creds_ctx);

            // Should fail (either Err or Error response)
            match result {
                Err(e) => {
                    let msg = format!("{e}");
                    assert!(
                        msg.contains("peer credentials"),
                        "Error should mention peer credentials: {msg}"
                    );
                },
                Ok(PrivilegedResponse::Error(err)) => {
                    assert!(
                        err.message.contains("peer credentials")
                            || err.message.contains("credential"),
                        "Error should mention credentials: {}",
                        err.message
                    );
                },
                Ok(other) => panic!("Expected error for missing peer credentials, got: {other:?}"),
            }

            // Verify session is still in the registry (not cleaned up)
            assert!(
                dispatcher
                    .session_registry()
                    .get_session(&session_id)
                    .is_some(),
                "Session MUST be preserved when credentials are missing"
            );

            // Verify no session_terminated events were emitted
            let events = dispatcher.event_emitter.get_events_by_work_id(&work_id);
            let terminated_count = events
                .iter()
                .filter(|e| e.event_type == "session_terminated")
                .count();
            assert_eq!(
                terminated_count, 0,
                "No session_terminated events should be emitted without credentials"
            );
        }

        /// Security MAJOR 2: `SpawnEpisode` fails closed when peer
        /// credentials are missing.
        #[test]
        fn spawn_episode_rejects_missing_peer_credentials() {
            let dispatcher = PrivilegedDispatcher::new();

            // ClaimWork with valid credentials
            let valid_ctx = ConnectionContext::privileged_session_open(Some(PeerCredentials {
                uid: 1000,
                gid: 1000,
                pid: Some(12345),
            }));

            let claim_request = ClaimWorkRequest {
                actor_id: "test-actor".to_string(),
                role: WorkRole::Implementer.into(),
                credential_signature: vec![1, 2, 3],
                nonce: vec![4, 5, 6],
            };
            let claim_frame = encode_claim_work_request(&claim_request);
            let claim_response = dispatcher.dispatch(&claim_frame, &valid_ctx).unwrap();
            let (work_id, lease_id) = match claim_response {
                PrivilegedResponse::ClaimWork(resp) => (resp.work_id, resp.lease_id),
                _ => panic!("Expected ClaimWork response"),
            };

            // SpawnEpisode WITHOUT peer credentials
            let no_creds_ctx = ConnectionContext::privileged_session_open(None);
            let spawn_request = SpawnEpisodeRequest {
                workspace_root: test_workspace_root(),
                work_id,
                role: WorkRole::Implementer.into(),
                lease_id: Some(lease_id),
                adapter_profile_hash: None,
                max_episodes: None,
                escalation_predicate: None,
            };
            let spawn_frame = encode_spawn_episode_request(&spawn_request);
            let result = dispatcher.dispatch(&spawn_frame, &no_creds_ctx);

            // Should fail
            match result {
                Err(e) => {
                    let msg = format!("{e}");
                    assert!(
                        msg.contains("peer credentials"),
                        "Error should mention peer credentials: {msg}"
                    );
                },
                Ok(PrivilegedResponse::Error(err)) => {
                    assert!(
                        err.message.contains("peer credentials")
                            || err.message.contains("credential"),
                        "Error should mention credentials: {}",
                        err.message
                    );
                },
                Ok(other) => panic!("Expected error for missing peer credentials, got: {other:?}"),
            }
        }

        /// Security MAJOR 1: `EndSession` fails closed on malformed
        /// `episode_id`. A corrupted `episode_id` must not silently skip
        /// the runtime stop while still emitting termination facts.
        #[test]
        fn end_session_rejects_malformed_episode_id() {
            let dispatcher = PrivilegedDispatcher::new();
            let ctx = ConnectionContext::privileged_session_open(Some(PeerCredentials {
                uid: 1000,
                gid: 1000,
                pid: Some(12345),
            }));

            // Register a session directly with a malformed episode_id
            // (empty string after Some() - which is non-empty but invalid)
            let session = crate::session::SessionState {
                session_id: "SESS-MALFORMED-001".to_string(),
                work_id: "W-MALFORMED-001".to_string(),
                role: WorkRole::Implementer.into(),
                lease_id: "L-MALFORMED-001".to_string(),
                ephemeral_handle: "handle-malformed-001".to_string(),
                policy_resolved_ref: String::new(),
                capability_manifest_hash: vec![],
                // Malformed: contains forbidden '/' characters for an EpisodeId
                episode_id: Some("invalid/episode/id".to_string()),
            };

            dispatcher
                .session_registry()
                .register_session(session)
                .expect("session registration should succeed");

            let end_request = EndSessionRequest {
                session_id: "SESS-MALFORMED-001".to_string(),
                reason: "test".to_string(),
                outcome: TerminationOutcome::Success as i32,
            };
            let end_frame = encode_end_session_request(&end_request);
            let response = dispatcher.dispatch(&end_frame, &ctx).unwrap();

            match response {
                PrivilegedResponse::Error(err) => {
                    assert!(
                        err.message.contains("malformed episode_id")
                            || err.message.contains("episode"),
                        "Error should mention malformed episode_id: {}",
                        err.message
                    );
                },
                other => panic!("Expected error for malformed episode_id, got: {other:?}"),
            }

            // Session should be preserved since termination was rejected
            assert!(
                dispatcher
                    .session_registry()
                    .get_session("SESS-MALFORMED-001")
                    .is_some(),
                "Session MUST be preserved when episode_id parsing fails"
            );

            // No session_terminated events should have been emitted
            let events = dispatcher
                .event_emitter
                .get_events_by_work_id("W-MALFORMED-001");
            let terminated_count = events
                .iter()
                .filter(|e| e.event_type == "session_terminated")
                .count();
            assert_eq!(
                terminated_count, 0,
                "No termination events should be emitted when episode_id is malformed"
            );
        }

        /// BLOCKER 1 (v2): `stop_all_running` stops all running episodes
        /// and emits `SessionTerminated` events for each.
        #[tokio::test]
        async fn stop_all_running_emits_session_terminated_for_all() {
            use crate::episode::registry::InMemorySessionRegistry;

            let emitter = Arc::new(StubLedgerEventEmitter::new());
            let registry = Arc::new(InMemorySessionRegistry::new());
            let config = EpisodeRuntimeConfig {
                emit_events: true,
                ..EpisodeRuntimeConfig::default()
            };

            let runtime =
                EpisodeRuntime::new(config)
                    .with_ledger_emitter(emitter.clone())
                    .with_session_registry(
                        registry.clone() as Arc<dyn crate::session::SessionRegistry>
                    );

            // Create and start two episodes
            let ep1 = runtime.create([10u8; 32], 1_000_000_000).await.unwrap();
            let h1 = runtime
                .start_with_workspace(
                    &ep1,
                    "lease-001",
                    1_000_001_000,
                    std::path::Path::new("/tmp"),
                )
                .await
                .unwrap();
            let sid1 = h1.session_id().to_string();

            let ep2 = runtime.create([20u8; 32], 1_000_000_000).await.unwrap();
            let h2 = runtime
                .start_with_workspace(
                    &ep2,
                    "lease-002",
                    1_000_001_000,
                    std::path::Path::new("/tmp"),
                )
                .await
                .unwrap();
            let sid2 = h2.session_id().to_string();

            // Register sessions with known work IDs
            let s1 = crate::session::SessionState {
                session_id: sid1.clone(),
                work_id: "W-ALL-001".to_string(),
                role: 1,
                ephemeral_handle: "h1".to_string(),
                lease_id: "lease-001".to_string(),
                policy_resolved_ref: "pol-001".to_string(),
                capability_manifest_hash: vec![0u8; 32],
                episode_id: Some(ep1.as_str().to_string()),
            };
            registry.register_session(s1).unwrap();

            let s2 = crate::session::SessionState {
                session_id: sid2.clone(),
                work_id: "W-ALL-002".to_string(),
                role: 1,
                ephemeral_handle: "h2".to_string(),
                lease_id: "lease-002".to_string(),
                policy_resolved_ref: "pol-002".to_string(),
                capability_manifest_hash: vec![0u8; 32],
                episode_id: Some(ep2.as_str().to_string()),
            };
            registry.register_session(s2).unwrap();

            assert_eq!(runtime.active_count().await, 2);

            // Stop all running
            let stopped = runtime
                .stop_all_running(2_000_000_000, crate::episode::TerminationClass::Cancelled)
                .await;
            assert_eq!(stopped, 2, "Should have stopped 2 episodes");
            assert_eq!(runtime.active_count().await, 0);

            // Verify SessionTerminated events for both work IDs
            let ev1 = emitter.get_events_by_work_id("W-ALL-001");
            assert!(
                ev1.iter().any(|e| e.event_type == "session_terminated"),
                "W-ALL-001 should have session_terminated event"
            );
            let ev2 = emitter.get_events_by_work_id("W-ALL-002");
            assert!(
                ev2.iter().any(|e| e.event_type == "session_terminated"),
                "W-ALL-002 should have session_terminated event"
            );

            // Verify actor_id is daemon:shutdown
            let t1: Vec<_> = ev1
                .iter()
                .filter(|e| e.event_type == "session_terminated")
                .collect();
            let p1: serde_json::Value = serde_json::from_slice(&t1[0].payload).unwrap();
            assert_eq!(p1["actor_id"], "daemon:shutdown");
        }
    }

    // ========================================================================
    // TCK-00397: Spawn adapter profile hash binding tests
    // ========================================================================
    mod tck_00397_adapter_profile_binding {
        use apm2_core::evidence::{ContentAddressedStore, MemoryCas};
        use apm2_core::fac::builtin_profiles;

        use super::*;

        fn dispatcher_with_cas() -> (PrivilegedDispatcher, Arc<MemoryCas>, ConnectionContext) {
            let cas = Arc::new(MemoryCas::default());
            let dispatcher = PrivilegedDispatcher::new()
                .with_cas(Arc::clone(&cas) as Arc<dyn ContentAddressedStore>);
            let ctx = ConnectionContext::privileged_session_open(Some(PeerCredentials {
                uid: 1000,
                gid: 1000,
                pid: Some(12345),
            }));
            (dispatcher, cas, ctx)
        }

        fn claim_work(
            dispatcher: &PrivilegedDispatcher,
            ctx: &ConnectionContext,
        ) -> (String, String) {
            let claim_request = ClaimWorkRequest {
                actor_id: "adapter-profile-test".to_string(),
                role: WorkRole::Implementer.into(),
                credential_signature: vec![1, 2, 3],
                nonce: vec![4, 5, 6],
            };
            let claim_frame = encode_claim_work_request(&claim_request);
            let claim_response = dispatcher.dispatch(&claim_frame, ctx).unwrap();
            match claim_response {
                PrivilegedResponse::ClaimWork(resp) => (resp.work_id, resp.lease_id),
                other => panic!("Expected ClaimWork response, got {other:?}"),
            }
        }

        #[test]
        fn spawn_episode_rejects_missing_adapter_profile_hash_in_cas() {
            let (dispatcher, _cas, ctx) = dispatcher_with_cas();
            let (work_id, lease_id) = claim_work(&dispatcher, &ctx);

            let spawn_request = SpawnEpisodeRequest {
                workspace_root: test_workspace_root(),
                work_id: work_id.clone(),
                role: WorkRole::Implementer.into(),
                lease_id: Some(lease_id),
                adapter_profile_hash: Some(vec![0x55; 32]),
                max_episodes: None,
                escalation_predicate: None,
            };
            let spawn_frame = encode_spawn_episode_request(&spawn_request);
            let response = dispatcher.dispatch(&spawn_frame, &ctx).unwrap();

            match response {
                PrivilegedResponse::Error(err) => {
                    assert_eq!(
                        err.code,
                        PrivilegedErrorCode::CapabilityRequestRejected as i32
                    );
                    assert!(
                        err.message
                            .contains("adapter_profile_hash not found in CAS"),
                        "error should fail-closed on CAS miss: {}",
                        err.message
                    );
                },
                other => panic!("expected spawn rejection on CAS miss, got {other:?}"),
            }

            assert!(
                dispatcher
                    .session_registry()
                    .get_session_by_work_id(&work_id)
                    .is_none(),
                "CAS-miss rejection must not persist a session"
            );
        }

        #[test]
        fn spawn_episode_accepts_present_adapter_profile_hash_and_records_attribution() {
            let (dispatcher, cas, ctx) = dispatcher_with_cas();
            let (work_id, lease_id) = claim_work(&dispatcher, &ctx);

            let adapter_hash = builtin_profiles::claude_code_profile()
                .store_in_cas(cas.as_ref())
                .expect("builtin profile should store in CAS");

            let spawn_request = SpawnEpisodeRequest {
                workspace_root: test_workspace_root(),
                work_id: work_id.clone(),
                role: WorkRole::Implementer.into(),
                lease_id: Some(lease_id),
                adapter_profile_hash: Some(adapter_hash.to_vec()),
                max_episodes: None,
                escalation_predicate: None,
            };
            let spawn_frame = encode_spawn_episode_request(&spawn_request);
            let response = dispatcher.dispatch(&spawn_frame, &ctx).unwrap();
            assert!(
                matches!(response, PrivilegedResponse::SpawnEpisode(_)),
                "expected spawn success with CAS-present adapter hash"
            );

            let events = dispatcher.event_emitter.get_events_by_work_id(&work_id);
            let session_started: Vec<_> = events
                .iter()
                .filter(|e| e.event_type == "session_started")
                .collect();
            assert_eq!(
                session_started.len(),
                1,
                "expected one SessionStarted event"
            );

            let payload: serde_json::Value =
                serde_json::from_slice(&session_started[0].payload).expect("valid JSON payload");
            assert_eq!(payload["adapter_profile_hash"], hex::encode(adapter_hash));
            assert_eq!(payload["waiver_id"], "WVR-0002");
            assert_eq!(payload["role_spec_hash_absent"], true);
        }

        #[test]
        fn test_session_context_records_profile_hash() {
            let (dispatcher, cas, mut ctx) = dispatcher_with_cas();
            let (work_id, lease_id) = claim_work(&dispatcher, &ctx);

            let adapter_hash = builtin_profiles::claude_code_profile()
                .store_in_cas(cas.as_ref())
                .expect("builtin profile should store in CAS");
            let identity_profile_hash =
                crate::identity::IdentityProofProfileV1::baseline_smt_10e12()
                    .content_hash()
                    .expect("baseline identity proof profile hash should compute");
            ctx.set_identity_proof_profile_hash(identity_profile_hash)
                .expect("non-zero profile hash should be accepted");

            let spawn_request = SpawnEpisodeRequest {
                workspace_root: test_workspace_root(),
                work_id: work_id.clone(),
                role: WorkRole::Implementer.into(),
                lease_id: Some(lease_id),
                adapter_profile_hash: Some(adapter_hash.to_vec()),
                max_episodes: None,
                escalation_predicate: None,
            };
            let spawn_frame = encode_spawn_episode_request(&spawn_request);
            let response = dispatcher.dispatch(&spawn_frame, &ctx).unwrap();
            assert!(
                matches!(response, PrivilegedResponse::SpawnEpisode(_)),
                "expected spawn success with context-bound identity profile hash"
            );

            let events = dispatcher.event_emitter.get_events_by_work_id(&work_id);
            let session_started: Vec<_> = events
                .iter()
                .filter(|e| e.event_type == "session_started")
                .collect();
            assert_eq!(
                session_started.len(),
                1,
                "expected one SessionStarted event"
            );

            let payload: serde_json::Value =
                serde_json::from_slice(&session_started[0].payload).expect("valid JSON payload");
            assert_eq!(
                payload["identity_proof_profile_hash"],
                hex::encode(identity_profile_hash)
            );
        }

        /// TCK-00358: Verify that the production spawn path (without explicit
        /// `set_identity_proof_profile_hash`) emits the baseline identity
        /// proof profile hash in the `SessionStarted` payload. This exercises
        /// the fallback to `IdentityProofProfileV1::baseline_smt_10e12()` and
        /// proves REQ-0012 is met end-to-end without test-only injection.
        #[test]
        fn test_production_spawn_emits_baseline_identity_proof_profile_hash() {
            let (dispatcher, cas, ctx) = dispatcher_with_cas();
            let (work_id, lease_id) = claim_work(&dispatcher, &ctx);

            // No call to ctx.set_identity_proof_profile_hash() — simulates
            // production where the connection context has no pre-set value.
            assert!(
                ctx.identity_proof_profile_hash().is_none(),
                "precondition: ctx should not have identity_proof_profile_hash set"
            );

            let adapter_hash = builtin_profiles::claude_code_profile()
                .store_in_cas(cas.as_ref())
                .expect("builtin profile should store in CAS");

            let spawn_request = SpawnEpisodeRequest {
                workspace_root: test_workspace_root(),
                work_id: work_id.clone(),
                role: WorkRole::Implementer.into(),
                lease_id: Some(lease_id),
                adapter_profile_hash: Some(adapter_hash.to_vec()),
                max_episodes: None,
                escalation_predicate: None,
            };
            let spawn_frame = encode_spawn_episode_request(&spawn_request);
            let response = dispatcher.dispatch(&spawn_frame, &ctx).unwrap();
            assert!(
                matches!(response, PrivilegedResponse::SpawnEpisode(_)),
                "expected spawn success without explicit identity proof profile hash"
            );

            let events = dispatcher.event_emitter.get_events_by_work_id(&work_id);
            let session_started: Vec<_> = events
                .iter()
                .filter(|e| e.event_type == "session_started")
                .collect();
            assert_eq!(
                session_started.len(),
                1,
                "expected one SessionStarted event"
            );

            let payload: serde_json::Value =
                serde_json::from_slice(&session_started[0].payload).expect("valid JSON payload");

            // The baseline identity proof profile hash should be present even
            // though we never called set_identity_proof_profile_hash on ctx.
            let expected_hash = crate::identity::IdentityProofProfileV1::baseline_smt_10e12()
                .content_hash()
                .expect("baseline identity proof profile hash should compute");
            assert_eq!(
                payload["identity_proof_profile_hash"],
                hex::encode(expected_hash),
                "production spawn must emit baseline identity_proof_profile_hash"
            );
        }

        /// TCK-00358 Round 2: Verify that the production session-open path
        /// wires the identity proof profile hash into `ConnectionContext` and
        /// that `SessionStarted` carries that hash rather than the spawn-time
        /// baseline fallback.
        ///
        /// This test uses a **non-baseline** profile hash to prove the value
        /// flows from the session-open wiring
        /// (`set_identity_proof_profile_hash`) rather than the
        /// spawn-time fallback to `baseline_smt_10e12()`.
        #[test]
        fn test_session_open_wired_profile_hash_overrides_spawn_fallback() {
            let (dispatcher, cas, mut ctx) = dispatcher_with_cas();
            let (work_id, lease_id) = claim_work(&dispatcher, &ctx);

            // Construct a non-baseline profile by tweaking max_proof_bytes.
            // This produces a distinct hash from the baseline profile,
            // proving the emitted hash comes from session-open wiring.
            let mut custom_profile = crate::identity::IdentityProofProfileV1::baseline_smt_10e12();
            custom_profile.max_proof_bytes = 4096; // differs from baseline 8192
            let custom_hash = custom_profile
                .content_hash()
                .expect("custom profile hash should compute");

            // Sanity check: the custom hash differs from the baseline.
            let baseline_hash = crate::identity::IdentityProofProfileV1::baseline_smt_10e12()
                .content_hash()
                .expect("baseline hash should compute");
            assert_ne!(
                custom_hash, baseline_hash,
                "precondition: custom profile hash must differ from baseline"
            );

            // Simulate the production session-open wiring in main.rs:
            // set_identity_proof_profile_hash is called after handshake,
            // before entering the dispatch loop.
            ctx.set_identity_proof_profile_hash(custom_hash)
                .expect("non-zero custom hash should be accepted");

            let adapter_hash = builtin_profiles::claude_code_profile()
                .store_in_cas(cas.as_ref())
                .expect("builtin profile should store in CAS");

            let spawn_request = SpawnEpisodeRequest {
                workspace_root: test_workspace_root(),
                work_id: work_id.clone(),
                role: WorkRole::Implementer.into(),
                lease_id: Some(lease_id),
                adapter_profile_hash: Some(adapter_hash.to_vec()),
                max_episodes: None,
                escalation_predicate: None,
            };
            let spawn_frame = encode_spawn_episode_request(&spawn_request);
            let response = dispatcher.dispatch(&spawn_frame, &ctx).unwrap();
            assert!(
                matches!(response, PrivilegedResponse::SpawnEpisode(_)),
                "expected spawn success with session-open wired profile hash"
            );

            let events = dispatcher.event_emitter.get_events_by_work_id(&work_id);
            let session_started: Vec<_> = events
                .iter()
                .filter(|e| e.event_type == "session_started")
                .collect();
            assert_eq!(
                session_started.len(),
                1,
                "expected one SessionStarted event"
            );

            let payload: serde_json::Value =
                serde_json::from_slice(&session_started[0].payload).expect("valid JSON payload");

            // The emitted hash must be the custom (non-baseline) hash set
            // at session-open time, NOT the baseline fallback.
            assert_eq!(
                payload["identity_proof_profile_hash"],
                hex::encode(custom_hash),
                "SessionStarted must carry the session-open wired profile hash, \
                 not the spawn-time baseline fallback"
            );
            assert_ne!(
                payload["identity_proof_profile_hash"],
                hex::encode(baseline_hash),
                "SessionStarted must NOT carry the baseline fallback hash \
                 when session-open wiring provides a different profile"
            );
        }
    }

    // ========================================================================
    // TCK-00394: PublishChangeSet IPC endpoint tests
    // ========================================================================
    mod publish_changeset {
        use apm2_core::evidence::MemoryCas;
        use apm2_core::fac::{ChangeKind, ChangeSetBundleV1, FileChange, GitObjectRef, HashAlgo};

        use super::*;

        fn make_valid_bundle(changeset_id: &str) -> ChangeSetBundleV1 {
            ChangeSetBundleV1::builder()
                .changeset_id(changeset_id)
                .base(GitObjectRef {
                    algo: HashAlgo::Sha1,
                    object_kind: "commit".to_string(),
                    object_id: "a".repeat(40),
                })
                .diff_hash([0x42; 32])
                .file_manifest(vec![FileChange {
                    path: "src/main.rs".to_string(),
                    change_kind: ChangeKind::Modify,
                    old_path: None,
                }])
                .binary_detected(false)
                .build()
                .expect("bundle should build")
        }

        /// Helper to create a valid `ChangeSetBundleV1` JSON payload.
        fn make_bundle_json(changeset_id: &str) -> Vec<u8> {
            serde_json::to_vec(&make_valid_bundle(changeset_id)).unwrap()
        }

        /// Helper to create a semantically equivalent payload with different
        /// JSON formatting and key order.
        fn make_noncanonical_bundle_json(changeset_id: &str) -> Vec<u8> {
            let bundle = make_valid_bundle(changeset_id);
            let noncanonical = serde_json::json!({
                "binary_detected": bundle.binary_detected,
                "file_manifest": [{
                    "change_kind": "MODIFY",
                    "path": "src/main.rs"
                }],
                "diff_hash": hex::encode(bundle.diff_hash),
                "diff_format": bundle.diff_format,
                "changeset_digest": hex::encode(bundle.changeset_digest),
                "base": {
                    "object_id": bundle.base.object_id,
                    "object_kind": bundle.base.object_kind,
                    "algo": "sha1"
                },
                "changeset_id": bundle.changeset_id,
                "schema_version": bundle.schema_version,
                "schema": bundle.schema
            });
            serde_json::to_vec_pretty(&noncanonical).unwrap()
        }

        /// Helper to build a dispatcher with CAS and a privileged context.
        fn make_dispatcher_with_cas() -> (PrivilegedDispatcher, Arc<MemoryCas>) {
            let cas = Arc::new(MemoryCas::default());
            let dispatcher = PrivilegedDispatcher::new()
                .with_cas(Arc::clone(&cas) as Arc<dyn ContentAddressedStore>);
            (dispatcher, cas)
        }

        fn privileged_ctx() -> ConnectionContext {
            ConnectionContext::privileged_session_open(Some(PeerCredentials {
                uid: 1000,
                gid: 1000,
                pid: Some(12345),
            }))
        }

        /// Helper to first claim work so we have a registered `work_id`.
        fn claim_work(dispatcher: &PrivilegedDispatcher, ctx: &ConnectionContext) -> String {
            let request = ClaimWorkRequest {
                actor_id: "test:actor".to_string(),
                role: WorkRole::Implementer.into(),
                credential_signature: vec![1, 2, 3],
                nonce: vec![4, 5, 6],
            };
            let frame = encode_claim_work_request(&request);
            let response = dispatcher.dispatch(&frame, ctx).unwrap();
            match response {
                PrivilegedResponse::ClaimWork(resp) => resp.work_id,
                other => panic!("Expected ClaimWork response, got: {other:?}"),
            }
        }

        #[test]
        fn test_publish_changeset_routing() {
            let (dispatcher, _cas) = make_dispatcher_with_cas();
            let ctx = privileged_ctx();

            // First claim work to get a valid work_id
            let work_id = claim_work(&dispatcher, &ctx);

            let bundle_bytes = make_bundle_json("cs-001");
            let request = PublishChangeSetRequest {
                work_id,
                bundle_bytes,
            };
            let frame = encode_publish_changeset_request(&request);

            let response = dispatcher.dispatch(&frame, &ctx).unwrap();
            assert!(
                matches!(response, PrivilegedResponse::PublishChangeSet(_)),
                "Expected PublishChangeSet response, got: {response:?}"
            );
        }

        #[test]
        fn test_publish_changeset_returns_digest_and_cas_hash() {
            let (dispatcher, cas) = make_dispatcher_with_cas();
            let ctx = privileged_ctx();
            let work_id = claim_work(&dispatcher, &ctx);

            let bundle_bytes = make_bundle_json("cs-002");
            let request = PublishChangeSetRequest {
                work_id: work_id.clone(),
                bundle_bytes: bundle_bytes.clone(),
            };
            let frame = encode_publish_changeset_request(&request);

            let response = dispatcher.dispatch(&frame, &ctx).unwrap();
            match response {
                PrivilegedResponse::PublishChangeSet(resp) => {
                    // changeset_digest should be a 64-char hex string
                    assert_eq!(
                        resp.changeset_digest.len(),
                        64,
                        "changeset_digest should be 64 hex chars"
                    );
                    // cas_hash should be a 64-char hex string
                    assert_eq!(resp.cas_hash.len(), 64, "cas_hash should be 64 hex chars");
                    // work_id should match
                    assert_eq!(resp.work_id, work_id);
                    // event_id should be non-empty
                    assert!(!resp.event_id.is_empty(), "event_id should be non-empty");

                    // Verify the bundle was stored in CAS
                    let hash_bytes: [u8; 32] =
                        hex::decode(&resp.cas_hash).unwrap().try_into().unwrap();
                    assert!(
                        cas.exists(&hash_bytes).unwrap(),
                        "Bundle should exist in CAS"
                    );

                    // Verify CAS content matches bundle bytes
                    let stored = cas.retrieve(&hash_bytes).unwrap();
                    assert_eq!(stored, bundle_bytes, "CAS content should match input");
                },
                other => panic!("Expected PublishChangeSet response, got: {other:?}"),
            }
        }

        #[test]
        fn test_publish_changeset_emits_ledger_event() {
            let (dispatcher, _cas) = make_dispatcher_with_cas();
            let ctx = privileged_ctx();
            let work_id = claim_work(&dispatcher, &ctx);

            let bundle_bytes = make_bundle_json("cs-003");
            let request = PublishChangeSetRequest {
                work_id: work_id.clone(),
                bundle_bytes,
            };
            let frame = encode_publish_changeset_request(&request);
            let response = dispatcher.dispatch(&frame, &ctx).unwrap();

            let event_id = match &response {
                PrivilegedResponse::PublishChangeSet(resp) => resp.event_id.clone(),
                other => panic!("Expected PublishChangeSet response, got: {other:?}"),
            };

            // Query ledger for the emitted event
            let events = dispatcher.event_emitter.get_events_by_work_id(&work_id);
            let changeset_events: Vec<_> = events
                .iter()
                .filter(|e| e.event_type == "changeset_published")
                .collect();
            assert_eq!(
                changeset_events.len(),
                1,
                "Should have exactly one changeset_published event"
            );
            assert_eq!(changeset_events[0].event_id, event_id);

            // Verify event payload contains expected fields
            let payload: serde_json::Value =
                serde_json::from_slice(&changeset_events[0].payload).unwrap();
            assert_eq!(payload["event_type"], "changeset_published");
            assert_eq!(payload["work_id"], work_id);
            assert!(payload["changeset_digest"].is_string());
            assert!(payload["cas_hash"].is_string());
            assert!(payload["actor_id"].is_string());
            // Binding test evidence: timestamp_ns must be > 0
            assert!(
                payload["timestamp_ns"].as_u64().unwrap() > 0,
                "timestamp_ns must be > 0"
            );
        }

        #[test]
        fn test_publish_changeset_idempotent() {
            let (dispatcher, _cas) = make_dispatcher_with_cas();
            let ctx = privileged_ctx();
            let work_id = claim_work(&dispatcher, &ctx);

            let bundle_bytes = make_bundle_json("cs-004");

            // First publish
            let request1 = PublishChangeSetRequest {
                work_id: work_id.clone(),
                bundle_bytes: bundle_bytes.clone(),
            };
            let frame1 = encode_publish_changeset_request(&request1);
            let response1 = dispatcher.dispatch(&frame1, &ctx).unwrap();
            let (digest1, event_id1) = match &response1 {
                PrivilegedResponse::PublishChangeSet(resp) => {
                    (resp.changeset_digest.clone(), resp.event_id.clone())
                },
                other => panic!("Expected PublishChangeSet response, got: {other:?}"),
            };

            // Second publish with same bundle
            let request2 = PublishChangeSetRequest {
                work_id: work_id.clone(),
                bundle_bytes,
            };
            let frame2 = encode_publish_changeset_request(&request2);
            let response2 = dispatcher.dispatch(&frame2, &ctx).unwrap();
            let (digest2, event_id2) = match &response2 {
                PrivilegedResponse::PublishChangeSet(resp) => {
                    (resp.changeset_digest.clone(), resp.event_id.clone())
                },
                other => panic!("Expected PublishChangeSet response, got: {other:?}"),
            };

            // Should return same digest
            assert_eq!(digest1, digest2, "Idempotent: digest should match");
            // Should return same event_id (no duplicate events)
            assert_eq!(
                event_id1, event_id2,
                "Idempotent: should return same event_id"
            );

            // Only one changeset_published event should exist
            let events = dispatcher.event_emitter.get_events_by_work_id(&work_id);
            let changeset_count = events
                .iter()
                .filter(|e| e.event_type == "changeset_published")
                .count();
            assert_eq!(
                changeset_count, 1,
                "Idempotent: only one changeset_published event"
            );
        }

        #[test]
        fn test_publish_changeset_semantic_idempotent_noncanonical_json() {
            let (dispatcher, _cas) = make_dispatcher_with_cas();
            let ctx = privileged_ctx();
            let work_id = claim_work(&dispatcher, &ctx);

            let canonical_bundle = make_bundle_json("cs-semantic-idempotent");
            let noncanonical_bundle = make_noncanonical_bundle_json("cs-semantic-idempotent");

            let response1 = dispatcher
                .dispatch(
                    &encode_publish_changeset_request(&PublishChangeSetRequest {
                        work_id: work_id.clone(),
                        bundle_bytes: canonical_bundle,
                    }),
                    &ctx,
                )
                .unwrap();
            let (digest1, cas_hash1, event_id1) = match response1 {
                PrivilegedResponse::PublishChangeSet(resp) => {
                    (resp.changeset_digest, resp.cas_hash, resp.event_id)
                },
                other => panic!("Expected first PublishChangeSet response, got: {other:?}"),
            };

            let response2 = dispatcher
                .dispatch(
                    &encode_publish_changeset_request(&PublishChangeSetRequest {
                        work_id: work_id.clone(),
                        bundle_bytes: noncanonical_bundle,
                    }),
                    &ctx,
                )
                .unwrap();
            let (digest2, cas_hash2, event_id2) = match response2 {
                PrivilegedResponse::PublishChangeSet(resp) => {
                    (resp.changeset_digest, resp.cas_hash, resp.event_id)
                },
                other => panic!("Expected second PublishChangeSet response, got: {other:?}"),
            };

            assert_eq!(digest1, digest2, "semantic duplicate must reuse digest");
            assert_eq!(
                event_id1, event_id2,
                "semantic duplicate must replay event_id"
            );
            assert_eq!(
                cas_hash1, cas_hash2,
                "semantic duplicate must replay bound CAS hash"
            );

            let events = dispatcher.event_emitter.get_events_by_work_id(&work_id);
            let changeset_count = events
                .iter()
                .filter(|e| e.event_type == "changeset_published")
                .count();
            assert_eq!(
                changeset_count, 1,
                "semantic duplicate must not create duplicate changeset events"
            );
        }

        #[test]
        fn test_publish_changeset_rejects_digest_mismatch() {
            let (dispatcher, _cas) = make_dispatcher_with_cas();
            let ctx = privileged_ctx();
            let work_id = claim_work(&dispatcher, &ctx);

            let mut bundle = make_valid_bundle("cs-digest-mismatch");
            bundle.changeset_digest = [0xAB; 32];
            let request = PublishChangeSetRequest {
                work_id,
                bundle_bytes: serde_json::to_vec(&bundle).unwrap(),
            };

            let response = dispatcher
                .dispatch(&encode_publish_changeset_request(&request), &ctx)
                .unwrap();
            match response {
                PrivilegedResponse::Error(err) => {
                    assert_eq!(
                        err.code,
                        PrivilegedErrorCode::CapabilityRequestRejected as i32
                    );
                    assert!(
                        err.message.contains("changeset_digest mismatch"),
                        "Expected digest mismatch rejection, got: {}",
                        err.message
                    );
                },
                other => panic!("Expected digest mismatch rejection, got: {other:?}"),
            }
        }

        #[test]
        fn test_publish_changeset_rejects_unknown_work_id() {
            let (dispatcher, _cas) = make_dispatcher_with_cas();
            let ctx = privileged_ctx();

            let bundle_bytes = make_bundle_json("cs-005");
            let request = PublishChangeSetRequest {
                work_id: "W-nonexistent-001".to_string(),
                bundle_bytes,
            };
            let frame = encode_publish_changeset_request(&request);
            let response = dispatcher.dispatch(&frame, &ctx).unwrap();

            assert!(
                matches!(response, PrivilegedResponse::Error(_)),
                "Should reject unknown work_id"
            );
        }

        #[test]
        fn test_publish_changeset_rejects_empty_work_id() {
            let (dispatcher, _cas) = make_dispatcher_with_cas();
            let ctx = privileged_ctx();

            let request = PublishChangeSetRequest {
                work_id: String::new(),
                bundle_bytes: make_bundle_json("cs-006"),
            };
            let frame = encode_publish_changeset_request(&request);
            let response = dispatcher.dispatch(&frame, &ctx).unwrap();

            assert!(
                matches!(response, PrivilegedResponse::Error(_)),
                "Should reject empty work_id"
            );
        }

        #[test]
        fn test_publish_changeset_rejects_empty_bundle() {
            let (dispatcher, _cas) = make_dispatcher_with_cas();
            let ctx = privileged_ctx();
            let work_id = claim_work(&dispatcher, &ctx);

            let request = PublishChangeSetRequest {
                work_id,
                bundle_bytes: vec![],
            };
            let frame = encode_publish_changeset_request(&request);
            let response = dispatcher.dispatch(&frame, &ctx).unwrap();

            assert!(
                matches!(response, PrivilegedResponse::Error(_)),
                "Should reject empty bundle_bytes"
            );
        }

        #[test]
        fn test_publish_changeset_rejects_invalid_json() {
            let (dispatcher, _cas) = make_dispatcher_with_cas();
            let ctx = privileged_ctx();
            let work_id = claim_work(&dispatcher, &ctx);

            let request = PublishChangeSetRequest {
                work_id,
                bundle_bytes: b"not valid json".to_vec(),
            };
            let frame = encode_publish_changeset_request(&request);
            let response = dispatcher.dispatch(&frame, &ctx).unwrap();

            match response {
                PrivilegedResponse::Error(err) => {
                    assert_eq!(
                        err.code,
                        PrivilegedErrorCode::CapabilityRequestRejected as i32
                    );
                    assert!(
                        err.message.contains("invalid ChangeSetBundleV1 JSON"),
                        "Expected invalid JSON rejection, got: {}",
                        err.message
                    );
                },
                other => panic!("Expected invalid JSON error, got: {other:?}"),
            }
        }

        #[test]
        fn test_publish_changeset_rejects_empty_file_manifest() {
            let (dispatcher, _cas) = make_dispatcher_with_cas();
            let ctx = privileged_ctx();
            let work_id = claim_work(&dispatcher, &ctx);

            let bundle = ChangeSetBundleV1::builder()
                .changeset_id("cs-empty-manifest")
                .base(GitObjectRef {
                    algo: HashAlgo::Sha1,
                    object_kind: "commit".to_string(),
                    object_id: "a".repeat(40),
                })
                .diff_hash([0x42; 32])
                .file_manifest(vec![])
                .binary_detected(false)
                .build()
                .expect("bundle should build");
            let bundle_bytes = serde_json::to_vec(&bundle).unwrap();

            let request = PublishChangeSetRequest {
                work_id,
                bundle_bytes,
            };
            let frame = encode_publish_changeset_request(&request);
            let response = dispatcher.dispatch(&frame, &ctx).unwrap();

            match response {
                PrivilegedResponse::Error(err) => {
                    assert_eq!(
                        err.code,
                        PrivilegedErrorCode::CapabilityRequestRejected as i32
                    );
                    assert!(
                        err.message.contains("file_manifest"),
                        "Expected bundle validation rejection, got: {}",
                        err.message
                    );
                },
                other => panic!("Expected file_manifest rejection, got: {other:?}"),
            }
        }

        #[test]
        fn test_publish_changeset_rejects_no_cas() {
            // Dispatcher WITHOUT CAS
            let dispatcher = PrivilegedDispatcher::new();
            let ctx = privileged_ctx();
            let work_id = claim_work(&dispatcher, &ctx);

            let request = PublishChangeSetRequest {
                work_id,
                bundle_bytes: make_bundle_json("cs-no-cas"),
            };
            let frame = encode_publish_changeset_request(&request);
            let response = dispatcher.dispatch(&frame, &ctx).unwrap();

            assert!(
                matches!(response, PrivilegedResponse::Error(_)),
                "Should reject when CAS not configured"
            );
        }

        #[test]
        fn test_publish_changeset_permission_denied_for_non_privileged() {
            let (dispatcher, _cas) = make_dispatcher_with_cas();
            let ctx = ConnectionContext::session_open(
                Some(PeerCredentials {
                    uid: 1000,
                    gid: 1000,
                    pid: Some(12345),
                }),
                Some("session-test-001".to_string()),
            );

            let request = PublishChangeSetRequest {
                work_id: "W-test-001".to_string(),
                bundle_bytes: make_bundle_json("cs-perm"),
            };
            let frame = encode_publish_changeset_request(&request);
            let response = dispatcher.dispatch(&frame, &ctx).unwrap();

            assert!(
                matches!(response, PrivilegedResponse::Error(_)),
                "Should reject non-privileged connection"
            );
        }

        #[test]
        fn test_publish_changeset_tag_70_routing() {
            // Verify tag 70 routes correctly
            assert_eq!(
                PrivilegedMessageType::PublishChangeSet.tag(),
                70,
                "PublishChangeSet tag should be 70"
            );
            assert_eq!(
                PrivilegedMessageType::from_tag(70),
                Some(PrivilegedMessageType::PublishChangeSet),
                "Tag 70 should map to PublishChangeSet"
            );
        }

        /// TCK-00408: Verify actor ownership check rejects non-owner callers.
        ///
        /// A caller whose peer credentials produce a different `actor_id` than
        /// the work claim owner must be denied.
        #[test]
        fn test_publish_changeset_rejects_ownership_mismatch() {
            let (dispatcher, cas) = make_dispatcher_with_cas();
            let owner_ctx = privileged_ctx(); // uid=1000, gid=1000

            // Claim work as the owner
            let work_id = claim_work(&dispatcher, &owner_ctx);

            // Create a different caller context (different uid -> different actor_id)
            let non_owner_ctx = ConnectionContext::privileged_session_open(Some(PeerCredentials {
                uid: 9999,
                gid: 9999,
                pid: Some(99999),
            }));

            let bundle_bytes = make_bundle_json("cs-ownership-test");
            let expected_hash = *blake3::hash(&bundle_bytes).as_bytes();
            let request = PublishChangeSetRequest {
                work_id: work_id.clone(),
                bundle_bytes,
            };
            let frame = encode_publish_changeset_request(&request);
            let response = dispatcher.dispatch(&frame, &non_owner_ctx).unwrap();

            match &response {
                PrivilegedResponse::Error(err) => {
                    assert_eq!(
                        err.code,
                        PrivilegedErrorCode::PermissionDenied as i32,
                        "Ownership mismatch should return PermissionDenied"
                    );
                    assert!(
                        err.message.contains("does not own work claim"),
                        "Error message should mention ownership: {}",
                        err.message
                    );
                },
                other => {
                    panic!("Expected PermissionDenied error for ownership mismatch, got: {other:?}")
                },
            }

            // Ownership rejection must occur before CAS mutation or event emission.
            assert!(
                !cas.exists(&expected_hash)
                    .expect("CAS exists check should succeed"),
                "ownership mismatch must not write bundle to CAS"
            );
            let changeset_events = dispatcher
                .event_emitter
                .get_events_by_work_id(&work_id)
                .into_iter()
                .filter(|event| event.event_type == "changeset_published")
                .count();
            assert_eq!(
                changeset_events, 0,
                "ownership mismatch must not emit changeset_published events"
            );
        }
    }

    // ========================================================================
    // TCK-00389: IngestReviewReceipt Tests
    // ========================================================================
    mod ingest_review_receipt {
        use apm2_core::evidence::{ContentAddressedStore, MemoryCas};

        use super::*;

        /// Standard peer credentials used by all `IngestReviewReceipt` tests.
        fn test_peer_credentials() -> PeerCredentials {
            PeerCredentials {
                uid: 1000,
                gid: 1000,
                pid: Some(12345),
            }
        }

        /// Standard test artifact bundle content stored in CAS during setup.
        const TEST_ARTIFACT_CONTENT: &[u8] = b"test-artifact-bundle-content";

        /// Returns the CAS hash of `TEST_ARTIFACT_CONTENT`. Use this as
        /// `artifact_bundle_hash` in test requests that must pass CAS
        /// existence validation.
        fn test_artifact_bundle_hash() -> Vec<u8> {
            let cas = MemoryCas::default();
            let result = cas.store(TEST_ARTIFACT_CONTENT).unwrap();
            result.hash.to_vec()
        }

        /// Creates a dispatcher with CAS, a registered lease, and Tier0 work
        /// claim for testing. CAS is pre-populated with the standard test
        /// artifact bundle so that CAS existence checks pass on success paths.
        ///
        /// SECURITY (v6 Finding 1): The `executor_actor_id` is derived from the
        /// test peer credentials, ensuring the authenticated caller identity
        /// matches the lease executor. The `_executor_hint` parameter is
        /// ignored — it exists only for backward compatibility with test
        /// call sites that previously passed an arbitrary reviewer name.
        fn setup_dispatcher_with_lease(
            lease_id: &str,
            work_id: &str,
            gate_id: &str,
            _executor_hint: &str,
        ) -> (PrivilegedDispatcher, ConnectionContext) {
            setup_dispatcher_with_lease_and_tier(lease_id, work_id, gate_id, "", 0)
        }

        /// Creates a dispatcher with CAS, a registered lease, and a work claim
        /// at the specified risk tier. Used to test attestation ratcheting.
        ///
        /// TCK-00408: CAS is now mandatory for ingest (fail-closed). The
        /// standard test artifact bundle is pre-stored in CAS.
        ///
        /// SECURITY (v6 Finding 1): Registers the lease executor as the
        /// identity derived from `test_peer_credentials()`, binding the test
        /// lease to the same identity the handler will derive from the
        /// `ConnectionContext`.
        fn setup_dispatcher_with_lease_and_tier(
            lease_id: &str,
            work_id: &str,
            gate_id: &str,
            _executor_hint: &str,
            risk_tier: u8,
        ) -> (PrivilegedDispatcher, ConnectionContext) {
            let peer_creds = test_peer_credentials();
            let executor_actor_id = derive_actor_id(&peer_creds);

            // TCK-00408: CAS is mandatory for ingest (fail-closed).
            let cas = Arc::new(MemoryCas::default());
            cas.store(TEST_ARTIFACT_CONTENT).unwrap();

            let dispatcher = PrivilegedDispatcher::new()
                .with_cas(Arc::clone(&cas) as Arc<dyn ContentAddressedStore>);
            dispatcher.lease_validator.register_lease_with_executor(
                lease_id,
                work_id,
                gate_id,
                &executor_actor_id,
            );
            // Register work claim so risk-tier resolution succeeds
            let claim = WorkClaim {
                work_id: work_id.to_string(),
                lease_id: lease_id.to_string(),
                actor_id: executor_actor_id,
                role: WorkRole::Reviewer,
                policy_resolution: PolicyResolution {
                    policy_resolved_ref: format!("PolicyResolvedForChangeSet:{work_id}"),
                    resolved_policy_hash: [0u8; 32],
                    capability_manifest_hash: [0u8; 32],
                    context_pack_hash: [0u8; 32],
                    resolved_risk_tier: risk_tier,
                    resolved_scope_baseline: None,
                    expected_adapter_profile_hash: None,
                },
                executor_custody_domains: vec![],
                author_custody_domains: vec![],
            };
            dispatcher.work_registry.register_claim(claim).unwrap();
            let ctx = ConnectionContext::privileged_session_open(Some(peer_creds));
            (dispatcher, ctx)
        }

        #[test]
        fn test_ingest_review_receipt_approve_success() {
            let (dispatcher, ctx) =
                setup_dispatcher_with_lease("lease-001", "W-001", "gate-001", "reviewer-alpha");

            let request = IngestReviewReceiptRequest {
                lease_id: "lease-001".to_string(),
                receipt_id: "RR-001".to_string(),
                reviewer_actor_id: "reviewer-alpha".to_string(),
                changeset_digest: vec![0x42; 32],
                artifact_bundle_hash: test_artifact_bundle_hash(),
                verdict: ReviewReceiptVerdict::Approve.into(),
                blocked_reason_code: 0,
                blocked_log_hash: vec![],
                identity_proof_hash: vec![0x99; 32],
            };
            let frame = encode_ingest_review_receipt_request(&request);

            let response = dispatcher.dispatch(&frame, &ctx).unwrap();
            match response {
                PrivilegedResponse::IngestReviewReceipt(resp) => {
                    assert_eq!(resp.receipt_id, "RR-001");
                    assert_eq!(resp.event_type, "ReviewReceiptRecorded");
                    assert!(!resp.event_id.is_empty(), "event_id must be non-empty");
                },
                PrivilegedResponse::Error(err) => {
                    panic!("Expected IngestReviewReceipt, got error: {}", err.message);
                },
                other => panic!("Expected IngestReviewReceipt, got {other:?}"),
            }
        }

        #[test]
        fn test_ingest_review_receipt_blocked_success() {
            let (dispatcher, ctx) =
                setup_dispatcher_with_lease("lease-002", "W-002", "gate-002", "reviewer-beta");

            let request = IngestReviewReceiptRequest {
                lease_id: "lease-002".to_string(),
                receipt_id: "RB-001".to_string(),
                reviewer_actor_id: "reviewer-beta".to_string(),
                changeset_digest: vec![0x42; 32],
                artifact_bundle_hash: test_artifact_bundle_hash(),
                verdict: ReviewReceiptVerdict::Blocked.into(),
                blocked_reason_code: 1, // APPLY_FAILED
                blocked_log_hash: vec![0x55; 32],
                identity_proof_hash: vec![0x99; 32],
            };
            let frame = encode_ingest_review_receipt_request(&request);

            let response = dispatcher.dispatch(&frame, &ctx).unwrap();
            match response {
                PrivilegedResponse::IngestReviewReceipt(resp) => {
                    assert_eq!(resp.receipt_id, "RB-001");
                    assert_eq!(resp.event_type, "ReviewBlockedRecorded");
                    assert!(!resp.event_id.is_empty(), "event_id must be non-empty");
                },
                PrivilegedResponse::Error(err) => {
                    panic!("Expected IngestReviewReceipt, got error: {}", err.message);
                },
                other => panic!("Expected IngestReviewReceipt, got {other:?}"),
            }
        }

        /// v6 Finding 1: Reviewer identity is now derived from peer
        /// credentials, not from the request's `reviewer_actor_id`. To
        /// trigger a mismatch, the caller must have DIFFERENT peer
        /// credentials than the lease executor.
        #[test]
        fn test_ingest_review_receipt_reviewer_identity_mismatch() {
            let (dispatcher, _ctx) =
                setup_dispatcher_with_lease("lease-003", "W-003", "gate-003", "reviewer-gamma");

            // Create a context with DIFFERENT peer credentials (uid=9999)
            // so the authenticated identity won't match the lease executor.
            let wrong_ctx = ConnectionContext::privileged_session_open(Some(PeerCredentials {
                uid: 9999,
                gid: 9999,
                pid: Some(99999),
            }));

            let request = IngestReviewReceiptRequest {
                lease_id: "lease-003".to_string(),
                receipt_id: "RR-002".to_string(),
                reviewer_actor_id: "does-not-matter".to_string(), // Ignored by handler
                changeset_digest: vec![0x42; 32],
                artifact_bundle_hash: test_artifact_bundle_hash(),
                verdict: ReviewReceiptVerdict::Approve.into(),
                blocked_reason_code: 0,
                blocked_log_hash: vec![],
                identity_proof_hash: vec![0x99; 32],
            };
            let frame = encode_ingest_review_receipt_request(&request);

            let response = dispatcher.dispatch(&frame, &wrong_ctx).unwrap();
            match response {
                PrivilegedResponse::Error(err) => {
                    assert!(
                        err.message.contains("does not match gate lease executor"),
                        "Expected reviewer identity mismatch error, got: {}",
                        err.message
                    );
                },
                other => panic!("Expected error, got {other:?}"),
            }
        }

        /// v6 Finding 1 negative test: caller with wrong peer credentials is
        /// rejected even if they supply the correct `reviewer_actor_id` in the
        /// request payload. The handler must ignore the request field and
        /// use the authenticated identity.
        #[test]
        fn test_ingest_review_receipt_wrong_peer_creds_correct_request_field_rejected() {
            // Register lease with the derived identity from standard test creds
            let (dispatcher, _ctx) =
                setup_dispatcher_with_lease("lease-neg", "W-NEG", "gate-neg", "unused");

            // The correct executor_actor_id as registered on the lease
            let correct_actor_id = derive_actor_id(&test_peer_credentials());

            // Caller supplies the correct reviewer_actor_id in the request,
            // but connects with DIFFERENT peer credentials.
            let wrong_ctx = ConnectionContext::privileged_session_open(Some(PeerCredentials {
                uid: 7777,
                gid: 7777,
                pid: Some(77777),
            }));

            let request = IngestReviewReceiptRequest {
                lease_id: "lease-neg".to_string(),
                receipt_id: "RR-NEG".to_string(),
                reviewer_actor_id: correct_actor_id, // Correct value, but ignored
                changeset_digest: vec![0x42; 32],
                artifact_bundle_hash: test_artifact_bundle_hash(),
                verdict: ReviewReceiptVerdict::Approve.into(),
                blocked_reason_code: 0,
                blocked_log_hash: vec![],
                identity_proof_hash: vec![0x99; 32],
            };
            let frame = encode_ingest_review_receipt_request(&request);

            let response = dispatcher.dispatch(&frame, &wrong_ctx).unwrap();
            match response {
                PrivilegedResponse::Error(err) => {
                    assert!(
                        err.message.contains("does not match gate lease executor"),
                        "Should reject despite correct request field, got: {}",
                        err.message
                    );
                },
                other => panic!("Expected rejection for wrong peer creds, got {other:?}"),
            }
        }

        /// v6 Finding 1: missing peer credentials must be rejected.
        #[test]
        fn test_ingest_review_receipt_missing_peer_credentials_rejected() {
            let (dispatcher, _ctx) =
                setup_dispatcher_with_lease("lease-no-creds", "W-NC", "gate-nc", "unused");

            let ctx_no_creds = ConnectionContext::privileged_session_open(None);

            let request = IngestReviewReceiptRequest {
                lease_id: "lease-no-creds".to_string(),
                receipt_id: "RR-NC".to_string(),
                reviewer_actor_id: "any".to_string(),
                changeset_digest: vec![0x42; 32],
                artifact_bundle_hash: test_artifact_bundle_hash(),
                verdict: ReviewReceiptVerdict::Approve.into(),
                blocked_reason_code: 0,
                blocked_log_hash: vec![],
                identity_proof_hash: vec![0x99; 32],
            };
            let frame = encode_ingest_review_receipt_request(&request);

            let response = dispatcher.dispatch(&frame, &ctx_no_creds).unwrap();
            match response {
                PrivilegedResponse::Error(err) => {
                    assert!(
                        err.message.contains("peer credentials required"),
                        "Expected peer credentials error, got: {}",
                        err.message
                    );
                },
                other => panic!("Expected error for missing creds, got {other:?}"),
            }
        }

        #[test]
        fn test_ingest_review_receipt_lease_not_found() {
            let dispatcher = PrivilegedDispatcher::new();
            let ctx = ConnectionContext::privileged_session_open(Some(PeerCredentials {
                uid: 1000,
                gid: 1000,
                pid: Some(12345),
            }));

            let request = IngestReviewReceiptRequest {
                lease_id: "nonexistent-lease".to_string(),
                receipt_id: "RR-003".to_string(),
                reviewer_actor_id: "reviewer-delta".to_string(),
                changeset_digest: vec![0x42; 32],
                artifact_bundle_hash: test_artifact_bundle_hash(),
                verdict: ReviewReceiptVerdict::Approve.into(),
                blocked_reason_code: 0,
                blocked_log_hash: vec![],
                identity_proof_hash: vec![0x99; 32],
            };
            let frame = encode_ingest_review_receipt_request(&request);

            let response = dispatcher.dispatch(&frame, &ctx).unwrap();
            match response {
                PrivilegedResponse::Error(err) => {
                    assert_eq!(
                        err.code,
                        PrivilegedErrorCode::GateLeaseMissing as i32,
                        "Should return GateLeaseMissing for unknown lease_id"
                    );
                    assert!(
                        err.message.contains("gate lease not found"),
                        "Error message should indicate lease not found, got: {}",
                        err.message
                    );
                },
                other => panic!("Expected error, got {other:?}"),
            }
        }

        #[test]
        fn test_ingest_review_receipt_empty_lease_id() {
            let dispatcher = PrivilegedDispatcher::new();
            let ctx = ConnectionContext::privileged_session_open(Some(PeerCredentials {
                uid: 1000,
                gid: 1000,
                pid: Some(12345),
            }));

            let request = IngestReviewReceiptRequest {
                lease_id: String::new(),
                receipt_id: "RR-004".to_string(),
                reviewer_actor_id: "reviewer".to_string(),
                changeset_digest: vec![0x42; 32],
                artifact_bundle_hash: test_artifact_bundle_hash(),
                verdict: ReviewReceiptVerdict::Approve.into(),
                blocked_reason_code: 0,
                blocked_log_hash: vec![],
                identity_proof_hash: vec![0x99; 32],
            };
            let frame = encode_ingest_review_receipt_request(&request);

            let response = dispatcher.dispatch(&frame, &ctx).unwrap();
            match response {
                PrivilegedResponse::Error(err) => {
                    assert!(
                        err.message.contains("lease_id is required"),
                        "Expected lease_id required error, got: {}",
                        err.message
                    );
                },
                other => panic!("Expected error, got {other:?}"),
            }
        }

        #[test]
        fn test_ingest_review_receipt_empty_receipt_id() {
            let (dispatcher, ctx) =
                setup_dispatcher_with_lease("lease-004", "W-004", "gate-004", "reviewer");

            let request = IngestReviewReceiptRequest {
                lease_id: "lease-004".to_string(),
                receipt_id: String::new(),
                reviewer_actor_id: "reviewer".to_string(),
                changeset_digest: vec![0x42; 32],
                artifact_bundle_hash: test_artifact_bundle_hash(),
                verdict: ReviewReceiptVerdict::Approve.into(),
                blocked_reason_code: 0,
                blocked_log_hash: vec![],
                identity_proof_hash: vec![0x99; 32],
            };
            let frame = encode_ingest_review_receipt_request(&request);

            let response = dispatcher.dispatch(&frame, &ctx).unwrap();
            match response {
                PrivilegedResponse::Error(err) => {
                    assert!(
                        err.message.contains("receipt_id is required"),
                        "Expected receipt_id required error, got: {}",
                        err.message
                    );
                },
                other => panic!("Expected error, got {other:?}"),
            }
        }

        #[test]
        fn test_ingest_review_receipt_unspecified_verdict() {
            let (dispatcher, ctx) =
                setup_dispatcher_with_lease("lease-005", "W-005", "gate-005", "reviewer");

            let request = IngestReviewReceiptRequest {
                lease_id: "lease-005".to_string(),
                receipt_id: "RR-005".to_string(),
                reviewer_actor_id: "reviewer".to_string(),
                changeset_digest: vec![0x42; 32],
                artifact_bundle_hash: test_artifact_bundle_hash(),
                verdict: 0, // UNSPECIFIED
                blocked_reason_code: 0,
                blocked_log_hash: vec![],
                identity_proof_hash: vec![0x99; 32],
            };
            let frame = encode_ingest_review_receipt_request(&request);

            let response = dispatcher.dispatch(&frame, &ctx).unwrap();
            match response {
                PrivilegedResponse::Error(err) => {
                    assert!(
                        err.message.contains("verdict must be APPROVE or BLOCKED"),
                        "Expected verdict error, got: {}",
                        err.message
                    );
                },
                other => panic!("Expected error, got {other:?}"),
            }
        }

        #[test]
        fn test_ingest_review_receipt_invalid_changeset_digest_length() {
            let (dispatcher, ctx) =
                setup_dispatcher_with_lease("lease-006", "W-006", "gate-006", "reviewer");

            let request = IngestReviewReceiptRequest {
                lease_id: "lease-006".to_string(),
                receipt_id: "RR-006".to_string(),
                reviewer_actor_id: "reviewer".to_string(),
                changeset_digest: vec![0x42; 16], // Wrong length
                artifact_bundle_hash: test_artifact_bundle_hash(),
                verdict: ReviewReceiptVerdict::Approve.into(),
                blocked_reason_code: 0,
                blocked_log_hash: vec![],
                identity_proof_hash: vec![0x99; 32],
            };
            let frame = encode_ingest_review_receipt_request(&request);

            let response = dispatcher.dispatch(&frame, &ctx).unwrap();
            match response {
                PrivilegedResponse::Error(err) => {
                    assert!(
                        err.message
                            .contains("changeset_digest must be exactly 32 bytes"),
                        "Expected changeset_digest length error, got: {}",
                        err.message
                    );
                },
                other => panic!("Expected error, got {other:?}"),
            }
        }

        #[test]
        fn test_ingest_review_receipt_missing_identity_proof_hash_rejected() {
            let (dispatcher, ctx) =
                setup_dispatcher_with_lease("lease-006b", "W-006b", "gate-006b", "reviewer");

            let request = IngestReviewReceiptRequest {
                lease_id: "lease-006b".to_string(),
                receipt_id: "RR-006b".to_string(),
                reviewer_actor_id: "reviewer".to_string(),
                changeset_digest: vec![0x42; 32],
                artifact_bundle_hash: test_artifact_bundle_hash(),
                verdict: ReviewReceiptVerdict::Approve.into(),
                blocked_reason_code: 0,
                blocked_log_hash: vec![],
                identity_proof_hash: vec![],
            };
            let frame = encode_ingest_review_receipt_request(&request);

            let response = dispatcher.dispatch(&frame, &ctx).unwrap();
            match response {
                PrivilegedResponse::Error(err) => {
                    assert!(
                        err.message
                            .contains("identity_proof_hash validation failed"),
                        "Expected identity_proof_hash validation error, got: {}",
                        err.message
                    );
                },
                other => panic!("Expected error, got {other:?}"),
            }
        }

        #[test]
        fn test_ingest_review_receipt_zero_identity_proof_hash_rejected() {
            let (dispatcher, ctx) =
                setup_dispatcher_with_lease("lease-006c", "W-006c", "gate-006c", "reviewer");

            let request = IngestReviewReceiptRequest {
                lease_id: "lease-006c".to_string(),
                receipt_id: "RR-006c".to_string(),
                reviewer_actor_id: "reviewer".to_string(),
                changeset_digest: vec![0x42; 32],
                artifact_bundle_hash: test_artifact_bundle_hash(),
                verdict: ReviewReceiptVerdict::Approve.into(),
                blocked_reason_code: 0,
                blocked_log_hash: vec![],
                identity_proof_hash: vec![0x00; 32],
            };
            let frame = encode_ingest_review_receipt_request(&request);

            let response = dispatcher.dispatch(&frame, &ctx).unwrap();
            match response {
                PrivilegedResponse::Error(err) => {
                    assert!(
                        err.message.contains("null commitment rejected"),
                        "Expected null commitment rejection, got: {}",
                        err.message
                    );
                },
                other => panic!("Expected error for zero identity proof hash, got {other:?}"),
            }
        }

        #[test]
        fn test_ingest_review_receipt_blocked_missing_log_hash() {
            let (dispatcher, ctx) =
                setup_dispatcher_with_lease("lease-007", "W-007", "gate-007", "reviewer");

            let request = IngestReviewReceiptRequest {
                lease_id: "lease-007".to_string(),
                receipt_id: "RB-002".to_string(),
                reviewer_actor_id: "reviewer".to_string(),
                changeset_digest: vec![0x42; 32],
                artifact_bundle_hash: test_artifact_bundle_hash(),
                verdict: ReviewReceiptVerdict::Blocked.into(),
                blocked_reason_code: 1,
                blocked_log_hash: vec![], // Missing log hash for BLOCKED
                identity_proof_hash: vec![0x99; 32],
            };
            let frame = encode_ingest_review_receipt_request(&request);

            let response = dispatcher.dispatch(&frame, &ctx).unwrap();
            match response {
                PrivilegedResponse::Error(err) => {
                    assert!(
                        err.message
                            .contains("blocked_log_hash must be exactly 32 bytes"),
                        "Expected blocked_log_hash error, got: {}",
                        err.message
                    );
                },
                other => panic!("Expected error, got {other:?}"),
            }
        }

        #[test]
        fn test_ingest_review_receipt_blocked_zero_reason_code() {
            let (dispatcher, ctx) =
                setup_dispatcher_with_lease("lease-008", "W-008", "gate-008", "reviewer");

            let request = IngestReviewReceiptRequest {
                lease_id: "lease-008".to_string(),
                receipt_id: "RB-003".to_string(),
                reviewer_actor_id: "reviewer".to_string(),
                changeset_digest: vec![0x42; 32],
                artifact_bundle_hash: test_artifact_bundle_hash(),
                verdict: ReviewReceiptVerdict::Blocked.into(),
                blocked_reason_code: 0, // Zero is invalid for BLOCKED
                blocked_log_hash: vec![0x55; 32],
                identity_proof_hash: vec![0x99; 32],
            };
            let frame = encode_ingest_review_receipt_request(&request);

            let response = dispatcher.dispatch(&frame, &ctx).unwrap();
            match response {
                PrivilegedResponse::Error(err) => {
                    assert!(
                        err.message.contains("blocked_reason_code must be non-zero"),
                        "Expected blocked_reason_code error, got: {}",
                        err.message
                    );
                },
                other => panic!("Expected error, got {other:?}"),
            }
        }

        #[test]
        fn test_ingest_review_receipt_non_privileged_rejected() {
            let dispatcher = PrivilegedDispatcher::new();
            // Non-privileged context (session socket)
            let ctx = ConnectionContext::session_open(
                Some(PeerCredentials {
                    uid: 1000,
                    gid: 1000,
                    pid: Some(12345),
                }),
                None,
            );

            let request = IngestReviewReceiptRequest {
                lease_id: "lease-009".to_string(),
                receipt_id: "RR-009".to_string(),
                reviewer_actor_id: "reviewer".to_string(),
                changeset_digest: vec![0x42; 32],
                artifact_bundle_hash: test_artifact_bundle_hash(),
                verdict: ReviewReceiptVerdict::Approve.into(),
                blocked_reason_code: 0,
                blocked_log_hash: vec![],
                identity_proof_hash: vec![0x99; 32],
            };
            let frame = encode_ingest_review_receipt_request(&request);

            let response = dispatcher.dispatch(&frame, &ctx).unwrap();
            match response {
                PrivilegedResponse::Error(err) => {
                    assert_eq!(
                        err.code,
                        PrivilegedErrorCode::PermissionDenied as i32,
                        "Non-privileged connections should get PERMISSION_DENIED"
                    );
                },
                other => panic!("Expected PERMISSION_DENIED error, got {other:?}"),
            }
        }

        #[test]
        fn test_ingest_review_receipt_ledger_event_persisted() {
            let (dispatcher, ctx) =
                setup_dispatcher_with_lease("lease-010", "W-010", "gate-010", "reviewer-zeta");

            // v6 Finding 1: reviewer_actor_id in request is now ignored by
            // the handler — the authenticated identity from peer credentials
            // is used instead.
            let expected_actor_id = derive_actor_id(&test_peer_credentials());

            let request = IngestReviewReceiptRequest {
                lease_id: "lease-010".to_string(),
                receipt_id: "RR-010".to_string(),
                reviewer_actor_id: "reviewer-zeta".to_string(),
                changeset_digest: vec![0x42; 32],
                artifact_bundle_hash: test_artifact_bundle_hash(),
                verdict: ReviewReceiptVerdict::Approve.into(),
                blocked_reason_code: 0,
                blocked_log_hash: vec![],
                identity_proof_hash: vec![0x99; 32],
            };
            let frame = encode_ingest_review_receipt_request(&request);

            let response = dispatcher.dispatch(&frame, &ctx).unwrap();
            let event_id = match response {
                PrivilegedResponse::IngestReviewReceipt(resp) => resp.event_id,
                other => panic!("Expected IngestReviewReceipt, got {other:?}"),
            };

            // Verify the event was persisted to the emitter
            let stored_event = dispatcher.event_emitter.get_event(&event_id);
            assert!(
                stored_event.is_some(),
                "Event should be persisted in emitter"
            );
            let event = stored_event.unwrap();
            assert_eq!(event.event_type, "review_receipt_recorded");
            // v6 Finding 1: actor_id must be the authenticated identity,
            // not the caller-supplied reviewer_actor_id.
            assert_eq!(event.actor_id, expected_actor_id);
            assert!(event.timestamp_ns > 0, "Timestamp must be non-zero (HTF)");
        }

        #[test]
        fn test_ingest_review_receipt_response_encoding() {
            let (dispatcher, ctx) =
                setup_dispatcher_with_lease("lease-011", "W-011", "gate-011", "reviewer-enc");

            let request = IngestReviewReceiptRequest {
                lease_id: "lease-011".to_string(),
                receipt_id: "RR-011".to_string(),
                reviewer_actor_id: "reviewer-enc".to_string(),
                changeset_digest: vec![0x42; 32],
                artifact_bundle_hash: test_artifact_bundle_hash(),
                verdict: ReviewReceiptVerdict::Approve.into(),
                blocked_reason_code: 0,
                blocked_log_hash: vec![],
                identity_proof_hash: vec![0x99; 32],
            };
            let frame = encode_ingest_review_receipt_request(&request);

            let response = dispatcher.dispatch(&frame, &ctx).unwrap();

            // Verify encode/decode roundtrip
            let encoded = response.encode();
            assert!(!encoded.is_empty(), "Encoded response should not be empty");
            assert_eq!(
                encoded[0],
                PrivilegedMessageType::IngestReviewReceipt.tag(),
                "Response tag should match IngestReviewReceipt"
            );

            // Decode the payload
            let decoded = IngestReviewReceiptResponse::decode_bounded(
                &encoded[1..],
                &DecodeConfig::default(),
            )
            .expect("decode should succeed");
            assert_eq!(decoded.receipt_id, "RR-011");
            assert_eq!(decoded.event_type, "ReviewReceiptRecorded");
        }

        #[test]
        fn test_ingest_review_receipt_message_type_tag() {
            assert_eq!(
                PrivilegedMessageType::IngestReviewReceipt.tag(),
                17,
                "IngestReviewReceipt should have tag 17"
            );
            assert_eq!(
                PrivilegedMessageType::from_tag(17),
                Some(PrivilegedMessageType::IngestReviewReceipt),
                "Tag 17 should resolve to IngestReviewReceipt"
            );
        }

        #[test]
        fn test_ingest_review_receipt_oversized_lease_id() {
            let dispatcher = PrivilegedDispatcher::new();
            let ctx = ConnectionContext::privileged_session_open(Some(PeerCredentials {
                uid: 1000,
                gid: 1000,
                pid: Some(12345),
            }));

            let request = IngestReviewReceiptRequest {
                lease_id: "x".repeat(MAX_ID_LENGTH + 1),
                receipt_id: "RR-012".to_string(),
                reviewer_actor_id: "reviewer".to_string(),
                changeset_digest: vec![0x42; 32],
                artifact_bundle_hash: test_artifact_bundle_hash(),
                verdict: ReviewReceiptVerdict::Approve.into(),
                blocked_reason_code: 0,
                blocked_log_hash: vec![],
                identity_proof_hash: vec![0x99; 32],
            };
            let frame = encode_ingest_review_receipt_request(&request);

            let response = dispatcher.dispatch(&frame, &ctx).unwrap();
            match response {
                PrivilegedResponse::Error(err) => {
                    assert!(
                        err.message.contains("lease_id exceeds maximum length"),
                        "Expected oversized lease_id error, got: {}",
                        err.message
                    );
                },
                other => panic!("Expected error, got {other:?}"),
            }
        }

        // =================================================================
        // TCK-00340: Attestation enforcement tests
        // =================================================================

        #[test]
        fn test_ingest_review_receipt_attestation_validates_signer_identity() {
            // Verify the attestation ratchet is wired into the production path
            // by confirming that a valid Tier0 request with proper reviewer
            // identity passes the attestation check. The attestation check
            // verifies:
            // 1. Internal consistency (non-empty signer_identity)
            // 2. Policy hash binding (changeset_digest match)
            // 3. Ratchet level (SelfSigned >= Tier0 requirement of None)
            //
            // The pre-attestation validation already rejects empty
            // reviewer_actor_id, so the attestation check provides
            // defense-in-depth for any future code paths that skip that guard.
            let (dispatcher, ctx) = setup_dispatcher_with_lease(
                "lease-attest-002",
                "W-ATT-002",
                "gate-attest",
                "reviewer-attest",
            );

            let request = IngestReviewReceiptRequest {
                lease_id: "lease-attest-002".to_string(),
                receipt_id: "RR-ATT-002".to_string(),
                reviewer_actor_id: "reviewer-attest".to_string(),
                changeset_digest: vec![0x42; 32],
                artifact_bundle_hash: test_artifact_bundle_hash(),
                verdict: ReviewReceiptVerdict::Approve.into(),
                blocked_reason_code: 0,
                blocked_log_hash: vec![],
                identity_proof_hash: vec![0x99; 32],
            };
            let frame = encode_ingest_review_receipt_request(&request);

            let response = dispatcher.dispatch(&frame, &ctx).unwrap();
            match response {
                PrivilegedResponse::IngestReviewReceipt(resp) => {
                    assert_eq!(resp.receipt_id, "RR-ATT-002");
                    assert_eq!(
                        resp.event_type, "ReviewReceiptRecorded",
                        "Attestation-validated Tier0 receipt should be accepted"
                    );
                },
                PrivilegedResponse::Error(err) => {
                    panic!("Valid attestation should pass, got error: {}", err.message);
                },
                other => panic!("Expected IngestReviewReceipt, got {other:?}"),
            }
        }

        #[test]
        fn test_ingest_review_receipt_tier0_constraint_fail_closed_semantics() {
            // Verify fail-closed semantics: the risk_tier variable is currently
            // hardcoded to Tier0 with an explicit guard that rejects non-Tier0.
            // When the work registry is wired (TODO RFC-0019), a non-Tier0 work
            // item MUST be rejected because this endpoint only provides
            // SelfSigned attestation, which is insufficient for Tier1+.
            //
            // This test validates that the attestation requirements table
            // correctly rejects SelfSigned at higher tiers (proving the
            // fail-closed guard is necessary and would work if activated).
            use apm2_core::fac::RiskTier;
            use apm2_core::fac::policy_inheritance::{
                AttestationLevel, AttestationRequirements, ReceiptAttestation, ReceiptKind,
                validate_receipt_attestation,
            };

            let requirements = AttestationRequirements::new();
            let changeset_digest = [0x42; 32];

            // Tier0 with SelfSigned should pass (current behavior)
            let tier0_attestation = ReceiptAttestation {
                kind: ReceiptKind::Review,
                level: AttestationLevel::SelfSigned,
                policy_hash: changeset_digest,
                signer_identity: "reviewer-001".to_string(),
                counter_signer_identity: None,
                threshold_signer_count: None,
            };
            assert!(
                validate_receipt_attestation(
                    &tier0_attestation,
                    RiskTier::Tier0,
                    &changeset_digest,
                    &requirements,
                )
                .is_ok(),
                "Tier0 SelfSigned must pass"
            );

            // Tier2 with SelfSigned should FAIL (requires CounterSigned for Review)
            // This proves the Tier0-only constraint is security-relevant.
            let tier2_result = validate_receipt_attestation(
                &tier0_attestation,
                RiskTier::Tier2,
                &changeset_digest,
                &requirements,
            );
            assert!(
                tier2_result.is_err(),
                "Tier2 with only SelfSigned must be rejected - proves the \
                 Tier0-only endpoint constraint is necessary for security"
            );

            // Tier4 with SelfSigned should FAIL (requires ThresholdSigned for Review)
            let tier4_result = validate_receipt_attestation(
                &tier0_attestation,
                RiskTier::Tier4,
                &changeset_digest,
                &requirements,
            );
            assert!(
                tier4_result.is_err(),
                "Tier4 with only SelfSigned must be rejected - proves the \
                 Tier0-only endpoint constraint prevents attestation bypass"
            );
        }

        #[test]
        fn test_ingest_review_receipt_tier0_self_signed_accepted() {
            // Verify that a valid Tier0 receipt with proper SelfSigned attestation
            // passes through the attestation check in the production path.
            let (dispatcher, ctx) = setup_dispatcher_with_lease(
                "lease-t0-001",
                "W-T0-001",
                "gate-001",
                "reviewer-tier0",
            );

            let request = IngestReviewReceiptRequest {
                lease_id: "lease-t0-001".to_string(),
                receipt_id: "RR-T0-001".to_string(),
                reviewer_actor_id: "reviewer-tier0".to_string(),
                changeset_digest: vec![0x42; 32],
                artifact_bundle_hash: test_artifact_bundle_hash(),
                verdict: ReviewReceiptVerdict::Approve.into(),
                blocked_reason_code: 0,
                blocked_log_hash: vec![],
                identity_proof_hash: vec![0x99; 32],
            };
            let frame = encode_ingest_review_receipt_request(&request);

            let response = dispatcher.dispatch(&frame, &ctx).unwrap();
            match response {
                PrivilegedResponse::IngestReviewReceipt(resp) => {
                    assert_eq!(resp.receipt_id, "RR-T0-001");
                    assert_eq!(
                        resp.event_type, "ReviewReceiptRecorded",
                        "Tier0 SelfSigned review receipt should be accepted"
                    );
                },
                PrivilegedResponse::Error(err) => {
                    panic!(
                        "Tier0 SelfSigned should be accepted, got error: {}",
                        err.message
                    );
                },
                other => panic!("Expected IngestReviewReceipt, got {other:?}"),
            }
        }

        // ====================================================================
        // TCK-00340: Risk-tier resolution integration tests
        // ====================================================================

        #[test]
        fn test_ingest_review_receipt_tier2_self_signed_rejected() {
            // Tier2 work items require CounterSigned attestation for Review
            // receipts. SelfSigned is insufficient — must be rejected.
            let (dispatcher, ctx) = setup_dispatcher_with_lease_and_tier(
                "lease-t2-001",
                "W-T2-001",
                "gate-001",
                "reviewer-tier2",
                2, // Tier2
            );

            let request = IngestReviewReceiptRequest {
                lease_id: "lease-t2-001".to_string(),
                receipt_id: "RR-T2-001".to_string(),
                reviewer_actor_id: "reviewer-tier2".to_string(),
                changeset_digest: vec![0x42; 32],
                artifact_bundle_hash: test_artifact_bundle_hash(),
                verdict: ReviewReceiptVerdict::Approve.into(),
                blocked_reason_code: 0,
                blocked_log_hash: vec![],
                identity_proof_hash: vec![0x99; 32],
            };
            let frame = encode_ingest_review_receipt_request(&request);

            let response = dispatcher.dispatch(&frame, &ctx).unwrap();
            match response {
                PrivilegedResponse::Error(err) => {
                    assert!(
                        err.message.contains("SelfSigned") || err.message.contains("requires"),
                        "Tier2 SelfSigned must be rejected, got: {}",
                        err.message
                    );
                    assert!(
                        err.message.contains("Tier2"),
                        "Error must mention the resolved tier, got: {}",
                        err.message
                    );
                },
                other => {
                    panic!("Expected rejection for Tier2 SelfSigned attestation, got {other:?}")
                },
            }
        }

        #[test]
        fn test_ingest_review_receipt_tier4_self_signed_rejected() {
            // Tier4 is the highest risk tier — requires ThresholdSigned.
            // SelfSigned must be rejected (fail-closed).
            let (dispatcher, ctx) = setup_dispatcher_with_lease_and_tier(
                "lease-t4-001",
                "W-T4-001",
                "gate-001",
                "reviewer-tier4",
                4, // Tier4
            );

            let request = IngestReviewReceiptRequest {
                lease_id: "lease-t4-001".to_string(),
                receipt_id: "RR-T4-001".to_string(),
                reviewer_actor_id: "reviewer-tier4".to_string(),
                changeset_digest: vec![0x42; 32],
                artifact_bundle_hash: test_artifact_bundle_hash(),
                verdict: ReviewReceiptVerdict::Approve.into(),
                blocked_reason_code: 0,
                blocked_log_hash: vec![],
                identity_proof_hash: vec![0x99; 32],
            };
            let frame = encode_ingest_review_receipt_request(&request);

            let response = dispatcher.dispatch(&frame, &ctx).unwrap();
            match response {
                PrivilegedResponse::Error(err) => {
                    assert!(
                        err.message.contains("SelfSigned") || err.message.contains("requires"),
                        "Tier4 SelfSigned must be rejected, got: {}",
                        err.message
                    );
                    assert!(
                        err.message.contains("Tier4"),
                        "Error must mention the resolved tier, got: {}",
                        err.message
                    );
                },
                other => {
                    panic!("Expected rejection for Tier4 SelfSigned attestation, got {other:?}")
                },
            }
        }

        #[test]
        fn test_ingest_review_receipt_tier0_resolved_passes_end_to_end() {
            // Tier0 with SelfSigned through the FULL production path:
            // lease -> work_id -> work_claim -> risk_tier=0 -> attestation ok -> event
            let (dispatcher, ctx) = setup_dispatcher_with_lease_and_tier(
                "lease-e2e-001",
                "W-E2E-001",
                "gate-001",
                "reviewer-e2e",
                0, // Tier0
            );

            let request = IngestReviewReceiptRequest {
                lease_id: "lease-e2e-001".to_string(),
                receipt_id: "RR-E2E-001".to_string(),
                reviewer_actor_id: "reviewer-e2e".to_string(),
                changeset_digest: vec![0x42; 32],
                artifact_bundle_hash: test_artifact_bundle_hash(),
                verdict: ReviewReceiptVerdict::Approve.into(),
                blocked_reason_code: 0,
                blocked_log_hash: vec![],
                identity_proof_hash: vec![0x99; 32],
            };
            let frame = encode_ingest_review_receipt_request(&request);

            let response = dispatcher.dispatch(&frame, &ctx).unwrap();
            match response {
                PrivilegedResponse::IngestReviewReceipt(resp) => {
                    assert_eq!(resp.receipt_id, "RR-E2E-001");
                    assert_eq!(
                        resp.event_type, "ReviewReceiptRecorded",
                        "Tier0 work with SelfSigned must pass attestation end-to-end"
                    );
                    assert!(
                        !resp.event_id.is_empty(),
                        "event_id must be non-empty on success"
                    );
                },
                PrivilegedResponse::Error(err) => {
                    panic!(
                        "Tier0 SelfSigned should pass end-to-end, got error: {}",
                        err.message
                    );
                },
                other => panic!("Expected IngestReviewReceipt, got {other:?}"),
            }
        }

        #[test]
        fn test_ingest_review_receipt_unresolvable_risk_tier_fails_closed() {
            // When work_id can be resolved from the lease but no work claim
            // exists in the registry, the risk tier is unresolvable.
            // FAIL-CLOSED: must default to Tier4 and reject SelfSigned.
            let peer_creds = PeerCredentials {
                uid: 1000,
                gid: 1000,
                pid: Some(12345),
            };
            let executor_actor_id = derive_actor_id(&peer_creds);
            // TCK-00408: CAS is mandatory for ingest (fail-closed).
            let cas = Arc::new(MemoryCas::default());
            cas.store(TEST_ARTIFACT_CONTENT).unwrap();
            let dispatcher = PrivilegedDispatcher::new()
                .with_cas(Arc::clone(&cas) as Arc<dyn ContentAddressedStore>);
            // Register lease but do NOT register a work claim
            dispatcher.lease_validator.register_lease_with_executor(
                "lease-no-claim",
                "W-NO-CLAIM",
                "gate-001",
                &executor_actor_id,
            );
            let ctx = ConnectionContext::privileged_session_open(Some(peer_creds));

            let request = IngestReviewReceiptRequest {
                lease_id: "lease-no-claim".to_string(),
                receipt_id: "RR-NO-CLAIM".to_string(),
                reviewer_actor_id: "ignored-by-handler".to_string(),
                changeset_digest: vec![0x42; 32],
                artifact_bundle_hash: test_artifact_bundle_hash(),
                verdict: ReviewReceiptVerdict::Approve.into(),
                blocked_reason_code: 0,
                blocked_log_hash: vec![],
                identity_proof_hash: vec![0x99; 32],
            };
            let frame = encode_ingest_review_receipt_request(&request);

            let response = dispatcher.dispatch(&frame, &ctx).unwrap();
            match response {
                PrivilegedResponse::Error(err) => {
                    assert!(
                        err.message.contains("SelfSigned")
                            || err.message.contains("requires")
                            || err.message.contains("Tier4"),
                        "Unresolvable risk tier must fail-closed (Tier4 rejection), got: {}",
                        err.message
                    );
                },
                other => panic!(
                    "Expected fail-closed rejection for unresolvable risk tier, got {other:?}"
                ),
            }
        }

        #[test]
        fn test_ingest_review_receipt_tier1_self_signed_accepted() {
            // v5 Finding 4: Tier1 requires SelfSigned for review receipts per
            // the ratchet table. This endpoint provides SelfSigned attestation,
            // so Tier1 requests MUST be accepted (not hard-rejected).
            let (dispatcher, ctx) = setup_dispatcher_with_lease_and_tier(
                "lease-t1-001",
                "W-T1-001",
                "gate-001",
                "reviewer-tier1",
                1, // Tier1
            );

            let request = IngestReviewReceiptRequest {
                lease_id: "lease-t1-001".to_string(),
                receipt_id: "RR-T1-001".to_string(),
                reviewer_actor_id: "reviewer-tier1".to_string(),
                changeset_digest: vec![0x42; 32],
                artifact_bundle_hash: test_artifact_bundle_hash(),
                verdict: ReviewReceiptVerdict::Approve.into(),
                blocked_reason_code: 0,
                blocked_log_hash: vec![],
                identity_proof_hash: vec![0x99; 32],
            };
            let frame = encode_ingest_review_receipt_request(&request);

            let response = dispatcher.dispatch(&frame, &ctx).unwrap();
            match response {
                PrivilegedResponse::IngestReviewReceipt(resp) => {
                    assert_eq!(
                        resp.receipt_id, "RR-T1-001",
                        "Tier1 SelfSigned must be accepted per ratchet table"
                    );
                    assert_eq!(resp.event_type, "ReviewReceiptRecorded");
                    assert!(
                        !resp.event_id.is_empty(),
                        "Event ID must be non-empty for accepted Tier1 receipt"
                    );
                },
                PrivilegedResponse::Error(err) => {
                    panic!(
                        "Tier1 SelfSigned must be accepted per ratchet table, got error: {}",
                        err.message
                    );
                },
                other => {
                    panic!("Expected IngestReviewReceipt for Tier1 SelfSigned, got {other:?}")
                },
            }
        }

        /// v10 MAJOR 1: Duplicate `receipt_id` replay returns the original
        /// event (idempotent) instead of creating a new event. This verifies
        /// the fix for the broken idempotency check that previously looked up
        /// events by `event_id` (auto-generated UUID) instead of searching
        /// payloads for the caller-supplied `receipt_id`.
        #[test]
        fn test_ingest_review_receipt_duplicate_receipt_id_idempotent() {
            let (dispatcher, ctx) =
                setup_dispatcher_with_lease("lease-dup-001", "W-DUP-001", "gate-001", "reviewer-a");

            let request = IngestReviewReceiptRequest {
                lease_id: "lease-dup-001".to_string(),
                receipt_id: "RR-DUP-001".to_string(),
                reviewer_actor_id: "reviewer-a".to_string(),
                changeset_digest: vec![0x42; 32],
                artifact_bundle_hash: test_artifact_bundle_hash(),
                verdict: ReviewReceiptVerdict::Approve.into(),
                blocked_reason_code: 0,
                blocked_log_hash: vec![],
                identity_proof_hash: vec![0x99; 32],
            };

            // First submission should succeed
            let frame = encode_ingest_review_receipt_request(&request);
            let response1 = dispatcher.dispatch(&frame, &ctx).unwrap();
            let (event_id_1, event_type_1) = match &response1 {
                PrivilegedResponse::IngestReviewReceipt(resp) => {
                    assert_eq!(resp.receipt_id, "RR-DUP-001");
                    assert!(
                        !resp.event_id.is_empty(),
                        "First event_id must be non-empty"
                    );
                    (resp.event_id.clone(), resp.event_type.clone())
                },
                other => panic!("Expected IngestReviewReceipt, got {other:?}"),
            };

            // Second submission with same receipt_id should return idempotent
            // result with the SAME event_id as the first submission.
            let frame2 = encode_ingest_review_receipt_request(&request);
            let response2 = dispatcher.dispatch(&frame2, &ctx).unwrap();
            match &response2 {
                PrivilegedResponse::IngestReviewReceipt(resp) => {
                    assert_eq!(
                        resp.receipt_id, "RR-DUP-001",
                        "Idempotent replay must return same receipt_id"
                    );
                    assert_eq!(
                        resp.event_id, event_id_1,
                        "Idempotent replay must return the ORIGINAL event_id, not a new one"
                    );
                    assert_eq!(
                        resp.event_type, event_type_1,
                        "Idempotent replay must return same event_type"
                    );
                },
                other => panic!("Expected idempotent IngestReviewReceipt, got {other:?}"),
            }

            // Verify only ONE event was created in the ledger (not two)
            let events = dispatcher
                .event_emitter
                .get_events_by_work_id("lease-dup-001");
            let receipt_event_count = events
                .iter()
                .filter(|e| e.event_type == "review_receipt_recorded")
                .count();
            assert_eq!(
                receipt_event_count, 1,
                "Duplicate receipt_id submission must NOT create a second event"
            );
        }

        #[test]
        fn test_ingest_review_receipt_duplicate_receipt_id_different_identity_proof_hash_rejected()
        {
            let (dispatcher, ctx) = setup_dispatcher_with_lease(
                "lease-dup-proof-001",
                "W-DUP-PROOF-001",
                "gate-001",
                "reviewer-a",
            );

            let request1 = IngestReviewReceiptRequest {
                lease_id: "lease-dup-proof-001".to_string(),
                receipt_id: "RR-DUP-PROOF-001".to_string(),
                reviewer_actor_id: "reviewer-a".to_string(),
                changeset_digest: vec![0x42; 32],
                artifact_bundle_hash: test_artifact_bundle_hash(),
                verdict: ReviewReceiptVerdict::Approve.into(),
                blocked_reason_code: 0,
                blocked_log_hash: vec![],
                identity_proof_hash: vec![0x99; 32],
            };
            let frame1 = encode_ingest_review_receipt_request(&request1);
            let response1 = dispatcher.dispatch(&frame1, &ctx).unwrap();
            assert!(
                matches!(response1, PrivilegedResponse::IngestReviewReceipt(_)),
                "first receipt submission should succeed"
            );

            let request2 = IngestReviewReceiptRequest {
                lease_id: "lease-dup-proof-001".to_string(),
                receipt_id: "RR-DUP-PROOF-001".to_string(),
                reviewer_actor_id: "reviewer-a".to_string(),
                changeset_digest: vec![0x42; 32],
                artifact_bundle_hash: test_artifact_bundle_hash(),
                verdict: ReviewReceiptVerdict::Approve.into(),
                blocked_reason_code: 0,
                blocked_log_hash: vec![],
                identity_proof_hash: vec![0x98; 32],
            };
            let frame2 = encode_ingest_review_receipt_request(&request2);
            let response2 = dispatcher.dispatch(&frame2, &ctx).unwrap();
            match response2 {
                PrivilegedResponse::Error(err) => {
                    assert!(
                        err.message.contains("identity_proof_hash"),
                        "expected identity_proof_hash mismatch rejection, got: {}",
                        err.message
                    );
                },
                other => panic!(
                    "expected duplicate receipt_id + identity_proof_hash mismatch rejection, got {other:?}"
                ),
            }
        }

        #[test]
        fn test_ingest_review_receipt_duplicate_receipt_id_different_artifact_bundle_hash_rejected()
        {
            let (dispatcher, ctx) = setup_dispatcher_with_lease(
                "lease-dup-artifact-001",
                "W-DUP-ARTIFACT-001",
                "gate-001",
                "reviewer-a",
            );

            let request1 = IngestReviewReceiptRequest {
                lease_id: "lease-dup-artifact-001".to_string(),
                receipt_id: "RR-DUP-ARTIFACT-001".to_string(),
                reviewer_actor_id: "reviewer-a".to_string(),
                changeset_digest: vec![0x42; 32],
                artifact_bundle_hash: test_artifact_bundle_hash(),
                verdict: ReviewReceiptVerdict::Approve.into(),
                blocked_reason_code: 0,
                blocked_log_hash: vec![],
                identity_proof_hash: vec![0x99; 32],
            };
            let frame1 = encode_ingest_review_receipt_request(&request1);
            let response1 = dispatcher.dispatch(&frame1, &ctx).unwrap();
            assert!(
                matches!(response1, PrivilegedResponse::IngestReviewReceipt(_)),
                "first receipt submission should succeed"
            );

            let request2 = IngestReviewReceiptRequest {
                lease_id: "lease-dup-artifact-001".to_string(),
                receipt_id: "RR-DUP-ARTIFACT-001".to_string(),
                reviewer_actor_id: "reviewer-a".to_string(),
                changeset_digest: vec![0x42; 32],
                artifact_bundle_hash: vec![0x44; 32],
                verdict: ReviewReceiptVerdict::Approve.into(),
                blocked_reason_code: 0,
                blocked_log_hash: vec![],
                identity_proof_hash: vec![0x99; 32],
            };
            let frame2 = encode_ingest_review_receipt_request(&request2);
            let response2 = dispatcher.dispatch(&frame2, &ctx).unwrap();
            match response2 {
                PrivilegedResponse::Error(err) => {
                    assert!(
                        err.message.contains("artifact_bundle_hash"),
                        "expected artifact_bundle_hash mismatch rejection, got: {}",
                        err.message
                    );
                },
                other => panic!(
                    "expected duplicate receipt_id + artifact_bundle_hash mismatch rejection, got {other:?}"
                ),
            }
        }

        #[test]
        fn test_ingest_review_receipt_duplicate_blocked_receipt_different_artifact_bundle_hash_rejected()
         {
            let (dispatcher, ctx) = setup_dispatcher_with_lease(
                "lease-dup-blocked-artifact-001",
                "W-DUP-BLOCKED-ARTIFACT-001",
                "gate-001",
                "reviewer-a",
            );

            let request1 = IngestReviewReceiptRequest {
                lease_id: "lease-dup-blocked-artifact-001".to_string(),
                receipt_id: "RB-DUP-ARTIFACT-001".to_string(),
                reviewer_actor_id: "reviewer-a".to_string(),
                changeset_digest: vec![0x42; 32],
                artifact_bundle_hash: test_artifact_bundle_hash(),
                verdict: ReviewReceiptVerdict::Blocked.into(),
                blocked_reason_code: 1,
                blocked_log_hash: vec![0x55; 32],
                identity_proof_hash: vec![0x99; 32],
            };
            let frame1 = encode_ingest_review_receipt_request(&request1);
            let response1 = dispatcher.dispatch(&frame1, &ctx).unwrap();
            assert!(
                matches!(response1, PrivilegedResponse::IngestReviewReceipt(_)),
                "first blocked receipt submission should succeed"
            );

            let request2 = IngestReviewReceiptRequest {
                lease_id: "lease-dup-blocked-artifact-001".to_string(),
                receipt_id: "RB-DUP-ARTIFACT-001".to_string(),
                reviewer_actor_id: "reviewer-a".to_string(),
                changeset_digest: vec![0x42; 32],
                artifact_bundle_hash: vec![0x44; 32],
                verdict: ReviewReceiptVerdict::Blocked.into(),
                blocked_reason_code: 1,
                blocked_log_hash: vec![0x55; 32],
                identity_proof_hash: vec![0x99; 32],
            };
            let frame2 = encode_ingest_review_receipt_request(&request2);
            let response2 = dispatcher.dispatch(&frame2, &ctx).unwrap();
            match response2 {
                PrivilegedResponse::Error(err) => {
                    assert!(
                        err.message.contains("artifact_bundle_hash"),
                        "expected artifact_bundle_hash mismatch rejection, got: {}",
                        err.message
                    );
                },
                other => panic!(
                    "expected duplicate BLOCKED receipt_id + artifact_bundle_hash mismatch rejection, got {other:?}"
                ),
            }
        }

        #[test]
        fn test_ingest_review_receipt_duplicate_receipt_id_different_lease_id_rejected() {
            let (dispatcher, ctx) =
                setup_dispatcher_with_lease("lease-A", "W-DUP-LEASE-A", "gate-001", "reviewer-a");

            let peer_creds = test_peer_credentials();
            let executor_actor_id = derive_actor_id(&peer_creds);
            dispatcher.lease_validator.register_lease_with_executor(
                "lease-B",
                "W-DUP-LEASE-B",
                "gate-002",
                &executor_actor_id,
            );
            dispatcher
                .work_registry
                .register_claim(WorkClaim {
                    work_id: "W-DUP-LEASE-B".to_string(),
                    lease_id: "lease-B".to_string(),
                    actor_id: executor_actor_id,
                    role: WorkRole::Reviewer,
                    policy_resolution: PolicyResolution {
                        policy_resolved_ref: "PolicyResolvedForChangeSet:W-DUP-LEASE-B".to_string(),
                        resolved_policy_hash: [0u8; 32],
                        capability_manifest_hash: [0u8; 32],
                        context_pack_hash: [0u8; 32],
                        resolved_risk_tier: 0,
                        resolved_scope_baseline: None,
                        expected_adapter_profile_hash: None,
                    },
                    executor_custody_domains: vec![],
                    author_custody_domains: vec![],
                })
                .expect("second work claim registration should succeed");

            let request1 = IngestReviewReceiptRequest {
                lease_id: "lease-A".to_string(),
                receipt_id: "RR-DUP-LEASE-001".to_string(),
                reviewer_actor_id: "reviewer-a".to_string(),
                changeset_digest: vec![0x42; 32],
                artifact_bundle_hash: test_artifact_bundle_hash(),
                verdict: ReviewReceiptVerdict::Approve.into(),
                blocked_reason_code: 0,
                blocked_log_hash: vec![],
                identity_proof_hash: vec![0x99; 32],
            };
            let frame1 = encode_ingest_review_receipt_request(&request1);
            let response1 = dispatcher.dispatch(&frame1, &ctx).unwrap();
            assert!(
                matches!(response1, PrivilegedResponse::IngestReviewReceipt(_)),
                "first receipt submission should succeed"
            );

            let request2 = IngestReviewReceiptRequest {
                lease_id: "lease-B".to_string(),
                receipt_id: "RR-DUP-LEASE-001".to_string(),
                reviewer_actor_id: "reviewer-a".to_string(),
                changeset_digest: vec![0x42; 32],
                artifact_bundle_hash: test_artifact_bundle_hash(),
                verdict: ReviewReceiptVerdict::Approve.into(),
                blocked_reason_code: 0,
                blocked_log_hash: vec![],
                identity_proof_hash: vec![0x99; 32],
            };
            let frame2 = encode_ingest_review_receipt_request(&request2);
            let response2 = dispatcher.dispatch(&frame2, &ctx).unwrap();
            match response2 {
                PrivilegedResponse::Error(err) => {
                    assert!(
                        err.message.contains("lease"),
                        "expected lease mismatch rejection, got: {}",
                        err.message
                    );
                },
                other => panic!(
                    "expected duplicate receipt_id + lease mismatch rejection, got {other:?}"
                ),
            }
        }

        /// TCK-00408: Verify CAS existence validation rejects review receipts
        /// referencing artifact bundles that are not in CAS.
        #[test]
        fn test_ingest_review_receipt_rejects_missing_cas_artifact() {
            // Create dispatcher WITH CAS so the validation path is exercised
            let cas = Arc::new(MemoryCas::default());
            let peer_creds = test_peer_credentials();
            let executor_actor_id = derive_actor_id(&peer_creds);
            let dispatcher = PrivilegedDispatcher::new()
                .with_cas(Arc::clone(&cas) as Arc<dyn ContentAddressedStore>);

            // Register lease and work claim
            let lease_id = "lease-cas-check-001";
            let work_id = "W-CAS-CHECK-001";
            dispatcher.lease_validator.register_lease_with_executor(
                lease_id,
                work_id,
                "gate-cas-001",
                &executor_actor_id,
            );
            dispatcher
                .work_registry
                .register_claim(WorkClaim {
                    work_id: work_id.to_string(),
                    lease_id: lease_id.to_string(),
                    actor_id: executor_actor_id,
                    role: WorkRole::Reviewer,
                    policy_resolution: PolicyResolution {
                        policy_resolved_ref: format!("PolicyResolvedForChangeSet:{work_id}"),
                        resolved_policy_hash: [0u8; 32],
                        capability_manifest_hash: [0u8; 32],
                        context_pack_hash: [0u8; 32],
                        resolved_risk_tier: 0,
                        resolved_scope_baseline: None,
                        expected_adapter_profile_hash: None,
                    },
                    executor_custody_domains: vec![],
                    author_custody_domains: vec![],
                })
                .expect("work claim registration should succeed");

            let ctx = ConnectionContext::privileged_session_open(Some(peer_creds));

            // Submit review receipt with artifact_bundle_hash NOT in CAS
            let request = IngestReviewReceiptRequest {
                lease_id: lease_id.to_string(),
                receipt_id: "RR-CAS-MISSING-001".to_string(),
                reviewer_actor_id: "reviewer-a".to_string(),
                changeset_digest: vec![0x42; 32],
                artifact_bundle_hash: vec![0xAA; 32], // NOT stored in CAS
                verdict: ReviewReceiptVerdict::Approve.into(),
                blocked_reason_code: 0,
                blocked_log_hash: vec![],
                identity_proof_hash: vec![0x99; 32],
            };
            let frame = encode_ingest_review_receipt_request(&request);
            let response = dispatcher.dispatch(&frame, &ctx).unwrap();

            match &response {
                PrivilegedResponse::Error(err) => {
                    assert!(
                        err.message.contains("not found in CAS"),
                        "Error should mention CAS: {}",
                        err.message
                    );
                },
                other => panic!("Expected CAS existence rejection, got: {other:?}"),
            }
        }

        // ====================================================================
        // TCK-00340: DelegateSublease IPC Integration Tests
        // ====================================================================

        /// Helper: build a dispatcher with a gate orchestrator and a registered
        /// parent lease for sublease delegation tests.
        ///
        /// The parent lease's `executor_actor_id` is set to match the caller's
        /// derived actor ID (from uid=1000, gid=1000) so that the caller
        /// authorization check in `handle_delegate_sublease` passes.
        fn setup_dispatcher_with_orchestrator(
            parent_lease_id: &str,
            work_id: &str,
            gate_id: &str,
            _executor_actor_id: &str,
        ) -> (
            PrivilegedDispatcher,
            ConnectionContext,
            apm2_core::fac::GateLease,
        ) {
            let signer = Arc::new(apm2_core::crypto::Signer::generate());
            let orch = Arc::new(crate::gate::GateOrchestrator::new(
                crate::gate::GateOrchestratorConfig::default(),
                signer.clone(),
            ));

            // Derive the actor ID from the test peer credentials (uid=1000,
            // gid=1000) so the caller authorization check passes.
            let test_creds = PeerCredentials {
                uid: 1000,
                gid: 1000,
                pid: Some(12345),
            };
            let caller_actor = derive_actor_id(&test_creds);

            let parent_lease =
                apm2_core::fac::GateLeaseBuilder::new(parent_lease_id, work_id, gate_id)
                    .changeset_digest([0x42; 32])
                    .executor_actor_id(&caller_actor)
                    .issued_at(1_000_000)
                    .expires_at(2_000_000)
                    .policy_hash([0xAB; 32])
                    .issuer_actor_id("issuer-001")
                    .time_envelope_ref("htf:tick:100")
                    .build_and_sign(&signer);

            let dispatcher = PrivilegedDispatcher::new().with_gate_orchestrator(orch);
            dispatcher
                .lease_validator
                .register_full_lease(&parent_lease)
                .expect("register_full_lease should succeed in test");
            dispatcher.lease_validator.register_lease_with_executor(
                parent_lease_id,
                work_id,
                gate_id,
                &caller_actor,
            );

            let ctx = ConnectionContext::privileged_session_open(Some(PeerCredentials {
                uid: 1000,
                gid: 1000,
                pid: Some(12345),
            }));
            (dispatcher, ctx, parent_lease)
        }

        #[test]
        fn test_delegate_sublease_valid_succeeds() {
            let (dispatcher, ctx, _parent) = setup_dispatcher_with_orchestrator(
                "parent-lease-001",
                "W-DS-001",
                "gate-quality",
                "executor-001",
            );

            let request = DelegateSubleaseRequest {
                parent_lease_id: "parent-lease-001".to_string(),
                delegatee_actor_id: "child-executor-001".to_string(),
                requested_expiry_ns: 1_900_000_000_000,
                sublease_id: "sublease-001".to_string(),
                identity_proof_hash: vec![0x99; 32],
            };
            let frame = encode_delegate_sublease_request(&request);

            let response = dispatcher.dispatch(&frame, &ctx).unwrap();
            match response {
                PrivilegedResponse::DelegateSublease(resp) => {
                    assert_eq!(resp.sublease_id, "sublease-001");
                    assert_eq!(resp.parent_lease_id, "parent-lease-001");
                    assert_eq!(resp.delegatee_actor_id, "child-executor-001");
                    assert_eq!(resp.gate_id, "gate-quality");
                    // v5 Finding 2: expires_at_ns in response is in nanoseconds
                    // (converted from internal ms representation).
                    assert_eq!(resp.expires_at_ns, 1_900_000_000_000);
                    assert!(!resp.event_id.is_empty(), "event_id must be non-empty");
                },
                PrivilegedResponse::Error(err) => {
                    panic!(
                        "Expected DelegateSublease success, got error: {}",
                        err.message
                    );
                },
                other => panic!("Expected DelegateSublease, got {other:?}"),
            }
        }

        #[test]
        fn test_delegate_sublease_parent_not_found_rejected() {
            let signer = Arc::new(apm2_core::crypto::Signer::generate());
            let orch = Arc::new(crate::gate::GateOrchestrator::new(
                crate::gate::GateOrchestratorConfig::default(),
                signer,
            ));
            let dispatcher = PrivilegedDispatcher::new().with_gate_orchestrator(orch);
            // Do NOT register any parent lease
            let ctx = ConnectionContext::privileged_session_open(Some(PeerCredentials {
                uid: 1000,
                gid: 1000,
                pid: Some(12345),
            }));

            let request = DelegateSubleaseRequest {
                parent_lease_id: "nonexistent-lease".to_string(),
                delegatee_actor_id: "child-executor-001".to_string(),
                requested_expiry_ns: 1_900_000_000_000,
                sublease_id: "sublease-002".to_string(),
                identity_proof_hash: vec![0x99; 32],
            };
            let frame = encode_delegate_sublease_request(&request);

            let response = dispatcher.dispatch(&frame, &ctx).unwrap();
            match response {
                PrivilegedResponse::Error(err) => {
                    assert!(
                        err.message.contains("parent gate lease not found"),
                        "Error should mention parent lease not found: {}",
                        err.message
                    );
                    assert_eq!(
                        err.code,
                        i32::from(PrivilegedErrorCode::GateLeaseMissing),
                        "Error code should be GateLeaseMissing"
                    );
                },
                other => panic!("Expected error for missing parent, got {other:?}"),
            }
        }

        #[test]
        fn test_delegate_sublease_expired_parent_rejected() {
            let (dispatcher, ctx, _parent) = setup_dispatcher_with_orchestrator(
                "parent-lease-exp",
                "W-DS-EXP",
                "gate-quality",
                "executor-001",
            );

            // Request sublease with expiry EXCEEDING parent bounds (parent
            // expires at 2_000_000, sublease requests 3_000_000).
            let request = DelegateSubleaseRequest {
                parent_lease_id: "parent-lease-exp".to_string(),
                delegatee_actor_id: "child-executor-001".to_string(),
                requested_expiry_ns: 3_000_000_000_000, /* Exceeds parent's expires_at (3_000_000
                                                         * ms) */
                sublease_id: "sublease-overflow".to_string(),
                identity_proof_hash: vec![0x99; 32],
            };
            let frame = encode_delegate_sublease_request(&request);

            let response = dispatcher.dispatch(&frame, &ctx).unwrap();
            match response {
                PrivilegedResponse::Error(err) => {
                    assert!(
                        err.message.contains("sublease delegation failed"),
                        "Error should mention sublease delegation failure: {}",
                        err.message
                    );
                },
                other => panic!("Expected error for expiry overflow, got {other:?}"),
            }
        }

        #[test]
        fn test_delegate_sublease_empty_parent_lease_id_rejected() {
            let signer = Arc::new(apm2_core::crypto::Signer::generate());
            let orch = Arc::new(crate::gate::GateOrchestrator::new(
                crate::gate::GateOrchestratorConfig::default(),
                signer,
            ));
            let dispatcher = PrivilegedDispatcher::new().with_gate_orchestrator(orch);
            let ctx = ConnectionContext::privileged_session_open(Some(PeerCredentials {
                uid: 1000,
                gid: 1000,
                pid: Some(12345),
            }));

            let request = DelegateSubleaseRequest {
                parent_lease_id: String::new(),
                delegatee_actor_id: "child-executor-001".to_string(),
                requested_expiry_ns: 1_900_000_000_000,
                sublease_id: "sublease-003".to_string(),
                identity_proof_hash: vec![0x99; 32],
            };
            let frame = encode_delegate_sublease_request(&request);

            let response = dispatcher.dispatch(&frame, &ctx).unwrap();
            match response {
                PrivilegedResponse::Error(err) => {
                    assert!(
                        err.message.contains("parent_lease_id is required"),
                        "Error should mention missing parent_lease_id: {}",
                        err.message
                    );
                },
                other => panic!("Expected error for empty parent_lease_id, got {other:?}"),
            }
        }

        #[test]
        fn test_delegate_sublease_empty_delegatee_rejected() {
            let signer = Arc::new(apm2_core::crypto::Signer::generate());
            let orch = Arc::new(crate::gate::GateOrchestrator::new(
                crate::gate::GateOrchestratorConfig::default(),
                signer,
            ));
            let dispatcher = PrivilegedDispatcher::new().with_gate_orchestrator(orch);
            let ctx = ConnectionContext::privileged_session_open(Some(PeerCredentials {
                uid: 1000,
                gid: 1000,
                pid: Some(12345),
            }));

            let request = DelegateSubleaseRequest {
                parent_lease_id: "parent-lease-001".to_string(),
                delegatee_actor_id: String::new(),
                requested_expiry_ns: 1_900_000_000_000,
                sublease_id: "sublease-004".to_string(),
                identity_proof_hash: vec![0x99; 32],
            };
            let frame = encode_delegate_sublease_request(&request);

            let response = dispatcher.dispatch(&frame, &ctx).unwrap();
            match response {
                PrivilegedResponse::Error(err) => {
                    assert!(
                        err.message.contains("delegatee_actor_id is required"),
                        "Error should mention missing delegatee_actor_id: {}",
                        err.message
                    );
                },
                other => panic!("Expected error for empty delegatee_actor_id, got {other:?}"),
            }
        }

        #[test]
        fn test_delegate_sublease_missing_identity_proof_hash_rejected() {
            let signer = Arc::new(apm2_core::crypto::Signer::generate());
            let orch = Arc::new(crate::gate::GateOrchestrator::new(
                crate::gate::GateOrchestratorConfig::default(),
                signer,
            ));
            let dispatcher = PrivilegedDispatcher::new().with_gate_orchestrator(orch);
            let ctx = ConnectionContext::privileged_session_open(Some(PeerCredentials {
                uid: 1000,
                gid: 1000,
                pid: Some(12345),
            }));

            let request = DelegateSubleaseRequest {
                parent_lease_id: "parent-lease-001".to_string(),
                delegatee_actor_id: "child-executor-001".to_string(),
                requested_expiry_ns: 1_900_000_000_000,
                sublease_id: "sublease-no-proof".to_string(),
                identity_proof_hash: vec![],
            };
            let frame = encode_delegate_sublease_request(&request);

            let response = dispatcher.dispatch(&frame, &ctx).unwrap();
            match response {
                PrivilegedResponse::Error(err) => {
                    assert!(
                        err.message
                            .contains("identity_proof_hash validation failed"),
                        "Expected identity_proof_hash validation error, got: {}",
                        err.message
                    );
                },
                other => panic!("Expected error for missing identity proof hash, got {other:?}"),
            }
        }

        #[test]
        fn test_delegate_sublease_zero_identity_proof_hash_rejected() {
            let signer = Arc::new(apm2_core::crypto::Signer::generate());
            let orch = Arc::new(crate::gate::GateOrchestrator::new(
                crate::gate::GateOrchestratorConfig::default(),
                signer,
            ));
            let dispatcher = PrivilegedDispatcher::new().with_gate_orchestrator(orch);
            let ctx = ConnectionContext::privileged_session_open(Some(PeerCredentials {
                uid: 1000,
                gid: 1000,
                pid: Some(12345),
            }));

            let request = DelegateSubleaseRequest {
                parent_lease_id: "parent-lease-002".to_string(),
                delegatee_actor_id: "child-executor-002".to_string(),
                requested_expiry_ns: 1_900_000_000_000,
                sublease_id: "sublease-zero-proof".to_string(),
                identity_proof_hash: vec![0x00; 32],
            };
            let frame = encode_delegate_sublease_request(&request);

            let response = dispatcher.dispatch(&frame, &ctx).unwrap();
            match response {
                PrivilegedResponse::Error(err) => {
                    assert!(
                        err.message.contains("null commitment rejected"),
                        "Expected null commitment rejection, got: {}",
                        err.message
                    );
                },
                other => {
                    panic!("Expected error for zero identity proof hash, got {other:?}")
                },
            }
        }

        #[test]
        fn test_delegate_sublease_no_orchestrator_rejected() {
            // Dispatcher without gate orchestrator configured
            let dispatcher = PrivilegedDispatcher::new();
            let ctx = ConnectionContext::privileged_session_open(Some(PeerCredentials {
                uid: 1000,
                gid: 1000,
                pid: Some(12345),
            }));

            let request = DelegateSubleaseRequest {
                parent_lease_id: "parent-lease-001".to_string(),
                delegatee_actor_id: "child-001".to_string(),
                requested_expiry_ns: 1_900_000_000_000,
                sublease_id: "sublease-005".to_string(),
                identity_proof_hash: vec![0x99; 32],
            };
            let frame = encode_delegate_sublease_request(&request);

            let response = dispatcher.dispatch(&frame, &ctx).unwrap();
            match response {
                PrivilegedResponse::Error(err) => {
                    assert!(
                        err.message.contains("gate orchestrator not configured"),
                        "Error should mention orchestrator not configured: {}",
                        err.message
                    );
                },
                other => panic!("Expected error for missing orchestrator, got {other:?}"),
            }
        }

        #[test]
        fn test_delegate_sublease_non_privileged_rejected() {
            let (dispatcher, _ctx, _parent) = setup_dispatcher_with_orchestrator(
                "parent-lease-np",
                "W-DS-NP",
                "gate-quality",
                "executor-001",
            );
            // Non-privileged context
            let ctx = ConnectionContext::session_open(
                Some(PeerCredentials {
                    uid: 1000,
                    gid: 1000,
                    pid: Some(12345),
                }),
                Some("test-session".to_string()),
            );

            let request = DelegateSubleaseRequest {
                parent_lease_id: "parent-lease-np".to_string(),
                delegatee_actor_id: "child-001".to_string(),
                requested_expiry_ns: 1_900_000_000_000,
                sublease_id: "sublease-006".to_string(),
                identity_proof_hash: vec![0x99; 32],
            };
            let frame = encode_delegate_sublease_request(&request);

            let response = dispatcher.dispatch(&frame, &ctx).unwrap();
            match response {
                PrivilegedResponse::Error(err) => {
                    assert!(
                        err.message.contains("permission denied"),
                        "Non-privileged connections must be rejected: {}",
                        err.message
                    );
                },
                other => panic!("Expected permission denied, got {other:?}"),
            }
        }

        /// Security BLOCKER: Unauthorized caller (different actor ID) must be
        /// explicitly rejected when attempting to delegate from a parent lease
        /// they do not own.
        #[test]
        fn test_delegate_sublease_unauthorized_caller_rejected() {
            let signer = Arc::new(apm2_core::crypto::Signer::generate());
            let orch = Arc::new(crate::gate::GateOrchestrator::new(
                crate::gate::GateOrchestratorConfig::default(),
                signer.clone(),
            ));

            // Register parent lease with executor_actor_id that does NOT
            // match the caller (uid=1000, gid=1000).
            let parent_lease =
                apm2_core::fac::GateLeaseBuilder::new("parent-authz", "W-AUTHZ", "gate-authz")
                    .changeset_digest([0x42; 32])
                    .executor_actor_id("totally-different-actor")
                    .issued_at(1_000_000)
                    .expires_at(2_000_000)
                    .policy_hash([0xAB; 32])
                    .issuer_actor_id("also-different-issuer")
                    .time_envelope_ref("htf:tick:100")
                    .build_and_sign(&signer);

            let dispatcher = PrivilegedDispatcher::new().with_gate_orchestrator(orch);
            dispatcher
                .lease_validator
                .register_full_lease(&parent_lease)
                .expect("register_full_lease should succeed in test");
            dispatcher.lease_validator.register_lease_with_executor(
                "parent-authz",
                "W-AUTHZ",
                "gate-authz",
                "totally-different-actor",
            );

            let ctx = ConnectionContext::privileged_session_open(Some(PeerCredentials {
                uid: 1000,
                gid: 1000,
                pid: Some(12345),
            }));

            let request = DelegateSubleaseRequest {
                parent_lease_id: "parent-authz".to_string(),
                delegatee_actor_id: "child-001".to_string(),
                requested_expiry_ns: 1_900_000_000_000,
                sublease_id: "sublease-authz".to_string(),
                identity_proof_hash: vec![0x99; 32],
            };
            let frame = encode_delegate_sublease_request(&request);

            let response = dispatcher.dispatch(&frame, &ctx).unwrap();
            match response {
                PrivilegedResponse::Error(err) => {
                    assert!(
                        err.message.contains("not authorized to delegate"),
                        "Unauthorized caller must get explicit rejection: {}",
                        err.message
                    );
                    assert_eq!(
                        err.code,
                        i32::from(PrivilegedErrorCode::PermissionDenied),
                        "Error code should be PermissionDenied"
                    );
                },
                other => panic!("Expected PermissionDenied for unauthorized caller, got {other:?}"),
            }
        }

        /// Security BLOCKER: Caller with no peer credentials must be rejected
        /// (fail-closed).
        #[test]
        fn test_delegate_sublease_no_peer_credentials_rejected() {
            let (dispatcher, _ctx, _parent) = setup_dispatcher_with_orchestrator(
                "parent-no-creds",
                "W-NC",
                "gate-quality",
                "executor-001",
            );

            // Context with NO peer credentials
            let ctx = ConnectionContext::privileged_session_open(None);

            let request = DelegateSubleaseRequest {
                parent_lease_id: "parent-no-creds".to_string(),
                delegatee_actor_id: "child-001".to_string(),
                requested_expiry_ns: 1_900_000_000_000,
                sublease_id: "sublease-no-creds".to_string(),
                identity_proof_hash: vec![0x99; 32],
            };
            let frame = encode_delegate_sublease_request(&request);

            let response = dispatcher.dispatch(&frame, &ctx).unwrap();
            match response {
                PrivilegedResponse::Error(err) => {
                    assert!(
                        err.message.contains("peer credentials required"),
                        "Missing creds must fail closed: {}",
                        err.message
                    );
                },
                other => panic!("Expected rejection for missing peer creds, got {other:?}"),
            }
        }

        #[test]
        fn test_delegate_sublease_message_type_tag() {
            assert_eq!(PrivilegedMessageType::DelegateSublease.tag(), 72);
            assert_eq!(
                PrivilegedMessageType::from_tag(72),
                Some(PrivilegedMessageType::DelegateSublease)
            );
        }

        #[test]
        fn test_delegate_sublease_response_encoding() {
            let resp = PrivilegedResponse::DelegateSublease(DelegateSubleaseResponse {
                sublease_id: "sub-001".to_string(),
                parent_lease_id: "parent-001".to_string(),
                delegatee_actor_id: "child-001".to_string(),
                gate_id: "gate-quality".to_string(),
                expires_at_ns: 1_900_000,
                event_id: "evt-001".to_string(),
            });
            let encoded = resp.encode();
            assert_eq!(
                encoded[0],
                PrivilegedMessageType::DelegateSublease.tag(),
                "First byte should be DelegateSublease tag (72)"
            );
            assert!(
                encoded.len() > 1,
                "Encoded response should have payload after tag"
            );
        }

        #[test]
        fn test_delegate_sublease_ledger_event_emitted() {
            let (dispatcher, ctx, _parent) = setup_dispatcher_with_orchestrator(
                "parent-lease-evt",
                "W-DS-EVT",
                "gate-quality",
                "executor-001",
            );

            // Derive the expected caller actor_id (same credentials as
            // setup_dispatcher_with_orchestrator uses: uid=1000, gid=1000).
            let expected_caller_actor = derive_actor_id(&PeerCredentials {
                uid: 1000,
                gid: 1000,
                pid: Some(12345),
            });

            let request = DelegateSubleaseRequest {
                parent_lease_id: "parent-lease-evt".to_string(),
                delegatee_actor_id: "child-executor-evt".to_string(),
                requested_expiry_ns: 1_900_000_000_000, // ns
                sublease_id: "sublease-evt-001".to_string(),
                identity_proof_hash: vec![0x99; 32],
            };
            let frame = encode_delegate_sublease_request(&request);

            let response = dispatcher.dispatch(&frame, &ctx).unwrap();
            let event_id = match response {
                PrivilegedResponse::DelegateSublease(resp) => resp.event_id,
                other => panic!("Expected DelegateSublease, got {other:?}"),
            };

            // Verify the event was persisted to the emitter
            let stored_event = dispatcher.event_emitter.get_event(&event_id);
            assert!(
                stored_event.is_some(),
                "SubleaseIssued event should be persisted in emitter"
            );
            let event = stored_event.unwrap();
            assert_eq!(event.event_type, "SubleaseIssued");
            // v5 Finding 3: actor_id must be the authenticated CALLER, not the
            // caller-controlled delegatee_actor_id.
            assert_eq!(
                event.actor_id, expected_caller_actor,
                "SubleaseIssued event must record authenticated caller, not delegatee"
            );
            assert_ne!(
                event.actor_id, "child-executor-evt",
                "actor_id must NOT be the delegatee"
            );
            assert!(event.timestamp_ns > 0, "Timestamp must be non-zero (HTF)");
        }

        #[test]
        fn test_delegate_sublease_duplicate_id_idempotent() {
            let (dispatcher, ctx, _parent) = setup_dispatcher_with_orchestrator(
                "parent-lease-dup",
                "W-DS-DUP",
                "gate-quality",
                "executor-dup",
            );

            let request = DelegateSubleaseRequest {
                parent_lease_id: "parent-lease-dup".to_string(),
                delegatee_actor_id: "child-executor-dup".to_string(),
                requested_expiry_ns: 1_900_000_000_000,
                sublease_id: "sublease-dup-001".to_string(),
                identity_proof_hash: vec![0x99; 32],
            };
            let frame = encode_delegate_sublease_request(&request);

            // First call: should succeed and return a non-empty event_id
            let response = dispatcher.dispatch(&frame, &ctx).unwrap();
            let original_event_id = match &response {
                PrivilegedResponse::DelegateSublease(resp) => {
                    assert_eq!(resp.sublease_id, "sublease-dup-001");
                    assert!(
                        !resp.event_id.is_empty(),
                        "First delegation must return non-empty event_id"
                    );
                    resp.event_id.clone()
                },
                other => panic!("Expected DelegateSublease success, got {other:?}"),
            };

            // Second call with same parameters: should return idempotent
            // result with the ORIGINAL event_id (not empty).
            let response2 = dispatcher.dispatch(&frame, &ctx).unwrap();
            match response2 {
                PrivilegedResponse::DelegateSublease(resp) => {
                    assert_eq!(
                        resp.sublease_id, "sublease-dup-001",
                        "Idempotent return should have same sublease_id"
                    );
                    assert_eq!(
                        resp.event_id, original_event_id,
                        "Idempotent return must replay the original SubleaseIssued event_id"
                    );
                    assert!(
                        !resp.event_id.is_empty(),
                        "Idempotent event_id must not be empty"
                    );
                },
                other => panic!("Expected idempotent DelegateSublease, got {other:?}"),
            }
        }

        #[test]
        fn test_delegate_sublease_duplicate_id_different_identity_proof_hash_rejected() {
            let (dispatcher, ctx, _parent) = setup_dispatcher_with_orchestrator(
                "parent-lease-dup-proof",
                "W-DS-DUP-PROOF",
                "gate-quality",
                "executor-dup-proof",
            );

            let req1 = DelegateSubleaseRequest {
                parent_lease_id: "parent-lease-dup-proof".to_string(),
                delegatee_actor_id: "child-executor-dup-proof".to_string(),
                requested_expiry_ns: 1_900_000_000_000,
                sublease_id: "sublease-dup-proof-001".to_string(),
                identity_proof_hash: vec![0x99; 32],
            };
            let frame1 = encode_delegate_sublease_request(&req1);
            let resp1 = dispatcher.dispatch(&frame1, &ctx).unwrap();
            assert!(
                matches!(resp1, PrivilegedResponse::DelegateSublease(_)),
                "first sublease should succeed"
            );

            // Same sublease_id, same logical parameters, but different proof pointer.
            // This is not idempotent and must be rejected.
            let req2 = DelegateSubleaseRequest {
                parent_lease_id: "parent-lease-dup-proof".to_string(),
                delegatee_actor_id: "child-executor-dup-proof".to_string(),
                requested_expiry_ns: req1.requested_expiry_ns,
                sublease_id: "sublease-dup-proof-001".to_string(),
                identity_proof_hash: vec![0x98; 32],
            };
            let frame2 = encode_delegate_sublease_request(&req2);
            let resp2 = dispatcher.dispatch(&frame2, &ctx).unwrap();
            match resp2 {
                PrivilegedResponse::Error(err) => {
                    assert!(
                        err.message.contains("identity_proof_hash"),
                        "expected identity_proof_hash mismatch rejection, got: {}",
                        err.message
                    );
                },
                other => panic!("expected identity_proof_hash mismatch rejection, got {other:?}"),
            }
        }

        #[test]
        fn test_delegate_sublease_duplicate_id_conflict_rejected() {
            let signer = Arc::new(apm2_core::crypto::Signer::generate());
            let orch = Arc::new(crate::gate::GateOrchestrator::new(
                crate::gate::GateOrchestratorConfig::default(),
                signer.clone(),
            ));

            // Derive the actor ID from the test peer credentials so the
            // caller authorization check passes for both parent leases.
            let test_creds = PeerCredentials {
                uid: 1000,
                gid: 1000,
                pid: Some(12345),
            };
            let caller_actor = derive_actor_id(&test_creds);

            // Create parent lease in gate "gate-A"
            let parent_a = apm2_core::fac::GateLeaseBuilder::new("parent-A", "W-A", "gate-A")
                .changeset_digest([0x42; 32])
                .executor_actor_id(&caller_actor)
                .issued_at(1_000_000)
                .expires_at(2_000_000)
                .policy_hash([0xAB; 32])
                .issuer_actor_id("issuer-001")
                .time_envelope_ref("htf:tick:100")
                .build_and_sign(&signer);

            // Create parent lease in gate "gate-B" (different parameters)
            let parent_b = apm2_core::fac::GateLeaseBuilder::new("parent-B", "W-B", "gate-B")
                .changeset_digest([0x42; 32])
                .executor_actor_id(&caller_actor)
                .issued_at(1_000_000)
                .expires_at(2_000_000)
                .policy_hash([0xAB; 32])
                .issuer_actor_id("issuer-001")
                .time_envelope_ref("htf:tick:200")
                .build_and_sign(&signer);

            let dispatcher = PrivilegedDispatcher::new().with_gate_orchestrator(orch);
            dispatcher
                .lease_validator
                .register_full_lease(&parent_a)
                .expect("register_full_lease should succeed in test");
            dispatcher
                .lease_validator
                .register_full_lease(&parent_b)
                .expect("register_full_lease should succeed in test");
            dispatcher.lease_validator.register_lease_with_executor(
                "parent-A",
                "W-A",
                "gate-A",
                &caller_actor,
            );
            dispatcher.lease_validator.register_lease_with_executor(
                "parent-B",
                "W-B",
                "gate-B",
                &caller_actor,
            );

            let ctx = ConnectionContext::privileged_session_open(Some(PeerCredentials {
                uid: 1000,
                gid: 1000,
                pid: Some(12345),
            }));

            // First call: issue sublease under parent A
            let req1 = DelegateSubleaseRequest {
                parent_lease_id: "parent-A".to_string(),
                delegatee_actor_id: "child-001".to_string(),
                requested_expiry_ns: 1_900_000_000_000,
                sublease_id: "shared-sublease-id".to_string(),
                identity_proof_hash: vec![0x99; 32],
            };
            let frame1 = encode_delegate_sublease_request(&req1);
            let resp1 = dispatcher.dispatch(&frame1, &ctx).unwrap();
            assert!(
                matches!(resp1, PrivilegedResponse::DelegateSublease(_)),
                "First sublease should succeed"
            );

            // Second call: try to issue sublease with SAME ID under parent B
            // (different work_id/gate_id) — must be rejected as conflict
            let req2 = DelegateSubleaseRequest {
                parent_lease_id: "parent-B".to_string(),
                delegatee_actor_id: "child-002".to_string(),
                requested_expiry_ns: 1_900_000_000_000,
                sublease_id: "shared-sublease-id".to_string(),
                identity_proof_hash: vec![0x99; 32],
            };
            let frame2 = encode_delegate_sublease_request(&req2);
            let resp2 = dispatcher.dispatch(&frame2, &ctx).unwrap();
            match resp2 {
                PrivilegedResponse::Error(err) => {
                    assert!(
                        err.message
                            .contains("already exists with different parameters"),
                        "Must reject conflicting sublease_id, got: {}",
                        err.message
                    );
                },
                other => panic!("Expected conflict rejection, got {other:?}"),
            }
        }

        /// v6 Finding 3: Same `sublease_id`, same work/gate/delegatee, but a
        /// different `requested_expiry_ns` must NOT be treated as idempotent.
        /// The full request tuple (including expiry) must match for
        /// idempotent return.
        #[test]
        fn test_delegate_sublease_different_expiry_rejected_as_conflict() {
            let (dispatcher, ctx, _parent) = setup_dispatcher_with_orchestrator(
                "parent-lease-exp",
                "W-DS-EXP",
                "gate-quality",
                "executor-exp",
            );

            // First request with expiry = 1_900_000_000_000 ns
            let req1 = DelegateSubleaseRequest {
                parent_lease_id: "parent-lease-exp".to_string(),
                delegatee_actor_id: "child-exp".to_string(),
                requested_expiry_ns: 1_900_000_000_000,
                sublease_id: "sublease-exp-001".to_string(),
                identity_proof_hash: vec![0x99; 32],
            };
            let frame1 = encode_delegate_sublease_request(&req1);
            let resp1 = dispatcher.dispatch(&frame1, &ctx).unwrap();
            assert!(
                matches!(resp1, PrivilegedResponse::DelegateSublease(_)),
                "First sublease should succeed"
            );

            // Second request with DIFFERENT expiry but same sublease_id
            let req2 = DelegateSubleaseRequest {
                parent_lease_id: "parent-lease-exp".to_string(),
                delegatee_actor_id: "child-exp".to_string(),
                requested_expiry_ns: 1_800_000_000_000, // Different expiry
                sublease_id: "sublease-exp-001".to_string(),
                identity_proof_hash: vec![0x99; 32],
            };
            let frame2 = encode_delegate_sublease_request(&req2);
            let resp2 = dispatcher.dispatch(&frame2, &ctx).unwrap();
            match resp2 {
                PrivilegedResponse::Error(err) => {
                    assert!(
                        err.message
                            .contains("already exists with different parameters"),
                        "Different expiry should be rejected as conflict, got: {}",
                        err.message
                    );
                },
                other => panic!("Expected conflict rejection for different expiry, got {other:?}"),
            }
        }

        /// v7 Finding 2: Idempotent replay with the same `sublease_id` but a
        /// different `parent_lease_id` must be rejected even when inherited
        /// fields (changeset digest, policy hash) happen to match. The
        /// parent lineage check uses the `SubleaseIssued` event payload's
        /// parent lease ID field to enforce exact lineage binding.
        #[test]
        fn test_delegate_sublease_idempotent_rejects_different_parent_lineage() {
            let signer = Arc::new(apm2_core::crypto::Signer::generate());
            let orch = Arc::new(crate::gate::GateOrchestrator::new(
                crate::gate::GateOrchestratorConfig::default(),
                signer.clone(),
            ));

            let test_creds = PeerCredentials {
                uid: 1000,
                gid: 1000,
                pid: Some(12345),
            };
            let caller_actor = derive_actor_id(&test_creds);

            // Create two parent leases with IDENTICAL inherited fields
            // (changeset_digest, policy_hash) but different lease IDs.
            // This is the edge case where the changeset_digest/policy_hash
            // comparison alone would incorrectly treat them as equivalent.
            let parent_a =
                apm2_core::fac::GateLeaseBuilder::new("parent-lin-A", "W-LIN", "gate-lin")
                    .changeset_digest([0x42; 32])
                    .executor_actor_id(&caller_actor)
                    .issued_at(1_000_000)
                    .expires_at(2_000_000)
                    .policy_hash([0xAB; 32])
                    .issuer_actor_id("issuer-001")
                    .time_envelope_ref("htf:tick:100")
                    .build_and_sign(&signer);

            let parent_b = apm2_core::fac::GateLeaseBuilder::new("parent-lin-B", "W-LIN", "gate-lin")
                    .changeset_digest([0x42; 32]) // Same digest
                    .executor_actor_id(&caller_actor)
                    .issued_at(1_000_000)
                    .expires_at(2_000_000)
                    .policy_hash([0xAB; 32]) // Same policy hash
                    .issuer_actor_id("issuer-001")
                    .time_envelope_ref("htf:tick:200")
                    .build_and_sign(&signer);

            let dispatcher = PrivilegedDispatcher::new().with_gate_orchestrator(orch);
            dispatcher
                .lease_validator
                .register_full_lease(&parent_a)
                .expect("register_full_lease should succeed in test");
            dispatcher
                .lease_validator
                .register_full_lease(&parent_b)
                .expect("register_full_lease should succeed in test");
            dispatcher.lease_validator.register_lease_with_executor(
                "parent-lin-A",
                "W-LIN",
                "gate-lin",
                &caller_actor,
            );
            dispatcher.lease_validator.register_lease_with_executor(
                "parent-lin-B",
                "W-LIN",
                "gate-lin",
                &caller_actor,
            );

            let ctx = ConnectionContext::privileged_session_open(Some(PeerCredentials {
                uid: 1000,
                gid: 1000,
                pid: Some(12345),
            }));

            // First call: issue sublease under parent A
            let req1 = DelegateSubleaseRequest {
                parent_lease_id: "parent-lin-A".to_string(),
                delegatee_actor_id: "child-lin".to_string(),
                requested_expiry_ns: 1_900_000_000_000,
                sublease_id: "sublease-lineage-001".to_string(),
                identity_proof_hash: vec![0x99; 32],
            };
            let frame1 = encode_delegate_sublease_request(&req1);
            let resp1 = dispatcher.dispatch(&frame1, &ctx).unwrap();
            assert!(
                matches!(resp1, PrivilegedResponse::DelegateSublease(_)),
                "First sublease should succeed"
            );

            // Second call: same sublease_id but different parent_lease_id.
            // Since both parents have identical changeset_digest/policy_hash,
            // the indirect lineage check (changeset_digest + policy_hash) would
            // pass, but the direct parent_lease_id check in the event payload
            // MUST reject this.
            let req2 = DelegateSubleaseRequest {
                parent_lease_id: "parent-lin-B".to_string(),
                delegatee_actor_id: "child-lin".to_string(),
                requested_expiry_ns: 1_900_000_000_000,
                sublease_id: "sublease-lineage-001".to_string(),
                identity_proof_hash: vec![0x99; 32],
            };
            let frame2 = encode_delegate_sublease_request(&req2);
            let resp2 = dispatcher.dispatch(&frame2, &ctx).unwrap();
            match resp2 {
                PrivilegedResponse::Error(err) => {
                    assert!(
                        err.message.contains("originally delegated from parent")
                            || err
                                .message
                                .contains("already exists with different parameters"),
                        "Must reject sublease with different parent lineage, got: {}",
                        err.message
                    );
                },
                other => panic!("Expected parent lineage rejection, got {other:?}"),
            }
        }
    }

    // ========================================================================
    // TCK-00340: Serde fail-closed default tests
    // ========================================================================
    mod serde_fail_closed {
        use super::*;

        #[test]
        fn test_policy_resolution_missing_risk_tier_defaults_to_tier4() {
            // SECURITY: When `resolved_risk_tier` is missing from JSON,
            // it must default to Tier4 (4), not Tier0 (0).
            let json = r#"{
                "policy_resolved_ref": "test-ref",
                "resolved_policy_hash": [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],
                "capability_manifest_hash": [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],
                "context_pack_hash": [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0]
            }"#;

            let resolution: PolicyResolution = serde_json::from_str(json)
                .expect("PolicyResolution should deserialize without resolved_risk_tier");
            assert_eq!(
                resolution.resolved_risk_tier, 4,
                "Missing resolved_risk_tier must default to Tier4 (4), not Tier0 (0) — fail-closed"
            );
        }

        #[test]
        fn test_policy_resolution_explicit_tier0_preserved() {
            // When `resolved_risk_tier` is explicitly set to 0, it must be preserved.
            let json = r#"{
                "policy_resolved_ref": "test-ref",
                "resolved_policy_hash": [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],
                "capability_manifest_hash": [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],
                "context_pack_hash": [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],
                "resolved_risk_tier": 0
            }"#;

            let resolution: PolicyResolution = serde_json::from_str(json)
                .expect("PolicyResolution should deserialize with explicit Tier0");
            assert_eq!(
                resolution.resolved_risk_tier, 0,
                "Explicit Tier0 must be preserved"
            );
        }
    }

    // ========================================================================
    // TCK-00340: SQLite integration tests (production path)
    // ========================================================================
    mod sqlite_integration {
        use std::sync::{Arc, Mutex};

        use apm2_core::evidence::{ContentAddressedStore, MemoryCas};
        use rusqlite::Connection;

        use super::*;
        use crate::ledger::{SqliteLeaseValidator, SqliteLedgerEventEmitter, SqliteWorkRegistry};

        /// Standard test artifact bundle content for sqlite integration tests.
        const TEST_ARTIFACT_CONTENT: &[u8] = b"test-artifact-bundle-content";

        /// Returns the CAS hash of `TEST_ARTIFACT_CONTENT`.
        fn test_artifact_bundle_hash() -> Vec<u8> {
            let cas = MemoryCas::default();
            let result = cas.store(TEST_ARTIFACT_CONTENT).unwrap();
            result.hash.to_vec()
        }

        /// Creates a `PrivilegedDispatcher` backed by real `SQLite`
        /// implementations for testing the production persistence path.
        /// TCK-00408: CAS is now mandatory for ingest (fail-closed).
        fn setup_sqlite_dispatcher() -> (
            PrivilegedDispatcher,
            ConnectionContext,
            Arc<Mutex<Connection>>,
        ) {
            let conn = Connection::open_in_memory().unwrap();
            SqliteLedgerEventEmitter::init_schema(&conn).unwrap();
            SqliteWorkRegistry::init_schema(&conn).unwrap();
            let conn = Arc::new(Mutex::new(conn));

            let signing_key = ed25519_dalek::SigningKey::generate(&mut rand::rngs::OsRng);
            let policy_resolver = Arc::new(StubPolicyResolver);
            let work_registry = Arc::new(SqliteWorkRegistry::new(Arc::clone(&conn)));
            let event_emitter = Arc::new(SqliteLedgerEventEmitter::new(
                Arc::clone(&conn),
                signing_key,
            ));
            let lease_validator: Arc<dyn LeaseValidator> =
                Arc::new(SqliteLeaseValidator::new(Arc::clone(&conn)));
            let session_registry: Arc<dyn SessionRegistry> =
                Arc::new(InMemorySessionRegistry::new());
            let clock = Arc::new(
                HolonicClock::new(ClockConfig::default(), None).expect("clock creation failed"),
            );
            let token_minter = Arc::new(TokenMinter::new(TokenMinter::generate_secret()));
            let manifest_store = Arc::new(InMemoryManifestStore::new());
            let manifest_loader: Arc<dyn ManifestLoader> =
                Arc::new(InMemoryCasManifestLoader::with_reviewer_v0_manifest());
            let subscription_registry: SharedSubscriptionRegistry =
                Arc::new(SubscriptionRegistry::with_defaults());

            // TCK-00408: CAS is mandatory for ingest (fail-closed).
            let cas = Arc::new(MemoryCas::default());
            cas.store(TEST_ARTIFACT_CONTENT).unwrap();

            let mut dispatcher = PrivilegedDispatcher::with_dependencies(
                DecodeConfig::default(),
                policy_resolver,
                work_registry,
                event_emitter,
                Arc::new(EpisodeRuntime::new(EpisodeRuntimeConfig::default())),
                session_registry,
                lease_validator,
                clock,
                token_minter,
                manifest_store,
                manifest_loader,
                subscription_registry,
            );
            dispatcher = dispatcher.with_cas(cas as Arc<dyn ContentAddressedStore>);

            let ctx = ConnectionContext::privileged_session_open(Some(PeerCredentials {
                uid: 1000,
                gid: 1000,
                pid: Some(12345),
            }));

            (dispatcher, ctx, conn)
        }

        #[test]
        fn test_ingest_review_receipt_sqlite_tier0_passes() {
            let (dispatcher, ctx, _conn) = setup_sqlite_dispatcher();

            // v6 Finding 1: Derive executor_actor_id from peer credentials
            // to match what the handler will derive.
            let executor_actor_id = derive_actor_id(&PeerCredentials {
                uid: 1000,
                gid: 1000,
                pid: Some(12345),
            });

            // Register lease via SqliteLeaseValidator (production path)
            dispatcher.lease_validator.register_lease_with_executor(
                "sqlite-lease-001",
                "W-SQL-001",
                "gate-sql",
                &executor_actor_id,
            );

            // Register work claim via SqliteWorkRegistry (production path)
            let claim = WorkClaim {
                work_id: "W-SQL-001".to_string(),
                lease_id: "sqlite-lease-001".to_string(),
                actor_id: executor_actor_id,
                role: WorkRole::Reviewer,
                policy_resolution: PolicyResolution {
                    policy_resolved_ref: "PolicyResolvedForChangeSet:W-SQL-001".to_string(),
                    resolved_policy_hash: [0u8; 32],
                    capability_manifest_hash: [0u8; 32],
                    context_pack_hash: [0u8; 32],
                    resolved_risk_tier: 0, // Tier0
                    resolved_scope_baseline: None,
                    expected_adapter_profile_hash: None,
                },
                executor_custody_domains: vec![],
                author_custody_domains: vec![],
            };
            dispatcher.work_registry.register_claim(claim).unwrap();

            let request = IngestReviewReceiptRequest {
                lease_id: "sqlite-lease-001".to_string(),
                receipt_id: "RR-SQL-001".to_string(),
                reviewer_actor_id: "ignored-by-handler".to_string(),
                changeset_digest: vec![0x42; 32],
                artifact_bundle_hash: test_artifact_bundle_hash(),
                verdict: ReviewReceiptVerdict::Approve.into(),
                blocked_reason_code: 0,
                blocked_log_hash: vec![],
                identity_proof_hash: vec![0x99; 32],
            };
            let frame = encode_ingest_review_receipt_request(&request);

            let response = dispatcher.dispatch(&frame, &ctx).unwrap();
            match response {
                PrivilegedResponse::IngestReviewReceipt(resp) => {
                    assert_eq!(resp.receipt_id, "RR-SQL-001");
                    assert_eq!(
                        resp.event_type, "ReviewReceiptRecorded",
                        "Tier0 with SelfSigned must pass through sqlite path"
                    );
                    assert!(
                        !resp.event_id.is_empty(),
                        "Event ID must be non-empty on sqlite success"
                    );
                },
                PrivilegedResponse::Error(err) => {
                    panic!(
                        "Tier0 SelfSigned should pass on sqlite path, got error: {}",
                        err.message
                    );
                },
                other => panic!("Expected IngestReviewReceipt, got {other:?}"),
            }
        }

        #[test]
        fn test_ingest_review_receipt_sqlite_higher_tier_rejected() {
            let (dispatcher, ctx, _conn) = setup_sqlite_dispatcher();

            // v6 Finding 1: Derive executor_actor_id from peer credentials
            let executor_actor_id = derive_actor_id(&PeerCredentials {
                uid: 1000,
                gid: 1000,
                pid: Some(12345),
            });

            // Register lease via SqliteLeaseValidator
            dispatcher.lease_validator.register_lease_with_executor(
                "sqlite-lease-t2",
                "W-SQL-T2",
                "gate-sql",
                &executor_actor_id,
            );

            // Register work claim at Tier2 (should be rejected)
            let claim = WorkClaim {
                work_id: "W-SQL-T2".to_string(),
                lease_id: "sqlite-lease-t2".to_string(),
                actor_id: executor_actor_id,
                role: WorkRole::Reviewer,
                policy_resolution: PolicyResolution {
                    policy_resolved_ref: "PolicyResolvedForChangeSet:W-SQL-T2".to_string(),
                    resolved_policy_hash: [0u8; 32],
                    capability_manifest_hash: [0u8; 32],
                    context_pack_hash: [0u8; 32],
                    resolved_risk_tier: 2, // Tier2
                    resolved_scope_baseline: None,
                    expected_adapter_profile_hash: None,
                },
                executor_custody_domains: vec![],
                author_custody_domains: vec![],
            };
            dispatcher.work_registry.register_claim(claim).unwrap();

            let request = IngestReviewReceiptRequest {
                lease_id: "sqlite-lease-t2".to_string(),
                receipt_id: "RR-SQL-T2".to_string(),
                reviewer_actor_id: "ignored-by-handler".to_string(),
                changeset_digest: vec![0x42; 32],
                artifact_bundle_hash: test_artifact_bundle_hash(),
                verdict: ReviewReceiptVerdict::Approve.into(),
                blocked_reason_code: 0,
                blocked_log_hash: vec![],
                identity_proof_hash: vec![0x99; 32],
            };
            let frame = encode_ingest_review_receipt_request(&request);

            let response = dispatcher.dispatch(&frame, &ctx).unwrap();
            match response {
                PrivilegedResponse::Error(err) => {
                    assert!(
                        err.message.contains("SelfSigned")
                            || err.message.contains("requires")
                            || err.message.contains("attestation"),
                        "Tier2 SelfSigned must be rejected on sqlite path, got: {}",
                        err.message
                    );
                },
                other => {
                    panic!("Expected Tier2 rejection on sqlite path, got {other:?}");
                },
            }
        }

        #[test]
        fn test_delegate_sublease_sqlite_valid_succeeds() {
            let (dispatcher, ctx, _conn) = setup_sqlite_dispatcher();

            let signer = Arc::new(apm2_core::crypto::Signer::generate());
            let orch = Arc::new(crate::gate::GateOrchestrator::new(
                crate::gate::GateOrchestratorConfig::default(),
                Arc::clone(&signer),
            ));

            // Derive the actor ID from the test peer credentials (uid=1000,
            // gid=1000) so the caller authorization check passes.
            let caller_actor = derive_actor_id(&PeerCredentials {
                uid: 1000,
                gid: 1000,
                pid: Some(12345),
            });

            let parent_lease =
                apm2_core::fac::GateLeaseBuilder::new("sql-parent", "W-SQL-DS", "gate-sql-ds")
                    .changeset_digest([0x42; 32])
                    .executor_actor_id(&caller_actor)
                    .issued_at(1_000_000)
                    .expires_at(2_000_000)
                    .policy_hash([0xAB; 32])
                    .issuer_actor_id("issuer-001")
                    .time_envelope_ref("htf:tick:100")
                    .build_and_sign(&signer);

            // Use production SqliteLeaseValidator path to register full lease
            dispatcher
                .lease_validator
                .register_full_lease(&parent_lease)
                .expect("register_full_lease should succeed in test");
            dispatcher.lease_validator.register_lease_with_executor(
                "sql-parent",
                "W-SQL-DS",
                "gate-sql-ds",
                &caller_actor,
            );

            // Wire orchestrator after construction (mimics DispatcherState flow)
            let dispatcher = dispatcher.with_gate_orchestrator(orch);

            let request = DelegateSubleaseRequest {
                parent_lease_id: "sql-parent".to_string(),
                delegatee_actor_id: "child-sql-exec".to_string(),
                requested_expiry_ns: 1_900_000_000_000,
                sublease_id: "sublease-sql-001".to_string(),
                identity_proof_hash: vec![0x99; 32],
            };
            let frame = encode_delegate_sublease_request(&request);

            let response = dispatcher.dispatch(&frame, &ctx).unwrap();
            match response {
                PrivilegedResponse::DelegateSublease(resp) => {
                    assert_eq!(resp.sublease_id, "sublease-sql-001");
                    assert_eq!(resp.gate_id, "gate-sql-ds");
                    assert!(
                        !resp.event_id.is_empty(),
                        "Event ID must be non-empty on sqlite success"
                    );
                },
                PrivilegedResponse::Error(err) => {
                    panic!(
                        "DelegateSublease should succeed on sqlite path, got error: {}",
                        err.message
                    );
                },
                other => panic!("Expected DelegateSublease, got {other:?}"),
            }
        }

        #[test]
        fn test_delegate_sublease_sqlite_invalid_parent_rejected() {
            let (dispatcher, ctx, _conn) = setup_sqlite_dispatcher();

            let signer = Arc::new(apm2_core::crypto::Signer::generate());
            let orch = Arc::new(crate::gate::GateOrchestrator::new(
                crate::gate::GateOrchestratorConfig::default(),
                signer,
            ));

            let dispatcher = dispatcher.with_gate_orchestrator(orch);

            // Don't register any parent lease — should fail
            let request = DelegateSubleaseRequest {
                parent_lease_id: "nonexistent-parent".to_string(),
                delegatee_actor_id: "child-sql".to_string(),
                requested_expiry_ns: 1_900_000_000_000,
                sublease_id: "sublease-sql-invalid".to_string(),
                identity_proof_hash: vec![0x99; 32],
            };
            let frame = encode_delegate_sublease_request(&request);

            let response = dispatcher.dispatch(&frame, &ctx).unwrap();
            match response {
                PrivilegedResponse::Error(err) => {
                    assert!(
                        err.message.contains("parent gate lease not found"),
                        "Must reject when parent lease not in sqlite, got: {}",
                        err.message
                    );
                },
                other => panic!("Expected parent-not-found rejection, got {other:?}"),
            }
        }

        /// Quality MAJOR: Integration test using `DispatcherState` production
        /// wiring to exercise `DelegateSublease` and `IngestReviewReceipt`
        /// through the production composition path. This proves that
        /// `DispatcherState` properly wires `gate_orchestrator` into the
        /// privileged dispatcher.
        #[test]
        fn test_dispatcher_state_delegate_sublease_production_wiring() {
            use std::os::unix::fs::PermissionsExt;

            use crate::cas::{DurableCas, DurableCasConfig};
            use crate::state::DispatcherState;

            let conn = Connection::open_in_memory().unwrap();
            SqliteLedgerEventEmitter::init_schema(&conn).unwrap();
            SqliteWorkRegistry::init_schema(&conn).unwrap();
            let conn = Arc::new(Mutex::new(conn));

            let session_registry: Arc<dyn SessionRegistry> =
                Arc::new(InMemorySessionRegistry::new());

            // TCK-00408: Use with_persistence_and_cas so CAS is wired —
            // IngestReviewReceipt now requires CAS (fail-closed).
            let cas_dir = tempfile::tempdir().expect("tempdir for CAS");
            std::fs::set_permissions(cas_dir.path(), std::fs::Permissions::from_mode(0o700))
                .expect("set CAS dir permissions");
            // Pre-populate CAS with test artifact before state construction.
            {
                let cas = DurableCas::new(DurableCasConfig::new(cas_dir.path().to_path_buf()))
                    .expect("pre-populate CAS");
                cas.store(TEST_ARTIFACT_CONTENT)
                    .expect("store test artifact");
            }
            let state = DispatcherState::with_persistence_and_cas(
                session_registry,
                None, // no metrics
                Arc::clone(&conn),
                cas_dir.path(),
            )
            .expect("CAS initialization must succeed");

            // Wire gate orchestrator via production path
            let signer = Arc::new(apm2_core::crypto::Signer::generate());
            let orch = Arc::new(crate::gate::GateOrchestrator::new(
                crate::gate::GateOrchestratorConfig::default(),
                Arc::clone(&signer),
            ));
            let state = state.with_gate_orchestrator(orch);

            // Derive caller actor from test peer credentials
            let test_creds = PeerCredentials {
                uid: 1000,
                gid: 1000,
                pid: Some(12345),
            };
            let caller_actor = derive_actor_id(&test_creds);

            // Register parent lease in the sqlite-backed dispatcher
            let parent_lease =
                apm2_core::fac::GateLeaseBuilder::new("ds-parent-prod", "W-PROD-001", "gate-prod")
                    .changeset_digest([0x42; 32])
                    .executor_actor_id(&caller_actor)
                    .issued_at(1_000_000)
                    .expires_at(2_000_000)
                    .policy_hash([0xAB; 32])
                    .issuer_actor_id("issuer-prod")
                    .time_envelope_ref("htf:tick:100")
                    .build_and_sign(&signer);

            state
                .privileged_dispatcher()
                .lease_validator
                .register_full_lease(&parent_lease)
                .expect("register_full_lease should succeed in test");
            state
                .privileged_dispatcher()
                .lease_validator
                .register_lease_with_executor(
                    "ds-parent-prod",
                    "W-PROD-001",
                    "gate-prod",
                    &caller_actor,
                );

            let ctx = ConnectionContext::privileged_session_open(Some(test_creds));

            // Exercise DelegateSublease through production-wired dispatcher
            let request = DelegateSubleaseRequest {
                parent_lease_id: "ds-parent-prod".to_string(),
                delegatee_actor_id: "child-prod-exec".to_string(),
                requested_expiry_ns: 1_900_000_000_000,
                sublease_id: "sublease-prod-001".to_string(),
                identity_proof_hash: vec![0x99; 32],
            };
            let frame = encode_delegate_sublease_request(&request);

            let response = state
                .privileged_dispatcher()
                .dispatch(&frame, &ctx)
                .unwrap();
            match response {
                PrivilegedResponse::DelegateSublease(resp) => {
                    assert_eq!(resp.sublease_id, "sublease-prod-001");
                    assert_eq!(resp.gate_id, "gate-prod");
                    assert_eq!(resp.delegatee_actor_id, "child-prod-exec");
                    assert!(
                        !resp.event_id.is_empty(),
                        "Event ID must be non-empty in production wiring path"
                    );
                },
                PrivilegedResponse::Error(err) => {
                    panic!(
                        "DelegateSublease must succeed via DispatcherState wiring, got: {}",
                        err.message
                    );
                },
                other => panic!("Expected DelegateSublease via production path, got {other:?}"),
            }

            // Also exercise IngestReviewReceipt through production wiring
            // to verify get_lease_work_id works with SqliteLeaseValidator.
            //
            // v6 Finding 1: The lease executor must be the derived actor_id
            // from peer credentials, since the handler now authenticates the
            // reviewer identity via peer credentials (not the request field).
            state
                .privileged_dispatcher()
                .lease_validator
                .register_lease_with_executor(
                    "review-lease-prod",
                    "W-REVIEW-001",
                    "gate-review",
                    &caller_actor,
                );

            let claim = WorkClaim {
                work_id: "W-REVIEW-001".to_string(),
                lease_id: "review-lease-prod".to_string(),
                actor_id: caller_actor,
                role: WorkRole::Reviewer,
                policy_resolution: PolicyResolution {
                    policy_resolved_ref: "PolicyResolvedForChangeSet:W-REVIEW-001".to_string(),
                    resolved_policy_hash: [0u8; 32],
                    capability_manifest_hash: [0u8; 32],
                    context_pack_hash: [0u8; 32],
                    resolved_risk_tier: 0, // Tier0
                    resolved_scope_baseline: None,
                    expected_adapter_profile_hash: None,
                },
                executor_custody_domains: vec![],
                author_custody_domains: vec![],
            };
            state
                .privileged_dispatcher()
                .work_registry
                .register_claim(claim)
                .unwrap();

            let review_request = IngestReviewReceiptRequest {
                lease_id: "review-lease-prod".to_string(),
                receipt_id: "RR-PROD-001".to_string(),
                reviewer_actor_id: "reviewer-prod".to_string(),
                changeset_digest: vec![0x42; 32],
                artifact_bundle_hash: test_artifact_bundle_hash(),
                verdict: ReviewReceiptVerdict::Approve.into(),
                blocked_reason_code: 0,
                blocked_log_hash: vec![],
                identity_proof_hash: vec![0x99; 32],
            };
            let review_frame = encode_ingest_review_receipt_request(&review_request);

            let review_response = state
                .privileged_dispatcher()
                .dispatch(&review_frame, &ctx)
                .unwrap();
            match review_response {
                PrivilegedResponse::IngestReviewReceipt(resp) => {
                    assert_eq!(resp.receipt_id, "RR-PROD-001");
                    assert_eq!(
                        resp.event_type, "ReviewReceiptRecorded",
                        "Tier0 SelfSigned must pass through DispatcherState production path"
                    );
                    assert!(
                        !resp.event_id.is_empty(),
                        "Event ID must be non-empty on production path success"
                    );
                },
                PrivilegedResponse::Error(err) => {
                    panic!(
                        "IngestReviewReceipt must succeed via DispatcherState wiring, got: {}",
                        err.message
                    );
                },
                other => panic!("Expected IngestReviewReceipt via production path, got {other:?}"),
            }
        }

        /// Integration test: exercises the production `GovernancePolicyResolver
        /// -> ClaimWork -> IngestReviewReceipt` path end-to-end.
        /// Verifies that the governance resolver's transitional Tier1
        /// mapping permits `SelfSigned` attestation through the review
        /// receipt handler.
        ///
        /// This test was added as part of TCK-00340 quality fix to prevent
        /// regression where hardcoded Tier4 would block all production claims.
        #[test]
        fn test_governance_resolver_claim_then_ingest_review_receipt_production_path() {
            use std::os::unix::fs::PermissionsExt;

            use crate::cas::{DurableCas, DurableCasConfig};
            use crate::governance::GovernancePolicyResolver;
            use crate::state::DispatcherState;

            let conn = Connection::open_in_memory().unwrap();
            SqliteLedgerEventEmitter::init_schema(&conn).unwrap();
            SqliteWorkRegistry::init_schema(&conn).unwrap();
            let conn = Arc::new(Mutex::new(conn));

            let session_registry: Arc<dyn SessionRegistry> =
                Arc::new(InMemorySessionRegistry::new());

            // TCK-00408: Use with_persistence_and_cas — IngestReviewReceipt
            // now requires CAS (fail-closed).
            let cas_dir = tempfile::tempdir().expect("tempdir for CAS");
            std::fs::set_permissions(cas_dir.path(), std::fs::Permissions::from_mode(0o700))
                .expect("set CAS dir permissions");
            {
                let cas = DurableCas::new(DurableCasConfig::new(cas_dir.path().to_path_buf()))
                    .expect("pre-populate CAS");
                cas.store(TEST_ARTIFACT_CONTENT)
                    .expect("store test artifact");
            }
            let state = DispatcherState::with_persistence_and_cas(
                session_registry,
                None, // no metrics
                Arc::clone(&conn),
                cas_dir.path(),
            )
            .expect("CAS initialization must succeed");

            // Derive caller actor from test peer credentials
            let test_creds = PeerCredentials {
                uid: 1000,
                gid: 1000,
                pid: Some(12345),
            };
            let caller_actor = derive_actor_id(&test_creds);
            let ctx = ConnectionContext::privileged_session_open(Some(test_creds));

            // Step 1: Use GovernancePolicyResolver to produce the PolicyResolution
            // (this is what ClaimWork calls in production with_persistence path)
            let governance_resolver = GovernancePolicyResolver::new();
            let policy_resolution = governance_resolver
                .resolve_for_claim("W-GOV-001", WorkRole::Reviewer, &caller_actor)
                .expect("GovernancePolicyResolver must succeed");

            // Verify the governance resolver returns Tier1 (not Tier4)
            assert_eq!(
                policy_resolution.resolved_risk_tier, 1,
                "GovernancePolicyResolver must return Tier1 for transitional mapping"
            );

            // Step 2: Register the work claim with the governance-produced resolution
            let claim = WorkClaim {
                work_id: "W-GOV-001".to_string(),
                lease_id: "lease-gov-001".to_string(),
                actor_id: caller_actor.clone(),
                role: WorkRole::Reviewer,
                policy_resolution,
                executor_custody_domains: vec![],
                author_custody_domains: vec![],
            };
            state
                .privileged_dispatcher()
                .work_registry
                .register_claim(claim)
                .expect("Claim registration must succeed");

            // Step 3: Register the lease for reviewer identity validation
            state
                .privileged_dispatcher()
                .lease_validator
                .register_lease_with_executor(
                    "lease-gov-001",
                    "W-GOV-001",
                    "gate-gov",
                    &caller_actor,
                );

            // Step 4: Submit IngestReviewReceipt with SelfSigned attestation
            let review_request = IngestReviewReceiptRequest {
                lease_id: "lease-gov-001".to_string(),
                receipt_id: "RR-GOV-001".to_string(),
                reviewer_actor_id: caller_actor,
                changeset_digest: vec![0x42; 32],
                artifact_bundle_hash: test_artifact_bundle_hash(),
                verdict: ReviewReceiptVerdict::Approve.into(),
                blocked_reason_code: 0,
                blocked_log_hash: vec![],
                identity_proof_hash: vec![0x99; 32],
            };
            let review_frame = encode_ingest_review_receipt_request(&review_request);

            let review_response = state
                .privileged_dispatcher()
                .dispatch(&review_frame, &ctx)
                .unwrap();
            match review_response {
                PrivilegedResponse::IngestReviewReceipt(resp) => {
                    assert_eq!(resp.receipt_id, "RR-GOV-001");
                    assert_eq!(
                        resp.event_type, "ReviewReceiptRecorded",
                        "Governance-resolved Tier1 claim with SelfSigned attestation \
                         must pass through production path"
                    );
                    assert!(
                        !resp.event_id.is_empty(),
                        "Event ID must be non-empty for governance-resolved production path"
                    );
                },
                PrivilegedResponse::Error(err) => {
                    panic!(
                        "IngestReviewReceipt MUST succeed for governance-resolved Tier1 claim \
                         with SelfSigned attestation. Got error: {}. This is the TCK-00340 \
                         regression where hardcoded Tier4 blocked all production claims.",
                        err.message
                    );
                },
                other => panic!(
                    "Expected IngestReviewReceipt via governance-resolved path, got {other:?}"
                ),
            }
        }
    }

    // ========================================================================
    // TCK-00349: Session-typed state machine and fail-closed decoding tests
    // ========================================================================
    mod tck_00349_session_state_machine {
        use super::*;

        /// TCK-00349: Verify that `ConnectionContext` starts in `Connected`
        /// phase.
        #[test]
        fn test_context_starts_in_connected_phase() {
            use crate::protocol::connection_handler::ConnectionPhase;

            let ctx = ConnectionContext::privileged(Some(PeerCredentials {
                uid: 1000,
                gid: 1000,
                pid: Some(12345),
            }));
            assert_eq!(ctx.phase(), ConnectionPhase::Connected);
            assert!(!ctx.phase().allows_dispatch());
        }

        /// TCK-00349: Verify full phase progression on `ConnectionContext`.
        #[test]
        fn test_context_full_phase_progression() {
            use crate::protocol::connection_handler::ConnectionPhase;

            let mut ctx = ConnectionContext::privileged(Some(PeerCredentials {
                uid: 1000,
                gid: 1000,
                pid: Some(12345),
            }));

            // Connected -> HandshakeComplete
            ctx.advance_to_handshake_complete().unwrap();
            assert_eq!(ctx.phase(), ConnectionPhase::HandshakeComplete);
            assert!(!ctx.phase().allows_dispatch());

            // HandshakeComplete -> SessionOpen
            ctx.advance_to_session_open().unwrap();
            assert_eq!(ctx.phase(), ConnectionPhase::SessionOpen);
            assert!(ctx.phase().allows_dispatch());
        }

        /// TCK-00349: Verify that dispatch is rejected in Connected phase.
        #[test]
        fn test_dispatch_rejected_in_connected_phase() {
            let dispatcher = PrivilegedDispatcher::new();
            let ctx = ConnectionContext::privileged(Some(PeerCredentials {
                uid: 1000,
                gid: 1000,
                pid: Some(12345),
            }));

            // Create a valid ClaimWork frame
            let request = ClaimWorkRequest {
                actor_id: "test".to_string(),
                role: WorkRole::Implementer.into(),
                credential_signature: vec![],
                nonce: vec![],
            };
            let frame = encode_claim_work_request(&request);

            let response = dispatcher.dispatch(&frame, &ctx).unwrap();
            match response {
                PrivilegedResponse::Error(err) => {
                    assert!(
                        err.message.contains("not SessionOpen"),
                        "Expected phase error, got: {}",
                        err.message
                    );
                },
                other => panic!("Expected error for pre-SessionOpen dispatch, got {other:?}"),
            }
        }

        /// TCK-00349: Verify that dispatch is rejected in `HandshakeComplete`
        /// phase.
        #[test]
        fn test_dispatch_rejected_in_handshake_complete_phase() {
            let dispatcher = PrivilegedDispatcher::new();
            let mut ctx = ConnectionContext::privileged(Some(PeerCredentials {
                uid: 1000,
                gid: 1000,
                pid: Some(12345),
            }));
            ctx.advance_to_handshake_complete().unwrap();

            let request = ClaimWorkRequest {
                actor_id: "test".to_string(),
                role: WorkRole::Implementer.into(),
                credential_signature: vec![],
                nonce: vec![],
            };
            let frame = encode_claim_work_request(&request);

            let response = dispatcher.dispatch(&frame, &ctx).unwrap();
            match response {
                PrivilegedResponse::Error(err) => {
                    assert!(
                        err.message.contains("not SessionOpen"),
                        "Expected phase error, got: {}",
                        err.message
                    );
                },
                other => panic!("Expected error for HandshakeComplete dispatch, got {other:?}"),
            }
        }

        /// TCK-00349: Verify that dispatch succeeds in `SessionOpen` phase.
        #[test]
        fn test_dispatch_succeeds_in_session_open_phase() {
            let dispatcher = PrivilegedDispatcher::new();
            let ctx = ConnectionContext::privileged_session_open(Some(PeerCredentials {
                uid: 1000,
                gid: 1000,
                pid: Some(12345),
            }));

            let request = ClaimWorkRequest {
                actor_id: "test".to_string(),
                role: WorkRole::Implementer.into(),
                credential_signature: vec![],
                nonce: vec![],
            };
            let frame = encode_claim_work_request(&request);

            let response = dispatcher.dispatch(&frame, &ctx).unwrap();
            // Should route to handler (ClaimWork response, not error)
            assert!(
                matches!(response, PrivilegedResponse::ClaimWork(_)),
                "Dispatch in SessionOpen phase should route to handler"
            );
        }

        /// TCK-00349: Verify unknown tag is rejected as protocol error.
        #[test]
        fn test_unknown_tag_returns_protocol_error() {
            let dispatcher = PrivilegedDispatcher::new();
            let ctx = ConnectionContext::privileged_session_open(Some(PeerCredentials {
                uid: 1000,
                gid: 1000,
                pid: Some(12345),
            }));

            // Tag 200 is not a valid message type
            let frame = Bytes::from(vec![200u8, 0, 0, 0]);
            let result = dispatcher.dispatch(&frame, &ctx);
            assert!(
                result.is_err(),
                "Unknown tag must return protocol error (fail-closed)"
            );
        }

        /// TCK-00349: Verify `privileged_session_open` convenience constructor
        /// creates context in correct phase.
        #[test]
        fn test_privileged_session_open_constructor() {
            use crate::protocol::connection_handler::ConnectionPhase;

            let ctx = ConnectionContext::privileged_session_open(Some(PeerCredentials {
                uid: 1000,
                gid: 1000,
                pid: Some(12345),
            }));
            assert_eq!(ctx.phase(), ConnectionPhase::SessionOpen);
            assert!(ctx.is_privileged());
        }

        /// TCK-00349: Verify `session_open` convenience constructor creates
        /// context in correct phase.
        #[test]
        fn test_session_open_constructor() {
            use crate::protocol::connection_handler::ConnectionPhase;

            let ctx = ConnectionContext::session_open(
                Some(PeerCredentials {
                    uid: 1000,
                    gid: 1000,
                    pid: Some(12346),
                }),
                Some("sess-001".to_string()),
            );
            assert_eq!(ctx.phase(), ConnectionPhase::SessionOpen);
            assert!(!ctx.is_privileged());
        }
    }

    // ========================================================================
    // BLOCKER 2 v3: Risk tier ceiling from policy resolution (not manifest)
    // ========================================================================
    mod risk_tier_ceiling_policy_binding {
        use crate::episode::envelope::RiskTier;

        /// BLOCKER 2 v3: Risk tier ceiling MUST be derived from policy
        /// resolution, not from the manifest. This test proves that
        /// `RiskTier::from_u8(claim.policy_resolution.resolved_risk_tier)`
        /// correctly bounds the ceiling, and that an invalid tier value
        /// falls back to Tier4 (fail-closed).
        #[test]
        fn risk_tier_ceiling_from_policy_resolution_not_manifest() {
            // Policy says Tier1 -- even if manifest capabilities are Tier3,
            // the ceiling should be Tier1.
            let policy_resolved_tier: u8 = 1;
            let ceiling = RiskTier::from_u8(policy_resolved_tier).unwrap_or(RiskTier::Tier4);
            assert_eq!(
                ceiling,
                RiskTier::Tier1,
                "Ceiling should match policy-resolved tier, not manifest capabilities"
            );
        }

        /// BLOCKER 2 v3: Invalid risk tier in policy resolution fails closed
        /// to Tier4 (most restrictive).
        #[test]
        fn invalid_risk_tier_falls_back_to_tier4_fail_closed() {
            let invalid_tier: u8 = 255;
            let ceiling = RiskTier::from_u8(invalid_tier).unwrap_or(RiskTier::Tier4);
            assert_eq!(
                ceiling,
                RiskTier::Tier4,
                "Invalid risk tier must fail closed to Tier4"
            );
        }

        /// BLOCKER 2 v3: Each valid tier value round-trips correctly.
        #[test]
        fn all_valid_tiers_round_trip() {
            for tier_val in 0..=4u8 {
                let tier = RiskTier::from_u8(tier_val)
                    .unwrap_or_else(|| panic!("Tier {tier_val} should be valid"));
                assert_eq!(
                    tier.tier(),
                    tier_val,
                    "Tier value should round-trip: expected {tier_val}, got {}",
                    tier.tier()
                );
            }
        }
    }

    // ========================================================================
    // MAJOR 1 v3: Scope baseline from policy resolution (not manifest)
    // ========================================================================
    mod scope_baseline_policy_binding {
        use super::*;

        /// MAJOR 1 v3: When `resolved_scope_baseline` is None in
        /// `PolicyResolution`, V1 minting should not proceed (fail-closed).
        /// This test verifies the None check logic independently.
        #[test]
        fn none_scope_baseline_is_fail_closed() {
            let resolution = PolicyResolution {
                policy_resolved_ref: "test".to_string(),
                resolved_policy_hash: [0u8; 32],
                capability_manifest_hash: [0u8; 32],
                context_pack_hash: [0u8; 32],
                resolved_risk_tier: 0,
                resolved_scope_baseline: None,
                expected_adapter_profile_hash: None,
            };

            assert!(
                resolution.resolved_scope_baseline.is_none(),
                "None baseline must trigger fail-closed in V1 minting path"
            );
        }

        /// MAJOR 1 v3: When `resolved_scope_baseline` is Some with a valid
        /// baseline, the scope validation should use that baseline (not the
        /// manifest being validated).
        #[test]
        fn some_scope_baseline_used_for_validation() {
            use crate::episode::ToolClass;
            use crate::episode::capability::ScopeBaseline;

            let baseline = ScopeBaseline {
                tools: vec![ToolClass::Read, ToolClass::Git],
                write_paths: vec![],
                shell_patterns: vec![],
            };
            let resolution = PolicyResolution {
                policy_resolved_ref: "test".to_string(),
                resolved_policy_hash: [0u8; 32],
                capability_manifest_hash: [0u8; 32],
                context_pack_hash: [0u8; 32],
                resolved_risk_tier: 0,
                resolved_scope_baseline: Some(baseline),
                expected_adapter_profile_hash: None,
            };

            let resolved = resolution.resolved_scope_baseline.unwrap();
            assert_eq!(
                resolved.tools.len(),
                2,
                "Baseline should have 2 tools from policy"
            );
            assert_eq!(resolved.tools[0], ToolClass::Read);
            assert_eq!(resolved.tools[1], ToolClass::Git);
        }

        /// MAJOR 1 v3: A manifest with tools NOT in the policy baseline is
        /// rejected by `validate_manifest_scope_subset` when the baseline
        /// comes from `PolicyResolution`, not from the manifest itself.
        #[test]
        fn manifest_exceeding_policy_baseline_is_rejected() {
            use crate::episode::ToolClass;
            use crate::episode::capability::{ScopeBaseline, validate_manifest_scope_subset};

            // Policy only allows Read
            let policy_baseline = ScopeBaseline {
                tools: vec![ToolClass::Read],
                write_paths: vec![],
                shell_patterns: vec![],
            };

            // Manifest tries to use Read + Execute
            let manifest = crate::episode::CapabilityManifest::builder("attack")
                .delegator("attacker")
                .created_at(1000)
                .expires_at(2000)
                .tool_allowlist(vec![ToolClass::Read, ToolClass::Execute])
                .build()
                .unwrap();

            let result = validate_manifest_scope_subset(&manifest, &policy_baseline);
            assert!(
                result.is_err(),
                "manifest exceeding policy baseline must be rejected"
            );
        }
    }

    // ========================================================================
    // TCK-00352 v3 integration: SpawnEpisode with V1 store
    // ========================================================================
    mod v3_spawn_episode_v1_integration {
        use std::sync::Arc;

        use super::*;
        use crate::episode::capability::ScopeBaseline;
        use crate::episode::reviewer_manifest::{reviewer_v0_manifest, reviewer_v0_manifest_hash};
        use crate::episode::tool_class::ToolClass;
        use crate::protocol::session_dispatch::V1ManifestStore;

        fn priv_ctx() -> ConnectionContext {
            ConnectionContext::privileged_session_open(Some(PeerCredentials {
                uid: 1000,
                gid: 1000,
                pid: Some(12345),
            }))
        }

        /// TCK-00352 MAJOR 1 v3 integration: `SpawnEpisode` with V1 store
        /// enabled MUST reject when `resolved_scope_baseline` is `None`.
        #[test]
        fn spawn_v1_rejects_none_scope_baseline_integration() {
            let v1_store = Arc::new(V1ManifestStore::new());
            let dispatcher = PrivilegedDispatcher::new().with_v1_manifest_store(v1_store);
            let ctx = priv_ctx();

            // Pre-register claim with NO scope baseline
            let claim = WorkClaim {
                work_id: "W-V3-SCOPE-NONE".to_string(),
                lease_id: "L-V3-001".to_string(),
                actor_id: "actor:test-impl".to_string(),
                role: WorkRole::Implementer,
                policy_resolution: PolicyResolution {
                    policy_resolved_ref: "resolved-v3-none".to_string(),
                    resolved_policy_hash: [0u8; 32],
                    capability_manifest_hash: [0u8; 32],
                    context_pack_hash: [0u8; 32],
                    resolved_risk_tier: 1,
                    resolved_scope_baseline: None,
                    expected_adapter_profile_hash: None,
                },
                author_custody_domains: vec![],
                executor_custody_domains: vec![],
            };
            dispatcher.work_registry.register_claim(claim).unwrap();

            let request = SpawnEpisodeRequest {
                workspace_root: "/tmp".to_string(),
                work_id: "W-V3-SCOPE-NONE".to_string(),
                role: WorkRole::Implementer.into(),
                lease_id: Some("L-V3-001".to_string()),
                adapter_profile_hash: None,
                max_episodes: None,
                escalation_predicate: None,
            };
            let frame = encode_spawn_episode_request(&request);
            let response = dispatcher.dispatch(&frame, &ctx).unwrap();

            match response {
                PrivilegedResponse::Error(err) => {
                    assert_eq!(
                        err.code,
                        PrivilegedErrorCode::CapabilityRequestRejected as i32,
                        "Expected CapabilityRequestRejected for missing scope baseline"
                    );
                    assert!(
                        err.message.contains("scope baseline")
                            || err.message.contains("fail-closed"),
                        "Error should mention scope baseline or fail-closed: {}",
                        err.message
                    );
                },
                other => panic!("Expected rejection for None scope baseline, got: {other:?}"),
            }
        }

        /// TCK-00352 MAJOR 1 v3 integration: `SpawnEpisode` rejects a
        /// manifest whose tools exceed the policy-resolved scope baseline.
        #[test]
        fn spawn_v1_rejects_scope_exceeding_policy_baseline() {
            let v1_store = Arc::new(V1ManifestStore::new());
            let dispatcher = PrivilegedDispatcher::new().with_v1_manifest_store(v1_store);
            let ctx = priv_ctx();

            // Scope baseline only allows Read -- reviewer manifest has more
            let narrow_baseline = ScopeBaseline {
                tools: vec![ToolClass::Read],
                write_paths: Vec::new(),
                shell_patterns: Vec::new(),
            };
            let claim = WorkClaim {
                work_id: "W-V3-SCOPE-NARROW".to_string(),
                lease_id: "L-V3-002".to_string(),
                actor_id: "actor:test-reviewer".to_string(),
                role: WorkRole::Reviewer,
                policy_resolution: PolicyResolution {
                    policy_resolved_ref: "resolved-v3-narrow".to_string(),
                    resolved_policy_hash: [0u8; 32],
                    capability_manifest_hash: *reviewer_v0_manifest_hash(),
                    context_pack_hash: [0u8; 32],
                    resolved_risk_tier: 1,
                    resolved_scope_baseline: Some(narrow_baseline),
                    expected_adapter_profile_hash: None,
                },
                author_custody_domains: vec![],
                executor_custody_domains: vec![],
            };
            dispatcher.work_registry.register_claim(claim).unwrap();

            let request = SpawnEpisodeRequest {
                workspace_root: "/tmp".to_string(),
                work_id: "W-V3-SCOPE-NARROW".to_string(),
                role: WorkRole::Reviewer.into(),
                lease_id: Some("L-V3-002".to_string()),
                adapter_profile_hash: None,
                max_episodes: None,
                escalation_predicate: None,
            };
            let frame = encode_spawn_episode_request(&request);
            let response = dispatcher.dispatch(&frame, &ctx).unwrap();

            match response {
                PrivilegedResponse::Error(err) => {
                    assert_eq!(
                        err.code,
                        PrivilegedErrorCode::CapabilityRequestRejected as i32,
                        "Expected CapabilityRequestRejected for overbroad scope"
                    );
                    assert!(
                        err.message.contains("scope")
                            || err.message.contains("OverbroadScope")
                            || err.message.contains("baseline"),
                        "Error should mention scope failure: {}",
                        err.message
                    );
                },
                other => panic!("Expected rejection for overbroad scope, got: {other:?}"),
            }
        }

        /// TCK-00352 BLOCKER 2 v3 integration: Risk tier ceiling comes
        /// from `policy_resolution.resolved_risk_tier`, verified through
        /// the actual `SpawnEpisode` path with V1 store.
        #[test]
        fn spawn_v1_risk_ceiling_from_policy_integration() {
            let v1_store = Arc::new(V1ManifestStore::new());
            let dispatcher = PrivilegedDispatcher::new().with_v1_manifest_store(v1_store.clone());
            let ctx = priv_ctx();

            let reviewer = reviewer_v0_manifest();
            let matching_baseline = ScopeBaseline {
                tools: reviewer.tool_allowlist.clone(),
                write_paths: reviewer.write_allowlist.clone(),
                shell_patterns: reviewer.shell_allowlist.clone(),
            };

            // Policy says Tier0 -- the V1 ceiling must be Tier0
            let claim = WorkClaim {
                work_id: "W-V3-RISK-CEIL".to_string(),
                lease_id: "L-V3-003".to_string(),
                actor_id: "actor:test-reviewer".to_string(),
                role: WorkRole::Reviewer,
                policy_resolution: PolicyResolution {
                    policy_resolved_ref: "resolved-v3-risk".to_string(),
                    resolved_policy_hash: [0u8; 32],
                    capability_manifest_hash: *reviewer_v0_manifest_hash(),
                    context_pack_hash: [0u8; 32],
                    resolved_risk_tier: 0,
                    resolved_scope_baseline: Some(matching_baseline),
                    expected_adapter_profile_hash: None,
                },
                author_custody_domains: vec![],
                executor_custody_domains: vec![],
            };
            dispatcher.work_registry.register_claim(claim).unwrap();

            let request = SpawnEpisodeRequest {
                workspace_root: "/tmp".to_string(),
                work_id: "W-V3-RISK-CEIL".to_string(),
                role: WorkRole::Reviewer.into(),
                lease_id: Some("L-V3-003".to_string()),
                adapter_profile_hash: None,
                max_episodes: None,
                escalation_predicate: None,
            };
            let frame = encode_spawn_episode_request(&request);
            let response = dispatcher.dispatch(&frame, &ctx).unwrap();

            match &response {
                PrivilegedResponse::SpawnEpisode(resp) => {
                    let v1 = v1_store
                        .get(&resp.session_id)
                        .expect("V1 manifest should be registered");
                    assert_eq!(
                        v1.risk_tier_ceiling(),
                        crate::episode::envelope::RiskTier::Tier0,
                        "Risk tier ceiling must come from policy (Tier0)"
                    );
                },
                PrivilegedResponse::Error(err) => {
                    panic!("SpawnEpisode should succeed, got error: {err:?}");
                },
                other => panic!("Expected SpawnEpisode, got: {other:?}"),
            }
        }

        /// TCK-00352 BLOCKER 2 v3 integration: Invalid risk tier (255)
        /// must fail closed to Tier4 through the actual `SpawnEpisode` path.
        #[test]
        fn spawn_v1_invalid_risk_tier_fails_to_tier4_integration() {
            let v1_store = Arc::new(V1ManifestStore::new());
            let dispatcher = PrivilegedDispatcher::new().with_v1_manifest_store(v1_store.clone());
            let ctx = priv_ctx();

            let reviewer = reviewer_v0_manifest();
            let matching_baseline = ScopeBaseline {
                tools: reviewer.tool_allowlist.clone(),
                write_paths: reviewer.write_allowlist.clone(),
                shell_patterns: reviewer.shell_allowlist.clone(),
            };

            let claim = WorkClaim {
                work_id: "W-V3-RISK-INV".to_string(),
                lease_id: "L-V3-004".to_string(),
                actor_id: "actor:test-reviewer".to_string(),
                role: WorkRole::Reviewer,
                policy_resolution: PolicyResolution {
                    policy_resolved_ref: "resolved-v3-invalid".to_string(),
                    resolved_policy_hash: [0u8; 32],
                    capability_manifest_hash: *reviewer_v0_manifest_hash(),
                    context_pack_hash: [0u8; 32],
                    // Invalid tier value
                    resolved_risk_tier: 255,
                    resolved_scope_baseline: Some(matching_baseline),
                    expected_adapter_profile_hash: None,
                },
                author_custody_domains: vec![],
                executor_custody_domains: vec![],
            };
            dispatcher.work_registry.register_claim(claim).unwrap();

            let request = SpawnEpisodeRequest {
                workspace_root: "/tmp".to_string(),
                work_id: "W-V3-RISK-INV".to_string(),
                role: WorkRole::Reviewer.into(),
                lease_id: Some("L-V3-004".to_string()),
                adapter_profile_hash: None,
                max_episodes: None,
                escalation_predicate: None,
            };
            let frame = encode_spawn_episode_request(&request);
            let response = dispatcher.dispatch(&frame, &ctx).unwrap();

            match &response {
                PrivilegedResponse::SpawnEpisode(resp) => {
                    let v1 = v1_store
                        .get(&resp.session_id)
                        .expect("V1 manifest should be registered");
                    assert_eq!(
                        v1.risk_tier_ceiling(),
                        crate::episode::envelope::RiskTier::Tier4,
                        "Invalid risk tier must fail closed to Tier4"
                    );
                },
                PrivilegedResponse::Error(err) => {
                    // Also acceptable: minting fails due to Tier4 constraint
                    assert_eq!(
                        err.code,
                        PrivilegedErrorCode::CapabilityRequestRejected as i32,
                        "Error must be CapabilityRequestRejected: {err:?}"
                    );
                },
                other => panic!("Expected SpawnEpisode or Error, got: {other:?}"),
            }
        }
    }

    // =========================================================================
    // TCK-00399: build_harness_config tests
    // =========================================================================

    mod build_harness_config_tests {
        use apm2_core::fac::AgentAdapterProfileV1;
        use secrecy::ExposeSecret;

        use super::*;

        /// Creates a test profile by mutating the builtin `claude_code_profile`
        /// (which already passes all validation). We just override templates.
        fn test_profile_with_templates() -> AgentAdapterProfileV1 {
            let mut profile = apm2_core::fac::builtin_profiles::claude_code_profile();
            profile.args_template = vec![
                "--workspace".to_string(),
                "{workspace}".to_string(),
                "--prompt".to_string(),
                "{prompt}".to_string(),
            ];
            profile.env_template = vec![
                ("MY_WORKSPACE".to_string(), "{workspace}".to_string()),
                ("MY_PROMPT".to_string(), "{prompt}".to_string()),
            ];
            profile
        }

        #[test]
        fn test_template_expansion() {
            let profile = test_profile_with_templates();
            let token = secrecy::SecretString::from("test-token-123".to_string());

            let config = PrivilegedDispatcher::build_harness_config(
                &profile,
                "ep-001",
                "/home/user/workspace",
                "do something",
                "claude-code-v1",
                &token,
            )
            .expect("build_harness_config should succeed");

            // Check args were expanded
            assert_eq!(config.args[0], "--workspace");
            assert_eq!(config.args[1], "/home/user/workspace");
            assert_eq!(config.args[2], "--prompt");
            assert_eq!(config.args[3], "do something");

            // Check env was expanded
            assert_eq!(
                config.env.get("MY_WORKSPACE").unwrap().expose_secret(),
                "/home/user/workspace"
            );
            assert_eq!(
                config.env.get("MY_PROMPT").unwrap().expose_secret(),
                "do something"
            );

            // Check session token is in env (WVR-0002)
            assert_eq!(
                config
                    .env
                    .get("APM2_SESSION_TOKEN")
                    .unwrap()
                    .expose_secret(),
                "test-token-123"
            );
        }

        #[test]
        fn test_forbidden_env_rejected() {
            let mut profile = apm2_core::fac::builtin_profiles::claude_code_profile();
            profile.env_template = vec![("PATH".to_string(), "/malicious/path".to_string())];
            let token = secrecy::SecretString::from("token".to_string());

            let result = PrivilegedDispatcher::build_harness_config(
                &profile,
                "ep-001",
                "/workspace",
                "",
                "claude-code-v1",
                &token,
            );
            assert!(result.is_err());
            assert!(
                result.unwrap_err().contains("forbidden env var"),
                "Error should mention forbidden env var"
            );
        }

        #[test]
        fn test_ld_preload_rejected() {
            let mut profile = apm2_core::fac::builtin_profiles::claude_code_profile();
            profile.env_template = vec![("LD_PRELOAD".to_string(), "/evil.so".to_string())];
            let token = secrecy::SecretString::from("token".to_string());

            let result = PrivilegedDispatcher::build_harness_config(
                &profile,
                "ep-001",
                "/workspace",
                "",
                "claude-code-v1",
                &token,
            );
            assert!(result.is_err());
            assert!(result.unwrap_err().contains("LD_PRELOAD"));
        }

        #[test]
        fn test_session_token_in_argv_rejected() {
            let mut profile = apm2_core::fac::builtin_profiles::claude_code_profile();
            profile.args_template = vec!["--token".to_string(), "secret-token".to_string()];
            // Token value matches an argv value
            let token = secrecy::SecretString::from("secret-token".to_string());

            let result = PrivilegedDispatcher::build_harness_config(
                &profile,
                "ep-001",
                "/workspace",
                "",
                "claude-code-v1",
                &token,
            );
            assert!(result.is_err());
            assert!(
                result.unwrap_err().contains("session_token found in argv"),
                "Error should mention session_token in argv"
            );
        }

        #[test]
        fn test_model_template_expansion() {
            let mut profile = apm2_core::fac::builtin_profiles::claude_code_profile();
            profile.args_template = vec!["run".to_string(), "{model}".to_string()];
            profile.env_template = vec![];
            let token = secrecy::SecretString::from("token".to_string());

            let config = PrivilegedDispatcher::build_harness_config(
                &profile,
                "ep-001",
                "/workspace",
                "",
                "llama3",
                &token,
            )
            .expect("build_harness_config should succeed");

            assert_eq!(config.args[0], "run");
            assert_eq!(config.args[1], "llama3");
        }

        /// Known placeholder `{episode_id}` is NOT expanded by
        /// `build_harness_config` (only `{workspace}`, `{prompt}`, `{model}`
        /// are), so it must be rejected as unresolved.
        #[test]
        fn test_unresolved_known_placeholder_rejected() {
            let mut profile = apm2_core::fac::builtin_profiles::claude_code_profile();
            profile.args_template = vec!["run".to_string(), "{episode_id}".to_string()];
            profile.env_template = vec![];
            let token = secrecy::SecretString::from("token".to_string());

            let result = PrivilegedDispatcher::build_harness_config(
                &profile,
                "ep-001",
                "/workspace",
                "",
                "llama3",
                &token,
            );
            assert!(result.is_err());
            assert!(
                result
                    .unwrap_err()
                    .contains("unresolved template placeholder"),
                "Error should mention unresolved template placeholder"
            );
        }

        /// `{{workspace}}` IS expanded during template processing, so the
        /// result should succeed (no unresolved placeholder remains).
        #[test]
        fn test_expanded_known_placeholder_accepted() {
            let mut profile = apm2_core::fac::builtin_profiles::claude_code_profile();
            profile.args_template = vec!["run".to_string(), "prefix-{workspace}".to_string()];
            profile.env_template = vec![];
            let token = secrecy::SecretString::from("token".to_string());

            let result = PrivilegedDispatcher::build_harness_config(
                &profile,
                "ep-001",
                "/workspace",
                "",
                "llama3",
                &token,
            );
            assert!(
                result.is_ok(),
                "expanded known placeholder should be accepted, got error: {result:?}"
            );
        }

        /// Literal braces in args (e.g., JSON) must NOT be rejected.
        /// Only known placeholder tokens (`{workspace}`, `{prompt}`, etc.)
        /// should trigger the unresolved-placeholder guard.
        #[test]
        fn test_literal_braces_in_args_accepted() {
            let mut profile = apm2_core::fac::builtin_profiles::claude_code_profile();
            profile.args_template = vec![
                "--config".to_string(),
                r#"{"key": "value", "nested": {"a": 1}}"#.to_string(),
            ];
            profile.env_template = vec![];
            let token = secrecy::SecretString::from("token".to_string());

            let result = PrivilegedDispatcher::build_harness_config(
                &profile,
                "ep-001",
                "/workspace",
                "",
                "llama3",
                &token,
            );

            assert!(
                result.is_ok(),
                "literal braces in JSON args should be accepted, got error: {result:?}"
            );
        }

        /// Unknown placeholders (not in the known set) must be accepted.
        /// Only `{workspace}`, `{prompt}`, `{model}`, and `{episode_id}`
        /// are flagged.
        #[test]
        fn test_unknown_placeholder_not_in_known_set_accepted() {
            let mut profile = apm2_core::fac::builtin_profiles::claude_code_profile();
            profile.args_template = vec!["run".to_string(), "{custom_flag}".to_string()];
            profile.env_template = vec![];
            let token = secrecy::SecretString::from("token".to_string());

            let result = PrivilegedDispatcher::build_harness_config(
                &profile,
                "ep-001",
                "/workspace",
                "",
                "llama3",
                &token,
            );

            assert!(
                result.is_ok(),
                "unknown placeholder not in known set should be accepted, got error: {result:?}"
            );
        }
    }
}
