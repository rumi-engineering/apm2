//! Privileged endpoint dispatcher for RFC-0017 control-plane IPC.
//!
//! This module implements the privileged endpoint dispatcher per DD-001 and
//! DD-009. Privileged endpoints (ClaimWork, SpawnEpisode, IssueCapability,
//! Shutdown) are only accessible via the operator socket. Session socket
//! connections receive `PERMISSION_DENIED` for all privileged requests.
//!
//! # Security Invariants
//!
//! - [INV-0001] An agent cannot execute privileged IPC operations
//! - [TB-002] Privilege separation boundary: session connections blocked from
//!   privileged handlers
//! - [TCK-00253] Actor_id derived from credential, not user input
//!
//! # Message Flow
//!
//! ```text
//! ┌─────────────────┐     ┌─────────────────┐     ┌─────────────────┐
//! │ operator.sock   │────▶│ PrivilegedDispatch │──▶│ Handler Stubs  │
//! └─────────────────┘     └─────────────────┘     └─────────────────┘
//!                                │
//!                                │ `PERMISSION_DENIED`
//!                                ▼
//! ┌─────────────────┐     ┌─────────────────┐
//! │ session.sock    │────▶│  (Rejected)     │
//! └─────────────────┘     └─────────────────┘
//! ```

use std::collections::{BTreeMap, BTreeSet, HashMap, HashSet, VecDeque};
use std::sync::atomic::{AtomicBool, Ordering};
use std::sync::{Arc, Mutex};
use std::time::{Duration, SystemTime};

use apm2_core::channel::{
    BoundaryFlowPolicyBinding, ChannelBoundaryCheck, ChannelBoundaryDefect, ChannelSource,
    ChannelViolationClass, DeclassificationIntentScope, DisclosurePolicyBinding,
    LeakageBudgetReceipt, LeakageEstimatorFamily, RedundancyDeclassificationReceipt,
    TimingChannelBudget, derive_channel_source_witness, issue_channel_context_token,
    validate_channel_boundary,
};
use apm2_core::credentials::{
    AuthMethod, CredentialProfile as CoreCredentialProfile, CredentialStore, ProfileId, Provider,
};
use apm2_core::determinism::canonicalize_json;
use apm2_core::disclosure::{DisclosureChannelClass, DisclosurePolicyMode};
use apm2_core::events::{DefectRecorded, DefectSource, Validate};
use apm2_core::evidence::{ContentAddressedStore, MemoryCas};
use apm2_core::fac::{
    AdapterSelectionPolicy, AttestationLevel, AttestationRequirements, AuditorLaunchProjectionV1,
    CHANGESET_PUBLISHED_PREFIX, DenyCondition, DenyReasonCode,
    FAC_WORKOBJECT_IMPLEMENTOR_V2_ROLE_ID, OrchestratorLaunchProjectionV1, ProjectionUncertainty,
    REVIEW_BLOCKED_RECORDED_PREFIX, REVIEW_RECEIPT_RECORDED_PREFIX, ReceiptAttestation,
    ReceiptKind, RiskTier, SelectionDecision, builtin_profiles, digest_first_projection,
    fac_workobject_implementor_v2_role_contract, seed_builtin_role_contracts_v2_in_cas,
    validate_receipt_attestation,
};
use apm2_core::htf::{Canonicalizable, ClockProfile, TimeEnvelope, WallTimeSource};
use apm2_core::liveness::{
    HealthVerdict, LivenessHeartbeatReceiptV1, MAX_RESTARTS_LIMIT, MAX_RUN_ID_LENGTH,
};
use apm2_core::pcac::{
    AuthorityDenyClass, AuthorityJoinInputV1, DeterminismClass as PcacDeterminismClass,
    IdentityEvidenceLevel, RiskTier as PcacRiskTier,
};
use apm2_core::policy::permeability::{
    DELEGATION_SATISFIABILITY_SCHEMA_ID, DELEGATION_SATISFIABILITY_SCHEMA_MAJOR,
    DelegationSatisfiabilityReceiptV1,
};
use apm2_core::process::ProcessState;
use bytes::Bytes;
use prost::Message;
use secrecy::SecretString;
use sha2::{Digest, Sha256};
use subtle::ConstantTimeEq;
use tracing::{debug, error, info, warn};

use super::credentials::PeerCredentials;
use super::error::{ProtocolError, ProtocolResult};
use super::messages::{
    AddCredentialRequest,
    AddCredentialResponse,
    AuditorLaunchProjectionRequest,
    AuditorLaunchProjectionResponse,
    BoundedDecode,
    ClaimWorkRequest,
    ClaimWorkResponse,
    ConsensusByzantineEvidenceRequest,
    ConsensusByzantineEvidenceResponse,
    ConsensusErrorCode,
    ConsensusMetricsRequest,
    ConsensusMetricsResponse,
    ConsensusStatusRequest,
    ConsensusStatusResponse,
    ConsensusValidatorsRequest,
    ConsensusValidatorsResponse,
    CredentialAuthMethod as ProtoAuthMethod,
    CredentialProvider as ProtoProvider,
    DecodeConfig,
    // TCK-00340: Sublease delegation types
    DelegateSubleaseRequest,
    DelegateSubleaseResponse,
    EndSessionRequest,
    EndSessionResponse,
    // TCK-00389: Review receipt ingestion types
    IngestReviewReceiptRequest,
    IngestReviewReceiptResponse,
    IssueCapabilityRequest,
    IssueCapabilityResponse,
    ListCredentialsRequest,
    ListCredentialsResponse,
    ListProcessesRequest,
    ListProcessesResponse,
    LoginCredentialRequest,
    LoginCredentialResponse,
    OrchestratorLaunchProjectionRequest,
    OrchestratorLaunchProjectionResponse,
    PatternRejection,
    PrivilegedError,
    PrivilegedErrorCode,
    ProcessInfo,
    ProcessStateEnum,
    ProcessStatusRequest,
    ProcessStatusResponse,
    ProjectionUncertaintyFlag,
    // TCK-00394: ChangeSet publishing types
    PublishChangeSetRequest,
    PublishChangeSetResponse,
    RefreshCredentialRequest,
    RefreshCredentialResponse,
    RegisterRecoveryEvidenceRequest,
    RegisterRecoveryEvidenceResponse,
    ReloadProcessRequest,
    ReloadProcessResponse,
    RemoveCredentialRequest,
    RemoveCredentialResponse,
    RequestUnfreezeRequest,
    RequestUnfreezeResponse,
    RestartProcessRequest,
    RestartProcessResponse,
    ReviewReceiptVerdict,
    ShutdownRequest,
    ShutdownResponse,
    SpawnEpisodeRequest,
    SpawnEpisodeResponse,
    StartProcessRequest,
    StartProcessResponse,
    StopProcessRequest,
    StopProcessResponse,
    SubscribePulseRequest,
    SubscribePulseResponse,
    SwitchCredentialRequest,
    SwitchCredentialResponse,
    TerminationOutcome,
    UnsubscribePulseRequest,
    UnsubscribePulseResponse,
    UpdateStopFlagsRequest,
    UpdateStopFlagsResponse,
    VerifyLedgerChainRequest,
    VerifyLedgerChainResponse,
    WorkListRequest,
    WorkListResponse,
    WorkRole,
    WorkStatusRequest,
    WorkStatusResponse,
};
use super::pulse_acl::{
    AclDecision, AclError, PulseAclEvaluator, validate_client_sub_id, validate_subscription_id,
};
use super::resource_governance::{
    SharedSubscriptionRegistry, SubscriptionRegistry, SubscriptionState,
};
use super::session_dispatch::InMemoryManifestStore;
use super::session_token::TokenMinter;
use crate::episode::registry::InMemorySessionRegistry;
use crate::episode::{
    CapabilityManifest, CustodyDomainError, CustodyDomainId, EpisodeId, EpisodeRuntime,
    EpisodeRuntimeConfig, InMemoryCasManifestLoader, LeaseIssueDenialReason, ManifestLoader,
    RuntimePreconditionEvaluator, SharedSessionBrokerRegistry, SharedToolBroker, TerminationClass,
    ToolBroker, ToolBrokerConfig, ToolClass, validate_custody_domain_overlap,
};
use crate::evidence::keychain::{GitHubCredentialStore, SshCredentialStore};
use crate::governance::GovernanceFreshnessMonitor;
use crate::htf::{ClockConfig, HolonicClock};
use crate::metrics::SharedMetricsRegistry;
use crate::session::{EphemeralHandle, SessionRegistry, SessionState};
use crate::state::SharedState;
use crate::work::authority::{
    AliasReconciliationGate, ProjectionAliasReconciliationGate, ProjectionWorkAuthority,
    WorkAuthority, WorkAuthorityError, WorkAuthorityStatus, work_id_to_hash,
};

// ============================================================================
// Ledger Event Emitter Interface (TCK-00253)
// ============================================================================

/// A signed ledger event for persistence.
///
/// Per acceptance criteria: "`WorkClaimed` event signed and persisted"
/// This struct represents a signed event ready for ledger ingestion.
#[derive(Debug, Clone)]
#[non_exhaustive]
pub struct SignedLedgerEvent {
    /// Unique event identifier.
    pub event_id: String,

    /// Event type discriminant.
    pub event_type: String,

    /// Work ID this event relates to.
    pub work_id: String,

    /// Actor ID that produced this event.
    pub actor_id: String,

    /// Canonical event payload (JSON).
    pub payload: Vec<u8>,

    /// Ed25519 signature over canonical bytes.
    pub signature: Vec<u8>,

    /// Timestamp in nanoseconds since epoch (HTF-compliant).
    pub timestamp_ns: u64,
}

/// Security class for ledger event namespaces.
///
/// Governance-class events are authoritative for policy-root derivation.
#[derive(Debug, Clone, Copy, PartialEq, Eq)]
pub enum EventTypeClass {
    /// Governance-owned event namespace used for authoritative policy roots.
    Governance,
    /// Session-originated event namespace (default for untrusted emissions).
    Session,
    /// Daemon/system-managed operational namespace.
    System,
}

impl EventTypeClass {
    /// Returns the canonical storage encoding for this class.
    #[must_use]
    pub const fn as_str(self) -> &'static str {
        match self {
            Self::Governance => "governance",
            Self::Session => "session",
            Self::System => "system",
        }
    }
}

/// Returns true when an event type is a trusted governance policy event.
#[must_use]
pub fn is_governance_policy_event_type(event_type: &str) -> bool {
    matches!(
        event_type,
        "gate.policy_resolved"
            | "policy_root_published"
            | "policy_updated"
            | "gate_configuration_updated"
    )
}

/// Returns true when an actor is authorized to emit governance policy events.
#[must_use]
pub fn is_governance_policy_actor(actor_id: &str) -> bool {
    matches!(
        actor_id,
        "orchestrator:gate-lifecycle" | "governance:policy-root" | "governance:policy"
    )
}

/// Classifies a ledger event into governance/session/system namespaces.
#[must_use]
pub fn classify_event_type(event_type: &str, actor_id: &str) -> EventTypeClass {
    if is_governance_policy_event_type(event_type) && is_governance_policy_actor(actor_id) {
        return EventTypeClass::Governance;
    }

    if matches!(
        event_type,
        "work_claimed"
            | "session_started"
            | "session_terminated"
            | "work_transitioned"
            | "stop_flags_mutated"
            | "defect_recorded"
            | "changeset_published"
            | "review_receipt_recorded"
            | "review_blocked_recorded"
            | "redundancy_receipt_consumed"
            | "SubleaseIssued"
            | "gate_lease_issued"
            | "episode_run_attributed"
    ) {
        return EventTypeClass::System;
    }

    EventTypeClass::Session
}

/// Authoritative binding for a consumed redundancy declassification receipt.
#[derive(Debug, Clone, PartialEq, Eq)]
pub struct RedundancyReceiptConsumption {
    /// Canonical receipt identifier.
    pub receipt_id: String,
    /// Request ID bound to the first successful consumption.
    pub request_id: String,
    /// Tool class bound to the first successful consumption.
    pub tool_class: String,
    /// Request-scoped intent digest (`tool_class + channel_key +
    /// argument_digest`).
    ///
    /// `None` indicates a legacy consumption record emitted before intent
    /// binding was added; callers must treat it fail-closed.
    pub intent_digest: Option<[u8; 32]>,
    /// Digest of request argument/content bytes used in `intent_digest`.
    ///
    /// `None` indicates a legacy consumption record emitted before argument
    /// digest binding was added; callers must treat it fail-closed.
    pub argument_content_digest: Option<[u8; 32]>,
    /// Boundary channel key bound to the consumed intent.
    ///
    /// `None` indicates a legacy consumption record emitted before channel-key
    /// binding was added; callers must treat it fail-closed.
    pub channel_key: Option<String>,
    /// BLAKE3 hash of the declassification receipt fields, binding the
    /// receipt identity to the specific receipt content at consumption time.
    ///
    /// `None` indicates a legacy consumption record emitted before receipt-hash
    /// binding was added; callers must treat it fail-closed.
    pub receipt_hash: Option<[u8; 32]>,
    /// PCAC authority join certificate ID (`ajc_id`) that authorized this
    /// consumption. Provides O(1) correlation between the consumption event
    /// and the PCAC lifecycle admission that gated it.
    ///
    /// `None` indicates a legacy consumption record emitted before
    /// admission-digest binding was added, or a consumption that occurred
    /// outside PCAC lifecycle (must be treated fail-closed by auditors).
    pub admission_bundle_digest: Option<[u8; 32]>,
}

/// Error type for ledger event emission.
#[derive(Debug, Clone, PartialEq, Eq)]
pub enum LedgerEventError {
    /// Signing operation failed.
    SigningFailed {
        /// Error message.
        message: String,
    },

    /// Ledger persistence failed.
    PersistenceFailed {
        /// Error message.
        message: String,
    },

    /// Validation failed (TCK-00307 MAJOR 4).
    ///
    /// Per REQ-VAL-0001: All event payloads must be validated before emission
    /// to prevent denial-of-service via unbounded strings/bytes.
    ValidationFailed {
        /// Error message describing the validation failure.
        message: String,
    },
}

impl std::fmt::Display for LedgerEventError {
    fn fmt(&self, f: &mut std::fmt::Formatter<'_>) -> std::fmt::Result {
        match self {
            Self::SigningFailed { message } => write!(f, "signing failed: {message}"),
            Self::PersistenceFailed { message } => write!(f, "persistence failed: {message}"),
            Self::ValidationFailed { message } => write!(f, "validation failed: {message}"),
        }
    }
}

impl std::error::Error for LedgerEventError {}

/// Error type for HTF timestamp generation (TCK-00289).
///
/// # Security (Fail-Closed)
///
/// Per RFC-0016 and the security policy, HTF timestamp errors must be
/// propagated rather than returning a fallback value. Returning 0 would
/// violate fail-closed security posture and could allow operations to
/// proceed with invalid timestamps.
#[derive(Debug, Clone, PartialEq, Eq)]
pub enum HtfTimestampError {
    /// HLC is not enabled on the clock.
    HlcNotEnabled,
    /// Clock error occurred.
    ClockError {
        /// Error message from the clock.
        message: String,
    },
}

impl std::fmt::Display for HtfTimestampError {
    fn fmt(&self, f: &mut std::fmt::Formatter<'_>) -> std::fmt::Result {
        match self {
            Self::HlcNotEnabled => write!(f, "HLC not enabled on clock"),
            Self::ClockError { message } => write!(f, "clock error: {message}"),
        }
    }
}

impl std::error::Error for HtfTimestampError {}

/// Parameters for a work state transition event (TCK-00395).
///
/// Bundles the parameters needed to emit a `WorkTransitioned` event to the
/// ledger. This struct reduces the number of arguments passed to
/// `emit_work_transitioned`.
#[derive(Debug, Clone)]
pub struct WorkTransition<'a> {
    /// The work ID being transitioned.
    pub work_id: &'a str,
    /// The state before the transition.
    pub from_state: &'a str,
    /// The state after the transition.
    pub to_state: &'a str,
    /// Why the transition occurred (e.g., `work_claimed_via_ipc`).
    pub rationale_code: &'a str,
    /// The work item's `transition_count` before this transition.
    pub previous_transition_count: u32,
    /// The actor performing the transition.
    pub actor_id: &'a str,
    /// HTF-compliant timestamp in nanoseconds since epoch.
    pub timestamp_ns: u64,
}

/// Typed budget bindings required by lifecycle authority contracts (TCK-00416).
#[derive(Debug, Clone, PartialEq, Eq, serde::Serialize, serde::Deserialize)]
pub struct TypedBudgetBindings {
    /// Maximum tokens permitted for the lifecycle step.
    pub max_tokens: u64,
    /// Maximum tool calls permitted for the lifecycle step.
    pub max_tool_calls: u64,
    /// Maximum wall-clock runtime in milliseconds for the lifecycle step.
    pub max_wall_ms: u64,
}

/// Authority contract bound to authoritative lifecycle transitions (TCK-00416).
#[derive(Debug, Clone, PartialEq, Eq, serde::Serialize, serde::Deserialize)]
pub struct TransitionAuthorityBindings {
    /// Lease identifier delegating authority.
    pub lease_id: String,
    /// Actor identity bound to delegation.
    pub actor_id: String,
    /// Permeability receipt commitment hash.
    pub permeability_receipt_hash: [u8; 32],
    /// Capability manifest commitment hash.
    pub capability_manifest_hash: [u8; 32],
    /// Context pack commitment hash.
    pub context_pack_hash: [u8; 32],
    /// Stop-condition contract commitment hash.
    pub stop_condition_hash: [u8; 32],
    /// Canonical stop conditions used to derive `stop_condition_hash`.
    pub stop_conditions: crate::episode::envelope::StopConditions,
    /// Typed budget contract.
    pub typed_budgets: TypedBudgetBindings,
    /// Typed budget commitment hash.
    pub typed_budget_hash: [u8; 32],
    /// Policy resolution reference.
    pub policy_resolved_ref: String,
}

/// Parameters for a stop-flag mutation audit event (TCK-00351).
///
/// This captures the immutable evidence payload for `UpdateStopFlags` so
/// emergency/governance stop mutations are queryable from the ledger.
#[derive(Debug, Clone)]
#[allow(clippy::struct_excessive_bools)] // Captures explicit before/after stop-flag state for audit evidence.
pub struct StopFlagsMutation<'a> {
    /// Actor identity derived from peer credentials.
    pub actor_id: &'a str,
    /// Emergency stop flag value before mutation.
    pub emergency_stop_previous: bool,
    /// Emergency stop flag value after mutation.
    pub emergency_stop_current: bool,
    /// Governance stop flag value before mutation.
    pub governance_stop_previous: bool,
    /// Governance stop flag value after mutation.
    pub governance_stop_current: bool,
    /// HTF-compliant timestamp in nanoseconds since epoch.
    pub timestamp_ns: u64,
    /// Request-scoped context (connection identity + requested fields).
    pub request_context: &'a serde_json::Value,
}

/// Maximum number of ledger events loaded into launch projections per request.
///
/// Projection endpoints must replay a bounded recent-history window to prevent
/// unbounded CPU and memory consumption as the append-only ledger grows.
pub const MAX_PROJECTION_EVENTS: usize = 10_000;

/// Trait for emitting signed events to the ledger.
///
/// Per TCK-00253 acceptance criteria:
/// - "`WorkClaimed` event signed and persisted"
/// - "Ledger query returns signed event"
///
/// # Implementers
///
/// - `StubLedgerEventEmitter`: In-memory storage for testing
/// - `DurableLedgerEventEmitter`: SQLite-backed persistence with HTF timestamps
///   (TCK-00289)
pub trait LedgerEventEmitter: Send + Sync {
    /// Emits a signed `WorkClaimed` event to the ledger.
    ///
    /// # Arguments
    ///
    /// * `claim` - The work claim to record
    /// * `timestamp_ns` - HTF-compliant timestamp in nanoseconds since epoch
    ///
    /// # Returns
    ///
    /// The signed event that was persisted.
    ///
    /// # Errors
    ///
    /// Returns `LedgerEventError` if signing or persistence fails.
    fn emit_work_claimed(
        &self,
        claim: &WorkClaim,
        timestamp_ns: u64,
    ) -> Result<SignedLedgerEvent, LedgerEventError>;

    /// Emits a signed `SessionStarted` event to the ledger (TCK-00289).
    ///
    /// Per TCK-00348, the `SessionStarted` event is the **authoritative**
    /// record for contract binding metadata. When `contract_binding` is
    /// `Some`, the binding fields (client/server hashes, mismatch waived,
    /// risk tier) are included in the signed payload. Persistence failure
    /// MUST be propagated as an error (fail-closed).
    ///
    /// # Arguments
    ///
    /// * `session_id` - The session ID being started
    /// * `work_id` - The work ID this session is associated with
    /// * `lease_id` - The lease ID authorizing this session
    /// * `actor_id` - The actor starting the session
    /// * `adapter_profile_hash` - CAS hash of the adapter profile used
    /// * `role_spec_hash` - CAS hash of the role spec (if available)
    /// * `timestamp_ns` - HTF-compliant timestamp in nanoseconds since epoch
    /// * `contract_binding` - Contract binding from handshake (if available)
    /// * `identity_proof_profile_hash` - Active identity proof profile hash for
    ///   the session identity context (when available)
    /// * `selection_decision` - Adapter selection metadata when profile was
    ///   chosen by policy (TCK-00400)
    ///
    /// # Returns
    ///
    /// The signed event that was persisted.
    ///
    /// # Errors
    ///
    /// Returns `LedgerEventError` if signing or persistence fails.
    #[allow(clippy::too_many_arguments)]
    fn emit_session_started(
        &self,
        session_id: &str,
        work_id: &str,
        lease_id: &str,
        actor_id: &str,
        adapter_profile_hash: &[u8; 32],
        role_spec_hash: Option<&[u8; 32]>,
        timestamp_ns: u64,
        contract_binding: Option<&crate::hsi_contract::SessionContractBinding>,
        identity_proof_profile_hash: Option<&[u8; 32]>,
        selection_decision: Option<&SelectionDecision>,
    ) -> Result<SignedLedgerEvent, LedgerEventError>;

    /// Emits a generic session event to the ledger (TCK-00290).
    ///
    /// This method handles arbitrary session events from `EmitEvent` requests,
    /// preserving the actual `event_type` and payload from the request rather
    /// than coercing all events into `session_started` events.
    ///
    /// # Arguments
    ///
    /// * `session_id` - The session ID emitting the event
    /// * `event_type` - The actual event type from the request
    /// * `payload` - The event payload bytes from the request
    /// * `actor_id` - The actor emitting the event (session ID or agent ID)
    /// * `timestamp_ns` - HTF-compliant timestamp in nanoseconds since epoch
    ///
    /// # Returns
    ///
    /// The signed event that was persisted.
    ///
    /// # Errors
    ///
    /// Returns `LedgerEventError` if signing or persistence fails.
    fn emit_session_event(
        &self,
        session_id: &str,
        event_type: &str,
        payload: &[u8],
        actor_id: &str,
        timestamp_ns: u64,
    ) -> Result<SignedLedgerEvent, LedgerEventError>;

    /// Returns whether emitted events are durably persisted.
    ///
    /// `false` indicates in-memory/test-only emitters where audit evidence
    /// does not survive process restart.
    fn has_durable_storage(&self) -> bool {
        true
    }

    /// Returns the Ed25519 verifying (public) key for signature verification.
    ///
    /// Used by projection rebuilds to verify event signatures before
    /// admission. Implementations must return the key corresponding to the
    /// signing key used by this emitter instance.
    fn verifying_key(&self) -> ed25519_dalek::VerifyingKey;

    /// Emits a signed `StopFlagsMutated` event to the ledger (TCK-00351).
    ///
    /// This records runtime stop-flag mutations performed by the privileged
    /// `UpdateStopFlags` endpoint.
    fn emit_stop_flags_mutated(
        &self,
        mutation: &StopFlagsMutation<'_>,
    ) -> Result<SignedLedgerEvent, LedgerEventError>;

    /// Emits a signed `DefectRecorded` event to the ledger (TCK-00307).
    fn emit_defect_recorded(
        &self,
        defect: &DefectRecorded,
        timestamp_ns: u64,
    ) -> Result<SignedLedgerEvent, LedgerEventError>;

    /// Queries a signed event by event ID.
    fn get_event(&self, event_id: &str) -> Option<SignedLedgerEvent>;

    /// Queries events by work ID.
    fn get_events_by_work_id(&self, work_id: &str) -> Vec<SignedLedgerEvent>;

    /// Queries all persisted ledger events in deterministic replay order.
    ///
    /// Implementations must return events ordered by causal append ordering
    /// (timestamp with row-order tie-breakers) so projection rebuilds converge
    /// deterministically.
    fn get_all_events(&self) -> Vec<SignedLedgerEvent> {
        Vec::new()
    }

    /// Returns the total number of persisted ledger events in O(1) time.
    ///
    /// Used by projection caching to determine whether the event set has
    /// changed since the last rebuild, without fetching all rows. `SQLite`
    /// implementations should use `SELECT COUNT(*) FROM ledger_events`.
    fn get_event_count(&self) -> usize {
        0
    }

    /// Returns the most recent ledger event in O(1) time.
    ///
    /// Used by PCAC ledger anchor derivation to avoid full-table
    /// materialization. `SQLite` implementations should use
    /// `SELECT ... ORDER BY timestamp_ns DESC, rowid DESC LIMIT 1`.
    fn get_latest_event(&self) -> Option<SignedLedgerEvent> {
        None
    }

    /// Returns the latest governance policy event eligible for authoritative
    /// policy-root derivation.
    ///
    /// Default implementation scans replay-order events newest-first and
    /// selects the first governance-class policy event.
    fn get_latest_governance_policy_event(&self) -> Option<SignedLedgerEvent> {
        self.get_all_events().into_iter().rev().find(|event| {
            classify_event_type(&event.event_type, &event.actor_id) == EventTypeClass::Governance
                && is_governance_policy_event_type(&event.event_type)
        })
    }

    /// Returns the latest trusted `gate.policy_resolved` lifecycle event.
    ///
    /// The default implementation scans events from newest to oldest and
    /// returns the first event whose type and actor match gate orchestrator
    /// policy-resolution provenance.
    fn get_latest_gate_policy_resolved_event(&self) -> Option<SignedLedgerEvent> {
        self.get_all_events().into_iter().rev().find(|event| {
            event.event_type == "gate.policy_resolved"
                && classify_event_type(&event.event_type, &event.actor_id)
                    == EventTypeClass::Governance
        })
    }

    /// Returns the latest persisted event hash chain tip in O(1) time.
    ///
    /// `Ok(None)` indicates an empty ledger with no persisted events.
    /// Implementations should use a direct lookup against persisted chain-tip
    /// state (for `SQLite`: latest `event_hash`) rather than re-deriving the
    /// full chain.
    fn get_latest_event_hash(&self) -> Result<Option<String>, String> {
        if self.get_event_count() == 0 {
            return Ok(None);
        }
        self.derive_event_chain_hash().map(Some)
    }

    /// Derives the authoritative ledger chain tip hash.
    ///
    /// Implementations should fail-closed if chain integrity cannot be
    /// established. The default implementation derives a deterministic chain
    /// from replay-order events using `prev_hash -> payload -> signature`
    /// bindings.
    fn derive_event_chain_hash(&self) -> Result<String, String> {
        let mut prev_hash = "genesis".to_string();
        for event in self.get_all_events() {
            let mut hasher = Sha256::new();
            hasher.update(b"apm2-ledger-chain-default-v1");
            hasher.update(prev_hash.as_bytes());
            hasher.update(event.event_type.as_bytes());
            hasher.update(event.work_id.as_bytes());
            hasher.update(event.actor_id.as_bytes());
            hasher.update(event.timestamp_ns.to_le_bytes());
            hasher.update(&event.payload);
            hasher.update(&event.signature);
            prev_hash = hex::encode(hasher.finalize());
        }
        Ok(prev_hash)
    }

    /// Performs an explicit full-chain integrity audit from genesis.
    ///
    /// This maintenance path is intentionally separate from startup/checkpoint
    /// validation and is intended for operator/admin integrity audits.
    fn verify_chain_admin(&self) -> Result<String, String> {
        self.derive_event_chain_hash()
    }

    /// Emits an authoritative receipt-consumption ledger event.
    ///
    /// Default implementation emits through `emit_session_event` to preserve
    /// compatibility for in-memory test emitters.
    #[allow(clippy::too_many_arguments)]
    fn emit_redundancy_receipt_consumed(
        &self,
        session_id: &str,
        receipt_id: &str,
        request_id: &str,
        tool_class: &str,
        intent_digest: &[u8; 32],
        argument_content_digest: &[u8; 32],
        channel_key: &str,
        actor_id: &str,
        timestamp_ns: u64,
        receipt_hash: &[u8; 32],
        admission_bundle_digest: &[u8; 32],
    ) -> Result<SignedLedgerEvent, LedgerEventError> {
        let payload = serde_json::json!({
            "receipt_id": receipt_id,
            "request_id": request_id,
            "tool_class": tool_class,
            "intent_digest": hex::encode(intent_digest),
            "argument_content_digest": hex::encode(argument_content_digest),
            "channel_key": channel_key,
            "receipt_hash": hex::encode(receipt_hash),
            "admission_bundle_digest": hex::encode(admission_bundle_digest),
        });
        let payload_bytes =
            serde_json::to_vec(&payload).map_err(|e| LedgerEventError::SigningFailed {
                message: format!("receipt consumption payload serialization failed: {e}"),
            })?;
        self.emit_session_event(
            session_id,
            "redundancy_receipt_consumed",
            &payload_bytes,
            actor_id,
            timestamp_ns,
        )
    }

    /// Returns the authoritative receipt-consumption binding, if present.
    ///
    /// Default implementation scans replay-order events from newest to oldest
    /// and supports both direct JSON payloads and wrapped `session_event`
    /// payloads where the inner payload is hex-encoded.
    fn get_redundancy_receipt_consumption(
        &self,
        receipt_id: &str,
    ) -> Option<RedundancyReceiptConsumption> {
        fn parse_hex_hash32(value: Option<&str>) -> Option<[u8; 32]> {
            let hex = value?;
            let bytes = hex::decode(hex).ok()?;
            let arr: [u8; 32] = bytes.try_into().ok()?;
            Some(arr)
        }

        fn parse_consumption(payload: &serde_json::Value) -> Option<RedundancyReceiptConsumption> {
            let obj = payload.as_object()?;
            Some(RedundancyReceiptConsumption {
                receipt_id: obj.get("receipt_id")?.as_str()?.to_string(),
                request_id: obj.get("request_id")?.as_str()?.to_string(),
                tool_class: obj.get("tool_class")?.as_str()?.to_string(),
                intent_digest: parse_hex_hash32(
                    obj.get("intent_digest").and_then(serde_json::Value::as_str),
                ),
                argument_content_digest: parse_hex_hash32(
                    obj.get("argument_content_digest")
                        .and_then(serde_json::Value::as_str),
                ),
                channel_key: obj
                    .get("channel_key")
                    .and_then(serde_json::Value::as_str)
                    .map(str::to_string),
                receipt_hash: parse_hex_hash32(
                    obj.get("receipt_hash").and_then(serde_json::Value::as_str),
                ),
                admission_bundle_digest: parse_hex_hash32(
                    obj.get("admission_bundle_digest")
                        .and_then(serde_json::Value::as_str),
                ),
            })
        }

        let mut events = self.get_all_events();
        events.reverse();
        for event in events {
            if event.event_type != "redundancy_receipt_consumed" {
                continue;
            }

            let Ok(payload) = serde_json::from_slice::<serde_json::Value>(&event.payload) else {
                continue;
            };

            if let Some(consumption) = parse_consumption(&payload) {
                if consumption.receipt_id == receipt_id {
                    return Some(consumption);
                }
                continue;
            }

            let Some(inner_hex) = payload.get("payload").and_then(serde_json::Value::as_str) else {
                continue;
            };
            let Ok(inner_bytes) = hex::decode(inner_hex) else {
                continue;
            };
            let Ok(inner_payload) = serde_json::from_slice::<serde_json::Value>(&inner_bytes)
            else {
                continue;
            };
            let Some(consumption) = parse_consumption(&inner_payload) else {
                continue;
            };
            if consumption.receipt_id == receipt_id {
                return Some(consumption);
            }
        }
        None
    }

    /// Queries a signed event by `receipt_id` embedded in the payload.
    ///
    /// This searches for events of type `review_receipt_recorded` or
    /// `review_blocked_recorded` whose JSON payload contains a matching
    /// `receipt_id` field. Used by `handle_ingest_review_receipt` for
    /// idempotency: if a receipt with the same `receipt_id` was already
    /// ingested, the existing event is returned to avoid duplicate ledger
    /// entries.
    ///
    /// # Why not `get_event`?
    ///
    /// `get_event` looks up by `event_id` (the random `EVT-<uuid>` primary
    /// key), which is different from `receipt_id` (the caller-supplied
    /// idempotency key). Using `get_event(&receipt_id)` would never match
    /// because `receipt_id` is not stored as the `event_id`.
    ///
    /// # Default
    ///
    /// Returns `None`. Implementations that support receipt-based lookup
    /// should override this.
    fn get_event_by_receipt_id(&self, receipt_id: &str) -> Option<SignedLedgerEvent> {
        let _ = receipt_id;
        None
    }

    /// Returns the total number of authoritative receipt events used by launch
    /// projections.
    fn get_authoritative_receipt_event_count(&self) -> usize {
        self.get_all_events()
            .into_iter()
            .filter(|event| {
                event.event_type == "review_receipt_recorded"
                    || event.event_type == "review_blocked_recorded"
            })
            .count()
    }

    /// Queries authoritative receipt events used by launch projections.
    ///
    /// Implementations should return at most [`MAX_PROJECTION_EVENTS`] events
    /// in deterministic replay order. Retrieval MUST prefer the most recent
    /// events to avoid replaying unbounded historical ledgers per request.
    fn get_authoritative_receipt_events(&self) -> Vec<SignedLedgerEvent> {
        let events: Vec<SignedLedgerEvent> = self
            .get_all_events()
            .into_iter()
            .filter(|event| {
                event.event_type == "review_receipt_recorded"
                    || event.event_type == "review_blocked_recorded"
            })
            .collect();

        let start = events.len().saturating_sub(MAX_PROJECTION_EVENTS);
        events.into_iter().skip(start).collect()
    }

    /// Returns the total number of liveness-relevant launch events used by
    /// orchestrator projections.
    fn get_launch_liveness_projection_event_count(&self) -> usize {
        self.get_all_events()
            .into_iter()
            .filter(|event| {
                event.event_type == "session_started"
                    || event.event_type == "session_terminated"
                    || event.event_type == "review_receipt_recorded"
                    || event.event_type == "review_blocked_recorded"
            })
            .count()
    }

    /// Queries liveness-relevant launch events used by orchestrator
    /// projections.
    ///
    /// Implementations should return at most [`MAX_PROJECTION_EVENTS`] events
    /// in deterministic replay order. Retrieval MUST prefer the most recent
    /// events to avoid replaying unbounded historical ledgers per request.
    fn get_launch_liveness_projection_events(&self) -> Vec<SignedLedgerEvent> {
        let events: Vec<SignedLedgerEvent> = self
            .get_all_events()
            .into_iter()
            .filter(|event| {
                event.event_type == "session_started"
                    || event.event_type == "session_terminated"
                    || event.event_type == "review_receipt_recorded"
                    || event.event_type == "review_blocked_recorded"
            })
            .collect();

        let start = events.len().saturating_sub(MAX_PROJECTION_EVENTS);
        events.into_iter().skip(start).collect()
    }

    /// Queries a review-receipt event by semantic identity tuple.
    ///
    /// Canonical tuple: `(receipt_id, lease_id, work_id,
    /// changeset_digest_hex)`. Implementations should use this lookup for
    /// semantic idempotency in `IngestReviewReceipt`.
    fn get_event_by_receipt_identity(
        &self,
        receipt_id: &str,
        lease_id: &str,
        work_id: &str,
        changeset_digest_hex: &str,
    ) -> Option<SignedLedgerEvent> {
        let _ = (receipt_id, lease_id, work_id, changeset_digest_hex);
        None
    }

    /// Queries a `changeset_published` event by semantic identity tuple.
    ///
    /// Canonical tuple: `(work_id, changeset_digest_hex)`.
    fn get_event_by_changeset_identity(
        &self,
        work_id: &str,
        changeset_digest_hex: &str,
    ) -> Option<SignedLedgerEvent> {
        let _ = (work_id, changeset_digest_hex);
        None
    }

    /// Emits an episode lifecycle event to the ledger (TCK-00321).
    ///
    /// Per REQ-0005, episode events must be streamed directly to the ledger
    /// as they occur, rather than buffered in memory. This enables:
    /// - Events survive daemon restart (ledger-backed durability)
    /// - Receipt event appended atomically at completion
    /// - CAS-before-ledger ordering for events referencing CAS hashes
    ///
    /// # Arguments
    ///
    /// * `episode_id` - The episode ID for this event
    /// * `event_type` - The event type discriminant (e.g., "episode.created")
    /// * `payload` - The JSON-serialized event payload
    /// * `timestamp_ns` - HTF-compliant timestamp in nanoseconds since epoch
    ///
    /// # Returns
    ///
    /// The signed event that was persisted.
    ///
    /// # Errors
    ///
    /// Returns `LedgerEventError` if signing or persistence fails.
    fn emit_episode_event(
        &self,
        episode_id: &str,
        event_type: &str,
        payload: &[u8],
        timestamp_ns: u64,
    ) -> Result<SignedLedgerEvent, LedgerEventError>;

    /// Emits a `ReviewReceiptRecorded` ledger event (TCK-00321).
    ///
    /// Per REQ-0005, receipt events must be emitted atomically at episode
    /// completion. This method:
    /// - Validates that referenced CAS artifacts exist (CAS-before-event)
    /// - Persists the receipt to the ledger atomically
    /// - Returns the signed event for verification
    ///
    /// # Ledger Event vs Protocol Event
    ///
    /// This produces a **ledger event** (JCS-canonicalized JSON) for
    /// persistence and audit, which is distinct from the FAC protocol's
    /// `ReviewReceiptRecorded` event (binary canonical format) defined in
    /// `apm2_core::fac::review_receipt`.
    ///
    /// The ledger event format includes:
    /// - `event_type`: Event discriminant for querying
    /// - `timestamp_ns`: HTF-compliant timestamp in the signed payload
    /// - All required fields for audit trail reconstruction
    ///
    /// Both formats use the same domain prefix (`REVIEW_RECEIPT_RECORDED:`) to
    /// ensure namespace consistency, but serve different purposes in the
    /// system.
    ///
    /// # Arguments
    ///
    /// * `lease_id` - Gate lease identifier associated with this receipt
    /// * `work_id` - Canonical work identifier bound to the lease
    /// * `receipt_id` - Unique receipt identifier
    /// * `changeset_digest` - BLAKE3 digest of the reviewed changeset
    /// * `artifact_bundle_hash` - CAS hash of the artifact bundle
    /// * `reviewer_actor_id` - Actor ID of the reviewer
    /// * `timestamp_ns` - HTF-compliant timestamp in nanoseconds since epoch
    /// * `identity_proof_hash` - Identity proof hash binding (32 bytes)
    /// * `time_envelope_ref` - CAS hash reference of the authoritative
    ///   `TimeEnvelopeV1` binding this receipt
    /// * `pcac_lifecycle` - Optional privileged PCAC lifecycle selectors for
    ///   replay/audit linkage
    ///
    /// # Returns
    ///
    /// The signed event that was persisted.
    ///
    /// # Errors
    ///
    /// Returns `LedgerEventError` if signing, CAS validation, or persistence
    /// fails.
    #[allow(clippy::too_many_arguments)]
    fn emit_review_receipt(
        &self,
        lease_id: &str,
        work_id: &str,
        receipt_id: &str,
        changeset_digest: &[u8; 32],
        artifact_bundle_hash: &[u8; 32],
        capability_manifest_hash: &[u8; 32],
        context_pack_hash: &[u8; 32],
        role_spec_hash: &[u8; 32],
        reviewer_actor_id: &str,
        timestamp_ns: u64,
        identity_proof_hash: &[u8; 32],
        time_envelope_ref: &str,
        pcac_lifecycle: Option<&PrivilegedPcacLifecycleArtifacts>,
    ) -> Result<SignedLedgerEvent, LedgerEventError>;

    /// Emits a `ReviewBlockedRecorded` ledger event (TCK-00389).
    ///
    /// Records a blocked review outcome in the ledger. This is emitted when
    /// an external reviewer reports that the review was blocked due to tool
    /// failure, apply failure, or similar issues.
    ///
    /// # Arguments
    ///
    /// * `lease_id` - Gate lease identifier associated with this review
    /// * `work_id` - Canonical work identifier bound to the lease
    /// * `receipt_id` - Unique receipt identifier (used as `blocked_id`)
    /// * `changeset_digest` - BLAKE3 digest of the reviewed changeset
    /// * `artifact_bundle_hash` - CAS hash of the artifact bundle
    /// * `reason_code` - Numeric reason code for the blocked review
    /// * `blocked_log_hash` - CAS hash of blocked logs
    /// * `reviewer_actor_id` - Actor ID of the reviewer
    /// * `timestamp_ns` - HTF-compliant timestamp in nanoseconds since epoch
    /// * `identity_proof_hash` - Identity proof hash binding (32 bytes)
    /// * `time_envelope_ref` - CAS hash reference of the authoritative
    ///   `TimeEnvelopeV1` binding this receipt
    /// * `pcac_lifecycle` - Optional privileged PCAC lifecycle selectors for
    ///   replay/audit linkage
    ///
    /// # Returns
    ///
    /// The signed event that was persisted.
    ///
    /// # Errors
    ///
    /// Returns `LedgerEventError` if signing or persistence fails.
    #[allow(clippy::too_many_arguments)]
    fn emit_review_blocked_receipt(
        &self,
        lease_id: &str,
        work_id: &str,
        receipt_id: &str,
        changeset_digest: &[u8; 32],
        artifact_bundle_hash: &[u8; 32],
        capability_manifest_hash: &[u8; 32],
        context_pack_hash: &[u8; 32],
        role_spec_hash: &[u8; 32],
        reason_code: u32,
        blocked_log_hash: &[u8; 32],
        reviewer_actor_id: &str,
        timestamp_ns: u64,
        identity_proof_hash: &[u8; 32],
        time_envelope_ref: &str,
        pcac_lifecycle: Option<&PrivilegedPcacLifecycleArtifacts>,
    ) -> Result<SignedLedgerEvent, LedgerEventError>;

    /// Returns the number of `work_transitioned` events for a given work ID.
    ///
    /// This is the authoritative source for the `previous_transition_count`
    /// field in `WorkTransitioned` events. Callers MUST use this method
    /// rather than hardcoding transition counts.
    ///
    /// # Arguments
    ///
    /// * `work_id` - The work ID to count transitions for
    ///
    /// # Returns
    ///
    /// The number of `work_transitioned` events for the given work ID.
    fn get_work_transition_count(&self, work_id: &str) -> u32;

    /// Emits a signed `WorkTransitioned` event to the ledger (TCK-00395).
    ///
    /// Records a work item state transition in the ledger, enabling the FAC
    /// CLI (`apm2 fac work status`) to observe work lifecycle state changes
    /// and the `GateOrchestrator` (TCK-00388) to trigger gate lifecycle.
    ///
    /// # Arguments
    ///
    /// * `work_id` - The work ID being transitioned
    /// * `from_state` - The state before the transition
    /// * `to_state` - The state after the transition
    /// * `rationale_code` - Why the transition occurred (e.g.,
    ///   `work_claimed_via_ipc`)
    /// * `previous_transition_count` - The work item's `transition_count`
    ///   before this transition (replay protection)
    /// * `actor_id` - The actor performing the transition
    /// * `timestamp_ns` - HTF-compliant timestamp in nanoseconds since epoch
    ///
    /// # Returns
    ///
    /// The signed event that was persisted.
    ///
    /// # Errors
    ///
    /// Returns `LedgerEventError` if signing or persistence fails.
    fn emit_work_transitioned(
        &self,
        transition: &WorkTransition<'_>,
    ) -> Result<SignedLedgerEvent, LedgerEventError>;

    /// Emits a signed `SessionTerminated` event to the ledger (TCK-00395).
    ///
    /// Records session termination in the ledger, enabling the
    /// `GateOrchestrator` (TCK-00388) to trigger gate lifecycle after
    /// session completion.
    ///
    /// # Arguments
    ///
    /// * `session_id` - The session that terminated
    /// * `work_id` - The work ID this session was associated with
    /// * `exit_code` - The process exit code (0 = success)
    /// * `termination_reason` - Human-readable termination reason
    /// * `actor_id` - The actor associated with this session
    /// * `timestamp_ns` - HTF-compliant timestamp in nanoseconds since epoch
    ///
    /// # Returns
    ///
    /// The signed event that was persisted.
    ///
    /// # Errors
    ///
    /// Returns `LedgerEventError` if signing or persistence fails.
    fn emit_session_terminated(
        &self,
        session_id: &str,
        work_id: &str,
        exit_code: i32,
        termination_reason: &str,
        actor_id: &str,
        timestamp_ns: u64,
    ) -> Result<SignedLedgerEvent, LedgerEventError>;

    /// Atomically emits a `WorkClaimed` event followed by a
    /// `WorkTransitioned`(Open -> Claimed) event (TCK-00395).
    ///
    /// # Atomicity Contract
    ///
    /// Both events are persisted atomically per operation: either both
    /// succeed or neither is committed. This prevents partial state where
    /// `work_claimed` is persisted but the corresponding transition is not.
    ///
    /// For in-memory emitters, atomicity is trivially guaranteed. For
    /// durable emitters, this must be implemented with a database
    /// transaction.
    ///
    /// # Returns
    ///
    /// The signed `WorkClaimed` event on success.
    ///
    /// # Errors
    ///
    /// Returns `LedgerEventError` if signing or persistence of either
    /// event fails. On error, neither event is committed.
    fn emit_claim_lifecycle(
        &self,
        claim: &WorkClaim,
        actor_id: &str,
        timestamp_ns: u64,
    ) -> Result<SignedLedgerEvent, LedgerEventError> {
        // Default implementation: emit sequentially.
        // Override in durable emitters for transactional atomicity.
        let transition_count = self.get_work_transition_count(&claim.work_id);
        let claimed_event = self.emit_work_claimed(claim, timestamp_ns)?;
        if let Err(e) = self.emit_work_transitioned(&WorkTransition {
            work_id: &claim.work_id,
            from_state: "Open",
            to_state: "Claimed",
            rationale_code: "work_claimed_via_ipc",
            previous_transition_count: transition_count,
            actor_id,
            timestamp_ns,
        }) {
            // Partial failure: work_claimed was persisted but transition was not.
            // Log and propagate the error.
            warn!(
                error = %e,
                work_id = %claim.work_id,
                "Partial commit: work_claimed persisted but work_transitioned failed"
            );
            return Err(e);
        }
        Ok(claimed_event)
    }

    /// Atomically emits a `SessionStarted` event followed by a
    /// `WorkTransitioned`(Claimed -> `InProgress`) event (TCK-00395).
    ///
    /// # Atomicity Contract
    ///
    /// Both events are persisted atomically per operation. See
    /// [`Self::emit_claim_lifecycle`] for details.
    ///
    /// # Returns
    ///
    /// The signed `SessionStarted` event on success.
    ///
    /// # Errors
    ///
    /// Returns `LedgerEventError` if signing or persistence of either
    /// event fails.
    #[allow(clippy::too_many_arguments)]
    fn emit_spawn_lifecycle(
        &self,
        session_id: &str,
        work_id: &str,
        lease_id: &str,
        actor_id: &str,
        adapter_profile_hash: &[u8; 32],
        role_spec_hash: Option<&[u8; 32]>,
        timestamp_ns: u64,
        contract_binding: Option<&crate::hsi_contract::SessionContractBinding>,
        identity_proof_profile_hash: Option<&[u8; 32]>,
        selection_decision: Option<&SelectionDecision>,
    ) -> Result<SignedLedgerEvent, LedgerEventError> {
        // Default implementation: emit sequentially.
        let session_event = self.emit_session_started(
            session_id,
            work_id,
            lease_id,
            actor_id,
            adapter_profile_hash,
            role_spec_hash,
            timestamp_ns,
            contract_binding,
            identity_proof_profile_hash,
            selection_decision,
        )?;
        let transition_count = self.get_work_transition_count(work_id);
        if let Err(e) = self.emit_work_transitioned(&WorkTransition {
            work_id,
            from_state: "Claimed",
            to_state: "InProgress",
            rationale_code: "episode_spawned_via_ipc",
            previous_transition_count: transition_count,
            actor_id,
            timestamp_ns,
        }) {
            warn!(
                error = %e,
                work_id = %work_id,
                "Partial commit: session_started persisted but work_transitioned failed"
            );
            return Err(e);
        }
        Ok(session_event)
    }

    /// Emits an `EpisodeRunAttributed` event to the ledger (TCK-00330).
    ///
    /// This method records attribution for an episode run, binding:
    /// - `work_id`: The work item this run is associated with
    /// - `episode_id`: The episode identifier
    /// - `session_id`: The session that executed the run
    /// - `adapter_profile_hash`: CAS hash of the `AgentAdapterProfileV1` used
    ///
    /// Per REQ-0009, ledger events must include `adapter_profile_hash`
    /// attribution to enable audit trail reconstruction and
    /// profile-specific analysis.
    ///
    /// # Arguments
    ///
    /// * `work_id` - The work ID this run is associated with
    /// * `episode_id` - The episode ID for this run
    /// * `session_id` - The session ID that executed the run
    /// * `adapter_profile_hash` - CAS hash of the `AgentAdapterProfileV1`
    /// * `timestamp_ns` - HTF-compliant timestamp in nanoseconds since epoch
    ///
    /// # Returns
    ///
    /// The signed event that was persisted.
    ///
    /// # Errors
    ///
    /// Returns `LedgerEventError` if signing or persistence fails.
    fn emit_episode_run_attributed(
        &self,
        work_id: &str,
        episode_id: &str,
        session_id: &str,
        adapter_profile_hash: &[u8; 32],
        timestamp_ns: u64,
    ) -> Result<SignedLedgerEvent, LedgerEventError>;

    /// Emits a signed `ChangeSetPublished` event to the ledger (TCK-00394).
    ///
    /// Records that a changeset bundle was published to CAS and anchored in
    /// the ledger, enabling downstream gate orchestration (TCK-00388) to
    /// bind gate leases to the changeset.
    ///
    /// # Arguments
    ///
    /// * `work_id` - The work ID this changeset belongs to
    /// * `changeset_digest` - BLAKE3 digest of the canonical bundle
    /// * `cas_hash` - CAS hash of the stored bundle artifact
    /// * `actor_id` - The actor who published the changeset
    /// * `timestamp_ns` - HTF-compliant timestamp in nanoseconds since epoch
    ///
    /// # Returns
    ///
    /// The signed event that was persisted.
    ///
    /// # Errors
    ///
    /// Returns `LedgerEventError` if signing or persistence fails.
    fn emit_changeset_published(
        &self,
        work_id: &str,
        changeset_digest: &[u8; 32],
        cas_hash: &[u8; 32],
        actor_id: &str,
        timestamp_ns: u64,
    ) -> Result<SignedLedgerEvent, LedgerEventError>;

    /// Emits a signed receipt with envelope bindings (TCK-00350).
    ///
    /// Per REQ-0004, all authoritative effect receipts MUST carry
    /// `envelope_hash`, `capability_manifest_hash`, and
    /// `view_commitment_hash`. This method validates bindings before
    /// emission (fail-closed) and includes them in the signed payload.
    ///
    /// # Fail-closed
    ///
    /// Returns [`LedgerEventError::ValidationFailed`] if any binding
    /// hash is zero. Receipts MUST NOT be emitted without valid bindings.
    ///
    /// # Arguments
    ///
    /// * `episode_id` - The episode that produced this receipt
    /// * `receipt_id` - Unique receipt identifier
    /// * `changeset_digest` - BLAKE3 digest of the reviewed changeset
    /// * `artifact_bundle_hash` - CAS hash of the artifact bundle
    /// * `reviewer_actor_id` - Actor ID of the reviewer
    /// * `timestamp_ns` - HTF-compliant timestamp in nanoseconds
    /// * `bindings` - Envelope bindings to include in the receipt
    /// * `identity_proof_hash` - Identity proof hash binding (32 bytes)
    ///
    /// # Errors
    ///
    /// Returns `LedgerEventError` if validation, signing, or persistence
    /// fails.
    #[allow(clippy::too_many_arguments)]
    fn emit_receipt_with_bindings(
        &self,
        _episode_id: &str,
        _receipt_id: &str,
        _changeset_digest: &[u8; 32],
        _artifact_bundle_hash: &[u8; 32],
        _reviewer_actor_id: &str,
        _timestamp_ns: u64,
        _bindings: &crate::episode::EnvelopeBindings,
        _identity_proof_hash: &[u8; 32],
    ) -> Result<SignedLedgerEvent, LedgerEventError> {
        Err(LedgerEventError::ValidationFailed {
            message: "emit_receipt_with_bindings default is fail-closed; implementers must \
                      override to persist envelope bindings correctly"
                .to_string(),
        })
    }
}

/// Domain separation prefix for `WorkClaimed` events.
///
/// Per RFC-0017 and TCK-00264: domain prefixes prevent cross-context replay.
pub const WORK_CLAIMED_DOMAIN_PREFIX: &[u8] = b"apm2.event.work_claimed:";

/// Domain separation prefix for `DefectRecorded` events.
pub const DEFECT_RECORDED_DOMAIN_PREFIX: &[u8] = b"apm2.event.defect_recorded:";

/// Domain separation prefix for episode lifecycle events (TCK-00321).
///
/// Per RFC-0017 and TCK-00264: domain prefixes prevent cross-context replay.
pub const EPISODE_EVENT_DOMAIN_PREFIX: &[u8] = b"apm2.event.episode:";

/// Domain separation prefix for episode run attribution events (TCK-00330).
///
/// Per RFC-0017 and TCK-00264: domain prefixes prevent cross-context replay.
/// This prefix is used for events that attribute episode runs to specific
/// adapter profiles via their CAS hash.
pub const EPISODE_RUN_ATTRIBUTED_PREFIX: &[u8] = b"apm2.event.episode_run_attributed:";

/// Domain separation prefix for `WorkTransitioned` ledger events (TCK-00395).
///
/// Per RFC-0017 DD-006: domain prefixes prevent cross-context replay.
/// This prefix is used when emitting work state transition events to the
/// ledger, enabling the FAC CLI and `GateOrchestrator` to observe work
/// lifecycle state changes.
pub const WORK_TRANSITIONED_DOMAIN_PREFIX: &[u8] = b"apm2.event.work_transitioned:";

/// Domain separation prefix for `SessionTerminated` ledger events (TCK-00395).
///
/// Per RFC-0017 DD-006: domain prefixes prevent cross-context replay.
/// This prefix is used when emitting session termination events to the
/// ledger, enabling the `GateOrchestrator` to trigger gate lifecycle
/// after session completion.
///
/// # Note
///
/// This is distinct from `SESSION_TERMINATED_DOMAIN_PREFIX` in
/// `apm2_core::events::canonical` which is used for kernel event signing.
/// This prefix is for ledger-level JCS-canonicalized JSON events.
pub const SESSION_TERMINATED_LEDGER_DOMAIN_PREFIX: &[u8] = b"apm2.event.session_terminated_ledger:";

/// Domain separation prefix for `StopFlagsMutated` ledger events (TCK-00351).
pub const STOP_FLAGS_MUTATED_DOMAIN_PREFIX: &[u8] = b"apm2.event.stop_flags_mutated:";

/// Domain separation prefix for `gate_lease_issued` ledger events.
///
/// Lease issuance events are authority-bearing and must be signed with a
/// dedicated domain prefix so signature verification can fail-closed.
pub const GATE_LEASE_ISSUED_LEDGER_DOMAIN_PREFIX: &[u8] = b"apm2.event.gate_lease_issued:";

/// Ledger indexing key for daemon stop-flag mutation events.
pub const STOP_FLAGS_MUTATED_WORK_ID: &str = "daemon.stop_flags";

/// Domain separation prefix for `ChangeSetPublished` ledger events (TCK-00394).
///
/// Per RFC-0017 DD-006: domain prefixes prevent cross-context replay.
/// This prefix is used when emitting changeset publication events to the
/// ledger, enabling gate orchestration (TCK-00388) to bind gate leases
/// to published changesets.
pub const CHANGESET_PUBLISHED_LEDGER_DOMAIN_PREFIX: &[u8] = CHANGESET_PUBLISHED_PREFIX;

// Note: `REVIEW_RECEIPT_RECORDED_PREFIX` is imported from `apm2_core::fac`
// to ensure protocol compatibility across the daemon/core boundary (TCK-00321).
// See `apm2_core::fac::domain_separator` for the canonical definition.

/// Domain separation prefix for `ReviewBlockedRecorded` ledger events
/// (TCK-00389).
///
/// Per RFC-0017 DD-006: domain prefixes prevent cross-context replay.
/// This prefix is used when emitting review blocked events to the ledger.
pub const REVIEW_BLOCKED_RECORDED_LEDGER_PREFIX: &[u8] = REVIEW_BLOCKED_RECORDED_PREFIX;

/// Maximum length for ID fields (`work_id`, `lease_id`, etc.).
///
/// Per SEC-SCP-FAC-0020: Unbounded input processing can lead to
/// denial-of-service via OOM. This limit prevents attackers from supplying
/// multi-GB ID strings.
pub const MAX_ID_LENGTH: usize = 256;

/// Maximum allowed length for the `reason` field in `EndSessionRequest`.
///
/// Per SEC-SCP-FAC-0020: Caller-controlled text is written into signed ledger
/// payloads. Unbounded reason strings enable denial-of-service via OOM and
/// bloated ledger entries. This limit constrains the reason to a reasonable
/// diagnostic string.
pub const MAX_REASON_LENGTH: usize = 1024;

/// Maximum allowed length for `SpawnEpisodeRequest.escalation_predicate`.
///
/// Per SEC-SCP-FAC-0020: caller-controlled free-form predicates must be
/// bounded before persistence to prevent oversized payload retention.
pub const MAX_ESCALATION_PREDICATE_LEN: usize = 1024;

/// Maximum supported delegation depth for `DelegateSublease` lineage.
///
/// Keep this aligned with the core delegation ceiling so recursion-depth
/// laundering attempts fail-closed consistently across verifier surfaces.
pub const MAX_SUBLEASE_DELEGATION_DEPTH: u32 =
    apm2_core::policy::permeability::MAX_DELEGATION_DEPTH;

/// Hard cap on lineage traversal iterations for delegation-depth computation.
///
/// This bounds event-store traversal and prevents unbounded recursion/cycles.
pub const MAX_SUBLEASE_LINEAGE_TRAVERSAL_STEPS: usize =
    (MAX_SUBLEASE_DELEGATION_DEPTH as usize) + 1;

/// Deterministic budget for sublease delegation satisfiability evaluation.
///
/// The budget is expressed in integer ticks and consumed by fixed-cost lineage
/// traversal and binding checks.
pub const SUBLEASE_DELEGATION_SATISFIABILITY_BUDGET_TICKS: u64 =
    if (MAX_SUBLEASE_LINEAGE_TRAVERSAL_STEPS as u64) * 2 + 4 < 40 {
        40
    } else {
        (MAX_SUBLEASE_LINEAGE_TRAVERSAL_STEPS as u64) * 2 + 4
    };

/// Builds the canonical JSON payload for a `SessionStarted` ledger event.
///
/// Extracted to a single helper to eliminate triplicated payload construction
/// across `StubLedgerEventEmitter::emit_session_started`,
/// `SqliteLedgerEventEmitter::emit_session_started`, and
/// `SqliteLedgerEventEmitter::emit_spawn_lifecycle`.
///
/// TCK-00348: Includes contract binding fields when available.
#[allow(clippy::too_many_arguments)]
pub fn build_session_started_payload(
    session_id: &str,
    work_id: &str,
    lease_id: &str,
    actor_id: &str,
    adapter_profile_hash: &[u8; 32],
    role_spec_hash: Option<&[u8; 32]>,
    contract_binding: Option<&crate::hsi_contract::SessionContractBinding>,
    identity_proof_profile_hash: Option<&[u8; 32]>,
    selection_decision: Option<&SelectionDecision>,
) -> serde_json::Value {
    let mut payload = serde_json::json!({
        "event_type": "session_started",
        "session_id": session_id,
        "work_id": work_id,
        "lease_id": lease_id,
        "actor_id": actor_id,
        "adapter_profile_hash": hex::encode(adapter_profile_hash),
    });
    if let Some(hash) = role_spec_hash {
        payload.as_object_mut().expect("payload is object").insert(
            "role_spec_hash".to_string(),
            serde_json::Value::String(hex::encode(hash)),
        );
    } else {
        payload.as_object_mut().expect("payload is object").insert(
            "waiver_id".to_string(),
            serde_json::Value::String("WVR-0002".to_string()),
        );
        payload.as_object_mut().expect("payload is object").insert(
            "role_spec_hash_absent".to_string(),
            serde_json::Value::Bool(true),
        );
    }
    if let Some(binding) = contract_binding {
        let obj = payload.as_object_mut().expect("payload is object");
        obj.insert(
            "cli_contract_hash".to_string(),
            serde_json::Value::String(binding.cli_contract_hash.clone()),
        );
        obj.insert(
            "server_contract_hash".to_string(),
            serde_json::Value::String(binding.server_contract_hash.clone()),
        );
        obj.insert(
            "mismatch_waived".to_string(),
            serde_json::Value::Bool(binding.mismatch_waived),
        );
        obj.insert(
            "risk_tier".to_string(),
            serde_json::to_value(binding.risk_tier).expect("RiskTier serializes"),
        );
        obj.insert(
            "client_canonicalizers".to_string(),
            serde_json::to_value(&binding.client_canonicalizers)
                .expect("CanonicalizerInfo serializes"),
        );
    }
    if let Some(profile_hash) = identity_proof_profile_hash {
        payload.as_object_mut().expect("payload is object").insert(
            "identity_proof_profile_hash".to_string(),
            serde_json::Value::String(hex::encode(profile_hash)),
        );
    }
    if let Some(decision) = selection_decision {
        let mut weights = Vec::with_capacity(decision.selection_weights.len());
        for weight in &decision.selection_weights {
            weights.push(serde_json::json!({
                "profile_id": weight.profile_id,
                "profile_hash": hex::encode(weight.profile_hash),
                "configured_weight": weight.configured_weight,
                "effective_weight": weight.effective_weight,
                "enabled": weight.enabled,
                "adapter_available": weight.adapter_available,
                "rate_limited": weight.rate_limited,
                "fallback_priority": weight.fallback_priority,
            }));
        }

        let obj = payload.as_object_mut().expect("payload is object");
        obj.insert(
            "selected_profile_id".to_string(),
            serde_json::Value::String(decision.selected_profile_id.clone()),
        );
        obj.insert(
            "selection_weights".to_string(),
            serde_json::Value::Array(weights),
        );
        obj.insert(
            "selection_input_digest".to_string(),
            serde_json::Value::String(hex::encode(decision.selection_input_digest)),
        );
        obj.insert(
            "selection_seed".to_string(),
            serde_json::Value::String(hex::encode(decision.seed)),
        );
        obj.insert(
            "selection_policy_hash".to_string(),
            serde_json::Value::String(hex::encode(decision.policy_hash)),
        );
        obj.insert(
            "selection_attempt".to_string(),
            serde_json::Value::Number(serde_json::Number::from(u64::from(
                decision.selection_attempt,
            ))),
        );
        obj.insert(
            "selection_backoff_epoch".to_string(),
            serde_json::Value::Number(serde_json::Number::from(decision.backoff_epoch)),
        );
        obj.insert(
            "selection_strategy".to_string(),
            serde_json::to_value(decision.strategy).expect("selection strategy serializes"),
        );
        obj.insert(
            "selection_used_fallback".to_string(),
            serde_json::Value::Bool(decision.used_fallback),
        );
    }
    payload
}

/// Returns role-scoped typed budgets for lifecycle authority contracts.
#[must_use]
pub const fn typed_budgets_for_role(role: WorkRole) -> TypedBudgetBindings {
    match role {
        WorkRole::Implementer => TypedBudgetBindings {
            max_tokens: 1_000_000,
            max_tool_calls: 1_000,
            max_wall_ms: 3_600_000,
        },
        WorkRole::GateExecutor => TypedBudgetBindings {
            max_tokens: 500_000,
            max_tool_calls: 500,
            max_wall_ms: 1_800_000,
        },
        WorkRole::Reviewer => TypedBudgetBindings {
            max_tokens: 350_000,
            max_tool_calls: 350,
            max_wall_ms: 1_200_000,
        },
        WorkRole::Coordinator => TypedBudgetBindings {
            max_tokens: 250_000,
            max_tool_calls: 250,
            max_wall_ms: 900_000,
        },
        WorkRole::Unspecified => TypedBudgetBindings {
            max_tokens: 0,
            max_tool_calls: 0,
            max_wall_ms: 0,
        },
    }
}

fn canonical_json_bytes(value: &serde_json::Value) -> Result<Vec<u8>, String> {
    let payload = value.to_string();
    let canonical =
        canonicalize_json(&payload).map_err(|e| format!("JCS canonicalization failed: {e}"))?;
    Ok(canonical.into_bytes())
}

fn hash_bytes(bytes: &[u8]) -> [u8; 32] {
    *blake3::hash(bytes).as_bytes()
}

fn saturating_u32(value: usize) -> u32 {
    u32::try_from(value).unwrap_or(u32::MAX)
}

fn build_permeability_receipt_payload(
    work_id: &str,
    bindings: &TransitionAuthorityBindings,
) -> serde_json::Value {
    serde_json::json!({
        "schema": "apm2.permeability_receipt.v1",
        "work_id": work_id,
        "lease_id": bindings.lease_id,
        "actor_id": bindings.actor_id,
        "policy_resolved_ref": bindings.policy_resolved_ref,
        "capability_manifest_hash": hex::encode(bindings.capability_manifest_hash),
        "context_pack_hash": hex::encode(bindings.context_pack_hash),
    })
}

fn build_typed_budget_payload(budgets: &TypedBudgetBindings) -> serde_json::Value {
    serde_json::json!({
        "schema": "apm2.typed_budget_contract.v1",
        "max_tokens": budgets.max_tokens,
        "max_tool_calls": budgets.max_tool_calls,
        "max_wall_ms": budgets.max_wall_ms,
    })
}

/// Derives lifecycle authority bindings for transition validation/emission.
#[allow(clippy::too_many_arguments)]
pub fn derive_transition_authority_bindings(
    work_id: &str,
    lease_id: &str,
    actor_id: &str,
    role: WorkRole,
    policy_resolved_ref: &str,
    capability_manifest_hash: [u8; 32],
    context_pack_hash: [u8; 32],
    stop_conditions: crate::episode::envelope::StopConditions,
) -> Result<TransitionAuthorityBindings, String> {
    let typed_budgets = typed_budgets_for_role(role);
    let typed_budget_payload = build_typed_budget_payload(&typed_budgets);
    let typed_budget_hash = hash_bytes(&canonical_json_bytes(&typed_budget_payload)?);

    let stop_condition_hash = hash_bytes(&stop_conditions.canonical_bytes());

    let mut bindings = TransitionAuthorityBindings {
        lease_id: lease_id.to_string(),
        actor_id: actor_id.to_string(),
        permeability_receipt_hash: [0u8; 32],
        capability_manifest_hash,
        context_pack_hash,
        stop_condition_hash,
        stop_conditions,
        typed_budgets,
        typed_budget_hash,
        policy_resolved_ref: policy_resolved_ref.to_string(),
    };

    let permeability_payload = build_permeability_receipt_payload(work_id, &bindings);
    bindings.permeability_receipt_hash = hash_bytes(&canonical_json_bytes(&permeability_payload)?);

    Ok(bindings)
}

/// Derives claim-time authority bindings using fail-closed default stop policy.
pub fn derive_claim_transition_authority_bindings(
    claim: &WorkClaim,
) -> Result<TransitionAuthorityBindings, String> {
    let default_stop_conditions = crate::episode::envelope::StopConditions {
        max_episodes: 1,
        escalation_predicate: String::new(),
        goal_predicate: String::new(),
        failure_predicate: String::new(),
    };

    derive_transition_authority_bindings(
        &claim.work_id,
        &claim.lease_id,
        &claim.actor_id,
        claim.role,
        &claim.policy_resolution.policy_resolved_ref,
        claim.policy_resolution.capability_manifest_hash,
        claim.policy_resolution.context_pack_hash,
        default_stop_conditions,
    )
}

/// Appends authority binding fields to a JSON event payload (TCK-00416).
///
/// Used by transition emitters to embed complete binding references into
/// signed ledger events.
pub fn append_transition_authority_fields(
    payload: &mut serde_json::Map<String, serde_json::Value>,
    bindings: &TransitionAuthorityBindings,
) {
    payload.insert(
        "lease_id".to_string(),
        serde_json::Value::String(bindings.lease_id.clone()),
    );
    payload.insert(
        "permeability_receipt_hash".to_string(),
        serde_json::Value::String(hex::encode(bindings.permeability_receipt_hash)),
    );
    payload.insert(
        "capability_manifest_hash".to_string(),
        serde_json::Value::String(hex::encode(bindings.capability_manifest_hash)),
    );
    payload.insert(
        "context_pack_hash".to_string(),
        serde_json::Value::String(hex::encode(bindings.context_pack_hash)),
    );
    payload.insert(
        "stop_condition_hash".to_string(),
        serde_json::Value::String(hex::encode(bindings.stop_condition_hash)),
    );
    payload.insert(
        "typed_budgets".to_string(),
        serde_json::to_value(&bindings.typed_budgets).expect("typed budgets serialize"),
    );
    payload.insert(
        "typed_budget_hash".to_string(),
        serde_json::Value::String(hex::encode(bindings.typed_budget_hash)),
    );
    payload.insert(
        "policy_resolved_ref".to_string(),
        serde_json::Value::String(bindings.policy_resolved_ref.clone()),
    );
}

/// Maximum number of authority binding violations collected per validation
/// invocation before short-circuiting (TCK-00416).
///
/// Per bounded-resource-growth policy, a single validation pass never
/// allocates more than this many violation strings.
pub const MAX_BINDING_VIOLATIONS: usize = 32;

/// Structured error for authority binding validation failures (TCK-00416).
///
/// Captures all missing/invalid bindings in a single pass so the caller
/// can emit a comprehensive defect record.
#[derive(Debug, Clone, PartialEq, Eq)]
pub struct TransitionAuthorityError {
    /// Human-readable summary of the rejection.
    pub message: String,
    /// Individual violation descriptions (bounded by
    /// [`MAX_BINDING_VIOLATIONS`]).
    pub violations: Vec<String>,
}

impl std::fmt::Display for TransitionAuthorityError {
    fn fmt(&self, f: &mut std::fmt::Formatter<'_>) -> std::fmt::Result {
        write!(f, "{}", self.message)
    }
}

impl std::error::Error for TransitionAuthorityError {}

/// Validates that all required authority bindings are present and
/// CAS-resolvable at transition time (TCK-00416, REQ-HEF-0013).
///
/// # Fail-Closed Semantics
///
/// Missing, empty, or CAS-unresolvable bindings are hard failures.
/// Best-effort transition success is forbidden per the requirement.
///
/// # Arguments
///
/// * `bindings` - The authority bindings to validate.
/// * `cas` - Content-addressed store for resolvability checks.
///
/// # Errors
///
/// Returns `TransitionAuthorityError` with all violations collected
/// in a single pass (bounded by [`MAX_BINDING_VIOLATIONS`]).
pub fn validate_transition_authority_bindings(
    bindings: &TransitionAuthorityBindings,
    cas: &dyn ContentAddressedStore,
) -> Result<(), TransitionAuthorityError> {
    let mut violations: Vec<String> = Vec::new();

    // 1. Validate lease_id is non-empty
    if bindings.lease_id.is_empty() {
        violations.push("lease_id is empty".to_string());
    }

    // 2. Validate actor_id is non-empty
    if bindings.actor_id.is_empty() {
        violations.push("actor_id is empty".to_string());
    }

    // 3. Validate policy_resolved_ref is non-empty
    if bindings.policy_resolved_ref.is_empty() {
        violations.push("policy_resolved_ref is empty".to_string());
    }

    // 4. Validate permeability_receipt_hash is non-zero and CAS-resolvable
    if bool::from(bindings.permeability_receipt_hash.ct_eq(&[0u8; 32])) {
        violations.push("permeability_receipt_hash is zero (unset)".to_string());
    } else if violations.len() < MAX_BINDING_VIOLATIONS {
        match cas.exists(&bindings.permeability_receipt_hash) {
            Ok(true) => {},
            Ok(false) => violations.push(format!(
                "permeability_receipt_hash {} not resolvable in CAS",
                hex::encode(bindings.permeability_receipt_hash)
            )),
            Err(e) => violations.push(format!("permeability_receipt_hash CAS lookup failed: {e}")),
        }
    }

    // 5. Validate capability_manifest_hash is non-zero and CAS-resolvable
    if bool::from(bindings.capability_manifest_hash.ct_eq(&[0u8; 32])) {
        violations.push("capability_manifest_hash is zero (unset)".to_string());
    } else if violations.len() < MAX_BINDING_VIOLATIONS {
        match cas.exists(&bindings.capability_manifest_hash) {
            Ok(true) => {},
            Ok(false) => violations.push(format!(
                "capability_manifest_hash {} not resolvable in CAS",
                hex::encode(bindings.capability_manifest_hash)
            )),
            Err(e) => violations.push(format!("capability_manifest_hash CAS lookup failed: {e}")),
        }
    }

    // 6. Validate context_pack_hash is non-zero and CAS-resolvable
    if bool::from(bindings.context_pack_hash.ct_eq(&[0u8; 32])) {
        violations.push("context_pack_hash is zero (unset)".to_string());
    } else if violations.len() < MAX_BINDING_VIOLATIONS {
        match cas.exists(&bindings.context_pack_hash) {
            Ok(true) => {},
            Ok(false) => violations.push(format!(
                "context_pack_hash {} not resolvable in CAS",
                hex::encode(bindings.context_pack_hash)
            )),
            Err(e) => violations.push(format!("context_pack_hash CAS lookup failed: {e}")),
        }
    }

    // 7. Validate stop_condition_hash is non-zero and CAS-resolvable
    if bool::from(bindings.stop_condition_hash.ct_eq(&[0u8; 32])) {
        violations.push("stop_condition_hash is zero (unset)".to_string());
    } else if violations.len() < MAX_BINDING_VIOLATIONS {
        match cas.exists(&bindings.stop_condition_hash) {
            Ok(true) => {},
            Ok(false) => violations.push(format!(
                "stop_condition_hash {} not resolvable in CAS",
                hex::encode(bindings.stop_condition_hash)
            )),
            Err(e) => violations.push(format!("stop_condition_hash CAS lookup failed: {e}")),
        }
    }

    // 8. Validate typed_budget_hash is non-zero and CAS-resolvable
    if bool::from(bindings.typed_budget_hash.ct_eq(&[0u8; 32])) {
        violations.push("typed_budget_hash is zero (unset)".to_string());
    } else if violations.len() < MAX_BINDING_VIOLATIONS {
        match cas.exists(&bindings.typed_budget_hash) {
            Ok(true) => {},
            Ok(false) => violations.push(format!(
                "typed_budget_hash {} not resolvable in CAS",
                hex::encode(bindings.typed_budget_hash)
            )),
            Err(e) => violations.push(format!("typed_budget_hash CAS lookup failed: {e}")),
        }
    }

    // 9. Validate typed budgets have non-zero limits
    if bindings.typed_budgets.max_tokens == 0
        && bindings.typed_budgets.max_tool_calls == 0
        && bindings.typed_budgets.max_wall_ms == 0
    {
        violations.push("typed_budgets has all-zero limits (Unspecified role budget)".to_string());
    }

    // 10. Re-derive hashes and verify they match (tamper detection)
    let recomputed_stop_hash = hash_bytes(&bindings.stop_conditions.canonical_bytes());
    if !bool::from(bindings.stop_condition_hash.ct_eq(&[0u8; 32]))
        && !bool::from(bindings.stop_condition_hash.ct_eq(&recomputed_stop_hash))
    {
        violations.push(format!(
            "stop_condition_hash mismatch: declared={} recomputed={}",
            hex::encode(bindings.stop_condition_hash),
            hex::encode(recomputed_stop_hash),
        ));
    }

    if let Ok(budget_payload) =
        canonical_json_bytes(&build_typed_budget_payload(&bindings.typed_budgets))
    {
        let recomputed_budget_hash = hash_bytes(&budget_payload);
        if !bool::from(bindings.typed_budget_hash.ct_eq(&[0u8; 32]))
            && !bool::from(bindings.typed_budget_hash.ct_eq(&recomputed_budget_hash))
        {
            violations.push(format!(
                "typed_budget_hash mismatch: declared={} recomputed={}",
                hex::encode(bindings.typed_budget_hash),
                hex::encode(recomputed_budget_hash),
            ));
        }
    }

    if violations.is_empty() {
        Ok(())
    } else {
        let count = violations.len();
        Err(TransitionAuthorityError {
            message: format!("authority binding validation failed with {count} violation(s)"),
            violations,
        })
    }
}

/// Review/projection outcome bindings required by REQ-HEF-0013 (TCK-00416).
///
/// These bindings capture the evidence-index commitments that make review
/// and projection outcomes replayable without ambient state.
#[derive(Debug, Clone, PartialEq, Eq, serde::Serialize, serde::Deserialize)]
pub struct ReviewOutcomeBindings {
    /// BLAKE3 hash of the materialized workspace view at review time.
    pub view_commitment_hash: [u8; 32],
    /// BLAKE3 hash of the tool invocation log index.
    pub tool_log_index_hash: [u8; 32],
    /// BLAKE3 hash of the summary receipt artifact.
    pub summary_receipt_hash: [u8; 32],
}

/// Validates that review/projection outcome bindings are present and
/// CAS-resolvable (TCK-00416, REQ-HEF-0013).
///
/// # Fail-Closed Semantics
///
/// All three hashes must be non-zero and resolvable in the CAS at
/// validation time. Deferred resolution is forbidden.
pub fn validate_review_outcome_bindings(
    bindings: &ReviewOutcomeBindings,
    cas: &dyn ContentAddressedStore,
) -> Result<(), TransitionAuthorityError> {
    let mut violations: Vec<String> = Vec::new();

    // 1. view_commitment_hash
    if bool::from(bindings.view_commitment_hash.ct_eq(&[0u8; 32])) {
        violations.push("view_commitment_hash is zero (unset)".to_string());
    } else {
        match cas.exists(&bindings.view_commitment_hash) {
            Ok(true) => {},
            Ok(false) => violations.push(format!(
                "view_commitment_hash {} not resolvable in CAS",
                hex::encode(bindings.view_commitment_hash)
            )),
            Err(e) => violations.push(format!("view_commitment_hash CAS lookup failed: {e}")),
        }
    }

    // 2. tool_log_index_hash
    if bool::from(bindings.tool_log_index_hash.ct_eq(&[0u8; 32])) {
        violations.push("tool_log_index_hash is zero (unset)".to_string());
    } else if violations.len() < MAX_BINDING_VIOLATIONS {
        match cas.exists(&bindings.tool_log_index_hash) {
            Ok(true) => {},
            Ok(false) => violations.push(format!(
                "tool_log_index_hash {} not resolvable in CAS",
                hex::encode(bindings.tool_log_index_hash)
            )),
            Err(e) => violations.push(format!("tool_log_index_hash CAS lookup failed: {e}")),
        }
    }

    // 3. summary_receipt_hash
    if bool::from(bindings.summary_receipt_hash.ct_eq(&[0u8; 32])) {
        violations.push("summary_receipt_hash is zero (unset)".to_string());
    } else if violations.len() < MAX_BINDING_VIOLATIONS {
        match cas.exists(&bindings.summary_receipt_hash) {
            Ok(true) => {},
            Ok(false) => violations.push(format!(
                "summary_receipt_hash {} not resolvable in CAS",
                hex::encode(bindings.summary_receipt_hash)
            )),
            Err(e) => violations.push(format!("summary_receipt_hash CAS lookup failed: {e}")),
        }
    }

    if violations.is_empty() {
        Ok(())
    } else {
        let count = violations.len();
        Err(TransitionAuthorityError {
            message: format!("review outcome binding validation failed with {count} violation(s)"),
            violations,
        })
    }
}

/// Derives review outcome bindings from receipt fields using domain-tagged
/// hashing (TCK-00416).
///
/// Each outcome binding hash is derived independently from the receipt's
/// semantic source data using a unique domain prefix, ensuring no two
/// bindings alias the same field. This satisfies REQ-HEF-0013 by providing
/// proper independent evidence-index commitments.
///
/// # Arguments
///
/// * `changeset_digest` - BLAKE3 digest of the reviewed changeset.
/// * `artifact_bundle_hash` - CAS hash of the review artifact bundle.
/// * `receipt_id` - Unique receipt identifier for domain separation.
///
/// # Returns
///
/// A `ReviewOutcomeBindings` struct with independently-derived hashes.
pub fn derive_review_outcome_bindings(
    changeset_digest: &[u8; 32],
    artifact_bundle_hash: &[u8; 32],
    receipt_id: &str,
) -> ReviewOutcomeBindings {
    // view_commitment_hash: derived from the changeset digest (workspace view at
    // review time) using a domain-tagged hash to ensure it is NOT the raw
    // changeset_digest.
    let mut view_hasher = blake3::Hasher::new();
    view_hasher.update(b"apm2.review_outcome.view_commitment.v1:");
    view_hasher.update(changeset_digest);
    let view_commitment_hash = *view_hasher.finalize().as_bytes();

    // tool_log_index_hash: derived from the artifact bundle (which contains tool
    // logs) using a domain-tagged hash to ensure it is NOT the raw
    // artifact_bundle_hash or identity_proof_hash.
    let mut tool_hasher = blake3::Hasher::new();
    tool_hasher.update(b"apm2.review_outcome.tool_log_index.v1:");
    tool_hasher.update(artifact_bundle_hash);
    tool_hasher.update(receipt_id.as_bytes());
    let tool_log_index_hash = *tool_hasher.finalize().as_bytes();

    // summary_receipt_hash: derived from the artifact bundle hash itself
    // using a domain-tagged hash for binding independence.
    let mut summary_hasher = blake3::Hasher::new();
    summary_hasher.update(b"apm2.review_outcome.summary_receipt.v1:");
    summary_hasher.update(artifact_bundle_hash);
    let summary_receipt_hash = *summary_hasher.finalize().as_bytes();

    ReviewOutcomeBindings {
        view_commitment_hash,
        tool_log_index_hash,
        summary_receipt_hash,
    }
}

/// Stores review outcome binding artifacts in CAS so they are resolvable
/// at validation time (TCK-00416).
///
/// This must be called BEFORE `validate_review_outcome_bindings` to ensure
/// CAS-resolvability. The function stores domain-tagged preimages for each
/// outcome binding hash.
///
/// # Arguments
///
/// * `changeset_digest` - BLAKE3 digest of the reviewed changeset.
/// * `artifact_bundle_hash` - CAS hash of the review artifact bundle.
/// * `receipt_id` - Unique receipt identifier for domain separation.
/// * `cas` - Content-addressed store for artifact persistence.
///
/// # Errors
///
/// Returns an error string if any CAS store operation fails.
pub fn store_review_outcome_artifacts(
    changeset_digest: &[u8; 32],
    artifact_bundle_hash: &[u8; 32],
    receipt_id: &str,
    cas: &dyn ContentAddressedStore,
) -> Result<(), String> {
    // Store the preimage for view_commitment_hash
    let mut view_preimage = Vec::with_capacity(48 + 32);
    view_preimage.extend_from_slice(b"apm2.review_outcome.view_commitment.v1:");
    view_preimage.extend_from_slice(changeset_digest);
    cas.store(&view_preimage)
        .map_err(|e| format!("view_commitment CAS store failed: {e}"))?;

    // Store the preimage for tool_log_index_hash
    let mut tool_preimage = Vec::with_capacity(48 + 32 + receipt_id.len());
    tool_preimage.extend_from_slice(b"apm2.review_outcome.tool_log_index.v1:");
    tool_preimage.extend_from_slice(artifact_bundle_hash);
    tool_preimage.extend_from_slice(receipt_id.as_bytes());
    cas.store(&tool_preimage)
        .map_err(|e| format!("tool_log_index CAS store failed: {e}"))?;

    // Store the preimage for summary_receipt_hash
    let mut summary_preimage = Vec::with_capacity(48 + 32);
    summary_preimage.extend_from_slice(b"apm2.review_outcome.summary_receipt.v1:");
    summary_preimage.extend_from_slice(artifact_bundle_hash);
    cas.store(&summary_preimage)
        .map_err(|e| format!("summary_receipt CAS store failed: {e}"))?;

    Ok(())
}

/// Emits a structured defect event for authority binding violations
/// (TCK-00416).
///
/// Per REQ-HEF-0013, missing/unresolved bindings produce hard failures WITH
/// defect emission. This function builds and emits the `DefectRecorded` event.
///
/// # Arguments
///
/// * `emitter` - Ledger event emitter for defect persistence.
/// * `work_id` - The work ID associated with the rejected transition.
/// * `error` - The structured authority error with violation details.
/// * `timestamp_ns` - HTF-compliant timestamp.
pub fn emit_authority_binding_defect(
    emitter: &dyn LedgerEventEmitter,
    work_id: &str,
    error: &TransitionAuthorityError,
    timestamp_ns: u64,
) {
    let defect_id = format!("DEF-AUTH-{}", uuid::Uuid::new_v4());
    let violations_summary = error
        .violations
        .iter()
        .take(MAX_BINDING_VIOLATIONS)
        .cloned()
        .collect::<Vec<_>>()
        .join("; ");

    let cas_payload = serde_json::json!({
        "schema": "apm2.authority_binding_defect.v1",
        "work_id": work_id,
        "violations": error.violations,
        "message": error.message,
    });
    let cas_bytes = cas_payload.to_string().into_bytes();
    let cas_hash = hash_bytes(&cas_bytes);

    let defect = DefectRecorded {
        defect_id: defect_id.clone(),
        defect_type: "AUTHORITY_BINDING_VIOLATION".to_string(),
        cas_hash: cas_hash.to_vec(),
        source: DefectSource::SchemaReject as i32,
        work_id: work_id.to_string(),
        severity: "S1".to_string(),
        detected_at: timestamp_ns,
        time_envelope_ref: None,
    };

    if let Err(e) = emitter.emit_defect_recorded(&defect, timestamp_ns) {
        // Defect emission failure is logged but does NOT suppress the
        // original rejection (fail-closed).
        warn!(
            error = %e,
            defect_id = %defect_id,
            violations = %violations_summary,
            "Failed to persist authority binding defect (rejection still enforced)"
        );
    }
}

/// Stores authority binding artifacts in CAS so they are resolvable
/// at validation time (TCK-00416).
///
/// This must be called BEFORE `validate_transition_authority_bindings`
/// to ensure CAS-resolvability. The function stores:
/// - Permeability receipt payload
/// - Stop conditions canonical bytes
/// - Typed budget payload
///
/// # Arguments
///
/// * `work_id` - The work ID for permeability receipt context.
/// * `bindings` - The bindings whose hash preimages should be stored.
/// * `cas` - The content-addressed store.
///
/// # Errors
///
/// Returns an error message if any CAS store operation fails.
pub fn store_authority_binding_artifacts(
    work_id: &str,
    bindings: &TransitionAuthorityBindings,
    cas: &dyn ContentAddressedStore,
) -> Result<(), String> {
    // Store permeability receipt payload
    let permeability_payload = build_permeability_receipt_payload(work_id, bindings);
    let permeability_bytes = canonical_json_bytes(&permeability_payload)?;
    cas.store(&permeability_bytes)
        .map_err(|e| format!("CAS store permeability receipt failed: {e}"))?;

    // Store stop conditions canonical bytes
    let stop_bytes = bindings.stop_conditions.canonical_bytes();
    cas.store(&stop_bytes)
        .map_err(|e| format!("CAS store stop conditions failed: {e}"))?;

    // Store typed budget payload
    let budget_payload = build_typed_budget_payload(&bindings.typed_budgets);
    let budget_bytes = canonical_json_bytes(&budget_payload)?;
    cas.store(&budget_bytes)
        .map_err(|e| format!("CAS store typed budget failed: {e}"))?;

    Ok(())
}

/// Builds the deterministic context pack manifest used by policy resolvers.
///
/// Both [`StubPolicyResolver`] and
/// [`GovernancePolicyResolver`](crate::governance::GovernancePolicyResolver)
/// derive the context pack from `(work_id, actor_id)` using this shared
/// logic.  Extracting it into a helper ensures that
/// [`seed_policy_artifacts_in_cas`] reproduces the exact same preimage.
pub fn build_policy_context_pack(
    work_id: &str,
    actor_id: &str,
) -> apm2_core::context::ContextPackManifest {
    use apm2_core::context::{AccessLevel, ContextPackManifestBuilder, ManifestEntryBuilder};

    let content_hash = blake3::hash(format!("content:{work_id}:{actor_id}").as_bytes());

    ContextPackManifestBuilder::new(format!("manifest:{work_id}"), format!("profile:{actor_id}"))
        .add_entry(
            ManifestEntryBuilder::new(
                format!("work/{work_id}/context.yaml"),
                *content_hash.as_bytes(),
            )
            .stable_id("work-context")
            .access_level(AccessLevel::Read)
            .build(),
        )
        .build()
}

/// Returns the `capability_manifest_hash` that [`seed_policy_artifacts_in_cas`]
/// will store for the given `(work_id, actor_id, role)`.
///
/// Tests that construct `PolicyResolution` directly MUST use this hash
/// so that the CAS-seeded preimage matches.
pub fn policy_capability_manifest_hash(work_id: &str, actor_id: &str, role: WorkRole) -> [u8; 32] {
    if role == WorkRole::Reviewer {
        *crate::episode::reviewer_manifest::reviewer_v0_manifest_hash()
    } else {
        let preimage = format!("manifest:{work_id}:{actor_id}");
        *blake3::hash(preimage.as_bytes()).as_bytes()
    }
}

/// Returns the `context_pack_hash` that [`seed_policy_artifacts_in_cas`]
/// will store for the given `(work_id, actor_id)`.
///
/// Tests that construct `PolicyResolution` directly MUST use this hash
/// so that the CAS-seeded preimage matches.
pub fn policy_context_pack_hash(work_id: &str, actor_id: &str) -> [u8; 32] {
    let context_pack = build_policy_context_pack(work_id, actor_id);
    context_pack.manifest_hash()
}

/// Maps FAC deny reason codes to daemon privileged error codes.
const fn map_deny_reason_code_to_privileged_error(code: DenyReasonCode) -> PrivilegedErrorCode {
    match code {
        DenyReasonCode::MissingAuthority => PrivilegedErrorCode::PolicyResolutionMissing,
        DenyReasonCode::StaleAuthority
        | DenyReasonCode::UnknownRole
        | DenyReasonCode::UnverifiableContext => PrivilegedErrorCode::CapabilityRequestRejected,
    }
}

/// Builds a standardized fail-closed response for authority-context denials.
fn deny_response_for_authority_context(
    condition: DenyCondition,
    detail: impl Into<String>,
) -> PrivilegedResponse {
    let deny_reason = fac_workobject_implementor_v2_role_contract().deny_reason_for(condition);
    let detail = detail.into();
    PrivilegedResponse::error(
        map_deny_reason_code_to_privileged_error(deny_reason.code),
        format!(
            "{} ({condition}): {}; {detail}",
            deny_reason.code, deny_reason.message
        ),
    )
}

/// Resolves and seeds the authoritative `RoleSpecV2` hash for `WorkObject`
/// execution.
///
/// Returns an error when the built-in role registry cannot be seeded, the
/// role id is missing, or the resulting hash is not CAS-resolvable.
pub fn resolve_workobject_role_spec_hash(
    cas: &dyn ContentAddressedStore,
) -> Result<[u8; 32], String> {
    let registry = seed_builtin_role_contracts_v2_in_cas(cas)
        .map_err(|e| format!("failed to seed builtin RoleSpecV2 contracts in CAS: {e}"))?;
    let role_spec_hash = registry
        .get(FAC_WORKOBJECT_IMPLEMENTOR_V2_ROLE_ID)
        .copied()
        .ok_or_else(|| {
            format!(
                "missing role contract hash for role_id='{FAC_WORKOBJECT_IMPLEMENTOR_V2_ROLE_ID}'"
            )
        })?;

    if bool::from(role_spec_hash.ct_eq(&[0u8; 32])) {
        return Err("resolved role_spec_hash is zero".to_string());
    }

    match cas.exists(&role_spec_hash) {
        Ok(true) => Ok(role_spec_hash),
        Ok(false) => Err(format!(
            "resolved role_spec_hash {} is not present in CAS",
            hex::encode(role_spec_hash)
        )),
        Err(e) => Err(format!(
            "CAS existence check for role_spec_hash failed: {e}"
        )),
    }
}

fn compute_policy_required_read_digest_set_hash(
    required_read_digests: &BTreeMap<String, [u8; 32]>,
) -> Result<[u8; 32], String> {
    let mut payload = Vec::new();
    for (path, digest) in required_read_digests {
        let path_len = u32::try_from(path.len())
            .map_err(|_| "required read path length exceeds u32 range".to_string())?;
        payload.extend_from_slice(&path_len.to_be_bytes());
        payload.extend_from_slice(path.as_bytes());
        payload.extend_from_slice(digest);
    }
    Ok(*blake3::hash(&payload).as_bytes())
}

/// Builds a deterministic `CompiledContextPackRecipe` for policy lineage.
///
/// The compiled recipe binds role/context/budget hashes to the deterministic
/// policy context materialization used by `ClaimWork` and `SpawnEpisode`.
pub fn build_policy_context_pack_recipe(
    work_id: &str,
    actor_id: &str,
    role_spec_hash: [u8; 32],
    context_pack_hash: [u8; 32],
) -> Result<apm2_core::context::CompiledContextPackRecipe, String> {
    let required_read_path = format!("work/{work_id}/context.yaml");
    let required_read_digest =
        *blake3::hash(format!("content:{work_id}:{actor_id}").as_bytes()).as_bytes();
    let required_read_paths = vec![required_read_path.clone()];
    let mut required_read_digests = BTreeMap::new();
    required_read_digests.insert(required_read_path, required_read_digest);
    let required_read_digest_set_hash =
        compute_policy_required_read_digest_set_hash(&required_read_digests)?;
    let budget_profile_hash =
        *blake3::hash(format!("budget:{work_id}:{actor_id}").as_bytes()).as_bytes();

    let recipe = apm2_core::context::ContextPackRecipe::new(
        role_spec_hash,
        required_read_paths,
        required_read_digests,
        required_read_digest_set_hash,
        context_pack_hash,
        budget_profile_hash,
    )
    .map_err(|e| format!("failed to build ContextPackRecipe: {e}"))?;
    let recipe_hash = recipe
        .recipe_hash()
        .map_err(|e| format!("failed to compute ContextPackRecipe hash: {e}"))?;
    let fingerprint = apm2_core::context::DriftFingerprint {
        role_hash: role_spec_hash,
        required_read_digest_set_hash,
        context_manifest_hash: context_pack_hash,
        budget_profile_hash,
        recipe_hash,
        compiled_at_tick: 0,
    };

    Ok(apm2_core::context::CompiledContextPackRecipe {
        recipe,
        fingerprint,
    })
}

/// Returns the deterministic `ContextPackRecipe` hash for policy lineage.
///
/// # Errors
///
/// Returns an error if recipe materialization or hashing fails.
pub fn policy_context_pack_recipe_hash(
    work_id: &str,
    actor_id: &str,
    role_spec_hash: [u8; 32],
    context_pack_hash: [u8; 32],
) -> Result<[u8; 32], String> {
    let compiled =
        build_policy_context_pack_recipe(work_id, actor_id, role_spec_hash, context_pack_hash)?;
    compiled
        .recipe
        .recipe_hash()
        .map_err(|e| format!("failed to compute policy context recipe hash: {e}"))
}

/// Seeds CAS with the preimages of policy-provided hashes so they become
/// CAS-resolvable at validation time (REQ-HEF-0013).
///
/// MUST be called before [`validate_and_store_transition_authority`] in
/// handler paths (`handle_claim_work`, `handle_spawn_episode`).
///
/// For the **capability manifest**, the preimage is either:
/// - Reviewer role: the canonical bytes of the reviewer v0 manifest.
/// - Other roles: `format!("manifest:{work_id}:{actor_id}")`.
///
/// For the **context pack**, the preimage is the
/// [`canonical_seal_bytes`](apm2_core::context::ContextPackManifest::canonical_seal_bytes)
/// of the deterministic context pack built from `(work_id, actor_id)`.
///
/// For the **role spec**, built-in v2 contracts are seeded into CAS and the
/// policy-provided `role_spec_hash` must match the canonical built-in hash.
///
/// For the **context recipe**, deterministic canonical recipe bytes are stored
/// and must match the policy-provided `context_pack_recipe_hash`.
pub fn seed_policy_artifacts_in_cas(
    work_id: &str,
    actor_id: &str,
    role: WorkRole,
    role_spec_hash: [u8; 32],
    context_pack_recipe_hash: [u8; 32],
    cas: &dyn ContentAddressedStore,
) -> Result<(), String> {
    // ---- capability manifest preimage ----
    if role == WorkRole::Reviewer {
        let manifest = crate::episode::reviewer_manifest::reviewer_v0_manifest();
        let canonical = manifest.canonical_bytes();
        cas.store(&canonical)
            .map_err(|e| format!("CAS store reviewer manifest failed: {e}"))?;
    } else {
        let preimage = format!("manifest:{work_id}:{actor_id}");
        cas.store(preimage.as_bytes())
            .map_err(|e| format!("CAS store capability manifest failed: {e}"))?;
    }

    // ---- context pack preimage ----
    let context_pack = build_policy_context_pack(work_id, actor_id);
    let seal_bytes = context_pack.canonical_seal_bytes();
    cas.store(&seal_bytes)
        .map_err(|e| format!("CAS store context pack failed: {e}"))?;

    // ---- role spec contract preimage ----
    if bool::from(role_spec_hash.ct_eq(&[0u8; 32])) {
        return Err("role_spec_hash is zero (missing authority context)".to_string());
    }
    let cas_role_spec_hash = resolve_workobject_role_spec_hash(cas)?;
    if !bool::from(role_spec_hash.ct_eq(&cas_role_spec_hash)) {
        return Err(format!(
            "role_spec_hash mismatch: policy={} cas={}",
            hex::encode(role_spec_hash),
            hex::encode(cas_role_spec_hash),
        ));
    }

    // ---- context-pack recipe preimage ----
    if bool::from(context_pack_recipe_hash.ct_eq(&[0u8; 32])) {
        return Err("context_pack_recipe_hash is zero (missing authority context)".to_string());
    }
    let compiled = build_policy_context_pack_recipe(
        work_id,
        actor_id,
        role_spec_hash,
        context_pack.manifest_hash(),
    )?;
    let recipe_bytes = compiled
        .recipe
        .canonical_bytes()
        .map_err(|e| format!("failed to canonicalize policy context recipe: {e}"))?;
    let computed_recipe_hash = *blake3::hash(&recipe_bytes).as_bytes();
    if !bool::from(context_pack_recipe_hash.ct_eq(&computed_recipe_hash)) {
        return Err(format!(
            "context_pack_recipe_hash mismatch: policy={} computed={}",
            hex::encode(context_pack_recipe_hash),
            hex::encode(computed_recipe_hash),
        ));
    }
    cas.store(&recipe_bytes)
        .map_err(|e| format!("CAS store context pack recipe failed: {e}"))?;

    Ok(())
}

/// Handler-level authority binding validation for production transition paths
/// (TCK-00416).
///
/// This combines CAS artifact storage and binding validation into one
/// transactional call. ALL binding hashes (including policy-provided
/// `capability_manifest_hash` and `context_pack_hash`) are validated as
/// non-zero AND CAS-resolvable per REQ-HEF-0013. Zero values indicate
/// missing policy resolution and are hard failures.
///
/// Self-derived hashes (permeability receipt, stop conditions, typed budget)
/// are stored in CAS by [`store_authority_binding_artifacts`] in Step 1.
/// Policy-provided hashes (`capability_manifest_hash`, `context_pack_hash`)
/// MUST be seeded in CAS by the caller before invoking this function (see
/// [`seed_policy_artifacts_in_cas`]).
///
/// # Fail-Closed Semantics
///
/// Missing, empty, or zero bindings cause hard rejection. Called from
/// `handle_claim_work` and `handle_spawn_episode` BEFORE any state mutation.
pub fn validate_and_store_transition_authority(
    work_id: &str,
    bindings: &TransitionAuthorityBindings,
    cas: &dyn ContentAddressedStore,
) -> Result<(), TransitionAuthorityError> {
    // Step 1: Store self-derived artifacts so their hashes become CAS-resolvable.
    if let Err(e) = store_authority_binding_artifacts(work_id, bindings, cas) {
        return Err(TransitionAuthorityError {
            message: format!("authority binding CAS artifact storage failed: {e}"),
            violations: vec![e],
        });
    }

    // Step 2: Validate all binding fields.
    let mut violations: Vec<String> = Vec::new();

    // String field checks
    if bindings.lease_id.is_empty() {
        violations.push("lease_id is empty".to_string());
    }
    if bindings.actor_id.is_empty() {
        violations.push("actor_id is empty".to_string());
    }
    if bindings.policy_resolved_ref.is_empty() {
        violations.push("policy_resolved_ref is empty".to_string());
    }

    // Non-zero hash checks for ALL required hashes (REQ-HEF-0013).
    // capability_manifest_hash and context_pack_hash are mandatory per
    // REQ-HEF-0013 -- zero values indicate missing policy resolution and
    // MUST be rejected (fail-closed).
    if bool::from(bindings.permeability_receipt_hash.ct_eq(&[0u8; 32])) {
        violations.push("permeability_receipt_hash is zero (unset)".to_string());
    }
    if bool::from(bindings.capability_manifest_hash.ct_eq(&[0u8; 32])) {
        violations.push("capability_manifest_hash is zero (unset)".to_string());
    }
    if bool::from(bindings.context_pack_hash.ct_eq(&[0u8; 32])) {
        violations.push("context_pack_hash is zero (unset)".to_string());
    }
    if bool::from(bindings.stop_condition_hash.ct_eq(&[0u8; 32])) {
        violations.push("stop_condition_hash is zero (unset)".to_string());
    }
    if bool::from(bindings.typed_budget_hash.ct_eq(&[0u8; 32])) {
        violations.push("typed_budget_hash is zero (unset)".to_string());
    }

    // CAS resolvability for ALL required hashes (REQ-HEF-0013).
    // Both self-derived hashes (permeability, stop, budget) and
    // policy-provided hashes (capability_manifest, context_pack) MUST
    // be CAS-resolvable at validation time. Callers are responsible for
    // storing policy-provided preimages in CAS before invoking this
    // function (see `seed_policy_artifacts_in_cas`).
    for (name, hash) in [
        (
            "permeability_receipt_hash",
            bindings.permeability_receipt_hash,
        ),
        (
            "capability_manifest_hash",
            bindings.capability_manifest_hash,
        ),
        ("context_pack_hash", bindings.context_pack_hash),
        ("stop_condition_hash", bindings.stop_condition_hash),
        ("typed_budget_hash", bindings.typed_budget_hash),
    ] {
        if !bool::from(hash.ct_eq(&[0u8; 32])) && violations.len() < MAX_BINDING_VIOLATIONS {
            match cas.exists(&hash) {
                Ok(true) => {},
                Ok(false) => violations.push(format!(
                    "{name} {} not resolvable in CAS",
                    hex::encode(hash)
                )),
                Err(e) => violations.push(format!("{name} CAS lookup failed: {e}")),
            }
        }
    }

    // Budget checks
    if bindings.typed_budgets.max_tokens == 0
        && bindings.typed_budgets.max_tool_calls == 0
        && bindings.typed_budgets.max_wall_ms == 0
    {
        violations.push("typed_budgets has all-zero limits (Unspecified role budget)".to_string());
    }

    // Hash integrity: re-derive and verify
    let recomputed_stop_hash = hash_bytes(&bindings.stop_conditions.canonical_bytes());
    if !bool::from(bindings.stop_condition_hash.ct_eq(&[0u8; 32]))
        && !bool::from(bindings.stop_condition_hash.ct_eq(&recomputed_stop_hash))
    {
        violations.push(format!(
            "stop_condition_hash mismatch: declared={} recomputed={}",
            hex::encode(bindings.stop_condition_hash),
            hex::encode(recomputed_stop_hash),
        ));
    }

    if let Ok(budget_payload) =
        canonical_json_bytes(&build_typed_budget_payload(&bindings.typed_budgets))
    {
        let recomputed_budget_hash = hash_bytes(&budget_payload);
        if !bool::from(bindings.typed_budget_hash.ct_eq(&[0u8; 32]))
            && !bool::from(bindings.typed_budget_hash.ct_eq(&recomputed_budget_hash))
        {
            violations.push(format!(
                "typed_budget_hash mismatch: declared={} recomputed={}",
                hex::encode(bindings.typed_budget_hash),
                hex::encode(recomputed_budget_hash),
            ));
        }
    }

    if violations.is_empty() {
        Ok(())
    } else {
        let count = violations.len();
        Err(TransitionAuthorityError {
            message: format!("authority binding validation failed with {count} violation(s)"),
            violations,
        })
    }
}

/// Appends review outcome binding fields to a JSON event payload (TCK-00416).
pub fn append_review_outcome_fields(
    payload: &mut serde_json::Map<String, serde_json::Value>,
    bindings: &ReviewOutcomeBindings,
) {
    payload.insert(
        "view_commitment_hash".to_string(),
        serde_json::Value::String(hex::encode(bindings.view_commitment_hash)),
    );
    payload.insert(
        "tool_log_index_hash".to_string(),
        serde_json::Value::String(hex::encode(bindings.tool_log_index_hash)),
    );
    payload.insert(
        "summary_receipt_hash".to_string(),
        serde_json::Value::String(hex::encode(bindings.summary_receipt_hash)),
    );
}

/// Appends privileged PCAC lifecycle selectors to an event payload.
pub fn append_privileged_pcac_lifecycle_fields(
    payload: &mut serde_json::Map<String, serde_json::Value>,
    artifacts: &PrivilegedPcacLifecycleArtifacts,
) {
    payload.insert(
        "ajc_id".to_string(),
        serde_json::Value::String(hex::encode(artifacts.ajc_id)),
    );
    payload.insert(
        "intent_digest".to_string(),
        serde_json::Value::String(hex::encode(artifacts.intent_digest)),
    );
    payload.insert(
        "consume_tick".to_string(),
        serde_json::Value::Number(serde_json::Number::from(artifacts.consume_tick)),
    );
    payload.insert(
        "pcac_time_envelope_ref".to_string(),
        serde_json::Value::String(hex::encode(artifacts.time_envelope_ref)),
    );
    payload.insert(
        "consume_selector_digest".to_string(),
        serde_json::Value::String(hex::encode(artifacts.consume_selector_digest)),
    );
}

/// Maximum number of events stored in `StubLedgerEventEmitter`.
///
/// Per CTR-1303: In-memory stores must have `max_entries` limit with O(1)
/// eviction. This prevents denial-of-service via memory exhaustion from
/// unbounded event emission.
pub const MAX_LEDGER_EVENTS: usize = MAX_PROJECTION_EVENTS;

/// Stub ledger event emitter for testing.
///
/// Stores events in memory with Ed25519 signatures using a test signing key.
/// In production, this will be replaced with `SqliteLedgerEventEmitter`.
///
/// # Capacity Limits (CTR-1303)
///
/// This emitter enforces a maximum of [`MAX_LEDGER_EVENTS`] entries to prevent
/// memory exhaustion. When the limit is reached, the oldest entry (by insertion
/// order) is evicted to make room for the new event.
#[derive(Debug)]
pub struct StubLedgerEventEmitter {
    /// Events stored with insertion order for LRU eviction.
    /// The `Vec` maintains insertion order; oldest entries are at the front.
    events: std::sync::RwLock<(
        Vec<String>,
        std::collections::HashMap<String, SignedLedgerEvent>,
    )>,
    /// Events indexed by work ID for efficient querying.
    events_by_work_id: std::sync::RwLock<std::collections::HashMap<String, Vec<String>>>,
    /// Signing key for event signatures (test key).
    signing_key: ed25519_dalek::SigningKey,
}

impl Default for StubLedgerEventEmitter {
    fn default() -> Self {
        Self::new()
    }
}

impl StubLedgerEventEmitter {
    /// Creates a new stub emitter with a random test signing key.
    #[must_use]
    pub fn new() -> Self {
        use rand::rngs::OsRng;
        Self {
            events: std::sync::RwLock::new((
                Vec::with_capacity(MAX_LEDGER_EVENTS.min(1000)), // Pre-allocate reasonably
                std::collections::HashMap::with_capacity(MAX_LEDGER_EVENTS.min(1000)),
            )),
            events_by_work_id: std::sync::RwLock::new(std::collections::HashMap::new()),
            signing_key: ed25519_dalek::SigningKey::generate(&mut OsRng),
        }
    }

    /// Creates a new stub emitter with a specific signing key.
    #[must_use]
    pub fn with_signing_key(signing_key: ed25519_dalek::SigningKey) -> Self {
        Self {
            events: std::sync::RwLock::new((
                Vec::with_capacity(MAX_LEDGER_EVENTS.min(1000)), // Pre-allocate reasonably
                std::collections::HashMap::with_capacity(MAX_LEDGER_EVENTS.min(1000)),
            )),
            events_by_work_id: std::sync::RwLock::new(std::collections::HashMap::new()),
            signing_key,
        }
    }

    /// Returns the verifying (public) key for signature verification.
    #[must_use]
    pub fn verifying_key(&self) -> ed25519_dalek::VerifyingKey {
        self.signing_key.verifying_key()
    }
}

impl LedgerEventEmitter for StubLedgerEventEmitter {
    fn emit_work_claimed(
        &self,
        claim: &WorkClaim,
        timestamp_ns: u64,
    ) -> Result<SignedLedgerEvent, LedgerEventError> {
        use ed25519_dalek::Signer;

        // Generate unique event ID
        let event_id = format!("EVT-{}", uuid::Uuid::new_v4());

        // Build canonical payload (deterministic JSON)
        // TCK-00416: Derive authority bindings and embed in signed payload.
        // This ensures transition authority fields are persisted in the
        // signed event and cannot be stripped post-signing.
        let mut payload_map = serde_json::Map::new();
        payload_map.insert("event_type".to_string(), serde_json::json!("work_claimed"));
        payload_map.insert("work_id".to_string(), serde_json::json!(claim.work_id));
        payload_map.insert("lease_id".to_string(), serde_json::json!(claim.lease_id));
        payload_map.insert("actor_id".to_string(), serde_json::json!(claim.actor_id));
        payload_map.insert(
            "role".to_string(),
            serde_json::json!(format!("{:?}", claim.role)),
        );
        payload_map.insert(
            "policy_resolved_ref".to_string(),
            serde_json::json!(claim.policy_resolution.policy_resolved_ref),
        );
        payload_map.insert(
            "capability_manifest_hash".to_string(),
            serde_json::json!(hex::encode(
                claim.policy_resolution.capability_manifest_hash
            )),
        );
        payload_map.insert(
            "context_pack_hash".to_string(),
            serde_json::json!(hex::encode(claim.policy_resolution.context_pack_hash)),
        );
        payload_map.insert(
            "role_spec_hash".to_string(),
            serde_json::json!(hex::encode(claim.policy_resolution.role_spec_hash)),
        );
        payload_map.insert(
            "context_pack_recipe_hash".to_string(),
            serde_json::json!(hex::encode(
                claim.policy_resolution.context_pack_recipe_hash
            )),
        );

        // TCK-00416 BLOCKER 3: Append transition authority binding fields
        // to the signed payload. These are derived from the claim and
        // persisted in the event so they are audit-bound.
        if let Ok(authority_bindings) = derive_claim_transition_authority_bindings(claim) {
            append_transition_authority_fields(&mut payload_map, &authority_bindings);
        }

        let payload = serde_json::Value::Object(payload_map);

        let payload_bytes =
            serde_json::to_vec(&payload).map_err(|e| LedgerEventError::SigningFailed {
                message: format!("payload serialization failed: {e}"),
            })?;

        // Build canonical bytes for signing (domain prefix + payload)
        let mut canonical_bytes =
            Vec::with_capacity(WORK_CLAIMED_DOMAIN_PREFIX.len() + payload_bytes.len());
        canonical_bytes.extend_from_slice(WORK_CLAIMED_DOMAIN_PREFIX);
        canonical_bytes.extend_from_slice(&payload_bytes);

        // Sign the canonical bytes
        let signature = self.signing_key.sign(&canonical_bytes);

        // TCK-00289: Use provided HTF-compliant timestamp from HolonicClock.
        // The timestamp_ns parameter is now provided by the caller from
        // HolonicClock.now_hlc() ensuring RFC-0016 HTF compliance.

        let signed_event = SignedLedgerEvent {
            event_id: event_id.clone(),
            event_type: "work_claimed".to_string(),
            work_id: claim.work_id.clone(),
            actor_id: claim.actor_id.clone(),
            payload: payload_bytes,
            signature: signature.to_bytes().to_vec(),
            timestamp_ns,
        };

        // CTR-1303: Persist to bounded in-memory store with LRU eviction
        {
            let mut guard = self.events.write().expect("lock poisoned");
            let mut events_by_work = self.events_by_work_id.write().expect("lock poisoned");
            let (order, events) = &mut *guard;

            // Evict oldest entries if at capacity, also pruning events_by_work_id index
            while events.len() >= MAX_LEDGER_EVENTS {
                if let Some(oldest_key) = order.first().cloned() {
                    order.remove(0);
                    // Remove from events and prune the events_by_work_id index
                    if let Some(evicted_event) = events.remove(&oldest_key) {
                        // Remove from work_id index
                        if let Some(work_id_events) = events_by_work.get_mut(&evicted_event.work_id)
                        {
                            work_id_events.retain(|id| id != &oldest_key);
                            // Remove the entry entirely if no events remain for this work_id
                            if work_id_events.is_empty() {
                                events_by_work.remove(&evicted_event.work_id);
                            }
                        }
                    }
                    debug!(
                        evicted_event_id = %oldest_key,
                        "Evicted oldest ledger event to maintain capacity limit"
                    );
                } else {
                    break;
                }
            }

            order.push(event_id.clone());
            events.insert(event_id.clone(), signed_event.clone());
            events_by_work
                .entry(claim.work_id.clone())
                .or_default()
                .push(event_id);
        }

        info!(
            event_id = %signed_event.event_id,
            work_id = %signed_event.work_id,
            actor_id = %signed_event.actor_id,
            "WorkClaimed event signed and persisted"
        );

        Ok(signed_event)
    }

    fn get_event(&self, event_id: &str) -> Option<SignedLedgerEvent> {
        let guard = self.events.read().expect("lock poisoned");
        guard.1.get(event_id).cloned()
    }

    fn emit_session_started(
        &self,
        session_id: &str,
        work_id: &str,
        lease_id: &str,
        actor_id: &str,
        adapter_profile_hash: &[u8; 32],
        role_spec_hash: Option<&[u8; 32]>,
        timestamp_ns: u64,
        contract_binding: Option<&crate::hsi_contract::SessionContractBinding>,
        identity_proof_profile_hash: Option<&[u8; 32]>,
        selection_decision: Option<&SelectionDecision>,
    ) -> Result<SignedLedgerEvent, LedgerEventError> {
        use ed25519_dalek::Signer;

        // Domain prefix for session events (must be at function start per clippy)
        const SESSION_STARTED_DOMAIN_PREFIX: &[u8] = b"apm2.event.session_started:";

        // Generate unique event ID
        let event_id = format!("EVT-{}", uuid::Uuid::new_v4());

        let payload = build_session_started_payload(
            session_id,
            work_id,
            lease_id,
            actor_id,
            adapter_profile_hash,
            role_spec_hash,
            contract_binding,
            identity_proof_profile_hash,
            selection_decision,
        );

        let payload_bytes =
            serde_json::to_vec(&payload).map_err(|e| LedgerEventError::SigningFailed {
                message: format!("payload serialization failed: {e}"),
            })?;

        // Build canonical bytes for signing (domain prefix + payload)
        let mut canonical_bytes =
            Vec::with_capacity(SESSION_STARTED_DOMAIN_PREFIX.len() + payload_bytes.len());
        canonical_bytes.extend_from_slice(SESSION_STARTED_DOMAIN_PREFIX);
        canonical_bytes.extend_from_slice(&payload_bytes);

        // Sign the canonical bytes
        let signature = self.signing_key.sign(&canonical_bytes);

        let signed_event = SignedLedgerEvent {
            event_id: event_id.clone(),
            event_type: "session_started".to_string(),
            work_id: work_id.to_string(),
            actor_id: actor_id.to_string(),
            payload: payload_bytes,
            signature: signature.to_bytes().to_vec(),
            timestamp_ns,
        };

        // CTR-1303: Persist to bounded in-memory store with LRU eviction
        {
            let mut guard = self.events.write().expect("lock poisoned");
            let mut events_by_work = self.events_by_work_id.write().expect("lock poisoned");
            let (order, events) = &mut *guard;

            // Evict oldest entries if at capacity
            while events.len() >= MAX_LEDGER_EVENTS {
                if let Some(oldest_key) = order.first().cloned() {
                    order.remove(0);
                    if let Some(evicted_event) = events.remove(&oldest_key) {
                        if let Some(work_id_events) = events_by_work.get_mut(&evicted_event.work_id)
                        {
                            work_id_events.retain(|id| id != &oldest_key);
                            if work_id_events.is_empty() {
                                events_by_work.remove(&evicted_event.work_id);
                            }
                        }
                    }
                } else {
                    break;
                }
            }

            order.push(event_id.clone());
            events.insert(event_id.clone(), signed_event.clone());
            events_by_work
                .entry(work_id.to_string())
                .or_default()
                .push(event_id);
        }

        info!(
            event_id = %signed_event.event_id,
            session_id = %session_id,
            work_id = %signed_event.work_id,
            "SessionStarted event signed and persisted"
        );

        Ok(signed_event)
    }

    fn emit_session_event(
        &self,
        session_id: &str,
        event_type: &str,
        payload: &[u8],
        actor_id: &str,
        timestamp_ns: u64,
    ) -> Result<SignedLedgerEvent, LedgerEventError> {
        use ed25519_dalek::Signer;

        // Domain prefix for generic session events (TCK-00290)
        const SESSION_EVENT_DOMAIN_PREFIX: &[u8] = b"apm2.event.session_event:";

        // Generate unique event ID
        let event_id = format!("EVT-{}", uuid::Uuid::new_v4());

        // Build payload as JSON with actual event type and hex-encoded payload
        let payload_json = serde_json::json!({
            "event_type": event_type,
            "session_id": session_id,
            "actor_id": actor_id,
            "payload": hex::encode(payload),
        });

        // MAJOR 1 FIX (TCK-00290): Use JCS (RFC 8785) canonicalization for signing.
        // This matches the production SqliteLedgerEventEmitter and ensures
        // deterministic JSON representation per RFC-0016. Using
        // serde_json::to_vec is non-deterministic because it does not guarantee
        // key ordering.
        let payload_string = payload_json.to_string();
        let canonical_payload =
            canonicalize_json(&payload_string).map_err(|e| LedgerEventError::SigningFailed {
                message: format!("JCS canonicalization failed: {e}"),
            })?;
        let payload_bytes = canonical_payload.as_bytes().to_vec();

        // Build canonical bytes for signing (domain prefix + JCS payload)
        let mut canonical_bytes =
            Vec::with_capacity(SESSION_EVENT_DOMAIN_PREFIX.len() + payload_bytes.len());
        canonical_bytes.extend_from_slice(SESSION_EVENT_DOMAIN_PREFIX);
        canonical_bytes.extend_from_slice(&payload_bytes);

        // Sign the canonical bytes
        let signature = self.signing_key.sign(&canonical_bytes);

        let signed_event = SignedLedgerEvent {
            event_id: event_id.clone(),
            event_type: event_type.to_string(),
            work_id: session_id.to_string(), // Use session_id as work_id for indexing
            actor_id: actor_id.to_string(),
            payload: payload_bytes,
            signature: signature.to_bytes().to_vec(),
            timestamp_ns,
        };

        // CTR-1303: Persist to bounded in-memory store with LRU eviction
        {
            let mut guard = self.events.write().expect("lock poisoned");
            let mut events_by_work = self.events_by_work_id.write().expect("lock poisoned");
            let (order, events) = &mut *guard;

            // Evict oldest entries if at capacity
            while events.len() >= MAX_LEDGER_EVENTS {
                if let Some(oldest_key) = order.first().cloned() {
                    order.remove(0);
                    if let Some(evicted_event) = events.remove(&oldest_key) {
                        if let Some(work_id_events) = events_by_work.get_mut(&evicted_event.work_id)
                        {
                            work_id_events.retain(|id| id != &oldest_key);
                            if work_id_events.is_empty() {
                                events_by_work.remove(&evicted_event.work_id);
                            }
                        }
                    }
                } else {
                    break;
                }
            }

            order.push(event_id.clone());
            events.insert(event_id.clone(), signed_event.clone());
            events_by_work
                .entry(session_id.to_string())
                .or_default()
                .push(event_id);
        }

        info!(
            event_id = %signed_event.event_id,
            session_id = %session_id,
            event_type = %event_type,
            actor_id = %actor_id,
            "SessionEvent signed and persisted"
        );

        Ok(signed_event)
    }

    fn has_durable_storage(&self) -> bool {
        false
    }

    fn emit_stop_flags_mutated(
        &self,
        mutation: &StopFlagsMutation<'_>,
    ) -> Result<SignedLedgerEvent, LedgerEventError> {
        use ed25519_dalek::Signer;

        let event_id = format!("EVT-{}", uuid::Uuid::new_v4());

        let payload_json = serde_json::json!({
            "event_type": "stop_flags_mutated",
            "actor_id": mutation.actor_id,
            "emergency_stop_previous": mutation.emergency_stop_previous,
            "emergency_stop_current": mutation.emergency_stop_current,
            "governance_stop_previous": mutation.governance_stop_previous,
            "governance_stop_current": mutation.governance_stop_current,
            "request_context": mutation.request_context,
            "timestamp_ns": mutation.timestamp_ns,
        });

        let payload_string = payload_json.to_string();
        let canonical_payload =
            canonicalize_json(&payload_string).map_err(|e| LedgerEventError::SigningFailed {
                message: format!("JCS canonicalization failed: {e}"),
            })?;
        let payload_bytes = canonical_payload.as_bytes().to_vec();

        let mut canonical_bytes =
            Vec::with_capacity(STOP_FLAGS_MUTATED_DOMAIN_PREFIX.len() + payload_bytes.len());
        canonical_bytes.extend_from_slice(STOP_FLAGS_MUTATED_DOMAIN_PREFIX);
        canonical_bytes.extend_from_slice(&payload_bytes);

        let signature = self.signing_key.sign(&canonical_bytes);

        let signed_event = SignedLedgerEvent {
            event_id: event_id.clone(),
            event_type: "stop_flags_mutated".to_string(),
            work_id: STOP_FLAGS_MUTATED_WORK_ID.to_string(),
            actor_id: mutation.actor_id.to_string(),
            payload: payload_bytes,
            signature: signature.to_bytes().to_vec(),
            timestamp_ns: mutation.timestamp_ns,
        };

        {
            let mut guard = self.events.write().expect("lock poisoned");
            let mut events_by_work = self.events_by_work_id.write().expect("lock poisoned");
            let (order, events) = &mut *guard;

            while events.len() >= MAX_LEDGER_EVENTS {
                if let Some(oldest_key) = order.first().cloned() {
                    order.remove(0);
                    if let Some(evicted_event) = events.remove(&oldest_key) {
                        if let Some(work_id_events) = events_by_work.get_mut(&evicted_event.work_id)
                        {
                            work_id_events.retain(|id| id != &oldest_key);
                            if work_id_events.is_empty() {
                                events_by_work.remove(&evicted_event.work_id);
                            }
                        }
                    }
                } else {
                    break;
                }
            }

            order.push(event_id.clone());
            events.insert(event_id.clone(), signed_event.clone());
            events_by_work
                .entry(STOP_FLAGS_MUTATED_WORK_ID.to_string())
                .or_default()
                .push(event_id);
        }

        info!(
            event_id = %signed_event.event_id,
            actor_id = %mutation.actor_id,
            emergency_stop_previous = mutation.emergency_stop_previous,
            emergency_stop_current = mutation.emergency_stop_current,
            governance_stop_previous = mutation.governance_stop_previous,
            governance_stop_current = mutation.governance_stop_current,
            "StopFlagsMutated event signed and persisted"
        );

        Ok(signed_event)
    }

    fn emit_defect_recorded(
        &self,
        defect: &DefectRecorded,
        timestamp_ns: u64,
    ) -> Result<SignedLedgerEvent, LedgerEventError> {
        use ed25519_dalek::Signer;

        // TCK-00307 MAJOR 4: Call validate() to enforce DoS protections
        defect
            .validate()
            .map_err(|e| LedgerEventError::ValidationFailed { message: e })?;

        // Generate unique event ID
        let event_id = format!("EVT-{}", uuid::Uuid::new_v4());

        // TCK-00307 BLOCKER: Use JCS/JSON wire format (not ProtoBuf) to match
        // SqliteLedgerEventEmitter and ensure consumer uniformity.
        // Include time_envelope_ref for temporal binding (MAJOR 1).
        let time_envelope_ref_hex = defect
            .time_envelope_ref
            .as_ref()
            .map(|ter| hex::encode(&ter.hash));

        let payload = serde_json::json!({
            "event_type": "defect_recorded",
            "defect_id": defect.defect_id,
            "defect_type": defect.defect_type,
            "cas_hash": hex::encode(&defect.cas_hash),
            "source": defect.source,
            "work_id": defect.work_id,
            "severity": defect.severity,
            "detected_at": defect.detected_at,
            "time_envelope_ref": time_envelope_ref_hex,
        });

        // Use JCS (RFC 8785) canonicalization for deterministic signing
        let payload_json = payload.to_string();
        let canonical_payload =
            canonicalize_json(&payload_json).map_err(|e| LedgerEventError::SigningFailed {
                message: format!("JCS canonicalization failed: {e}"),
            })?;
        let payload_bytes = canonical_payload.as_bytes().to_vec();

        // Build canonical bytes for signing (domain prefix + JCS payload)
        let mut canonical_bytes =
            Vec::with_capacity(DEFECT_RECORDED_DOMAIN_PREFIX.len() + payload_bytes.len());
        canonical_bytes.extend_from_slice(DEFECT_RECORDED_DOMAIN_PREFIX);
        canonical_bytes.extend_from_slice(&payload_bytes);

        // Sign the canonical bytes
        let signature = self.signing_key.sign(&canonical_bytes);

        let signed_event = SignedLedgerEvent {
            event_id: event_id.clone(),
            event_type: "defect_recorded".to_string(),
            work_id: defect.work_id.clone(),
            actor_id: "system".to_string(),
            payload: payload_bytes,
            signature: signature.to_bytes().to_vec(),
            timestamp_ns,
        };

        // CTR-1303: Persist to bounded in-memory store with LRU eviction
        {
            let mut guard = self.events.write().expect("lock poisoned");
            let mut events_by_work = self.events_by_work_id.write().expect("lock poisoned");
            let (order, events) = &mut *guard;

            // Evict oldest entries if at capacity
            while events.len() >= MAX_LEDGER_EVENTS {
                if let Some(oldest_key) = order.first().cloned() {
                    order.remove(0);
                    if let Some(evicted_event) = events.remove(&oldest_key) {
                        if let Some(work_id_events) = events_by_work.get_mut(&evicted_event.work_id)
                        {
                            work_id_events.retain(|id| id != &oldest_key);
                            if work_id_events.is_empty() {
                                events_by_work.remove(&evicted_event.work_id);
                            }
                        }
                    }
                } else {
                    break;
                }
            }

            order.push(event_id.clone());
            events.insert(event_id.clone(), signed_event.clone());
            events_by_work
                .entry(defect.work_id.clone())
                .or_default()
                .push(event_id);
        }

        info!(
            event_id = %signed_event.event_id,
            defect_id = %defect.defect_id,
            work_id = %signed_event.work_id,
            "DefectRecorded event signed and persisted"
        );

        Ok(signed_event)
    }

    fn get_events_by_work_id(&self, work_id: &str) -> Vec<SignedLedgerEvent> {
        let events_by_work = self.events_by_work_id.read().expect("lock poisoned");
        let guard = self.events.read().expect("lock poisoned");

        events_by_work
            .get(work_id)
            .map(|event_ids| {
                event_ids
                    .iter()
                    .filter_map(|id| guard.1.get(id).cloned())
                    .collect()
            })
            .unwrap_or_default()
    }

    fn get_all_events(&self) -> Vec<SignedLedgerEvent> {
        let guard = self.events.read().expect("lock poisoned");
        let (order, events) = &*guard;

        order
            .iter()
            .filter_map(|event_id| events.get(event_id).cloned())
            .collect()
    }

    fn get_event_count(&self) -> usize {
        let guard = self.events.read().expect("lock poisoned");
        guard.0.len()
    }

    fn get_latest_event(&self) -> Option<SignedLedgerEvent> {
        let guard = self.events.read().expect("lock poisoned");
        let (order, events) = &*guard;
        order
            .last()
            .and_then(|event_id| events.get(event_id).cloned())
    }

    fn get_latest_governance_policy_event(&self) -> Option<SignedLedgerEvent> {
        let guard = self.events.read().expect("lock poisoned");
        let (order, events) = &*guard;
        order.iter().rev().find_map(|event_id| {
            let event = events.get(event_id)?;
            (classify_event_type(&event.event_type, &event.actor_id) == EventTypeClass::Governance
                && is_governance_policy_event_type(&event.event_type))
            .then(|| event.clone())
        })
    }

    fn get_latest_gate_policy_resolved_event(&self) -> Option<SignedLedgerEvent> {
        let guard = self.events.read().expect("lock poisoned");
        let (order, events) = &*guard;
        order.iter().rev().find_map(|event_id| {
            let event = events.get(event_id)?;
            (event.event_type == "gate.policy_resolved"
                && classify_event_type(&event.event_type, &event.actor_id)
                    == EventTypeClass::Governance)
                .then(|| event.clone())
        })
    }

    fn get_event_by_receipt_id(&self, receipt_id: &str) -> Option<SignedLedgerEvent> {
        let guard = self.events.read().expect("lock poisoned");
        // Search all review receipt events for a matching receipt_id in the
        // JCS-canonicalized JSON payload. Both `review_receipt_recorded` and
        // `review_blocked_recorded` events embed `receipt_id` in the payload.
        for event in guard.1.values() {
            if event.event_type != "review_receipt_recorded"
                && event.event_type != "review_blocked_recorded"
            {
                continue;
            }
            // The payload is JCS-canonicalized JSON stored as bytes.
            if let Ok(payload) = serde_json::from_slice::<serde_json::Value>(&event.payload) {
                if payload
                    .get("receipt_id")
                    .and_then(serde_json::Value::as_str)
                    == Some(receipt_id)
                {
                    return Some(event.clone());
                }
            }
        }
        None
    }

    fn get_event_by_receipt_identity(
        &self,
        receipt_id: &str,
        lease_id: &str,
        work_id: &str,
        changeset_digest_hex: &str,
    ) -> Option<SignedLedgerEvent> {
        let guard = self.events.read().expect("lock poisoned");
        guard.1.values().find_map(|event| {
            if event.event_type != "review_receipt_recorded"
                && event.event_type != "review_blocked_recorded"
            {
                return None;
            }
            let payload = serde_json::from_slice::<serde_json::Value>(&event.payload).ok()?;
            let payload_receipt_id = payload.get("receipt_id")?.as_str()?;
            let payload_lease_id = payload.get("lease_id")?.as_str()?;
            let payload_work_id = payload.get("work_id")?.as_str()?;
            let payload_changeset_digest = payload.get("changeset_digest")?.as_str()?;
            if payload_receipt_id == receipt_id
                && payload_lease_id == lease_id
                && payload_work_id == work_id
                && payload_changeset_digest == changeset_digest_hex
            {
                Some(event.clone())
            } else {
                None
            }
        })
    }

    fn get_event_by_changeset_identity(
        &self,
        work_id: &str,
        changeset_digest_hex: &str,
    ) -> Option<SignedLedgerEvent> {
        let guard = self.events.read().expect("lock poisoned");
        guard.1.values().find_map(|event| {
            if event.event_type != "changeset_published" || event.work_id != work_id {
                return None;
            }
            let payload = serde_json::from_slice::<serde_json::Value>(&event.payload).ok()?;
            let payload_changeset_digest = payload.get("changeset_digest")?.as_str()?;
            if payload_changeset_digest == changeset_digest_hex {
                Some(event.clone())
            } else {
                None
            }
        })
    }

    fn get_work_transition_count(&self, work_id: &str) -> u32 {
        let events_by_work = self.events_by_work_id.read().expect("lock poisoned");
        let guard = self.events.read().expect("lock poisoned");

        events_by_work.get(work_id).map_or(0, |event_ids| {
            #[allow(clippy::cast_possible_truncation)]
            let count = event_ids
                .iter()
                .filter_map(|id| guard.1.get(id))
                .filter(|e| e.event_type == "work_transitioned")
                .count() as u32;
            count
        })
    }

    fn emit_episode_event(
        &self,
        episode_id: &str,
        event_type: &str,
        payload: &[u8],
        timestamp_ns: u64,
    ) -> Result<SignedLedgerEvent, LedgerEventError> {
        use ed25519_dalek::Signer;

        // Generate unique event ID
        let event_id = format!("EVT-{}", uuid::Uuid::new_v4());

        // Build payload as JSON with episode event metadata
        // The payload is already JSON-serialized episode event data
        // SECURITY: timestamp_ns is included in signed payload to prevent temporal
        // malleability per LAW-09 (Temporal Pinning & Freshness) and RS-40
        // (Time & Monotonicity)
        let payload_json = serde_json::json!({
            "event_type": event_type,
            "episode_id": episode_id,
            "payload": hex::encode(payload),
            "timestamp_ns": timestamp_ns,
        });

        // Use JCS (RFC 8785) canonicalization for deterministic signing
        let payload_string = payload_json.to_string();
        let canonical_payload =
            canonicalize_json(&payload_string).map_err(|e| LedgerEventError::SigningFailed {
                message: format!("JCS canonicalization failed: {e}"),
            })?;
        let payload_bytes = canonical_payload.as_bytes().to_vec();

        // Build canonical bytes for signing (domain prefix + JCS payload)
        let mut canonical_bytes =
            Vec::with_capacity(EPISODE_EVENT_DOMAIN_PREFIX.len() + payload_bytes.len());
        canonical_bytes.extend_from_slice(EPISODE_EVENT_DOMAIN_PREFIX);
        canonical_bytes.extend_from_slice(&payload_bytes);

        // Sign the canonical bytes
        let signature = self.signing_key.sign(&canonical_bytes);

        let signed_event = SignedLedgerEvent {
            event_id: event_id.clone(),
            event_type: event_type.to_string(),
            work_id: episode_id.to_string(), // Use episode_id as work_id for indexing
            actor_id: "daemon".to_string(),  // Episode events are daemon-authored
            payload: payload_bytes,
            signature: signature.to_bytes().to_vec(),
            timestamp_ns,
        };

        // CTR-1303: Persist to bounded in-memory store with LRU eviction
        {
            let mut guard = self.events.write().expect("lock poisoned");
            let mut events_by_work = self.events_by_work_id.write().expect("lock poisoned");
            let (order, events) = &mut *guard;

            // Evict oldest entries if at capacity
            while events.len() >= MAX_LEDGER_EVENTS {
                if let Some(oldest_key) = order.first().cloned() {
                    order.remove(0);
                    if let Some(evicted_event) = events.remove(&oldest_key) {
                        if let Some(work_id_events) = events_by_work.get_mut(&evicted_event.work_id)
                        {
                            work_id_events.retain(|id| id != &oldest_key);
                            if work_id_events.is_empty() {
                                events_by_work.remove(&evicted_event.work_id);
                            }
                        }
                    }
                } else {
                    break;
                }
            }

            order.push(event_id.clone());
            events.insert(event_id.clone(), signed_event.clone());
            events_by_work
                .entry(episode_id.to_string())
                .or_default()
                .push(event_id);
        }

        info!(
            event_id = %signed_event.event_id,
            episode_id = %episode_id,
            event_type = %event_type,
            "EpisodeEvent signed and persisted"
        );

        Ok(signed_event)
    }

    fn emit_review_receipt(
        &self,
        lease_id: &str,
        work_id: &str,
        receipt_id: &str,
        changeset_digest: &[u8; 32],
        artifact_bundle_hash: &[u8; 32],
        capability_manifest_hash: &[u8; 32],
        context_pack_hash: &[u8; 32],
        role_spec_hash: &[u8; 32],
        reviewer_actor_id: &str,
        timestamp_ns: u64,
        identity_proof_hash: &[u8; 32],
        time_envelope_ref: &str,
        pcac_lifecycle: Option<&PrivilegedPcacLifecycleArtifacts>,
    ) -> Result<SignedLedgerEvent, LedgerEventError> {
        use ed25519_dalek::Signer;

        for (name, hash) in [
            ("capability_manifest_hash", capability_manifest_hash),
            ("context_pack_hash", context_pack_hash),
            ("role_spec_hash", role_spec_hash),
        ] {
            if bool::from(hash.ct_eq(&[0u8; 32])) {
                return Err(LedgerEventError::ValidationFailed {
                    message: format!("{name} is zero (fail-closed)"),
                });
            }
        }

        // Generate unique event ID
        let event_id = format!("EVT-{}", uuid::Uuid::new_v4());

        // Build payload as JSON with review receipt data
        // SECURITY: timestamp_ns is included in signed payload to prevent temporal
        // malleability per LAW-09 (Temporal Pinning & Freshness) and RS-40
        // (Time & Monotonicity)
        //
        // SECURITY (TCK-00356 Fix 1): identity_proof_hash is included in
        // the signed payload so it is audit-bound and cannot be stripped
        // post-signing.
        let mut payload_map = serde_json::Map::new();
        payload_map.insert(
            "event_type".to_string(),
            serde_json::json!("review_receipt_recorded"),
        );
        payload_map.insert("episode_id".to_string(), serde_json::json!(lease_id));
        payload_map.insert("lease_id".to_string(), serde_json::json!(lease_id));
        payload_map.insert("work_id".to_string(), serde_json::json!(work_id));
        payload_map.insert("receipt_id".to_string(), serde_json::json!(receipt_id));
        payload_map.insert(
            "changeset_digest".to_string(),
            serde_json::json!(hex::encode(changeset_digest)),
        );
        payload_map.insert(
            "artifact_bundle_hash".to_string(),
            serde_json::json!(hex::encode(artifact_bundle_hash)),
        );
        payload_map.insert(
            "capability_manifest_hash".to_string(),
            serde_json::json!(hex::encode(capability_manifest_hash)),
        );
        payload_map.insert(
            "context_pack_hash".to_string(),
            serde_json::json!(hex::encode(context_pack_hash)),
        );
        payload_map.insert(
            "role_spec_hash".to_string(),
            serde_json::json!(hex::encode(role_spec_hash)),
        );
        payload_map.insert("verdict".to_string(), serde_json::json!("APPROVE"));
        payload_map.insert(
            "reviewer_actor_id".to_string(),
            serde_json::json!(reviewer_actor_id),
        );
        payload_map.insert("timestamp_ns".to_string(), serde_json::json!(timestamp_ns));
        payload_map.insert(
            "identity_proof_hash".to_string(),
            serde_json::json!(hex::encode(identity_proof_hash)),
        );
        payload_map.insert(
            "time_envelope_ref".to_string(),
            serde_json::json!(time_envelope_ref),
        );

        // TCK-00416 BLOCKER 3: Append review outcome binding fields to the
        // signed payload using domain-tagged derivation.
        let outcome_bindings =
            derive_review_outcome_bindings(changeset_digest, artifact_bundle_hash, receipt_id);
        append_review_outcome_fields(&mut payload_map, &outcome_bindings);
        if let Some(artifacts) = pcac_lifecycle {
            append_privileged_pcac_lifecycle_fields(&mut payload_map, artifacts);
        }

        let payload_json = serde_json::Value::Object(payload_map);

        // Use JCS (RFC 8785) canonicalization for deterministic signing
        let payload_string = payload_json.to_string();
        let canonical_payload =
            canonicalize_json(&payload_string).map_err(|e| LedgerEventError::SigningFailed {
                message: format!("JCS canonicalization failed: {e}"),
            })?;
        let payload_bytes = canonical_payload.as_bytes().to_vec();

        // Build canonical bytes for signing (domain prefix + JCS payload)
        // TCK-00321: Use REVIEW_RECEIPT_RECORDED_PREFIX from apm2_core::fac for
        // protocol compatibility across daemon/core boundary.
        let mut canonical_bytes =
            Vec::with_capacity(REVIEW_RECEIPT_RECORDED_PREFIX.len() + payload_bytes.len());
        canonical_bytes.extend_from_slice(REVIEW_RECEIPT_RECORDED_PREFIX);
        canonical_bytes.extend_from_slice(&payload_bytes);

        // Sign the canonical bytes
        let signature = self.signing_key.sign(&canonical_bytes);

        let signed_event = SignedLedgerEvent {
            event_id: event_id.clone(),
            event_type: "review_receipt_recorded".to_string(),
            work_id: work_id.to_string(),
            actor_id: reviewer_actor_id.to_string(),
            payload: payload_bytes,
            signature: signature.to_bytes().to_vec(),
            timestamp_ns,
        };

        // CTR-1303: Persist to bounded in-memory store with LRU eviction
        {
            let mut guard = self.events.write().expect("lock poisoned");
            let mut events_by_work = self.events_by_work_id.write().expect("lock poisoned");
            let (order, events) = &mut *guard;

            // Evict oldest entries if at capacity
            while events.len() >= MAX_LEDGER_EVENTS {
                if let Some(oldest_key) = order.first().cloned() {
                    order.remove(0);
                    if let Some(evicted_event) = events.remove(&oldest_key) {
                        if let Some(work_id_events) = events_by_work.get_mut(&evicted_event.work_id)
                        {
                            work_id_events.retain(|id| id != &oldest_key);
                            if work_id_events.is_empty() {
                                events_by_work.remove(&evicted_event.work_id);
                            }
                        }
                    }
                } else {
                    break;
                }
            }

            order.push(event_id.clone());
            events.insert(event_id.clone(), signed_event.clone());
            events_by_work
                .entry(work_id.to_string())
                .or_default()
                .push(event_id);
        }

        info!(
            event_id = %signed_event.event_id,
            lease_id = %lease_id,
            work_id = %work_id,
            receipt_id = %receipt_id,
            "ReviewReceiptRecorded event signed and persisted"
        );

        Ok(signed_event)
    }

    #[allow(clippy::too_many_arguments)]
    fn emit_review_blocked_receipt(
        &self,
        lease_id: &str,
        work_id: &str,
        receipt_id: &str,
        changeset_digest: &[u8; 32],
        artifact_bundle_hash: &[u8; 32],
        capability_manifest_hash: &[u8; 32],
        context_pack_hash: &[u8; 32],
        role_spec_hash: &[u8; 32],
        reason_code: u32,
        blocked_log_hash: &[u8; 32],
        reviewer_actor_id: &str,
        timestamp_ns: u64,
        identity_proof_hash: &[u8; 32],
        time_envelope_ref: &str,
        pcac_lifecycle: Option<&PrivilegedPcacLifecycleArtifacts>,
    ) -> Result<SignedLedgerEvent, LedgerEventError> {
        use ed25519_dalek::Signer;

        for (name, hash) in [
            ("capability_manifest_hash", capability_manifest_hash),
            ("context_pack_hash", context_pack_hash),
            ("role_spec_hash", role_spec_hash),
        ] {
            if bool::from(hash.ct_eq(&[0u8; 32])) {
                return Err(LedgerEventError::ValidationFailed {
                    message: format!("{name} is zero (fail-closed)"),
                });
            }
        }

        let event_id = format!("EVT-{}", uuid::Uuid::new_v4());

        // SECURITY (TCK-00356 Fix 2): identity_proof_hash is included in
        // the signed payload so it is audit-bound and cannot be stripped
        // post-signing, matching the APPROVE path's payload binding.
        let mut payload_map = serde_json::Map::new();
        payload_map.insert(
            "event_type".to_string(),
            serde_json::json!("review_blocked_recorded"),
        );
        payload_map.insert("lease_id".to_string(), serde_json::json!(lease_id));
        payload_map.insert("work_id".to_string(), serde_json::json!(work_id));
        payload_map.insert("receipt_id".to_string(), serde_json::json!(receipt_id));
        payload_map.insert(
            "changeset_digest".to_string(),
            serde_json::json!(hex::encode(changeset_digest)),
        );
        payload_map.insert(
            "artifact_bundle_hash".to_string(),
            serde_json::json!(hex::encode(artifact_bundle_hash)),
        );
        payload_map.insert(
            "capability_manifest_hash".to_string(),
            serde_json::json!(hex::encode(capability_manifest_hash)),
        );
        payload_map.insert(
            "context_pack_hash".to_string(),
            serde_json::json!(hex::encode(context_pack_hash)),
        );
        payload_map.insert(
            "role_spec_hash".to_string(),
            serde_json::json!(hex::encode(role_spec_hash)),
        );
        payload_map.insert("verdict".to_string(), serde_json::json!("BLOCKED"));
        payload_map.insert(
            "blocked_reason_code".to_string(),
            serde_json::json!(reason_code),
        );
        // Preserve legacy field for backward compatibility with old readers.
        payload_map.insert("reason_code".to_string(), serde_json::json!(reason_code));
        payload_map.insert(
            "blocked_log_hash".to_string(),
            serde_json::json!(hex::encode(blocked_log_hash)),
        );
        payload_map.insert(
            "reviewer_actor_id".to_string(),
            serde_json::json!(reviewer_actor_id),
        );
        payload_map.insert("timestamp_ns".to_string(), serde_json::json!(timestamp_ns));
        payload_map.insert(
            "identity_proof_hash".to_string(),
            serde_json::json!(hex::encode(identity_proof_hash)),
        );
        payload_map.insert(
            "time_envelope_ref".to_string(),
            serde_json::json!(time_envelope_ref),
        );

        // TCK-00416 BLOCKER 3: Append review outcome binding fields to the
        // signed payload using domain-tagged derivation.
        let outcome_bindings =
            derive_review_outcome_bindings(changeset_digest, artifact_bundle_hash, receipt_id);
        append_review_outcome_fields(&mut payload_map, &outcome_bindings);
        if let Some(artifacts) = pcac_lifecycle {
            append_privileged_pcac_lifecycle_fields(&mut payload_map, artifacts);
        }

        let payload_json = serde_json::Value::Object(payload_map);

        let payload_string = payload_json.to_string();
        let canonical_payload =
            canonicalize_json(&payload_string).map_err(|e| LedgerEventError::SigningFailed {
                message: format!("JCS canonicalization failed: {e}"),
            })?;
        let payload_bytes = canonical_payload.as_bytes().to_vec();

        let mut canonical_bytes =
            Vec::with_capacity(REVIEW_BLOCKED_RECORDED_LEDGER_PREFIX.len() + payload_bytes.len());
        canonical_bytes.extend_from_slice(REVIEW_BLOCKED_RECORDED_LEDGER_PREFIX);
        canonical_bytes.extend_from_slice(&payload_bytes);

        let signature = self.signing_key.sign(&canonical_bytes);

        let signed_event = SignedLedgerEvent {
            event_id: event_id.clone(),
            event_type: "review_blocked_recorded".to_string(),
            work_id: work_id.to_string(),
            actor_id: reviewer_actor_id.to_string(),
            payload: payload_bytes,
            signature: signature.to_bytes().to_vec(),
            timestamp_ns,
        };

        // CTR-1303: Persist to bounded in-memory store with LRU eviction
        {
            let mut guard = self.events.write().expect("lock poisoned");
            let mut events_by_work = self.events_by_work_id.write().expect("lock poisoned");
            let (order, events) = &mut *guard;

            while events.len() >= MAX_LEDGER_EVENTS {
                if let Some(oldest_key) = order.first().cloned() {
                    order.remove(0);
                    if let Some(evicted_event) = events.remove(&oldest_key) {
                        if let Some(work_id_events) = events_by_work.get_mut(&evicted_event.work_id)
                        {
                            work_id_events.retain(|id| id != &oldest_key);
                            if work_id_events.is_empty() {
                                events_by_work.remove(&evicted_event.work_id);
                            }
                        }
                    }
                } else {
                    break;
                }
            }

            order.push(event_id.clone());
            events.insert(event_id.clone(), signed_event.clone());
            events_by_work
                .entry(work_id.to_string())
                .or_default()
                .push(event_id);
        }

        info!(
            event_id = %signed_event.event_id,
            lease_id = %lease_id,
            work_id = %work_id,
            receipt_id = %receipt_id,
            reason_code = %reason_code,
            "ReviewBlockedRecorded event signed and persisted"
        );

        Ok(signed_event)
    }

    fn emit_episode_run_attributed(
        &self,
        work_id: &str,
        episode_id: &str,
        session_id: &str,
        adapter_profile_hash: &[u8; 32],
        timestamp_ns: u64,
    ) -> Result<SignedLedgerEvent, LedgerEventError> {
        use ed25519_dalek::Signer;

        // Generate unique event ID
        let event_id = format!("EVT-{}", uuid::Uuid::new_v4());

        // Build payload as JSON with run attribution data
        // SECURITY: timestamp_ns is included in signed payload to prevent temporal
        // malleability per LAW-09 (Temporal Pinning & Freshness) and RS-40
        // (Time & Monotonicity)
        // TCK-00330: adapter_profile_hash provides ledger attribution for profile-based
        // auditing
        let payload_json = serde_json::json!({
            "event_type": "episode_run_attributed",
            "work_id": work_id,
            "episode_id": episode_id,
            "session_id": session_id,
            "adapter_profile_hash": hex::encode(adapter_profile_hash),
            "timestamp_ns": timestamp_ns,
        });

        // Use JCS (RFC 8785) canonicalization for deterministic signing
        let payload_string = payload_json.to_string();
        let canonical_payload =
            canonicalize_json(&payload_string).map_err(|e| LedgerEventError::SigningFailed {
                message: format!("JCS canonicalization failed: {e}"),
            })?;
        let payload_bytes = canonical_payload.as_bytes().to_vec();

        // Build canonical bytes for signing (domain prefix + JCS payload)
        let mut canonical_bytes =
            Vec::with_capacity(EPISODE_RUN_ATTRIBUTED_PREFIX.len() + payload_bytes.len());
        canonical_bytes.extend_from_slice(EPISODE_RUN_ATTRIBUTED_PREFIX);
        canonical_bytes.extend_from_slice(&payload_bytes);

        // Sign the canonical bytes
        let signature = self.signing_key.sign(&canonical_bytes);

        let signed_event = SignedLedgerEvent {
            event_id: event_id.clone(),
            event_type: "episode_run_attributed".to_string(),
            work_id: work_id.to_string(),
            actor_id: session_id.to_string(), // Session is the actor for run attribution
            payload: payload_bytes,
            signature: signature.to_bytes().to_vec(),
            timestamp_ns,
        };

        // CTR-1303: Persist to bounded in-memory store with LRU eviction
        {
            let mut guard = self.events.write().expect("lock poisoned");
            let mut events_by_work = self.events_by_work_id.write().expect("lock poisoned");
            let (order, events) = &mut *guard;

            // Evict oldest entries if at capacity
            while events.len() >= MAX_LEDGER_EVENTS {
                if let Some(oldest_key) = order.first().cloned() {
                    order.remove(0);
                    if let Some(evicted_event) = events.remove(&oldest_key) {
                        if let Some(work_id_events) = events_by_work.get_mut(&evicted_event.work_id)
                        {
                            work_id_events.retain(|id| id != &oldest_key);
                            if work_id_events.is_empty() {
                                events_by_work.remove(&evicted_event.work_id);
                            }
                        }
                    }
                } else {
                    break;
                }
            }

            order.push(event_id.clone());
            events.insert(event_id.clone(), signed_event.clone());
            events_by_work
                .entry(work_id.to_string())
                .or_default()
                .push(event_id);
        }

        info!(
            event_id = %signed_event.event_id,
            work_id = %work_id,
            episode_id = %episode_id,
            session_id = %session_id,
            adapter_profile_hash = %hex::encode(adapter_profile_hash),
            "EpisodeRunAttributed event signed and persisted"
        );

        Ok(signed_event)
    }

    fn emit_work_transitioned(
        &self,
        transition: &WorkTransition<'_>,
    ) -> Result<SignedLedgerEvent, LedgerEventError> {
        use ed25519_dalek::Signer;

        // Generate unique event ID
        let event_id = format!("EVT-{}", uuid::Uuid::new_v4());

        // Build payload as JSON with work transition data
        // SECURITY: timestamp_ns is included in signed payload to prevent temporal
        // malleability per LAW-09 (Temporal Pinning & Freshness)
        let payload_json = serde_json::json!({
            "event_type": "work_transitioned",
            "work_id": transition.work_id,
            "from_state": transition.from_state,
            "to_state": transition.to_state,
            "rationale_code": transition.rationale_code,
            "previous_transition_count": transition.previous_transition_count,
            "actor_id": transition.actor_id,
            "timestamp_ns": transition.timestamp_ns,
        });

        // TCK-00395: Use JCS (RFC 8785) canonicalization for signing.
        let payload_string = payload_json.to_string();
        let canonical_payload =
            canonicalize_json(&payload_string).map_err(|e| LedgerEventError::SigningFailed {
                message: format!("JCS canonicalization failed: {e}"),
            })?;
        let payload_bytes = canonical_payload.as_bytes().to_vec();

        // Build canonical bytes for signing (domain prefix + JCS payload)
        let mut canonical_bytes =
            Vec::with_capacity(WORK_TRANSITIONED_DOMAIN_PREFIX.len() + payload_bytes.len());
        canonical_bytes.extend_from_slice(WORK_TRANSITIONED_DOMAIN_PREFIX);
        canonical_bytes.extend_from_slice(&payload_bytes);

        // Sign the canonical bytes
        let signature = self.signing_key.sign(&canonical_bytes);

        let signed_event = SignedLedgerEvent {
            event_id: event_id.clone(),
            event_type: "work_transitioned".to_string(),
            work_id: transition.work_id.to_string(),
            actor_id: transition.actor_id.to_string(),
            payload: payload_bytes,
            signature: signature.to_bytes().to_vec(),
            timestamp_ns: transition.timestamp_ns,
        };

        // CTR-1303: Persist to bounded in-memory store with LRU eviction
        {
            let mut guard = self.events.write().expect("lock poisoned");
            let mut events_by_work = self.events_by_work_id.write().expect("lock poisoned");
            let (order, events) = &mut *guard;

            // Evict oldest entries if at capacity
            while events.len() >= MAX_LEDGER_EVENTS {
                if let Some(oldest_key) = order.first().cloned() {
                    order.remove(0);
                    if let Some(evicted_event) = events.remove(&oldest_key) {
                        if let Some(work_id_events) = events_by_work.get_mut(&evicted_event.work_id)
                        {
                            work_id_events.retain(|id| id != &oldest_key);
                            if work_id_events.is_empty() {
                                events_by_work.remove(&evicted_event.work_id);
                            }
                        }
                    }
                } else {
                    break;
                }
            }

            order.push(event_id.clone());
            events.insert(event_id.clone(), signed_event.clone());
            events_by_work
                .entry(transition.work_id.to_string())
                .or_default()
                .push(event_id);
        }

        info!(
            event_id = %signed_event.event_id,
            work_id = %transition.work_id,
            from_state = %transition.from_state,
            to_state = %transition.to_state,
            rationale_code = %transition.rationale_code,
            "WorkTransitioned event signed and persisted"
        );

        Ok(signed_event)
    }

    fn emit_session_terminated(
        &self,
        session_id: &str,
        work_id: &str,
        exit_code: i32,
        termination_reason: &str,
        actor_id: &str,
        timestamp_ns: u64,
    ) -> Result<SignedLedgerEvent, LedgerEventError> {
        use ed25519_dalek::Signer;

        // Generate unique event ID
        let event_id = format!("EVT-{}", uuid::Uuid::new_v4());

        // Build payload as JSON with session termination data
        // SECURITY: timestamp_ns is included in signed payload to prevent temporal
        // malleability per LAW-09 (Temporal Pinning & Freshness)
        let payload_json = serde_json::json!({
            "event_type": "session_terminated",
            "session_id": session_id,
            "work_id": work_id,
            "exit_code": exit_code,
            "termination_reason": termination_reason,
            "actor_id": actor_id,
            "timestamp_ns": timestamp_ns,
        });

        // TCK-00395: Use JCS (RFC 8785) canonicalization for signing.
        let payload_string = payload_json.to_string();
        let canonical_payload =
            canonicalize_json(&payload_string).map_err(|e| LedgerEventError::SigningFailed {
                message: format!("JCS canonicalization failed: {e}"),
            })?;
        let payload_bytes = canonical_payload.as_bytes().to_vec();

        // Build canonical bytes for signing (domain prefix + JCS payload)
        let mut canonical_bytes =
            Vec::with_capacity(SESSION_TERMINATED_LEDGER_DOMAIN_PREFIX.len() + payload_bytes.len());
        canonical_bytes.extend_from_slice(SESSION_TERMINATED_LEDGER_DOMAIN_PREFIX);
        canonical_bytes.extend_from_slice(&payload_bytes);

        // Sign the canonical bytes
        let signature = self.signing_key.sign(&canonical_bytes);

        let signed_event = SignedLedgerEvent {
            event_id: event_id.clone(),
            event_type: "session_terminated".to_string(),
            work_id: work_id.to_string(),
            actor_id: actor_id.to_string(),
            payload: payload_bytes,
            signature: signature.to_bytes().to_vec(),
            timestamp_ns,
        };

        // CTR-1303: Persist to bounded in-memory store with LRU eviction
        {
            let mut guard = self.events.write().expect("lock poisoned");
            let mut events_by_work = self.events_by_work_id.write().expect("lock poisoned");
            let (order, events) = &mut *guard;

            // Evict oldest entries if at capacity
            while events.len() >= MAX_LEDGER_EVENTS {
                if let Some(oldest_key) = order.first().cloned() {
                    order.remove(0);
                    if let Some(evicted_event) = events.remove(&oldest_key) {
                        if let Some(work_id_events) = events_by_work.get_mut(&evicted_event.work_id)
                        {
                            work_id_events.retain(|id| id != &oldest_key);
                            if work_id_events.is_empty() {
                                events_by_work.remove(&evicted_event.work_id);
                            }
                        }
                    }
                } else {
                    break;
                }
            }

            order.push(event_id.clone());
            events.insert(event_id.clone(), signed_event.clone());
            events_by_work
                .entry(work_id.to_string())
                .or_default()
                .push(event_id);
        }

        info!(
            event_id = %signed_event.event_id,
            session_id = %session_id,
            work_id = %work_id,
            exit_code = %exit_code,
            termination_reason = %termination_reason,
            "SessionTerminated event signed and persisted"
        );

        Ok(signed_event)
    }

    fn emit_changeset_published(
        &self,
        work_id: &str,
        changeset_digest: &[u8; 32],
        cas_hash: &[u8; 32],
        actor_id: &str,
        timestamp_ns: u64,
    ) -> Result<SignedLedgerEvent, LedgerEventError> {
        use ed25519_dalek::Signer;

        // Generate unique event ID
        let event_id = format!("EVT-{}", uuid::Uuid::new_v4());

        // Build payload as JSON with changeset publication data.
        // SECURITY: timestamp_ns is included in signed payload to prevent
        // temporal malleability per LAW-09.
        let payload_json = serde_json::json!({
            "event_type": "changeset_published",
            "work_id": work_id,
            "changeset_digest": hex::encode(changeset_digest),
            "cas_hash": hex::encode(cas_hash),
            "actor_id": actor_id,
            "timestamp_ns": timestamp_ns,
        });

        // TCK-00394: Use JCS (RFC 8785) canonicalization for signing.
        let payload_string = payload_json.to_string();
        let canonical_payload =
            canonicalize_json(&payload_string).map_err(|e| LedgerEventError::SigningFailed {
                message: format!("JCS canonicalization failed: {e}"),
            })?;
        let payload_bytes = canonical_payload.as_bytes().to_vec();

        // Build canonical bytes for signing (domain prefix + JCS payload)
        let mut canonical_bytes = Vec::with_capacity(
            CHANGESET_PUBLISHED_LEDGER_DOMAIN_PREFIX.len() + payload_bytes.len(),
        );
        canonical_bytes.extend_from_slice(CHANGESET_PUBLISHED_LEDGER_DOMAIN_PREFIX);
        canonical_bytes.extend_from_slice(&payload_bytes);

        // Sign the canonical bytes
        let signature = self.signing_key.sign(&canonical_bytes);

        let signed_event = SignedLedgerEvent {
            event_id: event_id.clone(),
            event_type: "changeset_published".to_string(),
            work_id: work_id.to_string(),
            actor_id: actor_id.to_string(),
            payload: payload_bytes,
            signature: signature.to_bytes().to_vec(),
            timestamp_ns,
        };

        // CTR-1303: Persist to bounded in-memory store with LRU eviction
        {
            let mut guard = self.events.write().expect("lock poisoned");
            let mut events_by_work = self.events_by_work_id.write().expect("lock poisoned");
            let (order, events) = &mut *guard;

            // Evict oldest entries if at capacity
            while events.len() >= MAX_LEDGER_EVENTS {
                if let Some(oldest_key) = order.first().cloned() {
                    order.remove(0);
                    if let Some(evicted_event) = events.remove(&oldest_key) {
                        if let Some(work_id_events) = events_by_work.get_mut(&evicted_event.work_id)
                        {
                            work_id_events.retain(|id| id != &oldest_key);
                            if work_id_events.is_empty() {
                                events_by_work.remove(&evicted_event.work_id);
                            }
                        }
                    }
                } else {
                    break;
                }
            }

            order.push(event_id.clone());
            events.insert(event_id.clone(), signed_event.clone());
            events_by_work
                .entry(work_id.to_string())
                .or_default()
                .push(event_id);
        }

        info!(
            event_id = %signed_event.event_id,
            work_id = %work_id,
            changeset_digest = %hex::encode(changeset_digest),
            cas_hash = %hex::encode(cas_hash),
            "ChangeSetPublished event signed and persisted"
        );

        Ok(signed_event)
    }

    /// TCK-00350: Emits a receipt with envelope bindings in the stub
    /// implementation.
    #[allow(clippy::too_many_arguments)]
    fn emit_receipt_with_bindings(
        &self,
        episode_id: &str,
        receipt_id: &str,
        changeset_digest: &[u8; 32],
        artifact_bundle_hash: &[u8; 32],
        reviewer_actor_id: &str,
        timestamp_ns: u64,
        bindings: &crate::episode::EnvelopeBindings,
        identity_proof_hash: &[u8; 32],
    ) -> Result<SignedLedgerEvent, LedgerEventError> {
        use ed25519_dalek::Signer;

        // Fail-closed: validate bindings before emission
        bindings
            .validate()
            .map_err(|e| LedgerEventError::ValidationFailed {
                message: format!("envelope binding validation failed: {e}"),
            })?;

        let event_id = format!("EVT-{}", uuid::Uuid::new_v4());

        // Include bindings in the signed payload
        // SECURITY (TCK-00356 Fix 1): identity_proof_hash is included in
        // the signed payload so it is audit-bound.
        let (env_hex, cap_hex, view_hex) = bindings.to_hex_map();
        let payload_json = serde_json::json!({
            "event_type": "review_receipt_recorded",
            "episode_id": episode_id,
            "receipt_id": receipt_id,
            "changeset_digest": hex::encode(changeset_digest),
            "artifact_bundle_hash": hex::encode(artifact_bundle_hash),
            "reviewer_actor_id": reviewer_actor_id,
            "timestamp_ns": timestamp_ns,
            "envelope_hash": env_hex,
            "capability_manifest_hash": cap_hex,
            "view_commitment_hash": view_hex,
            "identity_proof_hash": hex::encode(identity_proof_hash),
        });

        let payload_string = payload_json.to_string();
        let canonical_payload =
            canonicalize_json(&payload_string).map_err(|e| LedgerEventError::SigningFailed {
                message: format!("JCS canonicalization failed: {e}"),
            })?;
        let payload_bytes = canonical_payload.as_bytes().to_vec();

        let mut canonical_bytes =
            Vec::with_capacity(REVIEW_RECEIPT_RECORDED_PREFIX.len() + payload_bytes.len());
        canonical_bytes.extend_from_slice(REVIEW_RECEIPT_RECORDED_PREFIX);
        canonical_bytes.extend_from_slice(&payload_bytes);

        let signature = self.signing_key.sign(&canonical_bytes);

        let signed_event = SignedLedgerEvent {
            event_id: event_id.clone(),
            event_type: "review_receipt_recorded".to_string(),
            work_id: episode_id.to_string(),
            actor_id: reviewer_actor_id.to_string(),
            payload: payload_bytes,
            signature: signature.to_bytes().to_vec(),
            timestamp_ns,
        };

        // CTR-1303: Persist to bounded in-memory store with LRU eviction
        {
            let mut guard = self.events.write().expect("lock poisoned");
            let mut events_by_work = self.events_by_work_id.write().expect("lock poisoned");
            let (order, events) = &mut *guard;

            while events.len() >= MAX_LEDGER_EVENTS {
                if let Some(oldest_key) = order.first().cloned() {
                    order.remove(0);
                    if let Some(evicted_event) = events.remove(&oldest_key) {
                        if let Some(work_id_events) = events_by_work.get_mut(&evicted_event.work_id)
                        {
                            work_id_events.retain(|id| id != &oldest_key);
                            if work_id_events.is_empty() {
                                events_by_work.remove(&evicted_event.work_id);
                            }
                        }
                    }
                } else {
                    break;
                }
            }

            order.push(event_id.clone());
            events.insert(event_id.clone(), signed_event.clone());
            events_by_work
                .entry(episode_id.to_string())
                .or_default()
                .push(event_id);
        }

        Ok(signed_event)
    }

    fn verifying_key(&self) -> ed25519_dalek::VerifyingKey {
        self.signing_key.verifying_key()
    }
}

// ============================================================================
// Policy Resolver Interface (TCK-00253)
// ============================================================================

use serde::{Deserialize, Serialize};

// ... (existing code)

/// Result of a policy resolution request.
///
/// Per DD-002, the daemon delegates policy resolution to the governance holon.
/// This struct captures the resolved policy state for work claiming.
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct PolicyResolution {
    /// Unique reference to the `PolicyResolvedForChangeSet` event.
    pub policy_resolved_ref: String,

    /// BLAKE3 hash of the resolved policy.
    pub resolved_policy_hash: [u8; 32],

    /// BLAKE3 hash of the capability manifest derived from policy.
    pub capability_manifest_hash: [u8; 32],

    /// BLAKE3 hash of the sealed context pack.
    pub context_pack_hash: [u8; 32],

    /// BLAKE3 hash of the authoritative `RoleSpecV2` contract.
    ///
    /// This hash is resolved from CAS-addressed role profiles and MUST be
    /// non-zero/cas-resolvable in production execution paths.
    #[serde(default = "zero_hash32")]
    pub role_spec_hash: [u8; 32],

    /// BLAKE3 hash of the compiled context-pack recipe bound to this policy.
    ///
    /// This hash closes lineage between role/context/budget selectors and the
    /// sealed context pack used during execution.
    #[serde(default = "zero_hash32")]
    pub context_pack_recipe_hash: [u8; 32],

    /// Resolved risk tier (0-4) from `PolicyResolvedForChangeSet`.
    ///
    /// Used by `handle_ingest_review_receipt` to enforce attestation
    /// ratcheting: higher tiers require stronger attestation levels.
    /// SECURITY: Defaults to `4` (Tier4, most restrictive) via
    /// `risk_tier_fail_closed` to prevent attestation downgrade via
    /// schema drift or tampered persistence (fail-closed semantics).
    #[serde(default = "risk_tier_fail_closed")]
    pub resolved_risk_tier: u8,

    /// Policy-resolved scope baseline for strict-subset validation.
    ///
    /// SECURITY (MAJOR 1 v3 fix): The scope baseline MUST come from an
    /// authoritative policy source (the policy resolver), NOT from the
    /// candidate manifest being validated. Building the baseline from the
    /// manifest itself makes the subset check tautological (always passes).
    ///
    /// When `None` (or absent from serialized data), V1 minting is
    /// denied (fail-closed) because no independent baseline is available
    /// to validate the candidate manifest against.
    #[serde(default)]
    pub resolved_scope_baseline: Option<crate::episode::capability::ScopeBaseline>,

    /// Policy-bound adapter profile hash (BLOCKER security fix).
    ///
    /// When present, `SpawnEpisode` MUST use this exact adapter profile hash.
    /// Any request supplying a different hash is rejected (confused-deputy
    /// prevention). CAS existence alone is NOT authorization -- the profile
    /// must be policy-bound to the caller's scope.
    ///
    /// When `None` (rollout waiver WVR-0003), callers may supply any
    /// CAS-present hash or fall back to the role-based default. This field
    /// will become mandatory once governance populates it.
    #[serde(default)]
    pub expected_adapter_profile_hash: Option<[u8; 32]>,

    /// PCAC policy knobs resolved from the governance event.
    #[serde(default)]
    pub pcac_policy: Option<apm2_core::pcac::PcacPolicyKnobs>,

    /// `PointerOnly` waiver resolved from the governance event.
    #[serde(default)]
    pub pointer_only_waiver: Option<apm2_core::pcac::PointerOnlyWaiver>,
}

/// Serde default for `resolved_risk_tier`: returns `4` (Tier4, most
/// restrictive).
///
/// SECURITY: This ensures that missing or omitted `resolved_risk_tier` values
/// fail closed to the most restrictive tier, preventing attestation downgrade
/// via schema drift or tampered persistence.
const fn risk_tier_fail_closed() -> u8 {
    4
}

/// Serde default helper for 32-byte hashes.
const fn zero_hash32() -> [u8; 32] {
    [0u8; 32]
}

/// Error type for policy resolution operations.
#[derive(Debug, Clone, PartialEq, Eq)]
pub enum PolicyResolutionError {
    /// Policy resolution not found for the given work/role combination.
    NotFound {
        /// The work ID that was queried.
        work_id: String,
        /// The role that was queried.
        role: WorkRole,
    },

    /// Policy resolution failed due to governance error.
    GovernanceFailed {
        /// Error message from governance.
        message: String,
    },

    /// Invalid credential for policy resolution.
    InvalidCredential {
        /// Error message describing the credential issue.
        message: String,
    },
}

impl std::fmt::Display for PolicyResolutionError {
    fn fmt(&self, f: &mut std::fmt::Formatter<'_>) -> std::fmt::Result {
        match self {
            Self::NotFound { work_id, role } => {
                write!(
                    f,
                    "policy resolution not found for work_id={work_id}, role={role:?}"
                )
            },
            Self::GovernanceFailed { message } => {
                write!(f, "governance failed: {message}")
            },
            Self::InvalidCredential { message } => {
                write!(f, "invalid credential: {message}")
            },
        }
    }
}

impl std::error::Error for PolicyResolutionError {}

/// Trait for policy resolution delegation to governance.
///
/// Per DD-002, the daemon does not embed governance logic. It delegates
/// policy resolution to the governance holon and mints capability manifests
/// based on the returned resolution.
///
/// # Implementers
///
/// - `StubPolicyResolver`: Returns stub data for testing
/// - `GovernancePolicyResolver`: Delegates to actual governance holon (future)
pub trait PolicyResolver: Send + Sync {
    /// Resolves policy for a work claim.
    ///
    /// # Arguments
    ///
    /// * `work_id` - Generated work ID for this claim
    /// * `role` - The role being claimed
    /// * `actor_id` - The authoritative actor ID (derived from credential)
    ///
    /// # Returns
    ///
    /// `PolicyResolution` containing the resolved policy hashes and references.
    ///
    /// # Errors
    ///
    /// Returns `PolicyResolutionError` if resolution fails.
    fn resolve_for_claim(
        &self,
        work_id: &str,
        role: WorkRole,
        actor_id: &str,
    ) -> Result<PolicyResolution, PolicyResolutionError>;
}

/// Stub policy resolver for testing and development.
///
/// Returns deterministic stub data. In production, this will be replaced
/// with a real governance holon integration.
///
/// # TCK-00255: Context Pack Sealing
///
/// This stub demonstrates the sealing pattern required by RFC-0017:
/// 1. Create a `ContextPackManifest` with work-specific entries
/// 2. Call `seal()` to get the deterministic content hash
/// 3. Return the seal hash in the `PolicyResolution.context_pack_hash`
///
/// # TCK-00317: Role-Based Manifest Hash Resolution
///
/// Per DOD item 2 (Policy Resolution Bypass fix), this resolver now returns
/// the canonical `reviewer_v0_manifest_hash()` for the Reviewer role. This
/// ensures `SpawnEpisode` loads the correct manifest from CAS using the hash
/// from `PolicyResolution`, rather than selecting manifests by role name.
#[derive(Debug, Clone, Default)]
pub struct StubPolicyResolver;

impl PolicyResolver for StubPolicyResolver {
    fn resolve_for_claim(
        &self,
        work_id: &str,
        role: WorkRole,
        actor_id: &str,
    ) -> Result<PolicyResolution, PolicyResolutionError> {
        use crate::episode::reviewer_manifest::reviewer_v0_manifest_hash;

        // Generate deterministic hash for policy
        let policy_hash = blake3::hash(format!("policy:{work_id}:{actor_id}").as_bytes());

        // TCK-00317: Return role-appropriate manifest hash
        //
        // Per DOD item 2, the policy resolver must return the correct manifest
        // hash for each role. SpawnEpisode uses this hash to load the manifest
        // from CAS, ensuring the manifest is not bypassed by role selection.
        //
        // - Reviewer: Use canonical reviewer v0 manifest hash
        // - Other roles: Use deterministic stub hash (fail-closed on CAS lookup)
        //
        // MAJOR 1 v3 fix: Also resolve the authoritative scope baseline and
        // risk tier ceiling from the policy (not from the candidate manifest).
        let (manifest_hash, resolved_scope_baseline) = if role == WorkRole::Reviewer {
            let reviewer = crate::episode::reviewer_manifest::reviewer_v0_manifest();
            let baseline = crate::episode::capability::ScopeBaseline {
                tools: reviewer.tool_allowlist.clone(),
                write_paths: reviewer.write_allowlist.clone(),
                shell_patterns: reviewer.shell_allowlist.clone(),
            };
            (*reviewer_v0_manifest_hash(), Some(baseline))
        } else {
            // For non-reviewer roles, generate a deterministic hash that will
            // fail closed when loaded from CAS (hash doesn't exist in store).
            // The fallback manifest (`from_hash_with_default_allowlist`) has
            // empty allowlists, so we provide a matching empty baseline. This
            // is NOT fail-open: the empty baseline means no tools, write paths,
            // or shell patterns are permitted, which matches the empty fallback
            // manifest. Any manifest with non-empty allowlists would be rejected
            // as exceeding this baseline.
            let hash =
                *blake3::hash(format!("manifest:{work_id}:{actor_id}").as_bytes()).as_bytes();
            (
                hash,
                Some(crate::episode::capability::ScopeBaseline::default()),
            )
        };

        // TCK-00255: Create and seal a context pack manifest
        // Uses the shared `build_policy_context_pack` helper so that
        // `seed_policy_artifacts_in_cas` can reproduce the same preimage.
        let context_pack = build_policy_context_pack(work_id, actor_id);

        // TCK-00255: Call seal() to get the context pack hash
        // This ensures the hash is deterministic and verifiable.
        let context_pack_hash =
            context_pack
                .seal()
                .map_err(|e| PolicyResolutionError::GovernanceFailed {
                    message: format!("context pack sealing failed: {e}"),
                })?;
        let role_spec_hash = fac_workobject_implementor_v2_role_contract()
            .compute_cas_hash()
            .map_err(|e| PolicyResolutionError::GovernanceFailed {
                message: format!("role spec hash computation failed: {e}"),
            })?;
        let context_pack_recipe_hash =
            policy_context_pack_recipe_hash(work_id, actor_id, role_spec_hash, context_pack_hash)
                .map_err(|e| PolicyResolutionError::GovernanceFailed {
                message: format!("context recipe hash computation failed: {e}"),
            })?;

        Ok(PolicyResolution {
            policy_resolved_ref: format!("PolicyResolvedForChangeSet:{work_id}"),
            pcac_policy: None,
            pointer_only_waiver: None,
            resolved_policy_hash: *policy_hash.as_bytes(),
            capability_manifest_hash: manifest_hash,
            context_pack_hash,
            role_spec_hash,
            context_pack_recipe_hash,
            resolved_risk_tier: 0, // Stub resolver: Tier0 default
            resolved_scope_baseline,
            expected_adapter_profile_hash: None, // TODO(TCK-00399): populate from governance
        })
    }
}

// ============================================================================
// Permeability Receipt Resolver (TCK-00373)
// ============================================================================

/// Resolves an optional permeability receipt for a work claim.
///
/// During `ClaimWork`, the dispatcher calls this resolver to determine
/// whether the actor's claim is subject to delegated authority constraints.
/// When the resolver returns `Some(receipt)`, the receipt is stored on the
/// `WorkClaim` and enforced by the delegated spawn gate in `SpawnEpisode`.
///
/// # Contract
///
/// * If the actor operates under a delegation, the resolver **MUST** return a
///   valid receipt whose `delegate_actor_id` matches `actor_id` and whose
///   `policy_root_hash` matches `resolved_policy_hash`.
/// * If the actor is a root (non-delegated) operator, the resolver returns
///   `None` and no delegated spawn gate is entered.
/// * Errors from the resolver are surfaced as `CapabilityRequestRejected` to
///   the caller (fail-closed).
pub trait PermeabilityReceiptResolver: Send + Sync {
    /// Resolves a permeability receipt for the given claim context.
    ///
    /// # Arguments
    ///
    /// * `actor_id` - The authoritative actor ID (derived from credential).
    /// * `work_id` - The work ID being claimed.
    /// * `resolved_policy_hash` - The policy hash from governance resolution.
    ///
    /// # Returns
    ///
    /// `Some(receipt)` if the actor operates under delegated authority,
    /// `None` for root (non-delegated) operators.
    ///
    /// # Errors
    ///
    /// Returns an error description if the receipt cannot be resolved
    /// (e.g., store unavailable, receipt invalid). Callers MUST treat
    /// errors as fail-closed.
    fn resolve_receipt(
        &self,
        actor_id: &str,
        work_id: &str,
        resolved_policy_hash: &[u8; 32],
    ) -> Result<Option<apm2_core::policy::permeability::PermeabilityReceipt>, String>;
}

/// Stub receipt resolver that always returns `None` (no delegation).
///
/// Used in tests and non-delegated deployments. In production, this
/// would be replaced with a governance-backed resolver that queries
/// active delegation receipts for the actor.
#[derive(Debug, Clone, Default)]
pub struct StubPermeabilityReceiptResolver;

impl PermeabilityReceiptResolver for StubPermeabilityReceiptResolver {
    fn resolve_receipt(
        &self,
        _actor_id: &str,
        _work_id: &str,
        _resolved_policy_hash: &[u8; 32],
    ) -> Result<Option<apm2_core::policy::permeability::PermeabilityReceipt>, String> {
        Ok(None)
    }
}

// ============================================================================
// Work Registry Interface (TCK-00253)
// ============================================================================

/// A claimed work item with its associated metadata.
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct WorkClaim {
    /// Unique work identifier.
    pub work_id: String,

    /// Lease identifier for this claim.
    pub lease_id: String,

    /// Authoritative actor ID (derived from credential).
    pub actor_id: String,

    /// Role claimed for this work.
    #[serde(with = "work_role_serde")]
    pub role: WorkRole,

    /// Policy resolution for this claim.
    pub policy_resolution: PolicyResolution,

    /// Custody domains associated with the executor (for `SoD` validation).
    ///
    /// Per TCK-00258, these are the domains assigned to the actor claiming
    /// the work. For `GATE_EXECUTOR` roles, spawn will be rejected if these
    /// domains overlap with author domains.
    pub executor_custody_domains: Vec<String>,

    /// Custody domains associated with changeset authors (for `SoD`
    /// validation).
    ///
    /// Per TCK-00258, these are the domains of the actors who authored the
    /// changeset being reviewed.
    pub author_custody_domains: Vec<String>,

    /// TCK-00373: Optional permeability receipt for delegated spawns.
    ///
    /// When present, `handle_spawn_episode` routes through the
    /// delegated spawn gate for full consumption-binding verification
    /// before allowing the spawn. The receipt is resolved during
    /// `ClaimWork` and bound to the policy resolution context.
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub permeability_receipt: Option<apm2_core::policy::permeability::PermeabilityReceipt>,
}

mod work_role_serde {
    use serde::{Deserialize, Deserializer, Serializer};

    use super::WorkRole;

    // Serde requires `&T` for custom serializers via `serialize_with`.
    #[allow(clippy::trivially_copy_pass_by_ref)]
    pub fn serialize<S>(role: &WorkRole, serializer: S) -> Result<S::Ok, S::Error>
    where
        S: Serializer,
    {
        serializer.serialize_i32(*role as i32)
    }

    pub fn deserialize<'de, D>(deserializer: D) -> Result<WorkRole, D::Error>
    where
        D: Deserializer<'de>,
    {
        let val = i32::deserialize(deserializer)?;
        WorkRole::try_from(val).map_err(serde::de::Error::custom)
    }
}

/// Trait for persisting and querying work claims.
///
/// The work registry tracks claimed work items and their associated
/// policy resolutions. It also handles `WorkClaimed` event signing.
pub trait WorkRegistry: Send + Sync {
    /// Registers a new work claim.
    ///
    /// # Arguments
    ///
    /// * `claim` - The work claim to register
    ///
    /// # Returns
    ///
    /// The registered `WorkClaim` (may be enriched with additional metadata).
    ///
    /// # Errors
    ///
    /// Returns an error if registration fails.
    fn register_claim(&self, claim: WorkClaim) -> Result<WorkClaim, WorkRegistryError>;

    /// Queries a work claim by work ID.
    fn get_claim(&self, work_id: &str) -> Option<WorkClaim>;
}

/// Error type for work registry operations.
#[derive(Debug, Clone, PartialEq, Eq)]
pub enum WorkRegistryError {
    /// Work ID already exists.
    DuplicateWorkId {
        /// The duplicate work ID.
        work_id: String,
    },

    /// Registration failed.
    RegistrationFailed {
        /// Error message.
        message: String,
    },
}

impl std::fmt::Display for WorkRegistryError {
    fn fmt(&self, f: &mut std::fmt::Formatter<'_>) -> std::fmt::Result {
        match self {
            Self::DuplicateWorkId { work_id } => {
                write!(f, "duplicate work_id: {work_id}")
            },
            Self::RegistrationFailed { message } => {
                write!(f, "registration failed: {message}")
            },
        }
    }
}

impl std::error::Error for WorkRegistryError {}

/// Maximum number of work claims stored in `StubWorkRegistry`.
///
/// Per CTR-1303: In-memory stores must have `max_entries` limit with O(1)
/// eviction. This prevents denial-of-service via memory exhaustion from
/// unbounded claim registration.
pub const MAX_WORK_CLAIMS: usize = 10_000;

/// In-memory stub work registry for testing.
///
/// # Capacity Limits (CTR-1303)
///
/// This registry enforces a maximum of [`MAX_WORK_CLAIMS`] entries to prevent
/// memory exhaustion. When the limit is reached, the oldest entry (by insertion
/// order) is evicted to make room for the new claim.
///
/// # Performance
///
/// Uses `VecDeque` for O(1) eviction via `pop_front()` instead of
/// `Vec::remove(0)` which is O(n).
#[derive(Debug)]
pub struct StubWorkRegistry {
    /// Claims stored with insertion order for LRU eviction.
    /// Uses `VecDeque` for O(1) eviction of oldest entries.
    claims: std::sync::RwLock<(
        VecDeque<String>,
        std::collections::HashMap<String, WorkClaim>,
    )>,
}

impl Default for StubWorkRegistry {
    fn default() -> Self {
        Self {
            claims: std::sync::RwLock::new((
                VecDeque::with_capacity(MAX_WORK_CLAIMS.min(1000)), // Pre-allocate reasonably
                std::collections::HashMap::with_capacity(MAX_WORK_CLAIMS.min(1000)),
            )),
        }
    }
}

impl WorkRegistry for StubWorkRegistry {
    fn register_claim(&self, claim: WorkClaim) -> Result<WorkClaim, WorkRegistryError> {
        let mut guard = self.claims.write().expect("lock poisoned");
        let (order, claims) = &mut *guard;

        if claims.contains_key(&claim.work_id) {
            return Err(WorkRegistryError::DuplicateWorkId {
                work_id: claim.work_id,
            });
        }

        // CTR-1303: Evict oldest entry if at capacity (O(1) via pop_front)
        while claims.len() >= MAX_WORK_CLAIMS {
            if let Some(oldest_key) = order.pop_front() {
                claims.remove(&oldest_key);
                debug!(
                    evicted_work_id = %oldest_key,
                    "Evicted oldest work claim to maintain capacity limit"
                );
            } else {
                break;
            }
        }

        let work_id = claim.work_id.clone();
        order.push_back(work_id.clone());
        claims.insert(work_id, claim.clone());
        Ok(claim)
    }

    fn get_claim(&self, work_id: &str) -> Option<WorkClaim> {
        let guard = self.claims.read().expect("lock poisoned");
        guard.1.get(work_id).cloned()
    }
}

// ============================================================================
// Actor ID Derivation (TCK-00253)
// ============================================================================

/// Derives the authoritative actor ID from peer credentials.
///
/// Per DD-001 and the proto definition, the `actor_id` in the request is a
/// "display hint" only. The authoritative `actor_id` is derived from the
/// credential. This implementation uses a fingerprint of the UID and GID
/// to create a **stable** identifier that does not change per request.
///
/// # Stability
///
/// The actor ID is derived ONLY from stable credential fields (UID, GID).
/// It intentionally excludes:
/// - PID: Changes per process
/// - Nonce: Changes per request
///
/// This ensures the same user always maps to the same `actor_id`.
///
/// # Arguments
///
/// * `credentials` - The peer credentials from `SO_PEERCRED`
///
/// # Returns
///
/// A stable actor ID string derived from the credential.
///
/// # TODO
///
/// - TCK-00253: `credential_signature` field is currently ignored. Integration
///   with credential verification infrastructure will allow deriving `actor_id`
///   from cryptographic identity rather than Unix UID/GID.
#[must_use]
pub fn derive_actor_id(credentials: &PeerCredentials) -> String {
    // Create a fingerprint from UID and GID only (stable across requests)
    // Per code quality review: exclude PID (changes per process) and nonce (changes
    // per request)
    let mut hasher = blake3::Hasher::new();
    hasher.update(&credentials.uid.to_le_bytes());
    hasher.update(&credentials.gid.to_le_bytes());

    let hash = hasher.finalize();

    // Use first 8 bytes (16 hex chars) for a shorter identifier
    // Per code quality review: use blake3::Hash::to_hex() instead of manual
    // formatting
    let hex = hash.to_hex();
    format!("actor:{}", &hex[..16])
}

/// Generates a unique work ID.
///
/// Uses UUID v4 for uniqueness per RFC-0016 (Hybrid Time Framework compliance).
/// HTF prohibits `SystemTime::now()` to ensure deterministic replay.
#[must_use]
pub fn generate_work_id() -> String {
    // RFC-0016 HTF compliance: Use UUID v4 instead of SystemTime::now()
    let uuid = uuid::Uuid::new_v4();
    format!("W-{uuid}")
}

/// Generates a unique lease ID.
///
/// Uses UUID v4 for uniqueness per RFC-0016 (Hybrid Time Framework compliance).
/// HTF prohibits `SystemTime::now()` to ensure deterministic replay.
#[must_use]
pub fn generate_lease_id() -> String {
    // RFC-0016 HTF compliance: Use UUID v4 instead of SystemTime::now()
    let uuid = uuid::Uuid::new_v4();
    format!("L-{uuid}")
}

/// Build a deterministic liveness heartbeat receipt for daemon progression
/// checks.
#[must_use]
pub fn build_liveness_heartbeat(
    episode_id: &[u8; 32],
    run_id: &str,
    current_tick: u64,
    health: HealthVerdict,
) -> LivenessHeartbeatReceiptV1 {
    let mut run_id = run_id.to_string();
    if run_id.len() > MAX_RUN_ID_LENGTH {
        let mut truncation_index = MAX_RUN_ID_LENGTH;
        while !run_id.is_char_boundary(truncation_index) {
            truncation_index = truncation_index.saturating_sub(1);
        }
        run_id.truncate(truncation_index);
    }

    LivenessHeartbeatReceiptV1 {
        run_id,
        episode_id: *episode_id,
        emitted_at_tick: current_tick,
        time_envelope_ref: *episode_id,
        health_verdict: health,
        restart_count: 0,
        max_restarts: MAX_RESTARTS_LIMIT,
        uptime_ms: 0,
        detail: None,
    }
}

fn decode_hash32_hex(field: &str, value: &str) -> Result<[u8; 32], String> {
    let decoded = hex::decode(value).map_err(|e| format!("{field} is not valid hex: {e}"))?;
    decoded
        .as_slice()
        .try_into()
        .map_err(|_| format!("{field} must decode to 32 bytes, got {}", decoded.len()))
}

/// Extracts replay-critical fields from a persisted `SubleaseIssued` payload.
///
/// Supports both payload shapes used by emitters:
/// - wrapper JSON with hex-encoded inner payload in `"payload"` (stub emitter)
/// - direct top-level fields (`SQLite` emitter)
fn extract_sublease_replay_bindings(event_payload: &[u8]) -> Result<(String, [u8; 32]), String> {
    let wrapper = serde_json::from_slice::<serde_json::Value>(event_payload)
        .map_err(|e| format!("cannot parse SubleaseIssued payload wrapper: {e}"))?;

    let inner_payload = wrapper
        .get("payload")
        .and_then(serde_json::Value::as_str)
        .and_then(|hex_payload| hex::decode(hex_payload).ok())
        .and_then(|inner_bytes| serde_json::from_slice::<serde_json::Value>(&inner_bytes).ok());

    let lookup_field = |field: &str| {
        inner_payload
            .as_ref()
            .and_then(|inner| inner.get(field).and_then(serde_json::Value::as_str))
            .or_else(|| wrapper.get(field).and_then(serde_json::Value::as_str))
    };

    let parent_lease_id = lookup_field("parent_lease_id")
        .map(str::to_owned)
        .ok_or_else(|| "missing parent_lease_id".to_string())?;

    let identity_proof_hash_hex = lookup_field("identity_proof_hash")
        .ok_or_else(|| "missing identity_proof_hash".to_string())?;
    let identity_proof_hash_vec = hex::decode(identity_proof_hash_hex)
        .map_err(|e| format!("identity_proof_hash is not valid hex: {e}"))?;
    crate::identity::validate_identity_proof_hash(&identity_proof_hash_vec)
        .map_err(|e| format!("identity_proof_hash validation failed: {e}"))?;
    let identity_proof_hash: [u8; 32] = identity_proof_hash_vec
        .as_slice()
        .try_into()
        .expect("validated to 32 bytes by validate_identity_proof_hash");

    Ok((parent_lease_id, identity_proof_hash))
}

#[derive(Debug)]
enum DelegationCommitEvent {
    PersistDelegatedLease,
    EmitSubleaseIssued(Vec<u8>),
}

fn compute_delegate_sublease_lineage_digest(
    parent_lease_id: &str,
    sublease_id: &str,
    parent_changeset_digest: &[u8; 32],
    parent_policy_hash: &[u8; 32],
    requested_depth: u32,
    ticks_used: u64,
    budget_ticks: u64,
) -> [u8; 32] {
    let mut hasher = blake3::Hasher::new();
    hasher.update(b"apm2.delegate_sublease.lineage_digest.v1");
    let update_len_prefixed = |hasher: &mut blake3::Hasher, bytes: &[u8]| {
        let len = u64::try_from(bytes.len())
            .expect("usize length always fits into u64 for digest framing");
        hasher.update(&len.to_le_bytes());
        hasher.update(bytes);
    };
    update_len_prefixed(&mut hasher, parent_lease_id.as_bytes());
    update_len_prefixed(&mut hasher, sublease_id.as_bytes());
    hasher.update(parent_changeset_digest);
    hasher.update(parent_policy_hash);
    hasher.update(&requested_depth.to_be_bytes());
    hasher.update(&ticks_used.to_be_bytes());
    hasher.update(&budget_ticks.to_be_bytes());
    hasher.finalize().into()
}

fn build_delegate_sublease_satisfiability_artifact(
    parent_lease_id: &str,
    sublease_id: &str,
    parent_changeset_digest: &[u8; 32],
    parent_policy_hash: &[u8; 32],
    requested_depth: u32,
    ticks_used: u64,
) -> Result<serde_json::Value, String> {
    let receipt = DelegationSatisfiabilityReceiptV1 {
        schema_id: DELEGATION_SATISFIABILITY_SCHEMA_ID.to_string(),
        schema_major: DELEGATION_SATISFIABILITY_SCHEMA_MAJOR,
        delegation_depth: requested_depth,
        budget_ticks: SUBLEASE_DELEGATION_SATISFIABILITY_BUDGET_TICKS,
        ticks_used,
        admissible_workset_non_empty: true,
    };

    let receipt_json = serde_json::to_value(&receipt)
        .map_err(|e| format!("delegation satisfiability receipt serialization failed: {e}"))?;
    let receipt_digest = hash_bytes(&canonical_json_bytes(&receipt_json)?);
    let parent_lease_id_digest = hash_bytes(parent_lease_id.as_bytes());
    let sublease_id_digest = hash_bytes(sublease_id.as_bytes());
    let lineage_digest = compute_delegate_sublease_lineage_digest(
        parent_lease_id,
        sublease_id,
        parent_changeset_digest,
        parent_policy_hash,
        requested_depth,
        ticks_used,
        SUBLEASE_DELEGATION_SATISFIABILITY_BUDGET_TICKS,
    );

    Ok(serde_json::json!({
        "schema": "apm2.delegate_sublease_satisfiability_artifact.v1",
        "schema_major": 1_u16,
        "delegation_satisfiability_receipt": receipt,
        "delegation_satisfiability_receipt_digest": hex::encode(receipt_digest),
        "canonical_parent_lease_id_digest": hex::encode(parent_lease_id_digest),
        "canonical_sublease_id_digest": hex::encode(sublease_id_digest),
        "canonical_parent_changeset_digest": hex::encode(parent_changeset_digest),
        "canonical_parent_policy_hash": hex::encode(parent_policy_hash),
        "canonical_lineage_digest": hex::encode(lineage_digest),
    }))
}

fn persist_delegate_sublease_satisfiability_artifact(
    cas: Option<&dyn ContentAddressedStore>,
    artifact: &serde_json::Value,
) -> Result<Option<[u8; 32]>, String> {
    let Some(cas) = cas else {
        return Ok(None);
    };

    let artifact_bytes = canonical_json_bytes(artifact)?;
    let stored = cas
        .store(&artifact_bytes)
        .map_err(|e| format!("CAS store delegation satisfiability artifact failed: {e}"))?;
    Ok(Some(stored.hash))
}

/// Deterministically computes parent delegation depth and evaluation ticks for
/// `DelegateSublease` admission.
///
/// Traverses persisted `SubleaseIssued` lineage by repeatedly reading the
/// latest event for each lease ID and following its `parent_lease_id`.
///
/// Fail-closed rule: if lineage metadata lookup fails, if a lease is marked as
/// delegated but no corresponding `SubleaseIssued` evidence exists, or if an
/// ancestor lease record is missing, traversal returns an error instead of
/// treating the lease as root.
///
/// Returns:
/// - `depth`: number of sublease hops from `parent_lease_id` to the root lease
/// - `ticks_used`: deterministic integer cost consumed by lineage evaluation
fn compute_sublease_parent_depth_and_ticks(
    event_emitter: &dyn LedgerEventEmitter,
    lease_validator: &dyn LeaseValidator,
    parent_lease_id: &str,
) -> Result<(u32, u64), String> {
    let mut current_lease = parent_lease_id.to_string();
    let mut visited = HashSet::with_capacity(MAX_SUBLEASE_LINEAGE_TRAVERSAL_STEPS);
    let mut depth = 0_u32;
    let mut ticks_used = 0_u64;

    // NOTE: Each loop iteration performs O(1) work plus up to two O(N) lookups
    // against bounded local collections (`get_events_by_work_id` + replay
    // binding parse). This remains acceptable because traversal is hard-capped
    // by MAX_SUBLEASE_LINEAGE_TRAVERSAL_STEPS (17 iterations).
    for _ in 0..MAX_SUBLEASE_LINEAGE_TRAVERSAL_STEPS {
        ticks_used = ticks_used
            .checked_add(1)
            .ok_or_else(|| "delegation satisfiability tick overflow".to_string())?;

        if !visited.insert(current_lease.clone()) {
            return Err(format!(
                "delegation lineage cycle detected at lease '{current_lease}'"
            ));
        }

        let current_events = event_emitter.get_events_by_work_id(&current_lease);
        let maybe_event = current_events
            .iter()
            .rev()
            .find(|event| event.event_type == "SubleaseIssued");

        let Some(sublease_event) = maybe_event else {
            let persisted_parent = lease_validator
                .get_delegation_parent_lease_id(&current_lease)
                .map_err(|error| {
                    format!(
                        "delegation lineage metadata lookup failed for lease '{current_lease}': \
                         {error}"
                    )
                })?;
            if persisted_parent.is_some() {
                return Err(format!(
                    "delegation lineage evidence missing for delegated lease '{current_lease}'"
                ));
            }
            let has_sublease_child_reference = current_events
                .iter()
                .any(|event| event.event_type == "SubleaseIssued");
            if has_sublease_child_reference {
                return Err(format!(
                    "delegation lineage root verification failed for lease '{current_lease}': \
                     SubleaseIssued child reference exists but parent metadata is missing"
                ));
            }
            return Ok((depth, ticks_used));
        };

        ticks_used = ticks_used
            .checked_add(1)
            .ok_or_else(|| "delegation satisfiability tick overflow".to_string())?;

        let (next_parent, _identity_proof_hash) =
            extract_sublease_replay_bindings(&sublease_event.payload).map_err(|e| {
                format!("invalid SubleaseIssued replay bindings for lease '{current_lease}': {e}")
            })?;
        if lease_validator.get_gate_lease(&next_parent).is_none() {
            return Err(format!(
                "delegation lineage broken: lease '{current_lease}' references ancestor \
                 '{next_parent}' with no authoritative gate_lease_issued record"
            ));
        }

        depth = depth
            .checked_add(1)
            .ok_or_else(|| "delegation depth overflow".to_string())?;
        current_lease = next_parent;
    }

    Err(format!(
        "delegation lineage traversal exceeded hard step cap \
         {MAX_SUBLEASE_LINEAGE_TRAVERSAL_STEPS}"
    ))
}

/// Replay-critical fields persisted for `IngestReviewReceipt` idempotency.
#[derive(Debug, Clone, PartialEq, Eq)]
struct ReceiptReplayBindings {
    lease_id: String,
    work_id: String,
    changeset_digest: [u8; 32],
    verdict: String,
    identity_proof_hash: [u8; 32],
    artifact_bundle_hash: [u8; 32],
    blocked_reason_code: Option<u32>,
    blocked_log_hash: Option<[u8; 32]>,
}

fn normalize_receipt_replay_verdict(verdict: &str) -> Result<String, String> {
    let canonical = verdict.trim().to_ascii_uppercase();
    match canonical.as_str() {
        "APPROVE" | "APPROVED" => Ok("APPROVE".to_string()),
        "BLOCKED" => Ok("BLOCKED".to_string()),
        _ => Err(format!("unsupported verdict value: {verdict}")),
    }
}

/// Extracts replay-critical fields from a persisted review receipt payload.
///
/// Supports both payload shapes used by emitters:
/// - wrapper JSON with hex-encoded inner payload in `"payload"` (stub emitter)
/// - direct top-level fields (`SQLite` emitter)
fn extract_receipt_replay_bindings(event_payload: &[u8]) -> Result<ReceiptReplayBindings, String> {
    let wrapper = serde_json::from_slice::<serde_json::Value>(event_payload)
        .map_err(|e| format!("cannot parse receipt payload wrapper: {e}"))?;

    let inner_payload = wrapper
        .get("payload")
        .and_then(serde_json::Value::as_str)
        .and_then(|hex_payload| hex::decode(hex_payload).ok())
        .and_then(|inner_bytes| serde_json::from_slice::<serde_json::Value>(&inner_bytes).ok());

    let lookup_value = |field: &str| {
        inner_payload
            .as_ref()
            .and_then(|inner| inner.get(field))
            .or_else(|| wrapper.get(field))
    };
    let lookup_field = |field: &str| lookup_value(field).and_then(serde_json::Value::as_str);

    let lease_id = lookup_field("lease_id")
        .or_else(|| lookup_field("episode_id"))
        .map(str::to_owned)
        .ok_or_else(|| "missing lease_id".to_string())?;
    if lease_id.is_empty() {
        return Err("lease_id is empty".to_string());
    }
    let work_id = lookup_field("work_id")
        .or_else(|| lookup_field("lease_id"))
        .or_else(|| lookup_field("episode_id"))
        .map(str::to_owned)
        .ok_or_else(|| "missing work_id".to_string())?;
    if work_id.is_empty() {
        return Err("work_id is empty".to_string());
    }

    let changeset_digest_hex =
        lookup_field("changeset_digest").ok_or_else(|| "missing changeset_digest".to_string())?;
    let changeset_digest = decode_hash32_hex("changeset_digest", changeset_digest_hex)?;

    let verdict_value = lookup_field("verdict")
        .map(str::to_owned)
        .or_else(|| {
            lookup_field("event_type").and_then(|event_type| match event_type {
                "review_receipt_recorded" | "ReviewReceiptRecorded" => Some("APPROVE".to_string()),
                "review_blocked_recorded" | "ReviewBlockedRecorded" => Some("BLOCKED".to_string()),
                _ => None,
            })
        })
        .ok_or_else(|| "missing verdict".to_string())?;
    let verdict = normalize_receipt_replay_verdict(&verdict_value)?;

    let identity_proof_hash_hex = lookup_field("identity_proof_hash")
        .ok_or_else(|| "missing identity_proof_hash".to_string())?;
    let identity_proof_hash_vec = hex::decode(identity_proof_hash_hex)
        .map_err(|e| format!("identity_proof_hash is not valid hex: {e}"))?;
    crate::identity::validate_identity_proof_hash(&identity_proof_hash_vec)
        .map_err(|e| format!("identity_proof_hash validation failed: {e}"))?;
    let identity_proof_hash: [u8; 32] = identity_proof_hash_vec
        .as_slice()
        .try_into()
        .expect("validated to 32 bytes by validate_identity_proof_hash");

    let artifact_bundle_hash_hex = lookup_field("artifact_bundle_hash")
        .ok_or_else(|| "missing artifact_bundle_hash".to_string())?;
    let artifact_bundle_hash = decode_hash32_hex("artifact_bundle_hash", artifact_bundle_hash_hex)?;

    let blocked_reason_code = lookup_value("blocked_reason_code")
        .or_else(|| lookup_value("reason_code"))
        .map(|value| match value {
            serde_json::Value::Number(num) => {
                let reason_u64 = num.as_u64().ok_or_else(|| {
                    "blocked_reason_code must be a non-negative integer".to_string()
                })?;
                u32::try_from(reason_u64)
                    .map_err(|_| format!("blocked_reason_code must fit in u32, got {reason_u64}"))
            },
            serde_json::Value::String(text) => text
                .parse::<u32>()
                .map_err(|e| format!("blocked_reason_code is not a valid u32: {e}")),
            other => Err(format!(
                "blocked_reason_code has unsupported JSON type: {other}"
            )),
        })
        .transpose()?;

    let blocked_log_hash = lookup_field("blocked_log_hash")
        .map(|value| decode_hash32_hex("blocked_log_hash", value))
        .transpose()?;

    Ok(ReceiptReplayBindings {
        lease_id,
        work_id,
        changeset_digest,
        verdict,
        identity_proof_hash,
        artifact_bundle_hash,
        blocked_reason_code,
        blocked_log_hash,
    })
}

/// Validates that an existing receipt event matches the current request fields.
///
/// Returns `Ok(())` if all fields match, or `Err(message)` with a
/// human-readable mismatch description.
#[allow(clippy::too_many_arguments)]
fn validate_receipt_replay_bindings(
    existing: &SignedLedgerEvent,
    request_lease_id: &str,
    request_work_id: &str,
    request_identity_proof_hash: &[u8; 32],
    request_changeset_digest: &[u8; 32],
    request_verdict: &str,
    request_artifact_bundle_hash: &[u8; 32],
    request_blocked_reason_code: Option<u32>,
    request_blocked_log_hash: Option<[u8; 32]>,
) -> Result<(), String> {
    let original = extract_receipt_replay_bindings(&existing.payload)
        .map_err(|e| format!("receipt exists but replay bindings could not be extracted: {e}"))?;

    if original.lease_id != request_lease_id {
        return Err(format!(
            "receipt was originally submitted for lease '{}', not '{}'",
            original.lease_id, request_lease_id
        ));
    }
    if original.work_id != request_work_id {
        return Err(format!(
            "receipt was originally submitted for work '{}', not '{}'",
            original.work_id, request_work_id
        ));
    }
    if !bool::from(
        original
            .identity_proof_hash
            .ct_eq(request_identity_proof_hash),
    ) {
        return Err(format!(
            "identity_proof_hash mismatch: original '{}', requested '{}'",
            hex::encode(original.identity_proof_hash),
            hex::encode(request_identity_proof_hash),
        ));
    }
    if !bool::from(original.changeset_digest.ct_eq(request_changeset_digest)) {
        return Err(format!(
            "changeset_digest mismatch: original '{}', requested '{}'",
            hex::encode(original.changeset_digest),
            hex::encode(request_changeset_digest),
        ));
    }
    if original.verdict != request_verdict {
        return Err(format!(
            "verdict mismatch: original '{}', requested '{}'",
            original.verdict, request_verdict
        ));
    }
    if !bool::from(
        original
            .artifact_bundle_hash
            .ct_eq(request_artifact_bundle_hash),
    ) {
        return Err(format!(
            "artifact_bundle_hash mismatch: original '{}', requested '{}'",
            hex::encode(original.artifact_bundle_hash),
            hex::encode(request_artifact_bundle_hash),
        ));
    }
    if let Some(original_code) = original.blocked_reason_code {
        if Some(original_code) != request_blocked_reason_code {
            return Err(format!(
                "blocked_reason_code mismatch: original '{}', requested '{}'",
                original_code,
                request_blocked_reason_code
                    .map_or_else(|| "<missing>".to_string(), |c| c.to_string()),
            ));
        }
    }
    if let Some(original_hash) = original.blocked_log_hash {
        let hash_matches =
            request_blocked_log_hash.is_some_and(|h| bool::from(original_hash.ct_eq(&h)));
        if !hash_matches {
            return Err(format!(
                "blocked_log_hash mismatch: original '{}', requested '{}'",
                hex::encode(original_hash),
                request_blocked_log_hash.map_or_else(|| "<missing>".to_string(), hex::encode),
            ));
        }
    }
    Ok(())
}

// ============================================================================
// Connection Context
// ============================================================================

/// Connection context tracking privilege level and authentication state.
///
/// Per DD-001 (`privilege_predicate`), connections are classified as privileged
/// based on the socket path:
/// - operator.sock: `is_privileged = true`
/// - session.sock: `is_privileged = false`
///
/// # TCK-00303: Connection Lifecycle Management
///
/// The `connection_id` field is used to track connections in the subscription
/// registry. When a connection closes, the connection handler MUST call
/// `subscription_registry.unregister_connection(connection_id)` to free
/// resources and prevent connection slot leaks.
#[derive(Debug, Clone)]
pub struct ConnectionContext {
    /// Whether this connection is privileged (operator socket).
    is_privileged: bool,

    /// Peer credentials extracted via `SO_PEERCRED`.
    peer_credentials: Option<PeerCredentials>,

    /// Session ID for session-scoped connections (None for operator
    /// connections).
    session_id: Option<String>,

    /// Connection ID for subscription registry tracking (TCK-00303).
    ///
    /// Generated once when the connection is established and used consistently
    /// across all subscribe/unsubscribe operations. Must be passed to
    /// `unregister_connection` when the connection closes to prevent leaks.
    connection_id: String,

    /// Contract binding metadata from the handshake (TCK-00348).
    ///
    /// Stored at connection time and threaded into `SessionStarted` events
    /// during `SpawnEpisode` so the authoritative record uses the real
    /// session ID (not a surrogate connection ID).
    contract_binding: Option<crate::hsi_contract::SessionContractBinding>,

    /// Active identity proof profile hash for this connection's identity
    /// context.
    ///
    /// This is persisted into `SessionStarted` payloads when present so
    /// session-open audit records bind to the verifier economics contract
    /// (REQ-0012). Full identity proof dereference wiring is deferred under
    /// WVR-0103 / TCK-00361.
    identity_proof_profile_hash: Option<[u8; 32]>,

    /// Connection phase for session-typed state machine (TCK-00349).
    ///
    /// Per REQ-0003, authority-bearing operations require valid session-state
    /// progression. The phase starts at `Connected`, advances to
    /// `HandshakeComplete` after successful handshake, then to `SessionOpen`
    /// when the connection is ready for IPC dispatch.
    ///
    /// The [`PrivilegedDispatcher::dispatch`] and
    /// [`SessionDispatcher::dispatch`] methods check this phase to ensure
    /// no dispatch occurs before `SessionOpen`.
    phase: crate::protocol::connection_handler::ConnectionPhase,
}

impl ConnectionContext {
    /// Creates a new privileged connection context (operator socket).
    ///
    /// # TCK-00303: Connection ID Generation
    ///
    /// The `connection_id` is generated from peer credentials (PID-based for
    /// operator connections) or a UUID if credentials are unavailable.
    #[must_use]
    pub fn privileged(peer_credentials: Option<PeerCredentials>) -> Self {
        let connection_id = peer_credentials.as_ref().and_then(|c| c.pid).map_or_else(
            || format!("CONN-OP-{}", uuid::Uuid::new_v4()),
            |pid| format!("CONN-OP-{pid}"),
        );
        Self {
            is_privileged: true,
            peer_credentials,
            session_id: None,
            connection_id,
            contract_binding: None,
            identity_proof_profile_hash: None,
            // TCK-00349: New connections start in Connected phase.
            // Callers MUST advance to SessionOpen before dispatch.
            phase: crate::protocol::connection_handler::ConnectionPhase::Connected,
        }
    }

    /// Creates a new session-scoped connection context (session socket).
    ///
    /// # TCK-00303: Connection ID Generation
    ///
    /// The `connection_id` is generated from the session ID (if available)
    /// or peer credentials (PID-based), or a UUID if neither is available.
    #[must_use]
    pub fn session(peer_credentials: Option<PeerCredentials>, session_id: Option<String>) -> Self {
        // For session connections, prefer session_id-based connection ID,
        // but fall back to PID or UUID if session_id is not yet known
        // (it may be set later via session token validation)
        let connection_id = session_id.as_ref().map_or_else(
            || {
                peer_credentials.as_ref().and_then(|c| c.pid).map_or_else(
                    || format!("CONN-SESS-{}", uuid::Uuid::new_v4()),
                    |pid| format!("CONN-SESS-{pid}"),
                )
            },
            |sid| format!("CONN-SESS-{sid}"),
        );
        Self {
            is_privileged: false,
            peer_credentials,
            session_id,
            connection_id,
            contract_binding: None,
            identity_proof_profile_hash: None,
            // TCK-00349: New connections start in Connected phase.
            // Callers MUST advance to SessionOpen before dispatch.
            phase: crate::protocol::connection_handler::ConnectionPhase::Connected,
        }
    }

    /// Returns `true` if this connection has privileged access.
    #[must_use]
    pub const fn is_privileged(&self) -> bool {
        self.is_privileged
    }

    /// Returns the peer credentials if available.
    #[must_use]
    pub const fn peer_credentials(&self) -> Option<&PeerCredentials> {
        self.peer_credentials.as_ref()
    }

    /// Returns the session ID for session-scoped connections.
    #[must_use]
    pub fn session_id(&self) -> Option<&str> {
        self.session_id.as_deref()
    }

    /// Returns the connection ID for subscription registry tracking.
    ///
    /// # TCK-00303: Connection Lifecycle
    ///
    /// This ID must be passed to `unregister_connection` when the connection
    /// closes to free subscription registry slots and prevent `DoS` via
    /// connection slot exhaustion.
    #[must_use]
    pub fn connection_id(&self) -> &str {
        &self.connection_id
    }

    /// Sets the contract binding metadata from the handshake (TCK-00348).
    ///
    /// Called after `perform_handshake` succeeds. The binding is later
    /// threaded into `SessionStarted` events during `emit_spawn_lifecycle`.
    pub fn set_contract_binding(&mut self, binding: crate::hsi_contract::SessionContractBinding) {
        self.contract_binding = Some(binding);
    }

    /// Returns the contract binding if set.
    #[must_use]
    pub const fn contract_binding(&self) -> Option<&crate::hsi_contract::SessionContractBinding> {
        self.contract_binding.as_ref()
    }

    /// Sets the active identity proof profile hash for this connection.
    ///
    /// Called from the production session-open path in `main.rs` when
    /// the active identity proof profile is resolved after handshake
    /// (TCK-00358). Also used by test code to inject specific profile
    /// hashes for verification. The spawn path emits a warning and
    /// falls back to the baseline SMT-256 10^12 profile hash if this
    /// method was not called at session-open time.
    pub fn set_identity_proof_profile_hash(
        &mut self,
        profile_hash: [u8; 32],
    ) -> Result<(), crate::identity::IdentityProofError> {
        if bool::from(profile_hash.ct_eq(&[0u8; 32])) {
            return Err(crate::identity::IdentityProofError::InvalidField {
                field: "identity_proof_profile_hash",
                reason: "must be non-zero".to_string(),
            });
        }
        self.identity_proof_profile_hash = Some(profile_hash);
        Ok(())
    }

    /// Returns the active identity proof profile hash, if known.
    #[must_use]
    pub const fn identity_proof_profile_hash(&self) -> Option<&[u8; 32]> {
        self.identity_proof_profile_hash.as_ref()
    }

    /// Returns the current connection phase (TCK-00349).
    #[must_use]
    pub const fn phase(&self) -> crate::protocol::connection_handler::ConnectionPhase {
        self.phase
    }

    /// Advances the connection phase to `HandshakeComplete` (TCK-00349).
    ///
    /// Called after `perform_handshake` succeeds. Must be called before
    /// `advance_to_session_open`.
    ///
    /// # Errors
    ///
    /// Returns `ConnectionPhaseError::IllegalTransition` if the current
    /// phase is not `Connected`.
    pub fn advance_to_handshake_complete(
        &mut self,
    ) -> Result<(), crate::protocol::connection_handler::ConnectionPhaseError> {
        self.phase = self.phase.advance_to_handshake_complete()?;
        Ok(())
    }

    /// Advances the connection phase to `SessionOpen` (TCK-00349).
    ///
    /// Called after handshake completion, before entering the message
    /// dispatch loop. This is the gate that permits authority-bearing
    /// IPC operations.
    ///
    /// # Errors
    ///
    /// Returns `ConnectionPhaseError::IllegalTransition` if the current
    /// phase is not `HandshakeComplete`.
    pub fn advance_to_session_open(
        &mut self,
    ) -> Result<(), crate::protocol::connection_handler::ConnectionPhaseError> {
        self.phase = self.phase.advance_to_session_open()?;
        Ok(())
    }

    /// Creates a privileged connection context already in `SessionOpen` phase.
    ///
    /// This is a convenience constructor for tests and situations where the
    /// handshake has already been validated externally (e.g., unit tests
    /// that exercise dispatch directly without a socket handshake).
    ///
    /// # Panics
    ///
    /// Panics if the phase transitions fail (should never happen for a fresh
    /// context).
    #[must_use]
    pub fn privileged_session_open(peer_credentials: Option<PeerCredentials>) -> Self {
        let mut ctx = Self::privileged(peer_credentials);
        ctx.advance_to_handshake_complete()
            .expect("fresh context should transition to HandshakeComplete");
        ctx.advance_to_session_open()
            .expect("HandshakeComplete should transition to SessionOpen");
        ctx
    }

    /// Creates a session-scoped connection context already in `SessionOpen`
    /// phase.
    ///
    /// This is a convenience constructor for tests and situations where the
    /// handshake has already been validated externally.
    ///
    /// # Panics
    ///
    /// Panics if the phase transitions fail (should never happen for a fresh
    /// context).
    #[must_use]
    pub fn session_open(
        peer_credentials: Option<PeerCredentials>,
        session_id: Option<String>,
    ) -> Self {
        let mut ctx = Self::session(peer_credentials, session_id);
        ctx.advance_to_handshake_complete()
            .expect("fresh context should transition to HandshakeComplete");
        ctx.advance_to_session_open()
            .expect("HandshakeComplete should transition to SessionOpen");
        ctx
    }
}

// ============================================================================
// Message Type Tags (for routing)
// ============================================================================

/// Message type tags for privileged endpoint routing.
///
/// These tags are used to identify the message type before decoding,
/// allowing the dispatcher to route to the appropriate handler.
///
/// # Consensus Query Tag Range (CTR-PROTO-011)
///
/// Consensus query messages use tags 5-8 per RFC-0014/TCK-00345:
/// - 5 = `ConsensusStatus`
/// - 6 = `ConsensusValidators`
/// - 7 = `ConsensusByzantineEvidence`
/// - 8 = `ConsensusMetrics`
///
/// # HEF Tag Range (CTR-PROTO-010)
///
/// HEF messages use tag range 64-79 per RFC-0018:
/// - 64 = `SubscribePulse`
/// - 65 = `SubscribePulseResponse` (response only)
/// - 66 = `UnsubscribePulse`
/// - 67 = `UnsubscribePulseResponse` (response only)
/// - 68 = `PulseEvent` (server->client only)
#[derive(Debug, Clone, Copy, PartialEq, Eq, Hash)]
#[repr(u8)]
pub enum PrivilegedMessageType {
    /// `ClaimWork` request (IPC-PRIV-001)
    ClaimWork           = 1,
    /// `SpawnEpisode` request (IPC-PRIV-002)
    SpawnEpisode        = 2,
    /// `IssueCapability` request (IPC-PRIV-003)
    IssueCapability     = 3,
    /// Shutdown request (IPC-PRIV-004)
    Shutdown            = 4,
    // --- Process Management (CTR-PROTO-011, TCK-00342) ---
    /// `ListProcesses` request (IPC-PRIV-005)
    ListProcesses       = 5,
    /// `ProcessStatus` request (IPC-PRIV-006)
    ProcessStatus       = 6,
    /// `StartProcess` request (IPC-PRIV-007)
    StartProcess        = 7,
    /// `StopProcess` request (IPC-PRIV-008)
    StopProcess         = 8,
    /// `RestartProcess` request (IPC-PRIV-009)
    RestartProcess      = 9,
    /// `ReloadProcess` request (IPC-PRIV-010)
    ReloadProcess       = 10,
    // --- Consensus Query Endpoints (CTR-PROTO-011, RFC-0014, TCK-00345) ---
    /// `ConsensusStatus` request (IPC-PRIV-011)
    ConsensusStatus     = 11,
    /// `ConsensusValidators` request (IPC-PRIV-012)
    ConsensusValidators = 12,
    /// `ConsensusByzantineEvidence` request (IPC-PRIV-013)
    ConsensusByzantineEvidence = 13,
    /// `ConsensusMetrics` request (IPC-PRIV-014)
    ConsensusMetrics    = 14,
    /// `WorkStatus` request (IPC-PRIV-015, TCK-00344)
    WorkStatus          = 15,
    /// `EndSession` request (IPC-PRIV-016, TCK-00395)
    EndSession          = 16,
    /// `IngestReviewReceipt` request (IPC-PRIV-017, TCK-00389)
    IngestReviewReceipt = 17,
    /// `UpdateStopFlags` request (IPC-PRIV-018, TCK-00351)
    UpdateStopFlags     = 18,
    /// `WorkList` request (IPC-PRIV-019, TCK-00415)
    WorkList            = 19,
    /// `AuditorLaunchProjection` request (IPC-PRIV-020, TCK-00452)
    AuditorLaunchProjection = 20,
    // --- Credential Management (CTR-PROTO-012, RFC-0018, TCK-00343) ---
    /// `ListCredentials` request (IPC-PRIV-021)
    ListCredentials     = 21,
    /// `AddCredential` request (IPC-PRIV-022)
    AddCredential       = 22,
    /// `RemoveCredential` request (IPC-PRIV-023)
    RemoveCredential    = 23,
    /// `RefreshCredential` request (IPC-PRIV-024)
    RefreshCredential   = 24,
    /// `SwitchCredential` request (IPC-PRIV-025)
    SwitchCredential    = 25,
    /// `LoginCredential` request (IPC-PRIV-026)
    LoginCredential     = 26,
    /// `OrchestratorLaunchProjection` request (IPC-PRIV-027, TCK-00452)
    OrchestratorLaunchProjection = 27,
    // --- HEF Pulse Plane (CTR-PROTO-010, RFC-0018) ---
    /// `SubscribePulse` request (IPC-HEF-001)
    SubscribePulse      = 64,
    /// `UnsubscribePulse` request (IPC-HEF-002)
    UnsubscribePulse    = 66,
    /// `PulseEvent` notification (server->client, IPC-HEF-003)
    PulseEvent          = 68,
    // --- ChangeSet Publishing (RFC-0018, TCK-00394) ---
    /// `PublishChangeSet` request (IPC-PRIV-017)
    PublishChangeSet    = 70,
    // --- Sublease Delegation (RFC-0019, TCK-00340) ---
    /// `DelegateSublease` request (IPC-PRIV-072)
    DelegateSublease    = 72,
    // --- Ledger Integrity Audit (TCK-00487) ---
    /// `VerifyLedgerChain` request (IPC-PRIV-073)
    VerifyLedgerChain   = 73,
    // --- Projection Recovery (TCK-00469) ---
    /// `RegisterRecoveryEvidence` request (IPC-PRIV-074)
    RegisterRecoveryEvidence = 74,
    /// `RequestUnfreeze` request (IPC-PRIV-075)
    RequestUnfreeze     = 75,
}

impl PrivilegedMessageType {
    /// Attempts to parse a message type from a tag byte.
    #[must_use]
    pub const fn from_tag(tag: u8) -> Option<Self> {
        match tag {
            1 => Some(Self::ClaimWork),
            2 => Some(Self::SpawnEpisode),
            3 => Some(Self::IssueCapability),
            4 => Some(Self::Shutdown),
            // Process Management tags (5-10)
            5 => Some(Self::ListProcesses),
            6 => Some(Self::ProcessStatus),
            7 => Some(Self::StartProcess),
            8 => Some(Self::StopProcess),
            9 => Some(Self::RestartProcess),
            10 => Some(Self::ReloadProcess),
            // Consensus query tags (11-14, TCK-00345)
            11 => Some(Self::ConsensusStatus),
            12 => Some(Self::ConsensusValidators),
            13 => Some(Self::ConsensusByzantineEvidence),
            14 => Some(Self::ConsensusMetrics),
            // TCK-00344: Work status query
            15 => Some(Self::WorkStatus),
            // TCK-00395: EndSession for session termination
            16 => Some(Self::EndSession),
            // TCK-00389: IngestReviewReceipt for external reviewer results
            17 => Some(Self::IngestReviewReceipt),
            // TCK-00351: stop flags update
            18 => Some(Self::UpdateStopFlags),
            // TCK-00415: projection-backed work listing
            19 => Some(Self::WorkList),
            // TCK-00452: auditor projection
            20 => Some(Self::AuditorLaunchProjection),
            // Credential management tags (21-26, TCK-00343)
            21 => Some(Self::ListCredentials),
            22 => Some(Self::AddCredential),
            23 => Some(Self::RemoveCredential),
            24 => Some(Self::RefreshCredential),
            25 => Some(Self::SwitchCredential),
            26 => Some(Self::LoginCredential),
            // TCK-00452: orchestrator projection
            27 => Some(Self::OrchestratorLaunchProjection),
            // HEF tags (64-68)
            64 => Some(Self::SubscribePulse),
            66 => Some(Self::UnsubscribePulse),
            68 => Some(Self::PulseEvent),
            // ChangeSet publishing (TCK-00394)
            70 => Some(Self::PublishChangeSet),
            // Sublease delegation (TCK-00340)
            72 => Some(Self::DelegateSublease),
            // Ledger integrity audit (TCK-00487)
            73 => Some(Self::VerifyLedgerChain),
            // Projection recovery (TCK-00469)
            74 => Some(Self::RegisterRecoveryEvidence),
            75 => Some(Self::RequestUnfreeze),
            _ => None,
        }
    }

    /// Returns the tag byte for this message type.
    #[must_use]
    pub const fn tag(self) -> u8 {
        self as u8
    }

    /// Returns all request-bearing variants of `PrivilegedMessageType`.
    ///
    /// This excludes response-only and notification-only variants such as
    /// `PulseEvent`. Adding a new request-bearing variant to the enum
    /// without adding it here will cause the HSI manifest completeness
    /// tests to fail.
    #[must_use]
    pub const fn all_request_variants() -> &'static [Self] {
        &[
            Self::ClaimWork,
            Self::SpawnEpisode,
            Self::IssueCapability,
            Self::Shutdown,
            Self::ListProcesses,
            Self::ProcessStatus,
            Self::StartProcess,
            Self::StopProcess,
            Self::RestartProcess,
            Self::ReloadProcess,
            Self::ConsensusStatus,
            Self::ConsensusValidators,
            Self::ConsensusByzantineEvidence,
            Self::ConsensusMetrics,
            Self::WorkStatus,
            Self::EndSession,
            Self::IngestReviewReceipt,
            Self::UpdateStopFlags,
            Self::WorkList,
            Self::AuditorLaunchProjection,
            Self::ListCredentials,
            Self::AddCredential,
            Self::RemoveCredential,
            Self::RefreshCredential,
            Self::SwitchCredential,
            Self::LoginCredential,
            Self::OrchestratorLaunchProjection,
            Self::SubscribePulse,
            Self::UnsubscribePulse,
            Self::PublishChangeSet,
            Self::DelegateSublease,
            Self::VerifyLedgerChain,
            Self::RegisterRecoveryEvidence,
            Self::RequestUnfreeze,
        ]
    }

    /// Returns `true` if this variant represents a client-initiated request,
    /// `false` if it is a server-to-client notification or response-only
    /// variant.
    ///
    /// This method uses an **exhaustive** match (no wildcard `_ =>` arm), so
    /// adding a new variant to the enum forces a compile error until it is
    /// classified here. This provides the non-self-referential completeness
    /// guarantee required by RFC-0020 section 3.1.1.
    #[must_use]
    pub const fn is_client_request(self) -> bool {
        // IMPORTANT: This match MUST remain exhaustive (no `_ =>` wildcard).
        // Adding a new enum variant forces a compile error here, ensuring the
        // developer must classify it as client-request (true) or not (false).
        match self {
            Self::ClaimWork
            | Self::SpawnEpisode
            | Self::IssueCapability
            | Self::Shutdown
            | Self::ListProcesses
            | Self::ProcessStatus
            | Self::StartProcess
            | Self::StopProcess
            | Self::RestartProcess
            | Self::ReloadProcess
            | Self::ConsensusStatus
            | Self::ConsensusValidators
            | Self::ConsensusByzantineEvidence
            | Self::ConsensusMetrics
            | Self::WorkStatus
            | Self::EndSession
            | Self::IngestReviewReceipt
            | Self::UpdateStopFlags
            | Self::WorkList
            | Self::AuditorLaunchProjection
            | Self::ListCredentials
            | Self::AddCredential
            | Self::RemoveCredential
            | Self::RefreshCredential
            | Self::SwitchCredential
            | Self::LoginCredential
            | Self::OrchestratorLaunchProjection
            | Self::SubscribePulse
            | Self::UnsubscribePulse
            | Self::PublishChangeSet
            | Self::DelegateSublease
            | Self::VerifyLedgerChain
            | Self::RegisterRecoveryEvidence
            | Self::RequestUnfreeze => true,
            // Server-to-client notification only — not a client request.
            Self::PulseEvent => false,
        }
    }

    /// Returns the HSI route path for this variant.
    ///
    /// Used by the HSI contract manifest to derive routes directly from the
    /// dispatch enum, ensuring the manifest stays in sync with the actual
    /// dispatch registry.
    #[must_use]
    pub const fn hsi_route(self) -> &'static str {
        match self {
            Self::ClaimWork => "hsi.work.claim",
            Self::SpawnEpisode => "hsi.episode.spawn",
            Self::IssueCapability => "hsi.capability.issue",
            Self::Shutdown => "hsi.daemon.shutdown",
            Self::ListProcesses => "hsi.process.list",
            Self::ProcessStatus => "hsi.process.status",
            Self::StartProcess => "hsi.process.start",
            Self::StopProcess => "hsi.process.stop",
            Self::RestartProcess => "hsi.process.restart",
            Self::ReloadProcess => "hsi.process.reload",
            Self::ConsensusStatus => "hsi.consensus.status",
            Self::ConsensusValidators => "hsi.consensus.validators",
            Self::ConsensusByzantineEvidence => "hsi.consensus.byzantine_evidence",
            Self::ConsensusMetrics => "hsi.consensus.metrics",
            Self::WorkStatus => "hsi.work.status",
            Self::EndSession => "hsi.session.end",
            Self::IngestReviewReceipt => "hsi.review.ingest_receipt",
            Self::UpdateStopFlags => "hsi.stop.update_flags",
            Self::WorkList => "hsi.work.list",
            Self::AuditorLaunchProjection => "hsi.launch.auditor_projection",
            Self::ListCredentials => "hsi.credential.list",
            Self::AddCredential => "hsi.credential.add",
            Self::RemoveCredential => "hsi.credential.remove",
            Self::RefreshCredential => "hsi.credential.refresh",
            Self::SwitchCredential => "hsi.credential.switch",
            Self::LoginCredential => "hsi.credential.login",
            Self::OrchestratorLaunchProjection => "hsi.launch.orchestrator_projection",
            Self::SubscribePulse => "hsi.pulse.subscribe",
            Self::UnsubscribePulse => "hsi.pulse.unsubscribe",
            Self::PublishChangeSet => "hsi.changeset.publish",
            Self::DelegateSublease => "hsi.sublease.delegate",
            Self::VerifyLedgerChain => "hsi.ledger.verify_chain",
            Self::RegisterRecoveryEvidence => "hsi.projection.register_recovery_evidence",
            Self::RequestUnfreeze => "hsi.projection.request_unfreeze",
            Self::PulseEvent => "hsi.pulse.event",
        }
    }

    /// Returns the HSI manifest route ID for this variant.
    #[must_use]
    pub const fn hsi_route_id(self) -> &'static str {
        match self {
            Self::ClaimWork => "CLAIM_WORK",
            Self::SpawnEpisode => "SPAWN_EPISODE",
            Self::IssueCapability => "ISSUE_CAPABILITY",
            Self::Shutdown => "SHUTDOWN",
            Self::ListProcesses => "LIST_PROCESSES",
            Self::ProcessStatus => "PROCESS_STATUS",
            Self::StartProcess => "START_PROCESS",
            Self::StopProcess => "STOP_PROCESS",
            Self::RestartProcess => "RESTART_PROCESS",
            Self::ReloadProcess => "RELOAD_PROCESS",
            Self::ConsensusStatus => "CONSENSUS_STATUS",
            Self::ConsensusValidators => "CONSENSUS_VALIDATORS",
            Self::ConsensusByzantineEvidence => "CONSENSUS_BYZANTINE_EVIDENCE",
            Self::ConsensusMetrics => "CONSENSUS_METRICS",
            Self::WorkStatus => "WORK_STATUS",
            Self::EndSession => "END_SESSION",
            Self::IngestReviewReceipt => "INGEST_REVIEW_RECEIPT",
            Self::UpdateStopFlags => "UPDATE_STOP_FLAGS",
            Self::WorkList => "WORK_LIST",
            Self::AuditorLaunchProjection => "AUDITOR_LAUNCH_PROJECTION",
            Self::ListCredentials => "LIST_CREDENTIALS",
            Self::AddCredential => "ADD_CREDENTIAL",
            Self::RemoveCredential => "REMOVE_CREDENTIAL",
            Self::RefreshCredential => "REFRESH_CREDENTIAL",
            Self::SwitchCredential => "SWITCH_CREDENTIAL",
            Self::LoginCredential => "LOGIN_CREDENTIAL",
            Self::OrchestratorLaunchProjection => "ORCHESTRATOR_LAUNCH_PROJECTION",
            Self::SubscribePulse => "SUBSCRIBE_PULSE",
            Self::UnsubscribePulse => "UNSUBSCRIBE_PULSE",
            Self::PublishChangeSet => "PUBLISH_CHANGESET",
            Self::DelegateSublease => "DELEGATE_SUBLEASE",
            Self::VerifyLedgerChain => "VERIFY_LEDGER_CHAIN",
            Self::RegisterRecoveryEvidence => "REGISTER_RECOVERY_EVIDENCE",
            Self::RequestUnfreeze => "REQUEST_UNFREEZE",
            Self::PulseEvent => "PULSE_EVENT",
        }
    }

    /// Returns the HSI request schema ID for this variant.
    #[must_use]
    pub const fn hsi_request_schema(self) -> &'static str {
        match self {
            Self::ClaimWork => "apm2.claim_work_request.v1",
            Self::SpawnEpisode => "apm2.spawn_episode_request.v1",
            Self::IssueCapability => "apm2.issue_capability_request.v1",
            Self::Shutdown => "apm2.shutdown_request.v1",
            Self::ListProcesses => "apm2.list_processes_request.v1",
            Self::ProcessStatus => "apm2.process_status_request.v1",
            Self::StartProcess => "apm2.start_process_request.v1",
            Self::StopProcess => "apm2.stop_process_request.v1",
            Self::RestartProcess => "apm2.restart_process_request.v1",
            Self::ReloadProcess => "apm2.reload_process_request.v1",
            Self::ConsensusStatus => "apm2.consensus_status_request.v1",
            Self::ConsensusValidators => "apm2.consensus_validators_request.v1",
            Self::ConsensusByzantineEvidence => "apm2.consensus_byzantine_evidence_request.v1",
            Self::ConsensusMetrics => "apm2.consensus_metrics_request.v1",
            Self::WorkStatus => "apm2.work_status_request.v1",
            Self::EndSession => "apm2.end_session_request.v1",
            Self::IngestReviewReceipt => "apm2.ingest_review_receipt_request.v1",
            Self::UpdateStopFlags => "apm2.update_stop_flags_request.v1",
            Self::WorkList => "apm2.work_list_request.v1",
            Self::AuditorLaunchProjection => "apm2.auditor_launch_projection_request.v1",
            Self::ListCredentials => "apm2.list_credentials_request.v1",
            Self::AddCredential => "apm2.add_credential_request.v1",
            Self::RemoveCredential => "apm2.remove_credential_request.v1",
            Self::RefreshCredential => "apm2.refresh_credential_request.v1",
            Self::SwitchCredential => "apm2.switch_credential_request.v1",
            Self::LoginCredential => "apm2.login_credential_request.v1",
            Self::OrchestratorLaunchProjection => "apm2.orchestrator_launch_projection_request.v1",
            Self::SubscribePulse => "apm2.subscribe_pulse_request.v1",
            Self::UnsubscribePulse => "apm2.unsubscribe_pulse_request.v1",
            Self::PublishChangeSet => "apm2.publish_changeset_request.v1",
            Self::DelegateSublease => "apm2.delegate_sublease_request.v1",
            Self::VerifyLedgerChain => "apm2.verify_ledger_chain_request.v1",
            Self::RegisterRecoveryEvidence => "apm2.register_recovery_evidence_request.v1",
            Self::RequestUnfreeze => "apm2.request_unfreeze_request.v1",
            Self::PulseEvent => "apm2.pulse_event_request.v1",
        }
    }

    /// Returns the HSI response schema ID for this variant.
    #[must_use]
    pub const fn hsi_response_schema(self) -> &'static str {
        match self {
            Self::ClaimWork => "apm2.claim_work_response.v1",
            Self::SpawnEpisode => "apm2.spawn_episode_response.v1",
            Self::IssueCapability => "apm2.issue_capability_response.v1",
            Self::Shutdown => "apm2.shutdown_response.v1",
            Self::ListProcesses => "apm2.list_processes_response.v1",
            Self::ProcessStatus => "apm2.process_status_response.v1",
            Self::StartProcess => "apm2.start_process_response.v1",
            Self::StopProcess => "apm2.stop_process_response.v1",
            Self::RestartProcess => "apm2.restart_process_response.v1",
            Self::ReloadProcess => "apm2.reload_process_response.v1",
            Self::ConsensusStatus => "apm2.consensus_status_response.v1",
            Self::ConsensusValidators => "apm2.consensus_validators_response.v1",
            Self::ConsensusByzantineEvidence => "apm2.consensus_byzantine_evidence_response.v1",
            Self::ConsensusMetrics => "apm2.consensus_metrics_response.v1",
            Self::WorkStatus => "apm2.work_status_response.v1",
            Self::EndSession => "apm2.end_session_response.v1",
            Self::IngestReviewReceipt => "apm2.ingest_review_receipt_response.v1",
            Self::UpdateStopFlags => "apm2.update_stop_flags_response.v1",
            Self::WorkList => "apm2.work_list_response.v1",
            Self::AuditorLaunchProjection => "apm2.auditor_launch_projection_response.v1",
            Self::ListCredentials => "apm2.list_credentials_response.v1",
            Self::AddCredential => "apm2.add_credential_response.v1",
            Self::RemoveCredential => "apm2.remove_credential_response.v1",
            Self::RefreshCredential => "apm2.refresh_credential_response.v1",
            Self::SwitchCredential => "apm2.switch_credential_response.v1",
            Self::LoginCredential => "apm2.login_credential_response.v1",
            Self::OrchestratorLaunchProjection => "apm2.orchestrator_launch_projection_response.v1",
            Self::SubscribePulse => "apm2.subscribe_pulse_response.v1",
            Self::UnsubscribePulse => "apm2.unsubscribe_pulse_response.v1",
            Self::PublishChangeSet => "apm2.publish_changeset_response.v1",
            Self::DelegateSublease => "apm2.delegate_sublease_response.v1",
            Self::VerifyLedgerChain => "apm2.verify_ledger_chain_response.v1",
            Self::RegisterRecoveryEvidence => "apm2.register_recovery_evidence_response.v1",
            Self::RequestUnfreeze => "apm2.request_unfreeze_response.v1",
            Self::PulseEvent => "apm2.pulse_event_response.v1",
        }
    }
}

// ============================================================================
// Response Envelope
// ============================================================================

/// Response envelope for privileged endpoint responses.
///
/// Contains either a successful response or an error.
#[derive(Debug)]
pub enum PrivilegedResponse {
    /// Successful `ClaimWork` response.
    ClaimWork(ClaimWorkResponse),
    /// Successful `SpawnEpisode` response.
    SpawnEpisode(SpawnEpisodeResponse),
    /// Successful `IssueCapability` response.
    IssueCapability(IssueCapabilityResponse),
    /// Successful Shutdown response.
    Shutdown(ShutdownResponse),
    // --- Process Management (TCK-00342) ---
    /// Successful `ListProcesses` response.
    ListProcesses(ListProcessesResponse),
    /// Successful `ProcessStatus` response.
    ProcessStatus(ProcessStatusResponse),
    /// Successful `StartProcess` response.
    StartProcess(StartProcessResponse),
    /// Successful `StopProcess` response.
    StopProcess(StopProcessResponse),
    /// Successful `RestartProcess` response.
    RestartProcess(RestartProcessResponse),
    /// Successful `ReloadProcess` response.
    ReloadProcess(ReloadProcessResponse),
    /// Successful `WorkStatus` response (TCK-00344).
    WorkStatus(WorkStatusResponse),
    /// Successful `WorkList` response (TCK-00415).
    WorkList(WorkListResponse),
    /// Successful `AuditorLaunchProjection` response (TCK-00452).
    AuditorLaunchProjection(AuditorLaunchProjectionResponse),
    /// Successful `OrchestratorLaunchProjection` response (TCK-00452).
    OrchestratorLaunchProjection(OrchestratorLaunchProjectionResponse),
    /// Successful `EndSession` response (TCK-00395).
    EndSession(EndSessionResponse),
    /// Successful `IngestReviewReceipt` response (TCK-00389).
    IngestReviewReceipt(IngestReviewReceiptResponse),
    /// Successful `UpdateStopFlags` response (TCK-00351).
    UpdateStopFlags(UpdateStopFlagsResponse),
    // --- Credential Management (CTR-PROTO-012, TCK-00343) ---
    /// Successful `ListCredentials` response.
    ListCredentials(ListCredentialsResponse),
    /// Successful `AddCredential` response.
    AddCredential(AddCredentialResponse),
    /// Successful `RemoveCredential` response.
    RemoveCredential(RemoveCredentialResponse),
    /// Successful `RefreshCredential` response.
    RefreshCredential(RefreshCredentialResponse),
    /// Successful `SwitchCredential` response.
    SwitchCredential(SwitchCredentialResponse),
    /// Successful `LoginCredential` response.
    LoginCredential(LoginCredentialResponse),
    /// Successful `SubscribePulse` response (TCK-00302).
    SubscribePulse(SubscribePulseResponse),
    /// Successful `UnsubscribePulse` response (TCK-00302).
    UnsubscribePulse(UnsubscribePulseResponse),
    /// Successful `ConsensusStatus` response (TCK-00345).
    ConsensusStatus(ConsensusStatusResponse),
    /// Successful `ConsensusValidators` response (TCK-00345).
    ConsensusValidators(ConsensusValidatorsResponse),
    /// Successful `ConsensusByzantineEvidence` response (TCK-00345).
    ConsensusByzantineEvidence(ConsensusByzantineEvidenceResponse),
    /// Successful `ConsensusMetrics` response (TCK-00345).
    ConsensusMetrics(ConsensusMetricsResponse),
    /// Successful `PublishChangeSet` response (TCK-00394).
    PublishChangeSet(PublishChangeSetResponse),
    /// Successful `DelegateSublease` response (TCK-00340).
    DelegateSublease(DelegateSubleaseResponse),
    /// Successful `VerifyLedgerChain` response (TCK-00487).
    VerifyLedgerChain(VerifyLedgerChainResponse),
    /// Successful `RegisterRecoveryEvidence` response (TCK-00469).
    RegisterRecoveryEvidence(RegisterRecoveryEvidenceResponse),
    /// Successful `RequestUnfreeze` response (TCK-00469).
    RequestUnfreeze(RequestUnfreezeResponse),
    /// Error response.
    Error(PrivilegedError),
}

impl PrivilegedResponse {
    /// Creates a `PERMISSION_DENIED` error response.
    #[must_use]
    pub fn permission_denied() -> Self {
        Self::Error(PrivilegedError {
            code: PrivilegedErrorCode::PermissionDenied.into(),
            message: "permission denied".to_string(),
        })
    }

    /// Creates a custom error response.
    #[must_use]
    pub fn error(code: PrivilegedErrorCode, message: impl Into<String>) -> Self {
        Self::Error(PrivilegedError {
            code: code.into(),
            message: message.into(),
        })
    }

    /// Encodes the response to bytes.
    ///
    /// The format is: [tag: u8][payload: protobuf]
    /// Tag 0 indicates an error response.
    #[must_use]
    pub fn encode(&self) -> Bytes {
        // Response tags for HEF messages (request tag + 1)
        const SUBSCRIBE_PULSE_RESPONSE_TAG: u8 = 65;
        const UNSUBSCRIBE_PULSE_RESPONSE_TAG: u8 = 67;

        let mut buf = Vec::new();
        match self {
            Self::ClaimWork(resp) => {
                buf.push(PrivilegedMessageType::ClaimWork.tag());
                resp.encode(&mut buf).expect("encode cannot fail");
            },
            Self::SpawnEpisode(resp) => {
                buf.push(PrivilegedMessageType::SpawnEpisode.tag());
                resp.encode(&mut buf).expect("encode cannot fail");
            },
            Self::IssueCapability(resp) => {
                buf.push(PrivilegedMessageType::IssueCapability.tag());
                resp.encode(&mut buf).expect("encode cannot fail");
            },
            Self::Shutdown(resp) => {
                buf.push(PrivilegedMessageType::Shutdown.tag());
                resp.encode(&mut buf).expect("encode cannot fail");
            },
            // Process Management (TCK-00342)
            Self::ListProcesses(resp) => {
                buf.push(PrivilegedMessageType::ListProcesses.tag());
                resp.encode(&mut buf).expect("encode cannot fail");
            },
            Self::ProcessStatus(resp) => {
                buf.push(PrivilegedMessageType::ProcessStatus.tag());
                resp.encode(&mut buf).expect("encode cannot fail");
            },
            Self::StartProcess(resp) => {
                buf.push(PrivilegedMessageType::StartProcess.tag());
                resp.encode(&mut buf).expect("encode cannot fail");
            },
            Self::StopProcess(resp) => {
                buf.push(PrivilegedMessageType::StopProcess.tag());
                resp.encode(&mut buf).expect("encode cannot fail");
            },
            Self::RestartProcess(resp) => {
                buf.push(PrivilegedMessageType::RestartProcess.tag());
                resp.encode(&mut buf).expect("encode cannot fail");
            },
            Self::ReloadProcess(resp) => {
                buf.push(PrivilegedMessageType::ReloadProcess.tag());
                resp.encode(&mut buf).expect("encode cannot fail");
            },
            Self::WorkStatus(resp) => {
                buf.push(PrivilegedMessageType::WorkStatus.tag());
                resp.encode(&mut buf).expect("encode cannot fail");
            },
            Self::WorkList(resp) => {
                buf.push(PrivilegedMessageType::WorkList.tag());
                resp.encode(&mut buf).expect("encode cannot fail");
            },
            Self::AuditorLaunchProjection(resp) => {
                buf.push(PrivilegedMessageType::AuditorLaunchProjection.tag());
                resp.encode(&mut buf).expect("encode cannot fail");
            },
            Self::OrchestratorLaunchProjection(resp) => {
                buf.push(PrivilegedMessageType::OrchestratorLaunchProjection.tag());
                resp.encode(&mut buf).expect("encode cannot fail");
            },
            Self::EndSession(resp) => {
                buf.push(PrivilegedMessageType::EndSession.tag());
                resp.encode(&mut buf).expect("encode cannot fail");
            },
            // TCK-00389: IngestReviewReceipt
            Self::IngestReviewReceipt(resp) => {
                buf.push(PrivilegedMessageType::IngestReviewReceipt.tag());
                resp.encode(&mut buf).expect("encode cannot fail");
            },
            Self::UpdateStopFlags(resp) => {
                buf.push(PrivilegedMessageType::UpdateStopFlags.tag());
                resp.encode(&mut buf).expect("encode cannot fail");
            },
            // Credential Management (CTR-PROTO-012, TCK-00343)
            Self::ListCredentials(resp) => {
                buf.push(PrivilegedMessageType::ListCredentials.tag());
                resp.encode(&mut buf).expect("encode cannot fail");
            },
            Self::AddCredential(resp) => {
                buf.push(PrivilegedMessageType::AddCredential.tag());
                resp.encode(&mut buf).expect("encode cannot fail");
            },
            Self::RemoveCredential(resp) => {
                buf.push(PrivilegedMessageType::RemoveCredential.tag());
                resp.encode(&mut buf).expect("encode cannot fail");
            },
            Self::RefreshCredential(resp) => {
                buf.push(PrivilegedMessageType::RefreshCredential.tag());
                resp.encode(&mut buf).expect("encode cannot fail");
            },
            Self::SwitchCredential(resp) => {
                buf.push(PrivilegedMessageType::SwitchCredential.tag());
                resp.encode(&mut buf).expect("encode cannot fail");
            },
            Self::LoginCredential(resp) => {
                buf.push(PrivilegedMessageType::LoginCredential.tag());
                resp.encode(&mut buf).expect("encode cannot fail");
            },
            Self::SubscribePulse(resp) => {
                buf.push(SUBSCRIBE_PULSE_RESPONSE_TAG);
                resp.encode(&mut buf).expect("encode cannot fail");
            },
            Self::UnsubscribePulse(resp) => {
                buf.push(UNSUBSCRIBE_PULSE_RESPONSE_TAG);
                resp.encode(&mut buf).expect("encode cannot fail");
            },
            Self::ConsensusStatus(resp) => {
                buf.push(PrivilegedMessageType::ConsensusStatus.tag());
                resp.encode(&mut buf).expect("encode cannot fail");
            },
            Self::ConsensusValidators(resp) => {
                buf.push(PrivilegedMessageType::ConsensusValidators.tag());
                resp.encode(&mut buf).expect("encode cannot fail");
            },
            Self::ConsensusByzantineEvidence(resp) => {
                buf.push(PrivilegedMessageType::ConsensusByzantineEvidence.tag());
                resp.encode(&mut buf).expect("encode cannot fail");
            },
            Self::ConsensusMetrics(resp) => {
                buf.push(PrivilegedMessageType::ConsensusMetrics.tag());
                resp.encode(&mut buf).expect("encode cannot fail");
            },
            Self::PublishChangeSet(resp) => {
                buf.push(PrivilegedMessageType::PublishChangeSet.tag());
                resp.encode(&mut buf).expect("encode cannot fail");
            },
            Self::DelegateSublease(resp) => {
                buf.push(PrivilegedMessageType::DelegateSublease.tag());
                resp.encode(&mut buf).expect("encode cannot fail");
            },
            Self::VerifyLedgerChain(resp) => {
                buf.push(PrivilegedMessageType::VerifyLedgerChain.tag());
                resp.encode(&mut buf).expect("encode cannot fail");
            },
            Self::RegisterRecoveryEvidence(resp) => {
                buf.push(PrivilegedMessageType::RegisterRecoveryEvidence.tag());
                resp.encode(&mut buf).expect("encode cannot fail");
            },
            Self::RequestUnfreeze(resp) => {
                buf.push(PrivilegedMessageType::RequestUnfreeze.tag());
                resp.encode(&mut buf).expect("encode cannot fail");
            },
            Self::Error(err) => {
                buf.push(0); // Error tag
                err.encode(&mut buf).expect("encode cannot fail");
            },
        }
        Bytes::from(buf)
    }
}

// ============================================================================
// Dispatcher
// ============================================================================

// ============================================================================
// TCK-00257: Gate Lease Validation
// ============================================================================

/// Maximum number of lease entries to store (CTR-1303: bounded capacity).
const MAX_LEASE_ENTRIES: usize = 10_000;

/// Error returned when lease validation fails.
#[derive(Debug, Clone, PartialEq, Eq)]
pub enum LeaseValidationError {
    /// Lease ID was not found in the ledger.
    LeaseNotFound {
        /// The lease ID that was not found.
        lease_id: String,
    },
    /// Lease exists but `work_id` does not match.
    ///
    /// # Security Note (SEC-HYG-001)
    ///
    /// The expected `work_id` is intentionally omitted from this error to
    /// prevent information leakage. Revealing the expected value could
    /// allow an attacker to enumerate valid work IDs.
    WorkIdMismatch {
        /// The actual `work_id` from the request (not the expected one).
        actual: String,
    },
    /// Lease has expired.
    LeaseExpired {
        /// The expired lease ID.
        lease_id: String,
    },
    /// Failed to query the ledger.
    LedgerQueryFailed {
        /// The error message from the ledger.
        message: String,
    },
}

impl std::fmt::Display for LeaseValidationError {
    fn fmt(&self, f: &mut std::fmt::Formatter<'_>) -> std::fmt::Result {
        match self {
            Self::LeaseNotFound { lease_id } => {
                write!(f, "lease not found: {lease_id}")
            },
            Self::WorkIdMismatch { actual } => {
                write!(f, "work_id mismatch for provided value: {actual}")
            },
            Self::LeaseExpired { lease_id } => {
                write!(f, "lease expired: {lease_id}")
            },
            Self::LedgerQueryFailed { message } => {
                write!(f, "ledger query failed: {message}")
            },
        }
    }
}

impl std::error::Error for LeaseValidationError {}

/// Trait for validating gate leases.
///
/// Per RFC-0017 IPC-PRIV-002, `GATE_EXECUTOR` role requires a valid
/// `GateLeaseIssued` event to exist in the ledger for the specified
/// `lease_id` and `work_id`.
///
/// # Security Contract
///
/// - `GATE_EXECUTOR` spawn MUST be rejected if lease validation fails
/// - Lease validation verifies the lease exists and matches the `work_id`
/// - This is a fail-closed check: validation errors reject the spawn
///
/// # Implementers
///
/// - `StubLeaseValidator`: In-memory storage for testing
/// - `LedgerLeaseValidator`: Ledger-backed validation (future)
pub trait LeaseValidator: Send + Sync {
    /// Validates that a gate lease exists and matches the `work_id`.
    ///
    /// # Arguments
    ///
    /// * `lease_id` - The lease ID to validate
    /// * `work_id` - The work ID that must match the lease
    ///
    /// # Returns
    ///
    /// `Ok(())` if the lease is valid and matches the `work_id`.
    ///
    /// # Errors
    ///
    /// Returns `LeaseValidationError` if:
    /// - The lease does not exist (`LeaseNotFound`)
    /// - The lease exists but the `work_id` doesn't match (`WorkIdMismatch`)
    /// - The lease has expired (`LeaseExpired`)
    /// - The ledger query failed (`LedgerQueryFailed`)
    fn validate_gate_lease(
        &self,
        lease_id: &str,
        work_id: &str,
    ) -> Result<(), LeaseValidationError>;

    /// Returns the `executor_actor_id` bound to a lease (TCK-00389).
    ///
    /// This is used by `IngestReviewReceipt` to validate that the reviewer
    /// identity matches the executor authorized by the gate lease.
    ///
    /// # Returns
    ///
    /// `Some(actor_id)` if the lease exists, `None` otherwise.
    fn get_lease_executor_actor_id(&self, lease_id: &str) -> Option<String>;

    /// Registers a gate lease for testing purposes.
    ///
    /// In production, leases are issued through a separate governance flow.
    /// This method exists to support test fixtures.
    fn register_lease(&self, lease_id: &str, work_id: &str, gate_id: &str);

    /// Registers a gate lease with an executor actor ID (TCK-00389).
    ///
    /// This method exists to support test fixtures that need to verify
    /// reviewer identity against the lease's `executor_actor_id`.
    fn register_lease_with_executor(
        &self,
        lease_id: &str,
        work_id: &str,
        gate_id: &str,
        executor_actor_id: &str,
    ) {
        // Default: delegate to register_lease (backward compat)
        let _ = executor_actor_id;
        self.register_lease(lease_id, work_id, gate_id);
    }

    /// Returns the `work_id` bound to a lease (TCK-00340).
    ///
    /// Used by `handle_ingest_review_receipt` to resolve the work claim
    /// and its associated risk tier for attestation ratcheting.
    ///
    /// # Returns
    ///
    /// `Some(work_id)` if the lease exists, `None` otherwise.
    fn get_lease_work_id(&self, lease_id: &str) -> Option<String> {
        let _ = lease_id;
        None
    }

    /// Returns the full `GateLease` for a given lease ID (TCK-00340).
    ///
    /// Used by `DelegateSublease` to retrieve the parent lease for
    /// strict-subset validation and sublease issuance.
    ///
    /// # Trust Model (v7 Finding 1)
    ///
    /// The returned lease is trusted because it comes from the daemon-owned
    /// `SQLite` ledger (same-process trust boundary). The lease's embedded
    /// `issuer_signature` field preserves the original Ed25519 signature
    /// for downstream verification, but callers within the daemon process
    /// do NOT need to call `GateLease::validate_signature()` -- the ledger
    /// integrity is guaranteed by the process trust boundary.
    ///
    /// See `SqliteLeaseValidator::register_full_lease()` for the full trust
    /// model documentation.
    ///
    /// # Returns
    ///
    /// `Some(GateLease)` if the lease exists, `None` otherwise.
    fn get_gate_lease(&self, lease_id: &str) -> Option<apm2_core::fac::GateLease> {
        let _ = lease_id;
        None
    }

    /// Registers a full `GateLease` for sublease delegation (TCK-00340).
    ///
    /// This method exists to support test fixtures and production paths
    /// that need full lease objects for sublease delegation.
    ///
    /// # Errors
    ///
    /// Returns an error string if the lease could not be persisted
    /// (serialization failure, database write failure, etc.). Callers
    /// MUST fail closed when this returns `Err` to preserve the
    /// single-effect guarantee (LAW-11) -- a sublease event emitted
    /// without a corresponding persisted lease anchor would leave the
    /// system in an inconsistent state.
    fn register_full_lease(&self, lease: &apm2_core::fac::GateLease) -> Result<(), String> {
        let _ = lease;
        Ok(())
    }

    /// Registers a delegated `GateLease` with authoritative parent lineage.
    ///
    /// Implementations should persist `parent_lease_id` so delegation-depth
    /// checks can fail closed if the corresponding `SubleaseIssued` evidence is
    /// missing.
    fn register_delegated_full_lease(
        &self,
        lease: &apm2_core::fac::GateLease,
        parent_lease_id: &str,
    ) -> Result<(), String> {
        let _ = parent_lease_id;
        self.register_full_lease(lease)
    }

    /// Returns the persisted delegation parent lease ID for `lease_id`.
    ///
    /// `Ok(None)` indicates root issuance or no persisted lineage metadata.
    /// Any storage/lookup/parse failures MUST return `Err` (fail-closed).
    fn get_delegation_parent_lease_id(&self, lease_id: &str) -> Result<Option<String>, String> {
        let _ = lease_id;
        Ok(None)
    }
}

/// Entry for a registered lease.
#[derive(Debug, Clone)]
#[allow(clippy::struct_field_names)]
struct LeaseEntry {
    work_id: String,
    #[allow(dead_code)]
    gate_id: String,
    /// The executor actor ID authorized by this lease (TCK-00389).
    executor_actor_id: String,
}

#[derive(Debug, Clone)]
struct FullLeaseEntry {
    lease: apm2_core::fac::GateLease,
    delegated_parent_lease_id: Option<String>,
}

/// Stub implementation of [`LeaseValidator`] for testing.
///
/// Stores leases in memory with no persistence.
///
/// # Capacity Limits (CTR-1303)
///
/// This validator enforces a maximum of 10,000 entries to prevent memory
/// exhaustion. When the limit is reached, the oldest entry (by insertion order)
/// is evicted to make room for the new lease.
///
/// # Security Notes
///
/// - **SEC-DoS-001**: Duplicate lease IDs are handled by updating in place
///   rather than adding a new entry, preventing unbounded memory growth.
/// - **SEC-DoS-002**: Uses `VecDeque` for O(1) eviction from the front,
///   consistent with the O(1) eviction pattern established in PR 329.
#[derive(Debug, Default)]
pub struct StubLeaseValidator {
    /// Leases stored with insertion order for O(1) LRU eviction.
    ///
    /// Uses `VecDeque` (SEC-DoS-002) for efficient front removal.
    leases: std::sync::RwLock<(
        std::collections::VecDeque<String>,
        std::collections::HashMap<String, LeaseEntry>,
    )>,
    /// Full gate leases for sublease delegation (TCK-00340).
    full_leases: std::sync::RwLock<std::collections::HashMap<String, FullLeaseEntry>>,
}

impl StubLeaseValidator {
    /// Creates a new empty lease validator.
    #[must_use]
    pub fn new() -> Self {
        Self {
            leases: std::sync::RwLock::new((
                std::collections::VecDeque::new(),
                std::collections::HashMap::new(),
            )),
            full_leases: std::sync::RwLock::new(std::collections::HashMap::new()),
        }
    }
}

impl LeaseValidator for StubLeaseValidator {
    fn validate_gate_lease(
        &self,
        lease_id: &str,
        work_id: &str,
    ) -> Result<(), LeaseValidationError> {
        let guard = self.leases.read().expect("lock poisoned");
        let (_, leases) = &*guard;

        leases.get(lease_id).map_or_else(
            || {
                Err(LeaseValidationError::LeaseNotFound {
                    lease_id: lease_id.to_string(),
                })
            },
            |entry| {
                let work_id_matches = entry.work_id.len() == work_id.len()
                    && bool::from(entry.work_id.as_bytes().ct_eq(work_id.as_bytes()));
                if work_id_matches {
                    Ok(())
                } else {
                    Err(LeaseValidationError::WorkIdMismatch {
                        actual: work_id.to_string(),
                    })
                }
            },
        )
    }

    fn get_lease_executor_actor_id(&self, lease_id: &str) -> Option<String> {
        let guard = self.leases.read().expect("lock poisoned");
        let (_, leases) = &*guard;
        leases
            .get(lease_id)
            .map(|entry| entry.executor_actor_id.clone())
    }

    fn get_lease_work_id(&self, lease_id: &str) -> Option<String> {
        let guard = self.leases.read().expect("lock poisoned");
        let (_, leases) = &*guard;
        leases.get(lease_id).map(|entry| entry.work_id.clone())
    }

    fn register_lease(&self, lease_id: &str, work_id: &str, gate_id: &str) {
        // Delegate to register_lease_with_executor with empty executor ID
        // for backward compatibility with existing tests.
        self.register_lease_with_executor(lease_id, work_id, gate_id, "");
    }

    fn register_lease_with_executor(
        &self,
        lease_id: &str,
        work_id: &str,
        gate_id: &str,
        executor_actor_id: &str,
    ) {
        let mut guard = self.leases.write().expect("lock poisoned");
        let (order, leases) = &mut *guard;

        // SEC-DoS-001: Check for duplicate lease_id and update in place
        if leases.contains_key(lease_id) {
            // Update existing entry without adding to order (already tracked)
            leases.insert(
                lease_id.to_string(),
                LeaseEntry {
                    work_id: work_id.to_string(),
                    gate_id: gate_id.to_string(),
                    executor_actor_id: executor_actor_id.to_string(),
                },
            );
            return;
        }

        // CTR-1303: Evict oldest entry if at capacity
        // SEC-DoS-002: Use pop_front() for O(1) eviction
        while leases.len() >= MAX_LEASE_ENTRIES {
            if let Some(oldest_key) = order.pop_front() {
                leases.remove(&oldest_key);
            } else {
                break;
            }
        }

        order.push_back(lease_id.to_string());
        leases.insert(
            lease_id.to_string(),
            LeaseEntry {
                work_id: work_id.to_string(),
                gate_id: gate_id.to_string(),
                executor_actor_id: executor_actor_id.to_string(),
            },
        );
    }

    fn get_gate_lease(&self, lease_id: &str) -> Option<apm2_core::fac::GateLease> {
        let guard = self.full_leases.read().expect("lock poisoned");
        guard.get(lease_id).map(|entry| entry.lease.clone())
    }

    fn register_full_lease(&self, lease: &apm2_core::fac::GateLease) -> Result<(), String> {
        let mut guard = self.full_leases.write().expect("lock poisoned");
        // SECURITY (v10 BLOCKER 2): Enforce uniqueness at the persistence
        // layer. If a lease with this ID already exists, return an error
        // so the caller can handle it (idempotent or conflict). This makes
        // the store the source of truth for uniqueness, preventing TOCTOU
        // races where two concurrent requests both pass the in-memory
        // pre-check and create conflicting leases.
        if guard.contains_key(&lease.lease_id) {
            return Err(format!("duplicate lease_id: {}", lease.lease_id));
        }
        guard.insert(
            lease.lease_id.clone(),
            FullLeaseEntry {
                lease: lease.clone(),
                delegated_parent_lease_id: None,
            },
        );
        Ok(())
    }

    fn register_delegated_full_lease(
        &self,
        lease: &apm2_core::fac::GateLease,
        parent_lease_id: &str,
    ) -> Result<(), String> {
        let mut guard = self.full_leases.write().expect("lock poisoned");
        if guard.contains_key(&lease.lease_id) {
            return Err(format!("duplicate lease_id: {}", lease.lease_id));
        }
        guard.insert(
            lease.lease_id.clone(),
            FullLeaseEntry {
                lease: lease.clone(),
                delegated_parent_lease_id: Some(parent_lease_id.to_string()),
            },
        );
        Ok(())
    }

    fn get_delegation_parent_lease_id(&self, lease_id: &str) -> Result<Option<String>, String> {
        let guard = self.full_leases.read().expect("lock poisoned");
        Ok(guard
            .get(lease_id)
            .and_then(|entry| entry.delegated_parent_lease_id.clone()))
    }
}

/// Privileged endpoint dispatcher.
///
/// Routes incoming messages to the appropriate handler based on message type.
/// Enforces privilege separation by checking
/// `ConnectionContext::is_privileged()` before dispatching to any handler.
///
/// # Security Contract
///
/// Per INV-0001 and TB-002:
/// - Session connections receive `PERMISSION_DENIED` for ALL privileged
///   requests
/// - No privileged handler logic executes for non-privileged connections
/// - Generic error messages prevent endpoint enumeration (TH-004)
///
/// # TCK-00253 Additions
///
/// - Policy resolver for governance delegation
/// - Work registry for claim persistence
/// - Ledger event emitter for signed event persistence
/// - Actor ID derivation from credentials
///
/// # TCK-00256 Additions
///
/// - Episode runtime for lifecycle management
/// - Session registry for session state persistence
///
/// # TCK-00257 Additions
///
/// - Lease validator for `GATE_EXECUTOR` spawn validation
///
/// # TCK-00289 Additions
///
/// - `HolonicClock` for HTF-compliant timestamps in `IssueCapability`
pub struct PrivilegedDispatcher {
    /// Decode configuration for bounded message decoding.
    decode_config: DecodeConfig,

    /// Policy resolver for governance delegation (TCK-00253).
    policy_resolver: Arc<dyn PolicyResolver>,

    /// Work registry for claim persistence (TCK-00253).
    work_registry: Arc<dyn WorkRegistry>,

    /// Ledger event emitter for signed event persistence (TCK-00253).
    event_emitter: Arc<dyn LedgerEventEmitter>,

    /// Shared projection-backed work authority (TCK-00415).
    ///
    /// Instantiated once and reused across requests so that the internal
    /// event-count cache is effective. Without sharing, each request would
    /// create a fresh `ProjectionWorkAuthority` with an empty cache,
    /// forcing a full O(N) ledger replay every time.
    work_authority: Arc<ProjectionWorkAuthority>,

    /// Episode runtime for lifecycle management (TCK-00256).
    episode_runtime: Arc<EpisodeRuntime>,

    /// Session registry for session state persistence (TCK-00256).
    session_registry: Arc<dyn SessionRegistry>,

    /// Lease validator for `GATE_EXECUTOR` spawn validation (TCK-00257).
    lease_validator: Arc<dyn LeaseValidator>,

    /// Token minter for session token generation (TCK-00287).
    ///
    /// Shared with `SessionDispatcher` to ensure tokens minted during
    /// `SpawnEpisode` can be validated on session endpoints.
    token_minter: Arc<TokenMinter>,

    /// Manifest store for capability manifest registration (TCK-00287).
    ///
    /// Shared with `SessionDispatcher` so that manifests registered during
    /// `SpawnEpisode` are accessible for tool request validation.
    manifest_store: Arc<InMemoryManifestStore>,

    /// CAS-backed manifest loader for capability manifest retrieval
    /// (TCK-00317).
    ///
    /// Per DOD item 1 (CAS Storage & Hash Loading), manifests are stored in
    /// CAS and loaded by hash. The policy resolver returns the manifest hash,
    /// and `handle_spawn_episode` uses this loader to retrieve the manifest.
    ///
    /// # Security Model
    ///
    /// - Manifests are stored with their BLAKE3 hash as the key
    /// - Load operations verify the hash matches the content
    /// - Missing manifests result in fail-closed rejection
    manifest_loader: Arc<dyn ManifestLoader>,

    /// Prometheus metrics registry for daemon health observability (TCK-00268).
    ///
    /// When present, the dispatcher emits metrics for:
    /// - `session_spawned`: When `SpawnEpisode` succeeds
    /// - `ipc_request_completed`: For each dispatched request
    /// - `capability_granted`: When `IssueCapability` succeeds
    ///
    /// # Integration Status
    ///
    /// **NOTE**: This dispatcher uses the binary protocol
    /// (`PrivilegedMessageType`) which is not currently wired into
    /// `main.rs`. The daemon's main connection handler uses JSON-based
    /// `IpcRequest` via `handlers::dispatch()` instead.
    ///
    /// These metrics will become active when the binary protocol path is
    /// integrated into the daemon's connection handling. Until then, the
    /// JSON-based IPC path in `handlers.rs` correctly records
    /// `ipc_request_completed` metrics.
    ///
    /// TODO(TCK-FUTURE): Wire `PrivilegedDispatcher` into `main.rs` to enable
    /// these metrics for binary protocol requests.
    metrics: Option<SharedMetricsRegistry>,

    /// HTF-compliant clock for timestamps (TCK-00289).
    ///
    /// Used to generate RFC-0016 compliant timestamps for:
    /// - `IssueCapability` `granted_at` / `expires_at` fields
    /// - `WorkClaimed` ledger event timestamps
    ///
    /// # HTF Compliance
    ///
    /// The clock provides:
    /// - Monotonic ticks: Never regress within a process lifetime
    /// - HLC stamps: Hybrid logical clock for cross-node causality
    /// - Wall time bounds: Observational only, with uncertainty interval
    holonic_clock: Arc<HolonicClock>,

    /// Subscription registry for HEF Pulse Plane resource governance
    /// (TCK-00303).
    ///
    /// Tracks per-connection subscription state and enforces limits per
    /// RFC-0018. Shared with `SessionDispatcher` to manage subscriptions
    /// across both operator and session sockets.
    subscription_registry: SharedSubscriptionRegistry,

    /// Shared daemon state for process management (TCK-00342).
    ///
    /// When present, process management handlers (`ListProcesses`,
    /// `ProcessStatus`, `StartProcess`, `StopProcess`, `RestartProcess`,
    /// `ReloadProcess`) query the `Supervisor` within `DaemonState` for
    /// process information. When `None`, handlers return stub responses
    /// (for testing without full daemon context).
    daemon_state: Option<SharedState>,

    /// Node ID for consensus status reporting (TCK-00345).
    ///
    /// Used in consensus query responses to identify this node.
    node_id: String,

    /// Consensus subsystem state handle (TCK-00345).
    ///
    /// When `Some`, consensus queries return real state data.
    /// When `None`, consensus queries return `CONSENSUS_NOT_CONFIGURED` error.
    ///
    /// # Future Work
    ///
    /// This will be wired to actual consensus state (`HotStuffState`,
    /// `ConsensusMetrics`, etc.) when the daemon consensus integration is
    /// complete. For now, presence/absence controls whether the subsystem
    /// is considered "configured".
    consensus_state: Option<()>,

    /// Credential store for secure credential persistence (TCK-00343).
    ///
    /// When present, credential management handlers (`ListCredentials`,
    /// `AddCredential`, `RemoveCredential`, `RefreshCredential`,
    /// `SwitchCredential`, `LoginCredential`) persist credentials to the
    /// `CredentialStore` backed by the OS keyring. When `None`, handlers
    /// return error responses indicating the credential store is not
    /// configured.
    credential_store: Option<Arc<CredentialStore>>,

    /// Session telemetry store for tracking tool calls, events emitted,
    /// and session start time (TCK-00384).
    ///
    /// When present, `SpawnEpisode` registers telemetry for new sessions
    /// with `started_at_ns` set to the current wall time. The store is
    /// shared with `SessionDispatcher` for counter updates and queries.
    telemetry_store: Option<Arc<crate::session::SessionTelemetryStore>>,

    /// Per-session tool broker registry (TCK-00401).
    ///
    /// When configured, `SpawnEpisode` initializes a broker scoped to the
    /// session capability manifest and registers it here. `RequestTool`
    /// resolves brokers by session ID.
    session_broker_registry:
        Option<SharedSessionBrokerRegistry<crate::episode::capability::StubManifestLoader>>,

    /// Configuration template for per-session broker construction.
    broker_config: ToolBrokerConfig,

    /// CAS backend for broker context artifact retrieval and firewall init.
    broker_cas: Option<Arc<dyn crate::episode::executor::ContentAddressedStore>>,

    /// Optional credential stores wired into per-session brokers.
    broker_github_store: Option<Arc<dyn GitHubCredentialStore>>,
    broker_ssh_store: Option<Arc<dyn SshCredentialStore>>,

    /// Content-addressed store for `ChangeSet` bundle persistence (TCK-00394).
    ///
    /// When present, `PublishChangeSet` stores the canonical bundle bytes in
    /// CAS and returns the content hash for ledger event binding. When `None`,
    /// the handler returns an error indicating CAS is not configured.
    cas: Option<Arc<dyn ContentAddressedStore>>,

    /// Shared V1 manifest store for TCK-00352 scope enforcement.
    ///
    /// Per Security Review MAJOR 2, `handle_spawn_episode` mints a V1
    /// capability manifest and registers it in this store. The
    /// `SessionDispatcher` holds a clone of the same `Arc` and enforces
    /// V1 scope checks in `handle_request_tool`.
    v1_manifest_store: Option<crate::protocol::session_dispatch::SharedV1ManifestStore>,

    /// Gate orchestrator for sublease delegation (TCK-00340).
    ///
    /// When present, `DelegateSublease` delegates to the orchestrator to
    /// issue signed subleases with strict-subset validation. When `None`,
    /// the handler returns an error indicating the orchestrator is not
    /// configured.
    gate_orchestrator: Option<Arc<crate::gate::GateOrchestrator>>,

    /// PCAC lifecycle gate for privileged authority-bearing handlers
    /// (TCK-00424).
    ///
    /// When present, `DelegateSublease` and `IngestReviewReceipt` can enforce
    /// join/revalidate/consume before authoritative state mutation.
    pcac_lifecycle_gate: Option<Arc<crate::pcac::LifecycleGate>>,

    /// Rollout policy for privileged PCAC lifecycle enforcement.
    privileged_pcac_policy: PrivilegedPcacPolicy,

    /// Per-session stop conditions store for pre-actuation gate enforcement
    /// (TCK-00351 v4).
    ///
    /// When present, `SpawnEpisode` registers stop conditions (from the
    /// episode envelope) for the new session. The `SessionDispatcher` holds
    /// a clone of the same `Arc` and reads conditions in the pre-actuation
    /// gate to enforce `max_episodes` / `escalation_predicate`.
    stop_conditions_store: Option<Arc<crate::session::SessionStopConditionsStore>>,

    /// Shared stop authority used by privileged control-plane handlers to
    /// mutate emergency/governance stop flags at runtime.
    stop_authority: Option<Arc<crate::episode::preactuation::StopAuthority>>,

    /// Governance freshness monitor wired from production `DispatcherState`.
    ///
    /// Successful governance-backed operations call `record_success()`.
    /// Only governance transport/communication failures call
    /// `record_failure()`.
    governance_freshness_monitor: Option<Arc<GovernanceFreshnessMonitor>>,

    /// TCK-00399: Adapter registry for spawning agent CLI processes.
    ///
    /// When present, `handle_spawn_episode` loads the adapter profile from
    /// CAS, builds a `HarnessConfig`, and spawns the agent process via the
    /// appropriate `HarnessAdapter`.
    adapter_registry: Option<Arc<crate::episode::AdapterRegistry>>,

    /// TCK-00400: Deterministic adapter selection policy state.
    ///
    /// Stored behind a daemon-only mutex so spawn failures can update
    /// backoff/failure counters without exposing mutable selection state
    /// through IPC surfaces.
    adapter_selection_policy: Option<Arc<Mutex<AdapterSelectionPolicy>>>,

    /// TCK-00400: Profile hashes currently eligible by runtime adapter support.
    adapter_available_profiles: BTreeSet<[u8; 32]>,

    /// TCK-00400: Reverse lookup for profile hash -> profile ID.
    adapter_profile_ids_by_hash: HashMap<[u8; 32], String>,

    /// TCK-00400: Dedicated CAS for adapter profile hash verification at
    /// selection time. Separate from the publish/ingest `cas` field so that
    /// `with_persistence` (no durable CAS) can still perform adapter selection
    /// without inadvertently enabling `PublishChangeSet` (TCK-00412
    /// fail-closed).
    adapter_profile_cas: Option<Arc<dyn ContentAddressedStore>>,

    /// TCK-00373: Permeability receipt resolver for delegated claim binding.
    ///
    /// During `ClaimWork`, the dispatcher queries this resolver to determine
    /// whether the actor operates under a delegated authority. When the
    /// resolver returns a receipt, it is stored on the `WorkClaim` and
    /// subsequently enforced by the delegated spawn gate.
    permeability_receipt_resolver: Arc<dyn PermeabilityReceiptResolver>,

    /// TCK-00420: Alias reconciliation gate for promotion gating.
    ///
    /// During `SpawnEpisode`, the dispatcher invokes this gate to verify that
    /// alias/`work_id` projections are consistent before proceeding.
    /// Infrastructure errors and unresolved defects both result in fail-closed
    /// rejection (CTR-2617, INV-ALIAS-002).
    ///
    /// # Current Limitation
    ///
    /// The `ClaimWork` handler registers identity-mapped aliases
    /// (`work_id -> work_id`) because the wire protocol does not carry a
    /// separate `ticket_alias` field. Real `TCK-*` style aliases must be
    /// registered by an operator layer or future policy resolver extension.
    ///
    /// TODO(TCK-00425): Wire real ticket aliases through policy resolution
    /// so `ClaimWork` can register true alias->work_id mappings.
    alias_reconciliation_gate: Arc<dyn AliasReconciliationGate>,

    /// TCK-00469: Divergence watchdog for projection compromise recovery.
    ///
    /// When present, `RegisterRecoveryEvidence` and `RequestUnfreeze`
    /// handlers call through to the watchdog to register durable recovery
    /// evidence and lift projection freezes. When `None`, both handlers
    /// return an error indicating the watchdog is not configured.
    divergence_watchdog:
        Option<Arc<crate::projection::DivergenceWatchdog<crate::projection::SystemTimeSource>>>,

    /// INV-BRK-HEALTH-GATE-001: Fail-closed health gate for token issuance.
    ///
    /// Starts `false` (fail-closed). Set to `true` during daemon startup
    /// after initial health validation succeeds. Must be checked before
    /// issuing channel context tokens via the dispatcher path.
    ///
    /// # Synchronization Protocol (RS-21)
    ///
    /// - **Protected data**: Whether the daemon health gate has been opened.
    /// - **Writers**: `set_admission_health_gate()` called from daemon startup
    ///   (`main.rs`) after successful initialization.
    /// - **Readers**:
    ///   `validate_channel_boundary_and_issue_context_token_with_flow()` on
    ///   every token issuance attempt.
    /// - **Ordering**: `Release` on store, `Acquire` on load — ensures all
    ///   initialization side-effects are visible before token issuance
    ///   proceeds.
    /// - **Happens-before**: store(true, Release) in main.rs startup
    ///   happens-before load(Acquire) in token issuance paths.
    admission_health_gate: AtomicBool,
}

impl Default for PrivilegedDispatcher {
    /// Creates a default dispatcher (TEST ONLY).
    ///
    /// # Warning: RSK-2503 Mixed Clock Domain Hazard
    ///
    /// See `new()` for details on clock domain hazards.
    fn default() -> Self {
        Self::new()
    }
}

/// Default session token TTL (5 minutes).
///
/// Per WVR-0002, env-only bootstrap tokens MUST have a TTL <= 300 seconds.
/// This was previously 3600s (1 hour) which violated the waiver constraint.
pub const DEFAULT_SESSION_TOKEN_TTL_SECS: u64 = 300;

/// Stop-condition policy floor for untrusted `SpawnEpisode` inputs.
///
/// This enforces authoritative minimum constraints before requester-supplied
/// stop conditions are persisted.
#[derive(Debug, Clone, Copy)]
struct StopConditionPolicy {
    /// Maximum allowed value for `max_episodes`.
    ///
    /// Values above this policy floor are rejected. A value of `0` means no
    /// floor is configured for this dimension.
    max_episodes_floor: u64,
}

impl StopConditionPolicy {
    /// Fail-closed policy floor used when no governance-provided floor is
    /// available.
    const fn fail_closed_default() -> Self {
        Self {
            // Transitional policy ceiling: requester cannot bypass with
            // unlimited `max_episodes=0` and cannot exceed this floor.
            max_episodes_floor: 10,
        }
    }

    /// Validates request stop conditions against the policy floor and returns
    /// canonicalized conditions to persist.
    ///
    /// # Errors
    ///
    /// Returns `Err` when request values violate the floor.
    fn validate_against_floor(
        self,
        request_max_episodes: Option<u64>,
        request_escalation_predicate: Option<&str>,
    ) -> Result<crate::episode::envelope::StopConditions, String> {
        // Fail-closed request default: missing max_episodes is constrained.
        let resolved_max_episodes = request_max_episodes.unwrap_or(1);

        if self.max_episodes_floor > 0 {
            if resolved_max_episodes == 0 {
                return Err(format!(
                    "max_episodes=0 is not allowed by policy floor; expected 1..={}",
                    self.max_episodes_floor
                ));
            }
            if resolved_max_episodes > self.max_episodes_floor {
                return Err(format!(
                    "max_episodes={} exceeds policy floor max={}",
                    resolved_max_episodes, self.max_episodes_floor
                ));
            }
        }

        Ok(crate::episode::envelope::StopConditions {
            max_episodes: resolved_max_episodes,
            escalation_predicate: request_escalation_predicate.unwrap_or("").to_string(),
            goal_predicate: String::new(),
            failure_predicate: String::new(),
        })
    }
}

/// Privileged PCAC lifecycle policy surface.
///
/// `DelegateSublease` and `IngestReviewReceipt` are both unconditionally
/// PCAC-gated (mandatory cutover).
#[derive(Debug, Clone, Copy, PartialEq, Eq, Default)]
pub struct PrivilegedPcacPolicy {}

/// Handler class for domain-tagged PCAC hash construction.
///
/// Each variant maps to a unique domain separation tag prefix, preventing
/// cross-handler confused-deputy attacks.
#[derive(Debug, Clone, Copy, PartialEq, Eq)]
pub(crate) enum PrivilegedHandlerClass {
    DelegateSublease,
    IngestReviewReceipt,
    RegisterRecoveryEvidence,
    RequestUnfreeze,
}

#[allow(clippy::missing_const_for_fn)]
impl PrivilegedHandlerClass {
    fn tag_prefix(self) -> &'static str {
        match self {
            Self::DelegateSublease => "pcac-privileged-delegate-sublease",
            Self::IngestReviewReceipt => "pcac-privileged-ingest-review",
            Self::RegisterRecoveryEvidence => "pcac-privileged-register-recovery-evidence",
            Self::RequestUnfreeze => "pcac-privileged-request-unfreeze",
        }
    }

    fn operation_name(self) -> &'static str {
        match self {
            Self::DelegateSublease => "DelegateSublease",
            Self::IngestReviewReceipt => "IngestReviewReceipt",
            Self::RegisterRecoveryEvidence => "RegisterRecoveryEvidence",
            Self::RequestUnfreeze => "RequestUnfreeze",
        }
    }
}

/// Construct a domain-tagged BLAKE3 hash for a privileged PCAC handler.
fn domain_tagged_hash(
    handler_class: PrivilegedHandlerClass,
    hash_type: &str,
    data: &[&[u8]],
) -> [u8; 32] {
    let mut hasher = blake3::Hasher::new();
    let tag = format!("{}-{}-v1", handler_class.tag_prefix(), hash_type);
    hasher.update(tag.as_bytes());
    for d in data {
        hasher.update(d);
    }
    *hasher.finalize().as_bytes()
}

/// Builder for PCAC `AuthorityJoinInputV1` for privileged handlers.
pub(crate) struct PrivilegedPcacInputBuilder {
    handler_class: PrivilegedHandlerClass,
    session_id: String,
    lease_id: String,
    boundary_intent_class: apm2_core::pcac::BoundaryIntentClass,
    identity_proof_hash: [u8; 32],
    identity_evidence_level: IdentityEvidenceLevel,
    risk_tier: PcacRiskTier,
    capability_manifest_hash: [u8; 32],
    scope_witness_hash: [u8; 32],
    freshness_policy_hash: [u8; 32],
    stop_budget_profile_digest: [u8; 32],
    effect_intent_digest: [u8; 32],
    lineage_receipt_hash: Option<[u8; 32]>,
}

#[allow(clippy::missing_const_for_fn)]
impl PrivilegedPcacInputBuilder {
    pub(crate) fn new(handler_class: PrivilegedHandlerClass) -> Self {
        Self {
            handler_class,
            session_id: String::new(),
            lease_id: String::new(),
            boundary_intent_class: apm2_core::pcac::BoundaryIntentClass::Assert,
            identity_proof_hash: [0u8; 32],
            identity_evidence_level: IdentityEvidenceLevel::PointerOnly,
            risk_tier: PcacRiskTier::Tier0,
            capability_manifest_hash: [0u8; 32],
            scope_witness_hash: [0u8; 32],
            freshness_policy_hash: [0u8; 32],
            stop_budget_profile_digest: [0u8; 32],
            effect_intent_digest: [0u8; 32],
            lineage_receipt_hash: None,
        }
    }

    pub(crate) fn session_id(mut self, v: String) -> Self {
        self.session_id = v;
        self
    }
    pub(crate) fn lease_id(mut self, v: String) -> Self {
        self.lease_id = v;
        self
    }
    pub(crate) fn boundary_intent_class(mut self, v: apm2_core::pcac::BoundaryIntentClass) -> Self {
        self.boundary_intent_class = v;
        self
    }
    pub(crate) fn identity_proof_hash(mut self, v: [u8; 32]) -> Self {
        self.identity_proof_hash = v;
        self
    }
    pub(crate) fn identity_evidence_level(mut self, v: IdentityEvidenceLevel) -> Self {
        self.identity_evidence_level = v;
        self
    }
    pub(crate) fn risk_tier(mut self, v: PcacRiskTier) -> Self {
        self.risk_tier = v;
        self
    }
    pub(crate) fn capability_manifest_hash(mut self, v: [u8; 32]) -> Self {
        self.capability_manifest_hash = v;
        self
    }
    pub(crate) fn scope_witness_hash(mut self, v: [u8; 32]) -> Self {
        self.scope_witness_hash = v;
        self
    }
    pub(crate) fn freshness_policy_hash(mut self, v: [u8; 32]) -> Self {
        self.freshness_policy_hash = v;
        self
    }
    pub(crate) fn stop_budget_profile_digest(mut self, v: [u8; 32]) -> Self {
        self.stop_budget_profile_digest = v;
        self
    }
    pub(crate) fn effect_intent_digest(mut self, v: [u8; 32]) -> Self {
        self.effect_intent_digest = v;
        self
    }
    pub(crate) fn lineage_receipt_hash(mut self, v: [u8; 32]) -> Self {
        self.lineage_receipt_hash = Some(v);
        self
    }

    /// Convenience: compute a domain-tagged BLAKE3 hash using this builder's
    /// handler class.
    pub(crate) fn hash(&self, hash_type: &str, data: &[&[u8]]) -> [u8; 32] {
        domain_tagged_hash(self.handler_class, hash_type, data)
    }

    /// Build the `AuthorityJoinInputV1`, consuming the builder.
    pub(crate) fn build(
        self,
        join_freshness_tick: u64,
        join_time_envelope_ref: [u8; 32],
        join_ledger_anchor: [u8; 32],
        join_revocation_head: [u8; 32],
    ) -> AuthorityJoinInputV1 {
        let join_tick_bytes = join_freshness_tick.to_le_bytes();
        let leakage_witness_hash = self.hash(
            "boundary_leakage_witness_hash",
            &[
                &self.effect_intent_digest,
                &self.scope_witness_hash,
                &join_tick_bytes,
            ],
        );
        let timing_witness_hash = self.hash(
            "boundary_timing_witness_hash",
            &[
                &join_time_envelope_ref,
                &join_ledger_anchor,
                &join_tick_bytes,
            ],
        );
        AuthorityJoinInputV1 {
            session_id: self.session_id,
            holon_id: None,
            intent_digest: self.effect_intent_digest,
            boundary_intent_class: self.boundary_intent_class,
            capability_manifest_hash: self.capability_manifest_hash,
            scope_witness_hashes: vec![self.scope_witness_hash],
            lease_id: self.lease_id,
            permeability_receipt_hash: self.lineage_receipt_hash,
            identity_proof_hash: self.identity_proof_hash,
            identity_evidence_level: self.identity_evidence_level,
            pointer_only_waiver_hash: None,
            directory_head_hash: join_revocation_head,
            freshness_policy_hash: self.freshness_policy_hash,
            freshness_witness_tick: join_freshness_tick,
            stop_budget_profile_digest: self.stop_budget_profile_digest,
            pre_actuation_receipt_hashes: Vec::new(),
            leakage_witness_hash,
            timing_witness_hash,
            risk_tier: self.risk_tier,
            determinism_class: PcacDeterminismClass::Deterministic,
            time_envelope_ref: join_time_envelope_ref,
            as_of_ledger_anchor: join_ledger_anchor,
        }
    }
}

type PrivilegedPcacRevalidationInputs = (u64, [u8; 32], [u8; 32], [u8; 32]);

/// Lifecycle selectors returned from privileged PCAC enforcement.
///
/// These bindings are persisted on authoritative effect events so replay and
/// audit paths can verify that the effect is anchored to a concrete
/// `join -> revalidate -> consume` chain.
#[derive(Debug, Clone, Copy, PartialEq, Eq)]
pub struct PrivilegedPcacLifecycleArtifacts {
    /// Single-use authority certificate ID consumed for the effect.
    pub ajc_id: [u8; 32],
    /// Effect intent digest consumed by PCAC.
    pub intent_digest: [u8; 32],
    /// Authoritative consume tick.
    pub consume_tick: u64,
    /// Consume-time HTF envelope witness.
    pub time_envelope_ref: [u8; 32],
    /// Consume selector digest from the durable consume record.
    pub consume_selector_digest: [u8; 32],
}

/// Runtime boundary-flow evidence used to evaluate RFC-0028 REQ-0004
/// predicate closure at channel admission.
#[derive(Debug, Clone, PartialEq, Eq, Serialize, Deserialize)]
#[serde(deny_unknown_fields)]
pub struct LeakageWitnessV1 {
    /// Daemon-side measurement method identifier.
    pub measurement_method: String,
    /// Daemon-observed leakage estimate in `leakage_bits`.
    pub measured_bits: u64,
    /// Confidence in basis points (`0..=10000`).
    pub confidence: u16,
    /// HTF timestamp (nanoseconds since Unix epoch).
    pub timestamp: u64,
}

/// Daemon-authoritative timing witness for boundary-flow admission.
#[derive(Debug, Clone, PartialEq, Eq, Serialize, Deserialize)]
#[serde(deny_unknown_fields)]
/// Daemon-authoritative timing-variance witness measured at boundary crossing.
pub struct TimingWitnessV1 {
    /// Daemon-side measurement method identifier.
    pub measurement_method: String,
    /// Daemon-observed timing variance in ticks.
    pub observed_variance_ticks: u64,
    /// Baseline timing expectation in ticks.
    pub baseline_ticks: u64,
    /// Confidence in basis points (`0..=10000`).
    pub confidence: u16,
    /// HTF timestamp (nanoseconds since Unix epoch).
    pub timestamp: u64,
}

const BOUNDARY_WITNESS_TIMING_TICK_QUANTUM_NS: u64 = 5_000_000;

#[inline]
const fn ceiling_div(value: u64, divisor: u64) -> u64 {
    if divisor == 0 {
        return value;
    }
    value.saturating_add(divisor.saturating_sub(1)) / divisor
}

/// Builds daemon-authoritative leakage/timing witnesses from runtime-observed
/// payload and elapsed-time signals.
#[must_use]
pub(crate) fn build_authoritative_witnesses(
    measurement_scope: &str,
    payload: &[u8],
    leakage_quantum_bytes: u64,
    baseline_ticks: u64,
    elapsed_ns: u64,
    timestamp_ns: u64,
) -> (LeakageWitnessV1, TimingWitnessV1) {
    let payload_len = u64::try_from(payload.len()).unwrap_or(u64::MAX);
    let measured_bits = ceiling_div(payload_len, leakage_quantum_bytes.max(1));
    let elapsed_ticks = ceiling_div(elapsed_ns, BOUNDARY_WITNESS_TIMING_TICK_QUANTUM_NS);
    let payload_ticks = ceiling_div(payload_len, 64);
    let observed_ticks = elapsed_ticks.max(payload_ticks);
    let baseline_ticks = baseline_ticks.max(1);
    let observed_variance_ticks = observed_ticks.abs_diff(baseline_ticks);

    let leakage = LeakageWitnessV1 {
        measurement_method: format!("daemon_{measurement_scope}_payload_quanta_v1"),
        measured_bits,
        confidence: 9_500,
        timestamp: timestamp_ns,
    };
    let timing = TimingWitnessV1 {
        measurement_method: format!("daemon_{measurement_scope}_elapsed_variance_v1"),
        observed_variance_ticks,
        baseline_ticks,
        confidence: 9_500,
        timestamp: timestamp_ns,
    };
    (leakage, timing)
}

/// Domain-tagged digest of a leakage witness for PCAC join binding.
#[must_use]
pub(crate) fn leakage_witness_hash(witness: &LeakageWitnessV1) -> [u8; 32] {
    let mut hasher = blake3::Hasher::new();
    hasher.update(b"pcac-boundary-leakage-witness-v1");
    hasher.update(witness.measurement_method.as_bytes());
    hasher.update(&witness.measured_bits.to_le_bytes());
    hasher.update(&witness.confidence.to_le_bytes());
    hasher.update(&witness.timestamp.to_le_bytes());
    *hasher.finalize().as_bytes()
}

/// Domain-tagged digest of a timing witness for PCAC join binding.
#[must_use]
pub(crate) fn timing_witness_hash(witness: &TimingWitnessV1) -> [u8; 32] {
    let mut hasher = blake3::Hasher::new();
    hasher.update(b"pcac-boundary-timing-witness-v1");
    hasher.update(witness.measurement_method.as_bytes());
    hasher.update(&witness.observed_variance_ticks.to_le_bytes());
    hasher.update(&witness.baseline_ticks.to_le_bytes());
    hasher.update(&witness.confidence.to_le_bytes());
    hasher.update(&witness.timestamp.to_le_bytes());
    *hasher.finalize().as_bytes()
}

/// Runtime boundary-flow evidence used to evaluate RFC-0028 REQ-0004
/// predicate closure at channel admission.
#[derive(Debug, Clone, PartialEq, Eq)]
pub struct BoundaryFlowRuntimeState {
    /// Dual-lattice taint admission predicate.
    pub taint_allow: bool,
    /// Dual-lattice confidentiality/classification admission predicate.
    pub classification_allow: bool,
    /// Declassification receipt validity predicate.
    pub declass_receipt_valid: bool,
    /// Declassification intent scope.
    pub declassification_intent: DeclassificationIntentScope,
    /// Optional redundancy-purpose receipt metadata.
    pub redundancy_declassification_receipt: Option<RedundancyDeclassificationReceipt>,
    /// Policy digest + canonicalizer tuple binding witness.
    pub policy_binding: BoundaryFlowPolicyBinding,
    /// Authoritative leakage witness measured by the daemon.
    pub leakage_witness: LeakageWitnessV1,
    /// Typed leakage-budget receipt.
    pub leakage_budget_receipt: LeakageBudgetReceipt,
    /// Authoritative timing witness measured by the daemon.
    pub timing_witness: TimingWitnessV1,
    /// Timing-channel release-bucketing witness.
    pub timing_channel_budget: TimingChannelBudget,
    /// Disclosure-control policy interlock binding.
    pub disclosure_policy_binding: DisclosurePolicyBinding,
    /// Policy-derived leakage budget ceiling for the request risk tier.
    pub leakage_budget_policy_max_bits: u64,
    /// Client-claimed leakage budget before policy clamping.
    pub claimed_leakage_budget_bits: Option<u64>,
    /// Policy-derived timing budget ceiling for the request risk tier.
    pub timing_budget_policy_max_ticks: u64,
    /// Client-claimed timing budget before policy clamping.
    pub claimed_timing_budget_ticks: Option<u64>,
}

impl BoundaryFlowRuntimeState {
    /// Baseline pass-state used when call-sites have not provided
    /// boundary-flow instrumentation.
    #[must_use]
    pub fn allow_all(policy_verified: bool) -> Self {
        let policy_digest = if policy_verified {
            [0x11; 32]
        } else {
            [0x22; 32]
        };
        let canonicalizer_digest = [0x33; 32];
        Self {
            taint_allow: true,
            classification_allow: true,
            declass_receipt_valid: true,
            declassification_intent: DeclassificationIntentScope::None,
            redundancy_declassification_receipt: None,
            policy_binding: BoundaryFlowPolicyBinding {
                policy_digest,
                admitted_policy_root_digest: policy_digest,
                canonicalizer_tuple_digest: canonicalizer_digest,
                admitted_canonicalizer_tuple_digest: canonicalizer_digest,
            },
            leakage_witness: LeakageWitnessV1 {
                measurement_method: "allow_all_default".to_string(),
                measured_bits: 0,
                confidence: 10_000,
                timestamp: 1,
            },
            leakage_budget_receipt: LeakageBudgetReceipt {
                leakage_bits: 0,
                budget_bits: 8,
                estimator_family: LeakageEstimatorFamily::MutualInformationUpperBound,
                confidence_bps: 10_000,
                confidence_label: "default".to_string(),
            },
            timing_witness: TimingWitnessV1 {
                measurement_method: "allow_all_default".to_string(),
                observed_variance_ticks: 0,
                baseline_ticks: 1,
                confidence: 10_000,
                timestamp: 1,
            },
            timing_channel_budget: TimingChannelBudget {
                release_bucket_ticks: 1,
                observed_variance_ticks: 0,
                budget_ticks: 1,
            },
            disclosure_policy_binding: DisclosurePolicyBinding {
                required_for_effect: false,
                state_valid: true,
                active_mode: DisclosurePolicyMode::TradeSecretOnly,
                expected_mode: DisclosurePolicyMode::TradeSecretOnly,
                attempted_channel: DisclosureChannelClass::Internal,
                policy_snapshot_digest: [0x44; 32],
                admitted_policy_epoch_root_digest: [0x44; 32],
                policy_epoch: 1,
                phase_id: "pre_federation".to_string(),
                state_reason: "not required".to_string(),
            },
            leakage_budget_policy_max_bits: 8,
            claimed_leakage_budget_bits: None,
            timing_budget_policy_max_ticks: 1,
            claimed_timing_budget_ticks: None,
        }
    }
}

impl PrivilegedDispatcher {
    /// Builds a channel-boundary check from daemon-classified tool context.
    ///
    /// Tool requests accepted by the daemon session plane are treated as
    /// typed tool-intent authority inputs. Unknown future tool classes
    /// fail-closed to `Unknown`.
    #[must_use]
    #[allow(clippy::fn_params_excessive_bools)]
    pub fn build_channel_boundary_check(
        &self,
        tool_class: &ToolClass,
        policy_verified: bool,
        broker_verified: bool,
        capability_verified: bool,
        context_firewall_verified: bool,
    ) -> ChannelBoundaryCheck {
        self.build_channel_boundary_check_with_flow(
            tool_class,
            policy_verified,
            broker_verified,
            capability_verified,
            context_firewall_verified,
            BoundaryFlowRuntimeState::allow_all(policy_verified),
        )
    }

    /// Builds a channel-boundary check with explicit boundary-flow runtime
    /// evidence for REQ-0004 enforcement.
    #[must_use]
    #[allow(clippy::fn_params_excessive_bools)]
    pub fn build_channel_boundary_check_with_flow(
        &self,
        tool_class: &ToolClass,
        policy_verified: bool,
        broker_verified: bool,
        capability_verified: bool,
        context_firewall_verified: bool,
        boundary_flow: BoundaryFlowRuntimeState,
    ) -> ChannelBoundaryCheck {
        let source = match tool_class {
            ToolClass::Read
            | ToolClass::Write
            | ToolClass::Execute
            | ToolClass::Network
            | ToolClass::Git
            | ToolClass::Inference
            | ToolClass::Artifact
            | ToolClass::ListFiles
            | ToolClass::Search => ChannelSource::TypedToolIntent,
            _ => ChannelSource::Unknown,
        };

        let channel_source_witness = if source == ChannelSource::TypedToolIntent {
            Some(derive_channel_source_witness(source))
        } else {
            None
        };
        let effective_source =
            if source == ChannelSource::TypedToolIntent && channel_source_witness.is_none() {
                ChannelSource::Unknown
            } else {
                source
            };

        ChannelBoundaryCheck {
            source: effective_source,
            channel_source_witness,
            broker_verified,
            capability_verified,
            context_firewall_verified,
            policy_ledger_verified: policy_verified,
            taint_allow: boundary_flow.taint_allow,
            classification_allow: boundary_flow.classification_allow,
            declass_receipt_valid: boundary_flow.declass_receipt_valid,
            declassification_intent: boundary_flow.declassification_intent,
            redundancy_declassification_receipt: boundary_flow.redundancy_declassification_receipt,
            boundary_flow_policy_binding: Some(boundary_flow.policy_binding),
            leakage_budget_receipt: Some(boundary_flow.leakage_budget_receipt),
            timing_channel_budget: Some(boundary_flow.timing_channel_budget),
            disclosure_policy_binding: Some(boundary_flow.disclosure_policy_binding),
            leakage_budget_policy_max_bits: Some(boundary_flow.leakage_budget_policy_max_bits),
            declared_leakage_budget_bits: boundary_flow.claimed_leakage_budget_bits,
            timing_budget_policy_max_ticks: Some(boundary_flow.timing_budget_policy_max_ticks),
            declared_timing_budget_ticks: boundary_flow.claimed_timing_budget_ticks,
        }
    }

    /// Validates channel boundaries and issues a daemon-signed context token.
    ///
    /// # Errors
    ///
    /// Returns boundary defects when validation fails or token issuance fails.
    #[deprecated(
        since = "0.1.0",
        note = "use validate_channel_boundary_and_issue_context_token_with_flow for explicit boundary-flow evidence"
    )]
    #[allow(clippy::fn_params_excessive_bools, clippy::too_many_arguments)]
    pub fn validate_channel_boundary_and_issue_context_token(
        &self,
        signer: &apm2_core::crypto::Signer,
        lease_id: &str,
        request_id: &str,
        issued_at_secs: u64,
        tool_class: &ToolClass,
        policy_verified: bool,
        broker_verified: bool,
        capability_verified: bool,
        context_firewall_verified: bool,
    ) -> Result<String, Vec<ChannelBoundaryDefect>> {
        self.validate_channel_boundary_and_issue_context_token_with_flow(
            signer,
            lease_id,
            request_id,
            issued_at_secs,
            tool_class,
            policy_verified,
            broker_verified,
            capability_verified,
            context_firewall_verified,
            BoundaryFlowRuntimeState::allow_all(policy_verified),
        )
    }

    /// Validates channel boundaries with explicit boundary-flow evidence and
    /// issues a daemon-signed context token on success.
    #[allow(clippy::fn_params_excessive_bools, clippy::too_many_arguments)]
    pub fn validate_channel_boundary_and_issue_context_token_with_flow(
        &self,
        signer: &apm2_core::crypto::Signer,
        lease_id: &str,
        request_id: &str,
        issued_at_secs: u64,
        tool_class: &ToolClass,
        policy_verified: bool,
        broker_verified: bool,
        capability_verified: bool,
        context_firewall_verified: bool,
        boundary_flow: BoundaryFlowRuntimeState,
    ) -> Result<String, Vec<ChannelBoundaryDefect>> {
        // INV-BRK-HEALTH-GATE-001: Fail-closed health gate check.
        // The gate starts closed and is opened by daemon startup after
        // successful initialization. Token issuance is denied until the
        // gate is opened.
        if !self.admission_health_gate.load(Ordering::Acquire) {
            return Err(vec![ChannelBoundaryDefect::new(
                ChannelViolationClass::MissingChannelMetadata,
                "admission denied: broker health gate not satisfied (INV-BRK-HEALTH-GATE-001)"
                    .to_string(),
            )]);
        }

        let check = self.build_channel_boundary_check_with_flow(
            tool_class,
            policy_verified,
            broker_verified,
            capability_verified,
            context_firewall_verified,
            boundary_flow,
        );
        let defects = validate_channel_boundary(&check);
        if !defects.is_empty() {
            return Err(defects);
        }

        issue_channel_context_token(&check, lease_id, request_id, issued_at_secs, signer).map_err(
            |error| {
                vec![ChannelBoundaryDefect::new(
                    ChannelViolationClass::MissingChannelMetadata,
                    format!("failed to issue channel context token: {error}"),
                )]
            },
        )
    }

    /// Creates a new dispatcher with default decode configuration (TEST ONLY).
    ///
    /// # Warning: RSK-2503 Mixed Clock Domain Hazard
    ///
    /// This constructor creates an internal `HolonicClock` instance. For
    /// production code, use `with_shared_state` or `with_dependencies` to
    /// inject a shared clock and prevent mixed clock domain hazards.
    ///
    /// # Usage
    ///
    /// This constructor is intended for unit tests only. Production code
    /// should use `with_shared_state` or `with_dependencies` with a
    /// properly initialized and shared `HolonicClock`.
    ///
    /// Uses stub implementations for policy resolver, work registry, event
    /// emitter, session registry, and lease validator. No metrics are emitted.
    /// Creates internal stub token minter, manifest store, and HTF clock for
    /// testing.
    #[must_use]
    pub fn new() -> Self {
        // TCK-00289: Create default HolonicClock for HTF-compliant timestamps
        // WARNING: This creates an internal clock which can cause RSK-2503
        // (Mixed Clock Domain Hazard) if used in production alongside other
        // components with their own clocks. Use with_shared_state or
        // with_dependencies for production code.
        let holonic_clock = Arc::new(
            HolonicClock::new(ClockConfig::default(), None)
                .expect("default ClockConfig should always succeed"),
        );
        // TCK-00303: Create subscription registry for HEF resource governance
        let subscription_registry = Arc::new(SubscriptionRegistry::with_defaults());

        // TCK-00415: Create shared event emitter and work authority.
        let event_emitter: Arc<dyn LedgerEventEmitter> = Arc::new(StubLedgerEventEmitter::new());
        let work_authority = Arc::new(ProjectionWorkAuthority::new(Arc::clone(&event_emitter)));
        // TCK-00420: Create alias reconciliation gate backed by shared emitter.
        let alias_reconciliation_gate: Arc<dyn AliasReconciliationGate> = Arc::new(
            ProjectionAliasReconciliationGate::new(Arc::clone(&event_emitter)),
        );

        Self {
            decode_config: DecodeConfig::default(),
            policy_resolver: Arc::new(StubPolicyResolver),
            work_registry: Arc::new(StubWorkRegistry::default()),
            event_emitter,
            work_authority,
            episode_runtime: Arc::new(EpisodeRuntime::new(EpisodeRuntimeConfig::default())),
            session_registry: Arc::new(InMemorySessionRegistry::default()),
            lease_validator: Arc::new(StubLeaseValidator::new()),
            token_minter: Arc::new(TokenMinter::new(TokenMinter::generate_secret())),
            manifest_store: Arc::new(InMemoryManifestStore::new()),
            // TCK-00317: Pre-seed CAS with reviewer v0 manifest
            manifest_loader: Arc::new(InMemoryCasManifestLoader::with_reviewer_v0_manifest()),
            metrics: None,
            holonic_clock,
            subscription_registry,
            daemon_state: None,
            // TCK-00345: Consensus state not configured in test mode
            node_id: "test-node".to_string(),
            consensus_state: None,
            credential_store: None,
            telemetry_store: None,
            session_broker_registry: None,
            broker_config: ToolBrokerConfig::default(),
            broker_cas: None,
            broker_github_store: None,
            broker_ssh_store: None,
            // TCK-00416: Default MemoryCas for authority binding validation in tests.
            cas: Some(Arc::new(MemoryCas::default())),
            v1_manifest_store: None,
            gate_orchestrator: None,
            pcac_lifecycle_gate: None,
            privileged_pcac_policy: PrivilegedPcacPolicy::default(),
            stop_conditions_store: None,
            stop_authority: None,
            governance_freshness_monitor: None,
            adapter_registry: None,
            adapter_selection_policy: None,
            adapter_available_profiles: BTreeSet::new(),
            adapter_profile_ids_by_hash: HashMap::new(),
            adapter_profile_cas: None,
            permeability_receipt_resolver: Arc::new(StubPermeabilityReceiptResolver),
            alias_reconciliation_gate,
            divergence_watchdog: None,
            admission_health_gate: AtomicBool::new(false),
        }
    }

    /// Creates a new dispatcher with custom decode configuration (TEST ONLY).
    ///
    /// # Warning: RSK-2503 Mixed Clock Domain Hazard
    ///
    /// This constructor creates an internal `HolonicClock` instance. For
    /// production code, use `with_shared_state` or `with_dependencies` to
    /// inject a shared clock and prevent mixed clock domain hazards.
    ///
    /// # Usage
    ///
    /// This constructor is intended for unit tests only. Production code
    /// should use `with_shared_state` or `with_dependencies` with a
    /// properly initialized and shared `HolonicClock`.
    ///
    /// Uses stub implementations for policy resolver, work registry, event
    /// emitter, session registry, and lease validator. No metrics are emitted.
    /// Creates internal stub token minter, manifest store, and HTF clock for
    /// testing.
    #[must_use]
    pub fn with_decode_config(decode_config: DecodeConfig) -> Self {
        // TCK-00289: Create default HolonicClock for HTF-compliant timestamps
        // WARNING: This creates an internal clock which can cause RSK-2503
        // (Mixed Clock Domain Hazard) if used in production alongside other
        // components with their own clocks. Use with_shared_state or
        // with_dependencies for production code.
        let holonic_clock = Arc::new(
            HolonicClock::new(ClockConfig::default(), None)
                .expect("default ClockConfig should always succeed"),
        );
        // TCK-00303: Create subscription registry for HEF resource governance
        let subscription_registry = Arc::new(SubscriptionRegistry::with_defaults());

        // TCK-00415: Create shared event emitter and work authority.
        let event_emitter: Arc<dyn LedgerEventEmitter> = Arc::new(StubLedgerEventEmitter::new());
        let work_authority = Arc::new(ProjectionWorkAuthority::new(Arc::clone(&event_emitter)));
        // TCK-00420: Create alias reconciliation gate backed by shared emitter.
        let alias_reconciliation_gate: Arc<dyn AliasReconciliationGate> = Arc::new(
            ProjectionAliasReconciliationGate::new(Arc::clone(&event_emitter)),
        );

        Self {
            decode_config,
            policy_resolver: Arc::new(StubPolicyResolver),
            work_registry: Arc::new(StubWorkRegistry::default()),
            event_emitter,
            work_authority,
            episode_runtime: Arc::new(EpisodeRuntime::new(EpisodeRuntimeConfig::default())),
            session_registry: Arc::new(InMemorySessionRegistry::default()),
            lease_validator: Arc::new(StubLeaseValidator::new()),
            token_minter: Arc::new(TokenMinter::new(TokenMinter::generate_secret())),
            manifest_store: Arc::new(InMemoryManifestStore::new()),
            // TCK-00317: Pre-seed CAS with reviewer v0 manifest
            manifest_loader: Arc::new(InMemoryCasManifestLoader::with_reviewer_v0_manifest()),
            metrics: None,
            holonic_clock,
            subscription_registry,
            daemon_state: None,
            // TCK-00345: Consensus state not configured in test mode
            node_id: "test-node".to_string(),
            consensus_state: None,
            credential_store: None,
            telemetry_store: None,
            session_broker_registry: None,
            broker_config: ToolBrokerConfig::default(),
            broker_cas: None,
            broker_github_store: None,
            broker_ssh_store: None,
            // TCK-00416: Default MemoryCas for authority binding validation in tests.
            cas: Some(Arc::new(MemoryCas::default())),
            v1_manifest_store: None,
            gate_orchestrator: None,
            pcac_lifecycle_gate: None,
            privileged_pcac_policy: PrivilegedPcacPolicy::default(),
            stop_conditions_store: None,
            stop_authority: None,
            governance_freshness_monitor: None,
            adapter_registry: None,
            adapter_selection_policy: None,
            adapter_available_profiles: BTreeSet::new(),
            adapter_profile_ids_by_hash: HashMap::new(),
            adapter_profile_cas: None,
            permeability_receipt_resolver: Arc::new(StubPermeabilityReceiptResolver),
            alias_reconciliation_gate,
            divergence_watchdog: None,
            admission_health_gate: AtomicBool::new(false),
        }
    }

    /// Creates a new dispatcher with custom dependencies (PRODUCTION).
    ///
    /// This is the production constructor for real governance integration.
    /// Does not include metrics; use `with_metrics` to add them.
    ///
    /// # TCK-00289: Clock Injection (RSK-2503 Prevention)
    ///
    /// The `clock` parameter MUST be a shared `HolonicClock` instance that is
    /// also used by other components in the system. This prevents the mixed
    /// clock domain hazard (RSK-2503) that would occur if each component
    /// created its own clock.
    ///
    /// # TCK-00287: State Sharing
    ///
    /// The `token_minter` and `manifest_store` parameters MUST be `Arc::clone`
    /// copies of the same instances used by `SessionDispatcher`. This ensures:
    /// - Tokens minted during `SpawnEpisode` can be validated by
    ///   `SessionDispatcher`
    /// - Capability manifests registered during `SpawnEpisode` are accessible
    ///   for tool request validation
    ///
    /// Callers must ensure proper sharing by cloning the Arcs BEFORE passing
    /// to this constructor:
    /// ```ignore
    /// let token_minter = Arc::new(TokenMinter::new(...));
    /// let manifest_store = Arc::new(InMemoryManifestStore::new());
    /// let priv_dispatcher = PrivilegedDispatcher::with_dependencies(
    ///     ...,
    ///     Arc::clone(&token_minter),  // Clone BEFORE passing
    ///     Arc::clone(&manifest_store), // Clone BEFORE passing
    /// );
    /// let session_dispatcher = SessionDispatcher::with_manifest_store(
    ///     (*token_minter).clone(),
    ///     manifest_store,
    /// );
    /// ```
    #[must_use]
    #[allow(clippy::too_many_arguments)]
    pub fn with_dependencies(
        decode_config: DecodeConfig,
        policy_resolver: Arc<dyn PolicyResolver>,
        work_registry: Arc<dyn WorkRegistry>,
        event_emitter: Arc<dyn LedgerEventEmitter>,
        episode_runtime: Arc<EpisodeRuntime>,
        session_registry: Arc<dyn SessionRegistry>,
        lease_validator: Arc<dyn LeaseValidator>,
        clock: Arc<HolonicClock>,
        token_minter: Arc<TokenMinter>,
        manifest_store: Arc<InMemoryManifestStore>,
        manifest_loader: Arc<dyn ManifestLoader>,
        subscription_registry: SharedSubscriptionRegistry,
    ) -> Self {
        // TCK-00415: Shared work authority over the provided emitter.
        let work_authority = Arc::new(ProjectionWorkAuthority::new(Arc::clone(&event_emitter)));
        // TCK-00420: Create alias reconciliation gate backed by shared emitter.
        let alias_reconciliation_gate: Arc<dyn AliasReconciliationGate> = Arc::new(
            ProjectionAliasReconciliationGate::new(Arc::clone(&event_emitter)),
        );

        Self {
            decode_config,
            policy_resolver,
            work_registry,
            event_emitter,
            work_authority,
            episode_runtime,
            session_registry,
            lease_validator,
            token_minter,
            manifest_store,
            manifest_loader,
            metrics: None,
            holonic_clock: clock,
            subscription_registry,
            daemon_state: None,
            // TCK-00345: Consensus state not configured by default
            node_id: "node-001".to_string(),
            consensus_state: None,
            credential_store: None,
            telemetry_store: None,
            session_broker_registry: None,
            broker_config: ToolBrokerConfig::default(),
            broker_cas: None,
            broker_github_store: None,
            broker_ssh_store: None,
            cas: None,
            v1_manifest_store: None,
            gate_orchestrator: None,
            pcac_lifecycle_gate: None,
            privileged_pcac_policy: PrivilegedPcacPolicy::default(),
            stop_conditions_store: None,
            stop_authority: None,
            governance_freshness_monitor: None,
            adapter_registry: None,
            adapter_selection_policy: None,
            adapter_available_profiles: BTreeSet::new(),
            adapter_profile_ids_by_hash: HashMap::new(),
            adapter_profile_cas: None,
            permeability_receipt_resolver: Arc::new(StubPermeabilityReceiptResolver),
            alias_reconciliation_gate,
            divergence_watchdog: None,
            admission_health_gate: AtomicBool::new(false),
        }
    }

    /// Creates a new dispatcher with shared token minter and manifest store
    /// (PRODUCTION).
    ///
    /// # TCK-00287: State Sharing
    ///
    /// This constructor is used by `DispatcherState` to wire up shared
    /// dependencies between `PrivilegedDispatcher` and `SessionDispatcher`:
    /// - `token_minter`: Ensures tokens minted during `SpawnEpisode` can be
    ///   validated by `SessionDispatcher`
    /// - `manifest_store`: Ensures capability manifests registered during
    ///   `SpawnEpisode` are accessible for tool request validation
    /// - `session_registry`: Uses the global daemon session registry instead of
    ///   an internal stub
    ///
    /// # TCK-00289: Clock Injection (RSK-2503 Prevention)
    ///
    /// The `clock` parameter MUST be a shared `HolonicClock` instance that is
    /// also used by other components in the system. This prevents the mixed
    /// clock domain hazard (RSK-2503) that would occur if each component
    /// created its own clock.
    #[must_use]
    pub fn with_shared_state(
        token_minter: Arc<TokenMinter>,
        manifest_store: Arc<InMemoryManifestStore>,
        session_registry: Arc<dyn SessionRegistry>,
        clock: Arc<HolonicClock>,
        subscription_registry: SharedSubscriptionRegistry,
    ) -> Self {
        // TCK-00415: Create shared event emitter and work authority.
        let event_emitter: Arc<dyn LedgerEventEmitter> = Arc::new(StubLedgerEventEmitter::new());
        let work_authority = Arc::new(ProjectionWorkAuthority::new(Arc::clone(&event_emitter)));
        // TCK-00420: Create alias reconciliation gate backed by shared emitter.
        let alias_reconciliation_gate: Arc<dyn AliasReconciliationGate> = Arc::new(
            ProjectionAliasReconciliationGate::new(Arc::clone(&event_emitter)),
        );

        Self {
            decode_config: DecodeConfig::default(),
            policy_resolver: Arc::new(StubPolicyResolver),
            work_registry: Arc::new(StubWorkRegistry::default()),
            event_emitter,
            work_authority,
            episode_runtime: Arc::new(EpisodeRuntime::new(EpisodeRuntimeConfig::default())),
            session_registry,
            lease_validator: Arc::new(StubLeaseValidator::new()),
            token_minter,
            manifest_store,
            // TCK-00317: Pre-seed CAS with reviewer v0 manifest
            manifest_loader: Arc::new(InMemoryCasManifestLoader::with_reviewer_v0_manifest()),
            metrics: None,
            holonic_clock: clock,
            subscription_registry,
            daemon_state: None,
            // TCK-00345: Consensus state not configured by default
            node_id: "node-001".to_string(),
            consensus_state: None,
            credential_store: None,
            telemetry_store: None,
            session_broker_registry: None,
            broker_config: ToolBrokerConfig::default(),
            broker_cas: None,
            broker_github_store: None,
            broker_ssh_store: None,
            cas: None,
            v1_manifest_store: None,
            gate_orchestrator: None,
            pcac_lifecycle_gate: None,
            privileged_pcac_policy: PrivilegedPcacPolicy::default(),
            stop_conditions_store: None,
            stop_authority: None,
            governance_freshness_monitor: None,
            adapter_registry: None,
            adapter_selection_policy: None,
            adapter_available_profiles: BTreeSet::new(),
            adapter_profile_ids_by_hash: HashMap::new(),
            adapter_profile_cas: None,
            permeability_receipt_resolver: Arc::new(StubPermeabilityReceiptResolver),
            alias_reconciliation_gate,
            divergence_watchdog: None,
            admission_health_gate: AtomicBool::new(false),
        }
    }

    /// Opens or closes the admission health gate (INV-BRK-HEALTH-GATE-001).
    ///
    /// Called from daemon startup (`main.rs`) after successful initialization
    /// to open the gate, enabling token issuance. The gate starts closed
    /// (fail-closed) and remains closed until this method is called with
    /// `true`.
    ///
    /// # Ordering
    ///
    /// Uses `Release` ordering so that all initialization side-effects
    /// are visible to subsequent `Acquire` loads in the token issuance
    /// path.
    pub fn set_admission_health_gate(&self, passed: bool) {
        self.admission_health_gate.store(passed, Ordering::Release);
    }

    /// Returns whether the admission health gate is currently open.
    ///
    /// Uses `Acquire` ordering to synchronize with the `Release` store
    /// in `set_admission_health_gate`.
    pub fn admission_health_gate_passed(&self) -> bool {
        self.admission_health_gate.load(Ordering::Acquire)
    }

    /// Sets the daemon state for process management (TCK-00342).
    ///
    /// When set, process management handlers (`ListProcesses`,
    /// `ProcessStatus`, `StartProcess`, `StopProcess`, `RestartProcess`,
    /// `ReloadProcess`) query the `Supervisor` within `DaemonState` for
    /// live process information instead of returning stub responses.
    #[must_use]
    pub fn with_daemon_state(mut self, state: SharedState) -> Self {
        self.daemon_state = Some(state);
        self
    }

    /// Sets the credential store for credential management (TCK-00343).
    ///
    /// When set, credential management handlers (`ListCredentials`,
    /// `AddCredential`, `RemoveCredential`, `RefreshCredential`,
    /// `SwitchCredential`) persist credentials via the `CredentialStore`
    /// backed by the OS keyring. When not set, handlers return error
    /// responses indicating the credential store is not configured.
    #[must_use]
    pub fn with_credential_store(mut self, store: Arc<CredentialStore>) -> Self {
        self.credential_store = Some(store);
        self
    }

    /// Sets the session telemetry store for tracking tool calls, events
    /// emitted, and session start time (TCK-00384).
    ///
    /// When set, `SpawnEpisode` registers telemetry for new sessions. The
    /// store should be shared with `SessionDispatcher` for counter
    /// updates and queries.
    #[must_use]
    pub fn with_telemetry_store(
        mut self,
        store: Arc<crate::session::SessionTelemetryStore>,
    ) -> Self {
        self.telemetry_store = Some(store);
        self
    }

    /// Sets the per-session broker registry (TCK-00401).
    #[must_use]
    pub fn with_session_broker_registry(
        mut self,
        registry: SharedSessionBrokerRegistry<crate::episode::capability::StubManifestLoader>,
    ) -> Self {
        self.session_broker_registry = Some(registry);
        self
    }

    /// Sets the per-session broker configuration template.
    #[must_use]
    pub fn with_broker_config(mut self, config: ToolBrokerConfig) -> Self {
        self.broker_config = config;
        self
    }

    /// Sets the CAS backend used by per-session brokers.
    #[must_use]
    pub fn with_broker_cas(
        mut self,
        cas: Arc<dyn crate::episode::executor::ContentAddressedStore>,
    ) -> Self {
        self.broker_cas = Some(cas);
        self
    }

    /// Sets the GitHub credential store used by per-session brokers.
    #[must_use]
    pub fn with_broker_github_store(mut self, store: Arc<dyn GitHubCredentialStore>) -> Self {
        self.broker_github_store = Some(store);
        self
    }

    /// Sets the SSH credential store used by per-session brokers.
    #[must_use]
    pub fn with_broker_ssh_store(mut self, store: Arc<dyn SshCredentialStore>) -> Self {
        self.broker_ssh_store = Some(store);
        self
    }

    /// Sets the content-addressed store for changeset bundle persistence
    /// (TCK-00394).
    ///
    /// When set, `PublishChangeSet` stores the canonical bundle bytes in CAS
    /// and returns the content hash for ledger event binding. When not set,
    /// the handler returns an error indicating CAS is not configured.
    #[must_use]
    pub fn with_cas(mut self, cas: Arc<dyn ContentAddressedStore>) -> Self {
        self.cas = Some(cas);
        self
    }

    /// Clears the CAS backend (TEST ONLY).
    ///
    /// Used by tests that need to verify fail-closed behavior when CAS is
    /// not configured. Since `new()` provides a default `MemoryCas` for
    /// authority binding validation, tests must call this to simulate the
    /// no-CAS production path for PublishChangeSet/IngestReviewReceipt.
    #[must_use]
    #[cfg(test)]
    pub fn without_cas(mut self) -> Self {
        self.cas = None;
        self
    }

    /// Sets the dedicated CAS for adapter profile hash verification
    /// (TCK-00400).
    ///
    /// This CAS is used to verify that selected adapter profile hashes
    /// exist at dispatch time. It is intentionally separate from the
    /// publish/ingest CAS (`with_cas`) so that adapter selection can work
    /// without enabling `PublishChangeSet` (TCK-00412 fail-closed).
    #[must_use]
    pub fn with_adapter_profile_cas(mut self, cas: Arc<dyn ContentAddressedStore>) -> Self {
        self.adapter_profile_cas = Some(cas);
        self
    }

    /// Sets the shared V1 manifest store (TCK-00352 Security Review MAJOR 2).
    ///
    /// When set, `handle_spawn_episode` mints a V1 capability manifest and
    /// registers it in this store. The `SessionDispatcher` holds a clone of
    /// the same `Arc` and enforces V1 scope checks in `handle_request_tool`.
    #[must_use]
    pub fn with_v1_manifest_store(
        mut self,
        store: crate::protocol::session_dispatch::SharedV1ManifestStore,
    ) -> Self {
        self.v1_manifest_store = Some(store);
        self
    }

    /// Sets the permeability receipt resolver for delegated claim binding
    /// (TCK-00373).
    #[must_use]
    pub fn with_permeability_receipt_resolver(
        mut self,
        resolver: Arc<dyn PermeabilityReceiptResolver>,
    ) -> Self {
        self.permeability_receipt_resolver = resolver;
        self
    }

    /// Sets the gate orchestrator for sublease delegation (TCK-00340).
    #[must_use]
    pub fn with_gate_orchestrator(
        mut self,
        orchestrator: Arc<crate::gate::GateOrchestrator>,
    ) -> Self {
        self.gate_orchestrator = Some(orchestrator);
        self
    }

    /// Sets the privileged PCAC lifecycle gate (TCK-00424).
    #[must_use]
    pub fn with_pcac_lifecycle_gate(mut self, gate: Arc<crate::pcac::LifecycleGate>) -> Self {
        self.pcac_lifecycle_gate = Some(gate);
        self
    }

    /// Clears the privileged PCAC lifecycle gate (TEST ONLY).
    #[cfg(test)]
    #[must_use]
    pub fn without_pcac_lifecycle_gate(mut self) -> Self {
        self.pcac_lifecycle_gate = None;
        self
    }

    /// Sets privileged PCAC rollout policy flags (TCK-00424).
    #[must_use]
    pub const fn with_privileged_pcac_policy(mut self, policy: PrivilegedPcacPolicy) -> Self {
        self.privileged_pcac_policy = policy;
        self
    }

    /// Sets the divergence watchdog for projection recovery (TCK-00469).
    ///
    /// When set, `RegisterRecoveryEvidence` and `RequestUnfreeze` handlers
    /// call through to the watchdog to register durable recovery evidence
    /// and lift projection freezes.
    #[must_use]
    pub fn with_divergence_watchdog(
        mut self,
        watchdog: Arc<crate::projection::DivergenceWatchdog<crate::projection::SystemTimeSource>>,
    ) -> Self {
        self.divergence_watchdog = Some(watchdog);
        self
    }

    /// Sets the per-session stop conditions store (TCK-00351 v4).
    ///
    /// When set, `SpawnEpisode` registers stop conditions from the episode
    /// envelope for each new session. The `SessionDispatcher` holds a clone
    /// of the same `Arc` and reads conditions in the pre-actuation gate.
    #[must_use]
    pub fn with_stop_conditions_store(
        mut self,
        store: Arc<crate::session::SessionStopConditionsStore>,
    ) -> Self {
        self.stop_conditions_store = Some(store);
        self
    }

    /// Sets the shared stop authority for runtime stop-flag mutation.
    #[must_use]
    pub fn with_stop_authority(
        mut self,
        authority: Arc<crate::episode::preactuation::StopAuthority>,
    ) -> Self {
        self.stop_authority = Some(authority);
        self
    }

    /// Sets the governance freshness monitor for production probe wiring.
    #[must_use]
    pub fn with_governance_freshness_monitor(
        mut self,
        monitor: Arc<GovernanceFreshnessMonitor>,
    ) -> Self {
        self.governance_freshness_monitor = Some(monitor);
        self
    }

    /// Sets the adapter registry for spawning agent CLI processes (TCK-00399).
    #[must_use]
    pub fn with_adapter_registry(mut self, registry: Arc<crate::episode::AdapterRegistry>) -> Self {
        self.adapter_registry = Some(registry);
        self
    }

    /// Sets adapter-selection policy state, adapter availability, and the
    /// dedicated CAS for profile hash verification (TCK-00400).
    ///
    /// `profile_cas` is the CAS instance used to verify that the selected
    /// adapter profile hash exists at dispatch time. This is intentionally
    /// separate from the `cas` field (used by `PublishChangeSet`) so that
    /// `with_persistence` can enable adapter selection without inadvertently
    /// enabling publish/ingest operations (TCK-00412 fail-closed).
    #[must_use]
    pub fn with_adapter_selection_policy(
        mut self,
        policy: AdapterSelectionPolicy,
        available_profiles: BTreeSet<[u8; 32]>,
        profile_hashes_by_id: HashMap<String, [u8; 32]>,
        profile_cas: Arc<dyn ContentAddressedStore>,
    ) -> Self {
        self.adapter_selection_policy = Some(Arc::new(Mutex::new(policy)));
        self.adapter_available_profiles = available_profiles;
        self.adapter_profile_ids_by_hash = profile_hashes_by_id
            .into_iter()
            .map(|(profile_id, profile_hash)| (profile_hash, profile_id))
            .collect();
        self.adapter_profile_cas = Some(profile_cas);
        self
    }

    /// Replaces the session registry used by this dispatcher (TEST ONLY).
    ///
    /// This allows tests to inject a concrete `InMemorySessionRegistry`
    /// (or any `SessionRegistry` impl) so they can inspect registry
    /// cardinality and content after dispatch operations.
    #[cfg(test)]
    #[must_use]
    pub fn with_session_registry(mut self, registry: Arc<dyn SessionRegistry>) -> Self {
        self.session_registry = registry;
        self
    }

    /// Adds a metrics registry to the dispatcher (TCK-00268).
    ///
    /// When set, the dispatcher will emit metrics for:
    /// - `session_spawned`: When `SpawnEpisode` succeeds
    /// - `ipc_request_completed`: For each dispatched request
    /// - `capability_granted`: When `IssueCapability` succeeds
    ///
    /// # Integration Status
    ///
    /// **NOTE**: This method is currently only exercised in tests. The binary
    /// protocol path is not yet wired into `main.rs`. See the `metrics` field
    /// documentation for details.
    #[must_use]
    pub fn with_metrics(mut self, metrics: SharedMetricsRegistry) -> Self {
        self.metrics = Some(metrics);
        self
    }

    /// Returns a reference to the event emitter.
    ///
    /// This is useful for testing to verify events were emitted.
    #[must_use]
    pub fn event_emitter(&self) -> &Arc<dyn LedgerEventEmitter> {
        &self.event_emitter
    }

    /// Returns a reference to the shared work authority (TCK-00415).
    ///
    /// The `ProjectionWorkAuthority` is instantiated once during dispatcher
    /// construction and shared across all work-related request handlers.
    #[must_use]
    pub const fn work_authority(&self) -> &Arc<ProjectionWorkAuthority> {
        &self.work_authority
    }

    /// Returns a reference to the alias reconciliation gate (TCK-00420).
    ///
    /// Used by integration tests to verify that the gate is wired into the
    /// dispatcher and produces expected reconciliation results.
    #[must_use]
    pub fn alias_reconciliation_gate(&self) -> &Arc<dyn AliasReconciliationGate> {
        &self.alias_reconciliation_gate
    }

    /// Records a successful governance probe when monitoring is wired.
    fn record_governance_probe_success(&self) {
        if let Some(ref monitor) = self.governance_freshness_monitor {
            monitor.record_success();
        }
    }

    /// Records a governance probe failure when monitoring is wired.
    ///
    /// IMPORTANT: Callers must use this only for real governance service
    /// transport/communication failures, never for local validation/state
    /// precondition errors.
    fn record_governance_probe_failure(&self) {
        if let Some(ref monitor) = self.governance_freshness_monitor {
            monitor.record_failure();
        }
    }

    /// Records an adapter spawn failure with a pre-classified `rate_limited`
    /// flag derived from typed error variants (not string matching).
    ///
    /// This is the preferred path for spawn failures from the typed
    /// `SpawnFailure` flow where the error has already been classified via
    /// `EpisodeError::is_rate_limited()` or `AdapterError` variant matching.
    fn record_adapter_profile_failure_typed(&self, profile_hash: &[u8; 32], rate_limited: bool) {
        let Some(policy) = &self.adapter_selection_policy else {
            return;
        };

        let now_secs = apm2_core::fac::adapter_selection::monotonic_secs();
        match policy.lock() {
            Ok(mut guard) => {
                if let Err(policy_err) = guard.record_failure(profile_hash, now_secs, rate_limited)
                {
                    warn!(
                        profile_hash = %hex::encode(profile_hash),
                        error = %policy_err,
                        "adapter selection failure tracking update failed"
                    );
                }
            },
            Err(lock_err) => {
                warn!(
                    profile_hash = %hex::encode(profile_hash),
                    error = %lock_err,
                    "adapter selection policy lock poisoned during failure update"
                );
            },
        }
    }

    fn record_adapter_profile_success(&self, profile_hash: &[u8; 32]) {
        let Some(policy) = &self.adapter_selection_policy else {
            return;
        };

        match policy.lock() {
            Ok(mut guard) => {
                if let Err(policy_err) = guard.record_success(profile_hash) {
                    warn!(
                        profile_hash = %hex::encode(profile_hash),
                        error = %policy_err,
                        "adapter selection success tracking update failed"
                    );
                }
            },
            Err(lock_err) => {
                warn!(
                    profile_hash = %hex::encode(profile_hash),
                    error = %lock_err,
                    "adapter selection policy lock poisoned during success update"
                );
            },
        }
    }

    /// Returns a reference to the episode runtime.
    ///
    /// This is useful for testing to verify episode state.
    #[must_use]
    pub const fn episode_runtime(&self) -> &Arc<EpisodeRuntime> {
        &self.episode_runtime
    }

    /// Returns a reference to the session registry.
    ///
    /// This is useful for testing to verify session state.
    #[must_use]
    pub fn session_registry(&self) -> &Arc<dyn SessionRegistry> {
        &self.session_registry
    }

    /// Returns a reference to the lease validator.
    ///
    /// Primarily for testing purposes to pre-register leases.
    #[must_use]
    pub fn lease_validator(&self) -> &Arc<dyn LeaseValidator> {
        &self.lease_validator
    }

    /// Runs an explicit full-chain ledger verification audit.
    ///
    /// This is an operator/admin maintenance path and is never called
    /// automatically during startup.
    pub fn verify_ledger_chain_admin(&self) -> Result<String, String> {
        self.event_emitter.verify_chain_admin()
    }

    fn handle_verify_ledger_chain(&self, payload: &[u8]) -> ProtocolResult<PrivilegedResponse> {
        VerifyLedgerChainRequest::decode_bounded(payload, &self.decode_config).map_err(|e| {
            ProtocolError::Serialization {
                reason: format!("invalid VerifyLedgerChainRequest: {e}"),
            }
        })?;

        let rows_validated =
            u64::try_from(self.event_emitter.get_event_count()).unwrap_or(u64::MAX);
        let response = match self.verify_ledger_chain_admin() {
            Ok(tip_hash) => VerifyLedgerChainResponse {
                verified: true,
                rows_validated,
                tip_hash: tip_hash.clone(),
                message: format!(
                    "full-chain verification succeeded (rows_validated={rows_validated}, tip_hash={tip_hash})"
                ),
            },
            Err(error) => VerifyLedgerChainResponse {
                verified: false,
                rows_validated,
                tip_hash: String::new(),
                message: format!(
                    "full-chain verification failed after validating {rows_validated} row(s): {error}"
                ),
            },
        };
        Ok(PrivilegedResponse::VerifyLedgerChain(response))
    }

    // =========================================================================
    // TCK-00469: Projection Recovery Handlers
    // =========================================================================

    /// Handles `RegisterRecoveryEvidence` requests (IPC-PRIV-074).
    ///
    /// Registers durable recovery evidence for a frozen projection, setting
    /// `has_durable_provenance = true` to enable subsequent unfreeze.
    ///
    /// # Security
    ///
    /// - Requires privileged connection (checked in dispatch)
    /// - Validates the watchdog is configured (fail-closed if missing)
    /// - Delegates admission checks to `DivergenceWatchdog`
    fn handle_register_recovery_evidence(
        &self,
        payload: &[u8],
        ctx: &ConnectionContext,
    ) -> ProtocolResult<PrivilegedResponse> {
        // Maximum size for receipts_json payload (256 KiB) to bound
        // deserialization memory and prevent memory exhaustion via oversized payloads.
        const MAX_RECEIPTS_JSON_SIZE: usize = 256 * 1024;
        // Maximum number of replay receipts per request to prevent CPU exhaustion.
        const MAX_RECEIPTS_PER_REQUEST: usize = 4096;

        let request = RegisterRecoveryEvidenceRequest::decode_bounded(payload, &self.decode_config)
            .map_err(|e| ProtocolError::Serialization {
                reason: format!("invalid RegisterRecoveryEvidenceRequest: {e}"),
            })?;

        // ---- Phase -1: Bind caller identity to authenticated peer ----
        let Some(peer_creds) = ctx.peer_credentials() else {
            return Ok(PrivilegedResponse::error(
                PrivilegedErrorCode::PermissionDenied,
                "peer credentials required for recovery evidence registration",
            ));
        };
        let _authenticated_caller_id = derive_actor_id(peer_creds);

        let watchdog =
            self.divergence_watchdog
                .as_ref()
                .ok_or_else(|| ProtocolError::Serialization {
                    reason: "divergence watchdog not configured".to_string(),
                })?;

        if request.freeze_id.is_empty() {
            return Ok(PrivilegedResponse::error(
                PrivilegedErrorCode::InvalidArgument,
                "freeze_id must not be empty",
            ));
        }

        if request.lease_id.is_empty() {
            return Ok(PrivilegedResponse::error(
                PrivilegedErrorCode::InvalidArgument,
                "lease_id is required for PCAC lifecycle enforcement",
            ));
        }
        if request.lease_id.len() > MAX_ID_LENGTH {
            return Ok(PrivilegedResponse::error(
                PrivilegedErrorCode::InvalidArgument,
                format!("lease_id exceeds maximum length of {MAX_ID_LENGTH} bytes"),
            ));
        }

        if request.durable_evidence_digest.len() != 32 {
            return Ok(PrivilegedResponse::error(
                PrivilegedErrorCode::InvalidArgument,
                "durable_evidence_digest must be exactly 32 bytes",
            ));
        }

        if request.receipts_json.len() > MAX_RECEIPTS_JSON_SIZE {
            return Ok(PrivilegedResponse::error(
                PrivilegedErrorCode::InvalidArgument,
                format!(
                    "receipts_json exceeds maximum size: {} > {MAX_RECEIPTS_JSON_SIZE}",
                    request.receipts_json.len()
                ),
            ));
        }

        let mut digest = [0u8; 32];
        digest.copy_from_slice(&request.durable_evidence_digest);

        let receipts: Vec<apm2_core::fac::projection_compromise::ProjectionReplayReceiptV1> =
            serde_json::from_slice(&request.receipts_json).map_err(|e| {
                ProtocolError::Serialization {
                    reason: format!("invalid receipts_json: {e}"),
                }
            })?;

        if receipts.len() > MAX_RECEIPTS_PER_REQUEST {
            return Ok(PrivilegedResponse::error(
                PrivilegedErrorCode::InvalidArgument,
                format!(
                    "receipts count exceeds maximum: {} > {MAX_RECEIPTS_PER_REQUEST}",
                    receipts.len(),
                ),
            ));
        }

        let sequence_bounds = apm2_core::fac::projection_compromise::ReplaySequenceBoundsV1 {
            required_start_sequence: request.required_start_sequence,
            required_end_sequence: request.required_end_sequence,
        };

        // ---- PCAC lifecycle enforcement (admission BEFORE mutation) ----
        let Some(pcac_gate) = self.pcac_lifecycle_gate.as_deref() else {
            return Ok(PrivilegedResponse::error(
                PrivilegedErrorCode::CapabilityRequestRejected,
                "PCAC authority gate not wired for RegisterRecoveryEvidence (fail-closed)",
            ));
        };

        let (join_freshness_tick, join_time_envelope_ref, join_ledger_anchor, join_revocation_head) =
            match self.derive_privileged_pcac_revalidation_inputs(&request.lease_id) {
                Ok(values) => values,
                Err(error) => {
                    return Ok(PrivilegedResponse::error(
                        PrivilegedErrorCode::CapabilityRequestRejected,
                        format!(
                            "PCAC authority denied for RegisterRecoveryEvidence: \
                             authoritative revalidation unavailable: {error}"
                        ),
                    ));
                },
            };

        let (risk_tier, _resolved_policy_hash) =
            self.resolve_risk_tier_for_lease(&request.lease_id, digest);
        let pcac_risk_tier = Self::map_fac_risk_tier_to_pcac(risk_tier);

        let pcac_builder =
            PrivilegedPcacInputBuilder::new(PrivilegedHandlerClass::RegisterRecoveryEvidence)
                .session_id(request.freeze_id.clone())
                .lease_id(request.lease_id.clone())
                .boundary_intent_class(apm2_core::pcac::BoundaryIntentClass::Assert)
                .identity_proof_hash(digest)
                .identity_evidence_level(IdentityEvidenceLevel::PointerOnly)
                .risk_tier(pcac_risk_tier);

        let capability_manifest_hash = pcac_builder.hash(
            "capability",
            &[
                request.lease_id.as_bytes(),
                request.freeze_id.as_bytes(),
                &digest,
            ],
        );

        let scope_witness_hash = pcac_builder.hash(
            "scope",
            &[
                request.freeze_id.as_bytes(),
                &digest,
                &request.required_start_sequence.to_le_bytes(),
                &request.required_end_sequence.to_le_bytes(),
            ],
        );

        let freshness_policy_hash = pcac_builder.hash(
            "freshness-policy",
            &[request.lease_id.as_bytes(), request.freeze_id.as_bytes()],
        );

        let stop_budget_profile_digest = pcac_builder.hash(
            "stop-budget",
            &[
                &[u8::from(self.stop_authority.as_ref().is_some_and(
                    |authority| authority.emergency_stop_active(),
                ))],
                &[u8::from(self.stop_authority.as_ref().is_some_and(
                    |authority| authority.governance_stop_active(),
                ))],
                request.freeze_id.as_bytes(),
            ],
        );

        let effect_intent_digest = domain_tagged_hash(
            PrivilegedHandlerClass::RegisterRecoveryEvidence,
            "intent",
            &[
                request.lease_id.as_bytes(),
                request.freeze_id.as_bytes(),
                &digest,
                &request.required_start_sequence.to_le_bytes(),
                &request.required_end_sequence.to_le_bytes(),
            ],
        );

        let pcac_builder = pcac_builder
            .capability_manifest_hash(capability_manifest_hash)
            .scope_witness_hash(scope_witness_hash)
            .freshness_policy_hash(freshness_policy_hash)
            .stop_budget_profile_digest(stop_budget_profile_digest)
            .effect_intent_digest(effect_intent_digest);

        let pcac_input = pcac_builder.build(
            join_freshness_tick,
            join_time_envelope_ref,
            join_ledger_anchor,
            join_revocation_head,
        );

        let _pcac_lifecycle_artifacts = match self.enforce_privileged_pcac_lifecycle(
            PrivilegedHandlerClass::RegisterRecoveryEvidence.operation_name(),
            pcac_gate,
            &pcac_input,
            &request.lease_id,
            join_freshness_tick,
            join_time_envelope_ref,
            join_ledger_anchor,
            join_revocation_head,
            effect_intent_digest,
        ) {
            Ok(artifacts) => {
                if artifacts.is_none() {
                    return Ok(PrivilegedResponse::error(
                        PrivilegedErrorCode::CapabilityRequestRejected,
                        "RegisterRecoveryEvidence requires PCAC lifecycle evidence \
                         (mandatory cutover); lifecycle_enforcement is disabled in claim policy",
                    ));
                }
                artifacts
            },
            Err(response) => return Ok(response),
        };

        // ---- Mutation: only after PCAC lifecycle succeeds ----
        match watchdog.register_durable_recovery_evidence(
            &request.freeze_id,
            receipts,
            digest,
            sequence_bounds,
        ) {
            Ok(()) => Ok(PrivilegedResponse::RegisterRecoveryEvidence(
                RegisterRecoveryEvidenceResponse {
                    accepted: true,
                    freeze_id: request.freeze_id.clone(),
                    message: format!(
                        "durable recovery evidence registered for freeze_id={}",
                        request.freeze_id
                    ),
                },
            )),
            Err(e) => Ok(PrivilegedResponse::error(
                PrivilegedErrorCode::InvalidArgument,
                format!("recovery evidence registration failed: {e}"),
            )),
        }
    }

    /// Handles `RequestUnfreeze` requests (IPC-PRIV-075).
    ///
    /// Creates an unfreeze event for a frozen projection, verifies durable
    /// provenance, and applies the unfreeze to clear the freeze state.
    ///
    /// # Security
    ///
    /// - Requires privileged connection (checked in dispatch)
    /// - Validates the watchdog is configured (fail-closed if missing)
    /// - Delegates admission and signature checks to `DivergenceWatchdog`
    /// - Follows create -> apply pattern per watchdog contract
    fn handle_request_unfreeze(
        &self,
        payload: &[u8],
        ctx: &ConnectionContext,
    ) -> ProtocolResult<PrivilegedResponse> {
        let request = RequestUnfreezeRequest::decode_bounded(payload, &self.decode_config)
            .map_err(|e| ProtocolError::Serialization {
                reason: format!("invalid RequestUnfreezeRequest: {e}"),
            })?;

        // ---- Phase -1: Bind caller identity to authenticated peer ----
        let Some(peer_creds) = ctx.peer_credentials() else {
            return Ok(PrivilegedResponse::error(
                PrivilegedErrorCode::PermissionDenied,
                "peer credentials required for unfreeze request",
            ));
        };
        let _authenticated_caller_id = derive_actor_id(peer_creds);

        let watchdog =
            self.divergence_watchdog
                .as_ref()
                .ok_or_else(|| ProtocolError::Serialization {
                    reason: "divergence watchdog not configured".to_string(),
                })?;

        if request.freeze_id.is_empty() {
            return Ok(PrivilegedResponse::error(
                PrivilegedErrorCode::InvalidArgument,
                "freeze_id must not be empty",
            ));
        }

        if request.lease_id.is_empty() {
            return Ok(PrivilegedResponse::error(
                PrivilegedErrorCode::InvalidArgument,
                "lease_id is required for PCAC lifecycle enforcement",
            ));
        }
        if request.lease_id.len() > MAX_ID_LENGTH {
            return Ok(PrivilegedResponse::error(
                PrivilegedErrorCode::InvalidArgument,
                format!("lease_id exceeds maximum length of {MAX_ID_LENGTH} bytes"),
            ));
        }

        if request.resolution_type.is_empty() {
            return Ok(PrivilegedResponse::error(
                PrivilegedErrorCode::InvalidArgument,
                "resolution_type must not be empty",
            ));
        }

        let resolution_type = match request.resolution_type.as_str() {
            "ADJUDICATION" => crate::projection::ResolutionType::Adjudication,
            "MANUAL" => crate::projection::ResolutionType::Manual,
            "ROLLBACK" => crate::projection::ResolutionType::Rollback,
            "ACCEPT_DIVERGENCE" => crate::projection::ResolutionType::AcceptDivergence,
            other => {
                return Ok(PrivilegedResponse::error(
                    PrivilegedErrorCode::InvalidArgument,
                    format!(
                        "unknown resolution_type: {other}; expected one of: \
                         ADJUDICATION, MANUAL, ROLLBACK, ACCEPT_DIVERGENCE"
                    ),
                ));
            },
        };

        let adjudication_id = if request.adjudication_id.is_empty() {
            None
        } else {
            Some(request.adjudication_id.as_str())
        };

        // ---- PCAC lifecycle enforcement (admission BEFORE mutation) ----
        let Some(pcac_gate) = self.pcac_lifecycle_gate.as_deref() else {
            return Ok(PrivilegedResponse::error(
                PrivilegedErrorCode::CapabilityRequestRejected,
                "PCAC authority gate not wired for RequestUnfreeze (fail-closed)",
            ));
        };

        let (join_freshness_tick, join_time_envelope_ref, join_ledger_anchor, join_revocation_head) =
            match self.derive_privileged_pcac_revalidation_inputs(&request.lease_id) {
                Ok(values) => values,
                Err(error) => {
                    return Ok(PrivilegedResponse::error(
                        PrivilegedErrorCode::CapabilityRequestRejected,
                        format!(
                            "PCAC authority denied for RequestUnfreeze: \
                             authoritative revalidation unavailable: {error}"
                        ),
                    ));
                },
            };

        let freeze_id_hash = *blake3::hash(request.freeze_id.as_bytes()).as_bytes();
        let (risk_tier, _resolved_policy_hash) =
            self.resolve_risk_tier_for_lease(&request.lease_id, freeze_id_hash);
        let pcac_risk_tier = Self::map_fac_risk_tier_to_pcac(risk_tier);

        let pcac_builder = PrivilegedPcacInputBuilder::new(PrivilegedHandlerClass::RequestUnfreeze)
            .session_id(request.freeze_id.clone())
            .lease_id(request.lease_id.clone())
            .boundary_intent_class(apm2_core::pcac::BoundaryIntentClass::Assert)
            .identity_proof_hash(freeze_id_hash)
            .identity_evidence_level(IdentityEvidenceLevel::PointerOnly)
            .risk_tier(pcac_risk_tier);

        let capability_manifest_hash = pcac_builder.hash(
            "capability",
            &[
                request.lease_id.as_bytes(),
                request.freeze_id.as_bytes(),
                request.resolution_type.as_bytes(),
            ],
        );

        let scope_witness_hash = pcac_builder.hash(
            "scope",
            &[
                request.freeze_id.as_bytes(),
                request.resolution_type.as_bytes(),
                request.adjudication_id.as_bytes(),
            ],
        );

        let freshness_policy_hash = pcac_builder.hash(
            "freshness-policy",
            &[request.lease_id.as_bytes(), request.freeze_id.as_bytes()],
        );

        let stop_budget_profile_digest = pcac_builder.hash(
            "stop-budget",
            &[
                &[u8::from(self.stop_authority.as_ref().is_some_and(
                    |authority| authority.emergency_stop_active(),
                ))],
                &[u8::from(self.stop_authority.as_ref().is_some_and(
                    |authority| authority.governance_stop_active(),
                ))],
                request.freeze_id.as_bytes(),
            ],
        );

        let effect_intent_digest = domain_tagged_hash(
            PrivilegedHandlerClass::RequestUnfreeze,
            "intent",
            &[
                request.lease_id.as_bytes(),
                request.freeze_id.as_bytes(),
                request.resolution_type.as_bytes(),
                request.adjudication_id.as_bytes(),
            ],
        );

        let pcac_builder = pcac_builder
            .capability_manifest_hash(capability_manifest_hash)
            .scope_witness_hash(scope_witness_hash)
            .freshness_policy_hash(freshness_policy_hash)
            .stop_budget_profile_digest(stop_budget_profile_digest)
            .effect_intent_digest(effect_intent_digest);

        let pcac_input = pcac_builder.build(
            join_freshness_tick,
            join_time_envelope_ref,
            join_ledger_anchor,
            join_revocation_head,
        );

        let _pcac_lifecycle_artifacts = match self.enforce_privileged_pcac_lifecycle(
            PrivilegedHandlerClass::RequestUnfreeze.operation_name(),
            pcac_gate,
            &pcac_input,
            &request.lease_id,
            join_freshness_tick,
            join_time_envelope_ref,
            join_ledger_anchor,
            join_revocation_head,
            effect_intent_digest,
        ) {
            Ok(artifacts) => {
                if artifacts.is_none() {
                    return Ok(PrivilegedResponse::error(
                        PrivilegedErrorCode::CapabilityRequestRejected,
                        "RequestUnfreeze requires PCAC lifecycle evidence \
                         (mandatory cutover); lifecycle_enforcement is disabled in claim policy",
                    ));
                }
                artifacts
            },
            Err(response) => return Ok(response),
        };

        // ---- Mutation: only after PCAC lifecycle succeeds ----

        // Step 1: Create the unfreeze event (validates durable provenance)
        let unfreeze =
            match watchdog.create_unfreeze(&request.freeze_id, resolution_type, adjudication_id) {
                Ok(u) => u,
                Err(e) => {
                    return Ok(PrivilegedResponse::error(
                        PrivilegedErrorCode::InvalidArgument,
                        format!("unfreeze creation failed: {e}"),
                    ));
                },
            };

        // Step 2: Apply unfreeze to local registry (signature verified)
        match watchdog.apply_unfreeze(&unfreeze) {
            Ok(()) => Ok(PrivilegedResponse::RequestUnfreeze(
                RequestUnfreezeResponse {
                    success: true,
                    freeze_id: request.freeze_id.clone(),
                    message: format!(
                        "projection freeze lifted for freeze_id={}",
                        request.freeze_id
                    ),
                },
            )),
            Err(e) => Ok(PrivilegedResponse::error(
                PrivilegedErrorCode::InvalidArgument,
                format!("unfreeze apply failed: {e}"),
            )),
        }
    }

    /// Returns a reference to the token minter.
    ///
    /// # TCK-00287
    ///
    /// This is used to share the token minter with `SessionDispatcher`.
    #[must_use]
    pub const fn token_minter(&self) -> &Arc<TokenMinter> {
        &self.token_minter
    }

    /// Returns a reference to the manifest store.
    ///
    /// # TCK-00287
    ///
    /// This is used to share the manifest store with `SessionDispatcher`.
    #[must_use]
    pub const fn manifest_store(&self) -> &Arc<InMemoryManifestStore> {
        &self.manifest_store
    }

    /// Returns a reference to the HTF-compliant clock (TCK-00289).
    ///
    /// This is primarily for testing to verify clock behavior.
    #[must_use]
    pub const fn holonic_clock(&self) -> &Arc<HolonicClock> {
        &self.holonic_clock
    }

    /// Returns a reference to the subscription registry (TCK-00303).
    ///
    /// # TCK-00303
    ///
    /// This is used to share the subscription registry with
    /// `SessionDispatcher`.
    #[must_use]
    pub const fn subscription_registry(&self) -> &SharedSubscriptionRegistry {
        &self.subscription_registry
    }

    // =========================================================================
    // TCK-00289: HTF-Compliant Timestamp Generation
    // =========================================================================

    /// Returns an HTF-compliant timestamp in nanoseconds since epoch.
    ///
    /// Per RFC-0016, all timestamps must come from the `HolonicClock` to
    /// ensure:
    /// - Monotonicity: Timestamps never regress within a process lifetime
    /// - Causality: HLC provides cross-node causal ordering
    /// - Determinism: Clock source is injectable for replay scenarios
    ///
    /// # Returns
    ///
    /// The current HLC wall time in nanoseconds since epoch. This is a u64
    /// value representing hybrid logical clock time, suitable for ledger
    /// event timestamps and capability grant/expiry times.
    ///
    /// # Panics
    ///
    /// This method expects the `HolonicClock` to have HLC enabled. If HLC is
    /// not enabled, this returns an error (fail-closed).
    ///
    /// # Errors
    ///
    /// Returns `HtfTimestampError` if the clock operation fails.
    ///
    /// # Security (Fail-Closed)
    ///
    /// Per RFC-0016 and TCK-00289 DOD, this method fails closed rather than
    /// returning a fallback value like 0. Returning 0 would violate security
    /// policy and allow operations to proceed with invalid timestamps.
    fn get_htf_timestamp_ns(&self) -> Result<u64, HtfTimestampError> {
        match self.holonic_clock.now_hlc() {
            Ok(hlc) => Ok(hlc.wall_ns),
            Err(e) => {
                // TCK-00289: Fail-closed - do not return 0 as fallback.
                // This is a security-critical operation that must not proceed
                // with invalid timestamps.
                warn!(error = %e, "HLC clock error - failing closed per RFC-0016");
                Err(HtfTimestampError::ClockError {
                    message: e.to_string(),
                })
            },
        }
    }

    /// Resolves the risk tier and policy hash associated with a lease.
    ///
    /// Fail-closed behavior:
    /// - Missing lease->work mapping => Tier4, fallback policy hash.
    /// - Missing work claim => Tier4, fallback policy hash.
    /// - Invalid risk tier value => Tier4, keep resolved policy hash.
    #[allow(clippy::option_if_let_else)]
    fn resolve_risk_tier_for_lease(
        &self,
        lease_id: &str,
        fallback_policy_hash: [u8; 32],
    ) -> (RiskTier, [u8; 32]) {
        if let Some(work_id) = self.lease_validator.get_lease_work_id(lease_id) {
            if let Some(claim) = self.work_registry.get_claim(&work_id) {
                let tier = RiskTier::try_from(claim.policy_resolution.resolved_risk_tier)
                    .unwrap_or_else(|_| {
                        warn!(
                            lease_id = %lease_id,
                            work_id = %work_id,
                            tier_value = claim.policy_resolution.resolved_risk_tier,
                            "invalid risk tier value in policy resolution — \
                             defaulting to Tier4 (fail-closed)"
                        );
                        RiskTier::Tier4
                    });
                (tier, claim.policy_resolution.resolved_policy_hash)
            } else {
                warn!(
                    lease_id = %lease_id,
                    work_id = %work_id,
                    "work claim not found for lease — defaulting to Tier4 (fail-closed)"
                );
                (RiskTier::Tier4, fallback_policy_hash)
            }
        } else {
            warn!(
                lease_id = %lease_id,
                "could not resolve work_id from lease — defaulting to Tier4 (fail-closed)"
            );
            (RiskTier::Tier4, fallback_policy_hash)
        }
    }

    /// Maps FAC risk tiers to PCAC risk tiers.
    #[allow(clippy::missing_const_for_fn)]
    fn map_fac_risk_tier_to_pcac(risk_tier: RiskTier) -> PcacRiskTier {
        match risk_tier {
            RiskTier::Tier0 => PcacRiskTier::Tier0,
            RiskTier::Tier1 => PcacRiskTier::Tier1,
            RiskTier::Tier2 | RiskTier::Tier3 | RiskTier::Tier4 => PcacRiskTier::Tier2Plus,
        }
    }

    /// Derive a fail-closed PCAC ledger anchor from the validated chain hash.
    fn derive_pcac_ledger_anchor(&self) -> Result<[u8; 32], String> {
        let mut hasher = blake3::Hasher::new();
        hasher.update(b"pcac-privileged-ledger-anchor-v1");
        let count = self.event_emitter.get_event_count() as u64;
        hasher.update(&count.to_le_bytes());
        let chain_hash = self
            .event_emitter
            .derive_event_chain_hash()
            .map_err(|e| format!("ledger hash-chain validation failed: {e}"))?;
        hasher.update(chain_hash.as_bytes());
        Ok(*hasher.finalize().as_bytes())
    }

    /// Derives fresh revalidation inputs for privileged PCAC lifecycle checks.
    ///
    /// Fail-closed: missing lease/work/policy bindings return an error.
    fn derive_privileged_pcac_revalidation_inputs(
        &self,
        lease_id: &str,
    ) -> Result<PrivilegedPcacRevalidationInputs, String> {
        let hlc = self
            .holonic_clock
            .now_hlc()
            .map_err(|e| format!("clock read failed: {e}"))?;
        let freshness_witness_tick = hlc.wall_ns / 1_000_000_000;
        if freshness_witness_tick == 0 {
            return Err("freshness witness tick is zero".to_string());
        }

        let current_time_envelope_ref = *blake3::hash(&hlc.wall_ns.to_le_bytes()).as_bytes();
        let current_ledger_anchor = self.derive_pcac_ledger_anchor()?;

        let work_id = self
            .lease_validator
            .get_lease_work_id(lease_id)
            .ok_or_else(|| format!("work_id missing for lease '{lease_id}'"))?;
        let claim = self.work_registry.get_claim(&work_id).ok_or_else(|| {
            format!("work claim missing for lease '{lease_id}' and work '{work_id}'")
        })?;
        if claim.policy_resolution.policy_resolved_ref.is_empty() {
            return Err(format!(
                "policy_resolved_ref missing for lease '{lease_id}' and work '{work_id}'"
            ));
        }

        let current_revocation_head =
            *blake3::hash(claim.policy_resolution.policy_resolved_ref.as_bytes()).as_bytes();
        Ok((
            freshness_witness_tick,
            current_time_envelope_ref,
            current_ledger_anchor,
            current_revocation_head,
        ))
    }

    fn map_pcac_deny_to_privileged_response(
        operation: &str,
        deny: &apm2_core::pcac::AuthorityDenyV1,
    ) -> PrivilegedResponse {
        PrivilegedResponse::error(
            PrivilegedErrorCode::CapabilityRequestRejected,
            format!("PCAC authority denied for {operation}: {}", deny.deny_class),
        )
    }

    /// Enforces join -> revalidate -> revalidate-before-execution -> consume
    /// for privileged handlers before authoritative mutation.
    #[allow(clippy::result_large_err)]
    #[allow(clippy::too_many_arguments)]
    fn enforce_privileged_pcac_lifecycle(
        &self,
        operation: &str,
        gate: &crate::pcac::LifecycleGate,
        join_input: &AuthorityJoinInputV1,
        lease_id: &str,
        join_freshness_tick: u64,
        join_time_envelope_ref: [u8; 32],
        join_ledger_anchor: [u8; 32],
        join_revocation_head: [u8; 32],
        effect_intent_digest: [u8; 32],
    ) -> Result<Option<PrivilegedPcacLifecycleArtifacts>, PrivilegedResponse> {
        let work_id = self
            .lease_validator
            .get_lease_work_id(lease_id)
            .ok_or_else(|| {
                PrivilegedResponse::error(
                    PrivilegedErrorCode::CapabilityRequestRejected,
                    format!(
                        "PCAC authority denied for {operation}: policy state missing for lease '{lease_id}'"
                    ),
                )
            })?;
        let claim = self.work_registry.get_claim(&work_id).ok_or_else(|| {
            PrivilegedResponse::error(
                PrivilegedErrorCode::CapabilityRequestRejected,
                format!(
                    "PCAC authority denied for {operation}: work claim missing for policy lookup \
                     (lease='{lease_id}', work_id='{work_id}')"
                ),
            )
        })?;

        let mut pcac_policy = claim.policy_resolution.pcac_policy.clone().ok_or_else(|| {
            PrivilegedResponse::error(
                PrivilegedErrorCode::CapabilityRequestRejected,
                format!(
                    "PCAC authority denied for {operation}: unknown policy state \
                             (pcac_policy missing for work_id='{work_id}')"
                ),
            )
        })?;
        if pcac_policy.pointer_only_waiver.is_none() {
            pcac_policy
                .pointer_only_waiver
                .clone_from(&claim.policy_resolution.pointer_only_waiver);
        }
        if !pcac_policy.lifecycle_enforcement {
            return Ok(None);
        }

        let mut effective_join_input = join_input.clone();
        if matches!(
            effective_join_input.identity_evidence_level,
            IdentityEvidenceLevel::PointerOnly
        ) && matches!(effective_join_input.risk_tier, PcacRiskTier::Tier2Plus)
            && effective_join_input.pointer_only_waiver_hash.is_none()
        {
            effective_join_input.pointer_only_waiver_hash = pcac_policy
                .pointer_only_waiver
                .as_ref()
                .map(apm2_core::pcac::PointerOnlyWaiver::content_hash);
        }

        gate.advance_tick(join_freshness_tick);

        let certificate = gate
            .join_and_revalidate(
                &effective_join_input,
                join_time_envelope_ref,
                join_ledger_anchor,
                join_revocation_head,
                &pcac_policy,
            )
            .map_err(|deny| {
                warn!(
                    operation = %operation,
                    lease_id = %lease_id,
                    deny_class = %deny.deny_class,
                    "Privileged handler denied by PCAC join/revalidate gate"
                );
                Self::map_pcac_deny_to_privileged_response(operation, &deny)
            })?;

        let (fresh_tick, current_time_envelope_ref, current_ledger_anchor, current_revocation_head) =
            self.derive_privileged_pcac_revalidation_inputs(lease_id)
                .map_err(|error| {
                    PrivilegedResponse::error(
                        PrivilegedErrorCode::CapabilityRequestRejected,
                        format!(
                            "PCAC authority denied for {operation}: \
                         authoritative revalidation unavailable: {error}"
                        ),
                    )
                })?;

        gate.advance_tick(fresh_tick);

        gate.revalidate_before_execution(
            &certificate,
            current_time_envelope_ref,
            current_ledger_anchor,
            current_revocation_head,
            &pcac_policy,
        )
        .map_err(|deny| {
            warn!(
                operation = %operation,
                lease_id = %lease_id,
                deny_class = %deny.deny_class,
                "Privileged handler denied by PCAC revalidate-before-execution gate"
            );
            Self::map_pcac_deny_to_privileged_response(operation, &deny)
        })?;

        if !bool::from(certificate.intent_digest.ct_eq(&effect_intent_digest)) {
            let deny_class = AuthorityDenyClass::IntentDigestMismatch {
                expected: certificate.intent_digest,
                actual: effect_intent_digest,
            };
            warn!(
                operation = %operation,
                lease_id = %lease_id,
                expected_intent = %hex::encode(certificate.intent_digest),
                actual_intent = %hex::encode(effect_intent_digest),
                deny_class = %deny_class,
                "Privileged handler denied by PCAC consume prerequisite"
            );
            return Err(PrivilegedResponse::error(
                PrivilegedErrorCode::CapabilityRequestRejected,
                format!("PCAC authority denied for {operation}: {deny_class}"),
            ));
        }

        let (consumed_witness, consume_record) = gate
            .consume_before_effect(
                &certificate,
                effect_intent_digest,
                effective_join_input.boundary_intent_class,
                true,
                current_time_envelope_ref,
                current_revocation_head,
                &pcac_policy,
            )
            .map_err(|deny| {
                warn!(
                    operation = %operation,
                    lease_id = %lease_id,
                    deny_class = %deny.deny_class,
                    "Privileged handler denied by PCAC consume-before-effect gate"
                );
                Self::map_pcac_deny_to_privileged_response(operation, &deny)
            })?;

        Ok(Some(PrivilegedPcacLifecycleArtifacts {
            ajc_id: certificate.ajc_id,
            intent_digest: effect_intent_digest,
            consume_tick: consumed_witness.consumed_at_tick,
            time_envelope_ref: consumed_witness.consumed_time_envelope_ref,
            consume_selector_digest: consume_record.effect_selector_digest,
        }))
    }

    /// Returns whether a resolved clock profile is admissible for a risk tier.
    ///
    /// Policy is fail-closed and intentionally conservative:
    /// - Tier0/Tier1: broad profile source compatibility.
    /// - Tier2: disallow weaker wall-time sources (`BEST_EFFORT_NTP`,
    ///   `MANUAL_OPERATOR`), attestation optional.
    /// - Tier3/Tier4: same source restrictions as Tier2 and require attestation
    ///   presence.
    #[allow(clippy::missing_const_for_fn)]
    fn is_clock_profile_admissible_for_risk_tier(
        risk_tier: RiskTier,
        profile: &ClockProfile,
    ) -> bool {
        let source_allowed = match risk_tier {
            RiskTier::Tier0 | RiskTier::Tier1 => matches!(
                profile.wall_time_source,
                WallTimeSource::None
                    | WallTimeSource::BestEffortNtp
                    | WallTimeSource::AuthenticatedNts
                    | WallTimeSource::Roughtime
                    | WallTimeSource::CloudBounded
            ),
            RiskTier::Tier2 | RiskTier::Tier3 | RiskTier::Tier4 => matches!(
                profile.wall_time_source,
                WallTimeSource::None
                    | WallTimeSource::AuthenticatedNts
                    | WallTimeSource::Roughtime
                    | WallTimeSource::CloudBounded
            ),
        };

        if !source_allowed {
            return false;
        }

        match risk_tier {
            RiskTier::Tier0 | RiskTier::Tier1 | RiskTier::Tier2 => true,
            RiskTier::Tier3 | RiskTier::Tier4 => profile.attestation.is_some(),
        }
    }

    /// Validates authoritative HTF binding for a lease's `time_envelope_ref`.
    ///
    /// This enforces:
    /// 1. `time_envelope_ref` is present and hex-decodable to a CAS hash.
    /// 2. Envelope exists in CAS and deserializes to `TimeEnvelope`.
    /// 3. Envelope canonical hash matches the referenced hash.
    /// 4. `clock_profile_hash` is pinned, CAS-resolvable, and canonical-hash
    ///    verified.
    /// 5. Clock profile is admissible for the resolved risk tier.
    fn validate_lease_time_authority(
        &self,
        lease: &apm2_core::fac::GateLease,
        risk_tier: RiskTier,
    ) -> Result<(), String> {
        if lease.time_envelope_ref.is_empty() {
            return Err("lease time_envelope_ref is missing".to_string());
        }

        let envelope_hash = decode_hash32_hex("time_envelope_ref", &lease.time_envelope_ref)?;
        let cas = self
            .cas
            .as_ref()
            .ok_or_else(|| "CAS is not configured; cannot resolve time_envelope_ref".to_string())?;

        let envelope_bytes = cas
            .retrieve(&envelope_hash)
            .map_err(|e| format!("failed to resolve time_envelope_ref from CAS: {e}"))?;

        let envelope: TimeEnvelope = serde_json::from_slice(&envelope_bytes)
            .map_err(|e| format!("time_envelope_ref payload is not TimeEnvelopeV1 JSON: {e}"))?;

        let envelope_canonical_hash = envelope
            .canonical_hash()
            .map_err(|e| format!("failed to canonicalize time envelope: {e}"))?;
        if !bool::from(envelope_canonical_hash.ct_eq(&envelope_hash)) {
            return Err(format!(
                "time_envelope_ref hash mismatch: ref={} actual={}",
                hex::encode(envelope_hash),
                hex::encode(envelope_canonical_hash)
            ));
        }

        let clock_profile_hash =
            decode_hash32_hex("clock_profile_hash", &envelope.clock_profile_hash)?;
        let clock_profile_bytes = cas
            .retrieve(&clock_profile_hash)
            .map_err(|e| format!("failed to resolve clock_profile_hash from CAS: {e}"))?;
        let clock_profile: ClockProfile = serde_json::from_slice(&clock_profile_bytes)
            .map_err(|e| format!("clock_profile_hash payload is not ClockProfileV1 JSON: {e}"))?;

        let profile_canonical_hash = clock_profile
            .canonical_hash()
            .map_err(|e| format!("failed to canonicalize clock profile: {e}"))?;
        if !bool::from(profile_canonical_hash.ct_eq(&clock_profile_hash)) {
            return Err(format!(
                "clock_profile_hash mismatch: envelope={} actual={}",
                hex::encode(clock_profile_hash),
                hex::encode(profile_canonical_hash)
            ));
        }

        if !Self::is_clock_profile_admissible_for_risk_tier(risk_tier, &clock_profile) {
            return Err(format!(
                "clock_profile_hash {} is not admissible for risk tier {:?}",
                envelope.clock_profile_hash, risk_tier
            ));
        }

        Ok(())
    }

    // =========================================================================
    // TCK-00258: Custody Domain Resolution (`SoD` Enforcement)
    // =========================================================================

    /// Resolves custody domains for an actor.
    ///
    /// Per TCK-00258, this method maps an actor ID to its custody domains
    /// for `SoD` validation. In production, this would query the `KeyPolicy`
    /// via a `CustodyDomainResolver` trait.
    ///
    /// # Stub Implementation
    ///
    /// Currently returns a single domain derived from the actor ID prefix.
    /// For example, `team-alpha:alice` -> `["team-alpha"]`.
    /// This enables testing of the `SoD` enforcement without full `KeyPolicy`
    /// integration.
    ///
    /// # Security (Fail-Closed)
    ///
    /// Returns an error if the actor ID doesn't match the expected
    /// `domain:actor` schema. This ensures that malformed or non-standard
    /// actor IDs cannot bypass `SoD` validation.
    #[allow(clippy::unused_self)] // Will use self in production for registry access
    fn resolve_actor_custody_domains(&self, actor_id: &str) -> Result<Vec<String>, String> {
        // Stub: derive domain from actor_id prefix (e.g., "team-alpha:alice" ->
        // "team-alpha") In production, this would query
        // KeyPolicy.custody_domains
        if let Some(colon_pos) = actor_id.find(':') {
            let domain = &actor_id[..colon_pos];
            if !domain.is_empty() {
                return Ok(vec![domain.to_string()]);
            }
        }
        // SEC-SoD-001: Fail-closed for malformed actor IDs.
        // If the actor_id doesn't match expected schema (domain:actor), return
        // an error. This prevents attackers from bypassing SoD by using
        // non-standard IDs.
        Err(format!(
            "malformed actor_id: {actor_id} (expected domain:actor)"
        ))
    }

    /// Resolves custody domains for changeset authors.
    ///
    /// Per TCK-00258, this method retrieves the custody domains of all actors
    /// who authored the changeset being reviewed. In production, this would
    /// query the changeset metadata and `KeyPolicy`.
    ///
    /// # Stub Implementation
    ///
    /// Currently returns a placeholder domain based on the `work_id`.
    /// For testing, set `work_id` to include domain information:
    /// e.g., `W-team-alpha-12345` -> `["team-alpha"]`
    ///
    /// # Security (Fail-Closed)
    ///
    /// Returns an error if the `work_id` doesn't match the expected schema.
    /// This ensures that malformed work IDs cannot bypass `SoD` validation.
    #[allow(clippy::unused_self)] // Will use self in production for registry access
    fn resolve_changeset_author_domains(&self, work_id: &str) -> Result<Vec<String>, String> {
        // Stub: derive author domain from work_id (e.g., "W-team-alpha-12345" ->
        // ["team-alpha"]) In production, this would query changeset metadata
        // for author list, then resolve each author's custody domains via
        // KeyPolicy
        if let Some(rest) = work_id.strip_prefix("W-") {
            if let Some(dash_pos) = rest.find('-') {
                let domain = &rest[..dash_pos];
                if !domain.is_empty() {
                    return Ok(vec![domain.to_string()]);
                }
            }
        }
        // SEC-SoD-001: Fail-closed for malformed work_ids.
        // Return an error if the work_id doesn't match the expected schema.
        // This prevents attackers from bypassing SoD by using malformed work_ids
        // that don't map to author domains.
        Err(format!(
            "malformed work_id: {work_id} (expected W-domain-suffix)"
        ))
    }

    /// Dispatches a privileged request to the appropriate handler.
    ///
    /// # Message Format
    ///
    /// The frame format is: [tag: u8][payload: protobuf]
    /// Where tag identifies the message type (see [`PrivilegedMessageType`]).
    ///
    /// # Security
    ///
    /// 1. Validates `ctx.is_privileged()` FIRST
    /// 2. Returns `PERMISSION_DENIED` immediately for non-privileged
    ///    connections
    /// 3. Only then decodes and routes the message
    ///
    /// # Errors
    ///
    /// Returns `Err` for protocol-level errors (malformed frames, etc.).
    /// Application-level errors are returned in [`PrivilegedResponse::Error`].
    pub fn dispatch(
        &self,
        frame: &Bytes,
        ctx: &ConnectionContext,
    ) -> ProtocolResult<PrivilegedResponse> {
        // TCK-00349: Check session phase BEFORE any message processing.
        // No authority-bearing IPC is permitted before SessionOpen.
        if !ctx.phase().allows_dispatch() {
            warn!(
                phase = %ctx.phase(),
                "Dispatch rejected: connection not in SessionOpen phase"
            );
            return Ok(PrivilegedResponse::error(
                PrivilegedErrorCode::PermissionDenied,
                format!(
                    "dispatch rejected: connection is in {} phase, not SessionOpen",
                    ctx.phase()
                ),
            ));
        }

        // INV-0001: Check privilege BEFORE any message processing
        if !ctx.is_privileged() {
            // TH-004: Generic error prevents endpoint enumeration
            debug!(
                peer_pid = ?ctx.peer_credentials().map(|c| c.pid),
                "Non-privileged connection attempted privileged endpoint"
            );
            return Ok(PrivilegedResponse::permission_denied());
        }

        // Validate frame has at least a tag byte
        if frame.is_empty() {
            return Err(ProtocolError::Serialization {
                reason: "empty frame".to_string(),
            });
        }

        let tag = frame[0];
        let payload = &frame[1..];

        // Route based on message type
        let msg_type =
            PrivilegedMessageType::from_tag(tag).ok_or_else(|| ProtocolError::Serialization {
                reason: format!("unknown privileged message type: {tag}"),
            })?;

        let result = match msg_type {
            PrivilegedMessageType::ClaimWork => self.handle_claim_work(payload, ctx),
            PrivilegedMessageType::SpawnEpisode => self.handle_spawn_episode(payload, ctx),
            PrivilegedMessageType::IssueCapability => self.handle_issue_capability(payload, ctx),
            PrivilegedMessageType::Shutdown => self.handle_shutdown(payload, ctx),
            // Process Management (TCK-00342)
            PrivilegedMessageType::ListProcesses => self.handle_list_processes(payload),
            PrivilegedMessageType::ProcessStatus => self.handle_process_status(payload),
            PrivilegedMessageType::StartProcess => self.handle_start_process(payload),
            PrivilegedMessageType::StopProcess => self.handle_stop_process(payload),
            PrivilegedMessageType::RestartProcess => self.handle_restart_process(payload),
            PrivilegedMessageType::ReloadProcess => self.handle_reload_process(payload),
            // Consensus Query Endpoints (TCK-00345)
            PrivilegedMessageType::ConsensusStatus => self.handle_consensus_status(payload),
            PrivilegedMessageType::ConsensusValidators => self.handle_consensus_validators(payload),
            PrivilegedMessageType::ConsensusByzantineEvidence => {
                self.handle_consensus_byzantine_evidence(payload)
            },
            PrivilegedMessageType::ConsensusMetrics => self.handle_consensus_metrics(payload),
            // TCK-00344: Work status query
            PrivilegedMessageType::WorkStatus => self.handle_work_status(payload, ctx),
            // TCK-00415: Projection-backed work listing
            PrivilegedMessageType::WorkList => self.handle_work_list(payload, ctx),
            // TCK-00452: Launch lineage projection for auditor consumers
            PrivilegedMessageType::AuditorLaunchProjection => {
                self.handle_auditor_launch_projection(payload, ctx)
            },
            // TCK-00452: Launch liveness projection for orchestrator consumers
            PrivilegedMessageType::OrchestratorLaunchProjection => {
                self.handle_orchestrator_launch_projection(payload, ctx)
            },
            // TCK-00395: EndSession for session termination with ledger event
            PrivilegedMessageType::EndSession => self.handle_end_session(payload, ctx),
            // TCK-00389: IngestReviewReceipt for external reviewer results
            PrivilegedMessageType::IngestReviewReceipt => {
                self.handle_ingest_review_receipt(payload, ctx)
            },
            // TCK-00351: Runtime stop-flag mutation
            PrivilegedMessageType::UpdateStopFlags => self.handle_update_stop_flags(payload, ctx),
            // Credential Management (CTR-PROTO-012, TCK-00343)
            PrivilegedMessageType::ListCredentials => self.handle_list_credentials(payload, ctx),
            PrivilegedMessageType::AddCredential => self.handle_add_credential(payload, ctx),
            PrivilegedMessageType::RemoveCredential => self.handle_remove_credential(payload, ctx),
            PrivilegedMessageType::RefreshCredential => {
                self.handle_refresh_credential(payload, ctx)
            },
            PrivilegedMessageType::SwitchCredential => self.handle_switch_credential(payload, ctx),
            PrivilegedMessageType::LoginCredential => self.handle_login_credential(payload, ctx),
            // HEF Pulse Plane (TCK-00302): Operator subscription handlers
            PrivilegedMessageType::SubscribePulse => self.handle_subscribe_pulse(payload, ctx),
            PrivilegedMessageType::UnsubscribePulse => self.handle_unsubscribe_pulse(payload, ctx),
            // PulseEvent is server-to-client only, reject if received from client
            PrivilegedMessageType::PulseEvent => Ok(PrivilegedResponse::error(
                PrivilegedErrorCode::PermissionDenied,
                "PulseEvent is server-to-client only",
            )),
            // TCK-00394: ChangeSet publishing for daemon-anchored submission
            PrivilegedMessageType::PublishChangeSet => self.handle_publish_changeset(payload, ctx),
            // TCK-00340: Sublease delegation for child holon authorization
            PrivilegedMessageType::DelegateSublease => self.handle_delegate_sublease(payload, ctx),
            // TCK-00487: explicit full-chain operator verification
            PrivilegedMessageType::VerifyLedgerChain => self.handle_verify_ledger_chain(payload),
            // TCK-00469: Projection recovery handlers
            PrivilegedMessageType::RegisterRecoveryEvidence => {
                self.handle_register_recovery_evidence(payload, ctx)
            },
            PrivilegedMessageType::RequestUnfreeze => self.handle_request_unfreeze(payload, ctx),
        };

        // TCK-00268: Emit IPC request completion metrics
        if let Some(ref metrics) = self.metrics {
            let endpoint = match msg_type {
                PrivilegedMessageType::ClaimWork => "ClaimWork",
                PrivilegedMessageType::SpawnEpisode => "SpawnEpisode",
                PrivilegedMessageType::IssueCapability => "IssueCapability",
                PrivilegedMessageType::Shutdown => "Shutdown",
                // Process Management (TCK-00342)
                PrivilegedMessageType::ListProcesses => "ListProcesses",
                PrivilegedMessageType::ProcessStatus => "ProcessStatus",
                PrivilegedMessageType::StartProcess => "StartProcess",
                PrivilegedMessageType::StopProcess => "StopProcess",
                PrivilegedMessageType::RestartProcess => "RestartProcess",
                PrivilegedMessageType::ReloadProcess => "ReloadProcess",
                // Consensus Query Endpoints (TCK-00345)
                PrivilegedMessageType::ConsensusStatus => "ConsensusStatus",
                PrivilegedMessageType::ConsensusValidators => "ConsensusValidators",
                PrivilegedMessageType::ConsensusByzantineEvidence => "ConsensusByzantineEvidence",
                PrivilegedMessageType::ConsensusMetrics => "ConsensusMetrics",
                // TCK-00344
                PrivilegedMessageType::WorkStatus => "WorkStatus",
                // TCK-00415
                PrivilegedMessageType::WorkList => "WorkList",
                // TCK-00452
                PrivilegedMessageType::AuditorLaunchProjection => "AuditorLaunchProjection",
                PrivilegedMessageType::OrchestratorLaunchProjection => {
                    "OrchestratorLaunchProjection"
                },
                // TCK-00395
                PrivilegedMessageType::EndSession => "EndSession",
                // TCK-00389
                PrivilegedMessageType::IngestReviewReceipt => "IngestReviewReceipt",
                // TCK-00351
                PrivilegedMessageType::UpdateStopFlags => "UpdateStopFlags",
                // Credential Management (CTR-PROTO-012, TCK-00343)
                PrivilegedMessageType::ListCredentials => "ListCredentials",
                PrivilegedMessageType::AddCredential => "AddCredential",
                PrivilegedMessageType::RemoveCredential => "RemoveCredential",
                PrivilegedMessageType::RefreshCredential => "RefreshCredential",
                PrivilegedMessageType::SwitchCredential => "SwitchCredential",
                PrivilegedMessageType::LoginCredential => "LoginCredential",
                // HEF Pulse Plane (TCK-00300)
                PrivilegedMessageType::SubscribePulse => "SubscribePulse",
                PrivilegedMessageType::UnsubscribePulse => "UnsubscribePulse",
                PrivilegedMessageType::PulseEvent => "PulseEvent",
                // TCK-00394
                PrivilegedMessageType::PublishChangeSet => "PublishChangeSet",
                // TCK-00340
                PrivilegedMessageType::DelegateSublease => "DelegateSublease",
                // TCK-00487
                PrivilegedMessageType::VerifyLedgerChain => "VerifyLedgerChain",
                // TCK-00469
                PrivilegedMessageType::RegisterRecoveryEvidence => "RegisterRecoveryEvidence",
                PrivilegedMessageType::RequestUnfreeze => "RequestUnfreeze",
            };
            let status = match &result {
                Ok(PrivilegedResponse::Error(_)) => "error",
                Ok(_) => "success",
                Err(_) => "protocol_error",
            };
            metrics
                .daemon_metrics()
                .ipc_request_completed(endpoint, status);
        }

        result
    }

    /// Handles `ClaimWork` requests (IPC-PRIV-001).
    ///
    /// # TCK-00253 Implementation
    ///
    /// This handler implements the work claim flow per DD-001 and DD-002:
    ///
    /// 1. Validate request structure
    /// 2. Derive authoritative `actor_id` from credential (not user input)
    /// 3. Query governance for `PolicyResolvedForChangeSet`
    /// 4. Mint capability manifest based on resolved policy
    /// 5. Register work claim in registry
    /// 6. Return work assignment
    ///
    /// # Security
    ///
    /// Per DD-001: The `actor_id` in the request is a display hint only.
    /// The authoritative `actor_id` is derived from the peer credential.
    fn handle_claim_work(
        &self,
        payload: &[u8],
        ctx: &ConnectionContext,
    ) -> ProtocolResult<PrivilegedResponse> {
        let request =
            ClaimWorkRequest::decode_bounded(payload, &self.decode_config).map_err(|e| {
                ProtocolError::Serialization {
                    reason: format!("invalid ClaimWorkRequest: {e}"),
                }
            })?;

        let role = WorkRole::try_from(request.role).unwrap_or(WorkRole::Unspecified);

        info!(
            actor_id_hint = %request.actor_id,
            role = ?role,
            peer_pid = ?ctx.peer_credentials().map(|c| c.pid),
            "ClaimWork request received"
        );

        // Validate role is specified
        if role == WorkRole::Unspecified {
            return Ok(PrivilegedResponse::error(
                PrivilegedErrorCode::CapabilityRequestRejected,
                "role is required",
            ));
        }

        // TCK-00253: Derive authoritative actor_id from credential (not user input)
        // Per DD-001: "actor_id is a display hint; authoritative actor_id derived from
        // credential"
        let peer_creds = ctx
            .peer_credentials()
            .ok_or_else(|| ProtocolError::Serialization {
                reason: "peer credentials required for work claim".to_string(),
            })?;

        let actor_id = derive_actor_id(peer_creds);

        debug!(
            actor_id_hint = %request.actor_id,
            derived_actor_id = %actor_id,
            "Actor ID derived from credential"
        );

        // Generate unique work and lease IDs
        let work_id = generate_work_id();
        let lease_id = generate_lease_id();

        // TCK-00253: Query governance for policy resolution
        // Per DD-002: "Daemon calls HOLON-KERNEL-GOVERNANCE for policy resolution"
        let mut policy_resolution = match self
            .policy_resolver
            .resolve_for_claim(&work_id, role, &actor_id)
        {
            Ok(resolution) => resolution,
            Err(e) => {
                if matches!(&e, PolicyResolutionError::GovernanceFailed { .. }) {
                    // Governance resolver reported a governance-side failure
                    // (transport/communication/service error), so this
                    // qualifies as a governance probe failure signal.
                    self.record_governance_probe_failure();
                }
                warn!(error = %e, "Policy resolution failed");
                // Return application-level error, not protocol error
                // Policy resolution failures are logic errors, not serialization errors
                return Ok(PrivilegedResponse::error(
                    PrivilegedErrorCode::PolicyResolutionFailed,
                    format!("policy resolution failed: {e}"),
                ));
            },
        };

        // Successful policy resolution is a direct governance-path health
        // signal, so refresh the governance probe watermark.
        self.record_governance_probe_success();

        // TCK-00448: Resolve authoritative role/context lineage from CAS.
        // Fail-closed on missing/unverifiable authority context.
        {
            let fallback_cas = MemoryCas::default();
            let cas: &dyn ContentAddressedStore = if let Some(ref c) = self.cas {
                c.as_ref()
            } else {
                &fallback_cas
            };

            let role_spec_hash = match resolve_workobject_role_spec_hash(cas) {
                Ok(hash) => hash,
                Err(e) => {
                    warn!(
                        work_id = %work_id,
                        error = %e,
                        "ClaimWork rejected: role spec hash is not CAS-resolvable"
                    );
                    return Ok(deny_response_for_authority_context(
                        DenyCondition::UnverifiableContextHash,
                        format!("role spec resolution failed: {e}"),
                    ));
                },
            };
            if bool::from(role_spec_hash.ct_eq(&[0u8; 32])) {
                warn!(
                    work_id = %work_id,
                    "ClaimWork rejected: resolved role spec hash is zero"
                );
                return Ok(deny_response_for_authority_context(
                    DenyCondition::MissingAuthorityContext,
                    "resolved role_spec_hash is zero",
                ));
            }
            policy_resolution.role_spec_hash = role_spec_hash;

            let context_compile_started = std::time::Instant::now();
            let context_pack_recipe_hash = match policy_context_pack_recipe_hash(
                &work_id,
                &actor_id,
                role_spec_hash,
                policy_resolution.context_pack_hash,
            ) {
                Ok(hash) => hash,
                Err(e) => {
                    warn!(
                        work_id = %work_id,
                        error = %e,
                        "ClaimWork rejected: context pack recipe hash derivation failed"
                    );
                    return Ok(deny_response_for_authority_context(
                        DenyCondition::UnverifiableContextHash,
                        format!("context recipe derivation failed: {e}"),
                    ));
                },
            };
            if bool::from(context_pack_recipe_hash.ct_eq(&[0u8; 32])) {
                warn!(
                    work_id = %work_id,
                    "ClaimWork rejected: context pack recipe hash is zero"
                );
                return Ok(deny_response_for_authority_context(
                    DenyCondition::MissingAuthorityContext,
                    "resolved context_pack_recipe_hash is zero",
                ));
            }
            policy_resolution.context_pack_recipe_hash = context_pack_recipe_hash;
            let context_compile_elapsed_ns =
                u64::try_from(context_compile_started.elapsed().as_nanos()).unwrap_or(u64::MAX);
            if let Ok(measurement_timestamp_ns) = self.get_htf_timestamp_ns() {
                let mut measurement_input = Vec::with_capacity(96);
                measurement_input.extend_from_slice(&role_spec_hash);
                measurement_input.extend_from_slice(&policy_resolution.context_pack_hash);
                measurement_input.extend_from_slice(&context_pack_recipe_hash);
                let (leakage_witness, timing_witness) = build_authoritative_witnesses(
                    "context_compilation",
                    &measurement_input,
                    16,
                    4,
                    context_compile_elapsed_ns,
                    measurement_timestamp_ns,
                );
                let payload = serde_json::json!({
                    "hook": "context_compilation",
                    "leakage_witness": leakage_witness,
                    "timing_witness": timing_witness,
                    "leakage_witness_hash": hex::encode(leakage_witness_hash(&leakage_witness)),
                    "timing_witness_hash": hex::encode(timing_witness_hash(&timing_witness)),
                });
                match serde_json::to_vec(&payload) {
                    Ok(payload_bytes) => {
                        if let Err(error) = self.event_emitter.emit_session_event(
                            &work_id,
                            "pcac_context_compilation_witness",
                            &payload_bytes,
                            &actor_id,
                            measurement_timestamp_ns,
                        ) {
                            warn!(
                                work_id = %work_id,
                                error = %error,
                                "context-compilation witness emission failed"
                            );
                        }
                    },
                    Err(error) => {
                        warn!(
                            work_id = %work_id,
                            error = %error,
                            "context-compilation witness payload serialization failed"
                        );
                    },
                }
            }

            if let Err(e) = seed_policy_artifacts_in_cas(
                &work_id,
                &actor_id,
                role,
                role_spec_hash,
                context_pack_recipe_hash,
                cas,
            ) {
                let condition = if e.contains("zero (missing authority context)") {
                    DenyCondition::MissingAuthorityContext
                } else {
                    DenyCondition::UnverifiableContextHash
                };
                warn!(
                    work_id = %work_id,
                    error = %e,
                    "ClaimWork rejected: authority lineage CAS seeding failed"
                );
                return Ok(deny_response_for_authority_context(
                    condition,
                    format!("authority lineage CAS seeding failed: {e}"),
                ));
            }
        }

        // SEC-SCP-FAC-0020: lease_id is redacted from logs to prevent capability
        // leakage
        info!(
            work_id = %work_id,
            lease_id = "[REDACTED]",
            actor_id = %actor_id,
            policy_resolved_ref = %policy_resolution.policy_resolved_ref,
            "Work claimed with policy resolution"
        );

        // TCK-00258: Extract custody domains for SoD validation
        // For now, use a stub implementation that derives domain from actor_id
        // In production, this would query the KeyPolicy via CustodyDomainResolver
        let executor_custody_domains = match self.resolve_actor_custody_domains(&actor_id) {
            Ok(domains) => domains,
            Err(e) => {
                warn!(error = %e, "Failed to resolve executor custody domains");
                return Ok(PrivilegedResponse::error(
                    PrivilegedErrorCode::CapabilityRequestRejected,
                    format!("failed to resolve executor custody domains: {e}"),
                ));
            },
        };

        let author_custody_domains = match self.resolve_changeset_author_domains(&work_id) {
            Ok(domains) => domains,
            Err(e) => {
                warn!(error = %e, "Failed to resolve author custody domains");
                return Ok(PrivilegedResponse::error(
                    PrivilegedErrorCode::CapabilityRequestRejected,
                    format!("failed to resolve author custody domains: {e}"),
                ));
            },
        };

        // TCK-00373 CQ MAJOR fix: Resolve permeability receipt for delegated
        // claims. The resolver queries governance/store for active delegations
        // bound to this actor and policy root. When a receipt is returned, it
        // is stored on the WorkClaim and subsequently enforced by the delegated
        // spawn gate in handle_spawn_episode. Errors are fail-closed.
        let permeability_receipt = match self.permeability_receipt_resolver.resolve_receipt(
            &actor_id,
            &work_id,
            &policy_resolution.resolved_policy_hash,
        ) {
            Ok(receipt) => {
                if receipt.is_some() {
                    debug!(
                        work_id = %work_id,
                        actor_id = %actor_id,
                        "Permeability receipt resolved for delegated claim"
                    );
                }
                receipt
            },
            Err(e) => {
                warn!(
                    work_id = %work_id,
                    actor_id = %actor_id,
                    error = %e,
                    "ClaimWork rejected: permeability receipt resolution failed (fail-closed)"
                );
                return Ok(PrivilegedResponse::error(
                    PrivilegedErrorCode::CapabilityRequestRejected,
                    format!("permeability receipt resolution failed: {e}"),
                ));
            },
        };

        // TCK-00395: Clone actor_id before it is moved into WorkClaim,
        // as we need it later for emit_work_transitioned.
        let actor_id_for_transition = actor_id.clone();

        // Register the work claim
        let claim = WorkClaim {
            work_id,
            lease_id,
            actor_id,
            role,
            policy_resolution: policy_resolution.clone(),
            executor_custody_domains,
            author_custody_domains,
            permeability_receipt,
        };

        // =====================================================================
        // TCK-00416: Lifecycle authority binding validation (ClaimWork)
        //
        // Per REQ-HEF-0013, authoritative transitions MUST carry valid
        // authority bindings. Derive bindings from the claim, store
        // CAS artifacts, validate, and fail-closed on any violation.
        // This MUST happen BEFORE state mutation (register_claim).
        //
        // If no dispatcher-level CAS is configured (e.g. the
        // `with_persistence` path without `--cas-dir`), a temporary
        // MemoryCas is used. Authority binding validation only needs CAS
        // for hash-store-and-verify within the same request; it does NOT
        // require durability. This preserves TCK-00412 fail-closed
        // semantics for PublishChangeSet (which requires durable CAS).
        // =====================================================================
        {
            let fallback_cas = MemoryCas::default();
            let cas: &dyn ContentAddressedStore = if let Some(ref c) = self.cas {
                c.as_ref()
            } else {
                &fallback_cas
            };

            let bindings = match derive_claim_transition_authority_bindings(&claim) {
                Ok(b) => b,
                Err(e) => {
                    warn!(
                        work_id = %claim.work_id,
                        error = %e,
                        "ClaimWork rejected: authority binding derivation failed"
                    );
                    return Ok(PrivilegedResponse::error(
                        PrivilegedErrorCode::CapabilityRequestRejected,
                        format!("authority binding derivation failed: {e}"),
                    ));
                },
            };

            // REQ-HEF-0013: Seed policy-provided CAS artifacts so
            // capability_manifest_hash and context_pack_hash are
            // CAS-resolvable during validation.
            if let Err(e) = seed_policy_artifacts_in_cas(
                &claim.work_id,
                &claim.actor_id,
                claim.role,
                claim.policy_resolution.role_spec_hash,
                claim.policy_resolution.context_pack_recipe_hash,
                cas,
            ) {
                warn!(
                    work_id = %claim.work_id,
                    error = %e,
                    "ClaimWork rejected: policy CAS artifact seeding failed"
                );
                return Ok(PrivilegedResponse::error(
                    PrivilegedErrorCode::CapabilityRequestRejected,
                    format!("policy CAS artifact seeding failed: {e}"),
                ));
            }

            if let Err(auth_err) =
                validate_and_store_transition_authority(&claim.work_id, &bindings, cas)
            {
                let defect_ts = self.get_htf_timestamp_ns().unwrap_or(0);
                emit_authority_binding_defect(
                    self.event_emitter.as_ref(),
                    &claim.work_id,
                    &auth_err,
                    defect_ts,
                );
                warn!(
                    work_id = %claim.work_id,
                    violations = ?auth_err.violations,
                    "ClaimWork rejected: authority binding validation failed (fail-closed)"
                );
                return Ok(PrivilegedResponse::error(
                    PrivilegedErrorCode::CapabilityRequestRejected,
                    format!("authority binding validation failed: {auth_err}"),
                ));
            }
        }

        let claim = match self.work_registry.register_claim(claim) {
            Ok(claim) => claim,
            Err(e) => {
                warn!(error = %e, "Work registration failed");
                // Return application-level error, not protocol error
                // Registration failures are logic errors, not serialization errors
                return Ok(PrivilegedResponse::error(
                    PrivilegedErrorCode::CapabilityRequestRejected,
                    format!("work registration failed: {e}"),
                ));
            },
        };

        // TCK-00253: Emit signed WorkClaimed + WorkTransitioned events atomically.
        // Per acceptance criteria: "`WorkClaimed` event signed and persisted"
        // The events are:
        // - Signed with the daemon's signing key (Ed25519)
        // - Includes work_id, lease_id, actor_id, role, and policy_resolved_ref
        // - Persisted to the append-only ledger for audit trail
        //
        // TCK-00289: Use HTF-compliant timestamp from HolonicClock.
        // Per RFC-0016, timestamps must come from the HTF clock source to ensure
        // monotonicity and causal ordering.
        //
        // TCK-00395: Uses emit_claim_lifecycle for atomic dual-event emission.
        // Both work_claimed and work_transitioned(Open->Claimed) are emitted
        // as a single atomic operation to prevent partial state commits.
        let timestamp_ns = match self.get_htf_timestamp_ns() {
            Ok(ts) => ts,
            Err(e) => {
                // TCK-00289: Fail-closed - do not proceed without valid timestamp
                warn!(error = %e, "HTF timestamp generation failed - failing closed");
                return Ok(PrivilegedResponse::error(
                    PrivilegedErrorCode::CapabilityRequestRejected,
                    format!("HTF timestamp error: {e}"),
                ));
            },
        };
        let signed_event = match self.event_emitter.emit_claim_lifecycle(
            &claim,
            &actor_id_for_transition,
            timestamp_ns,
        ) {
            Ok(event) => event,
            Err(e) => {
                warn!(error = %e, "Claim lifecycle event emission failed");
                return Ok(PrivilegedResponse::error(
                    PrivilegedErrorCode::CapabilityRequestRejected,
                    format!("claim lifecycle event emission failed: {e}"),
                ));
            },
        };

        debug!(
            event_id = %signed_event.event_id,
            work_id = %claim.work_id,
            "Claim lifecycle events emitted (work_claimed + work_transitioned)"
        );

        // Return the work assignment
        Ok(PrivilegedResponse::ClaimWork(ClaimWorkResponse {
            work_id: claim.work_id,
            lease_id: claim.lease_id,
            capability_manifest_hash: policy_resolution.capability_manifest_hash.to_vec(),
            policy_resolved_ref: policy_resolution.policy_resolved_ref,
            context_pack_hash: policy_resolution.context_pack_hash.to_vec(),
        }))
    }

    /// Builds and initializes a per-session tool broker from manifest and
    /// role-scoped allowlist policy (TCK-00401).
    ///
    /// # Errors
    ///
    /// Returns an error string when broker policy or manifest initialization
    /// fails, or when async runtime is unavailable in production.
    fn build_session_broker(
        &self,
        session_id: &str,
        manifest: &CapabilityManifest,
    ) -> Result<SharedToolBroker<crate::episode::capability::StubManifestLoader>, String> {
        let mut broker = match (&self.broker_github_store, &self.broker_ssh_store) {
            (Some(github), Some(ssh)) => ToolBroker::with_credential_stores(
                self.broker_config.clone(),
                Arc::clone(github),
                Arc::clone(ssh),
            ),
            (Some(github), None) => {
                ToolBroker::with_github_store(self.broker_config.clone(), Arc::clone(github))
            },
            (None, Some(ssh)) => {
                ToolBroker::with_ssh_store(self.broker_config.clone(), Arc::clone(ssh))
            },
            (None, None) => ToolBroker::new(self.broker_config.clone()),
        };

        if let Some(ref cas) = self.broker_cas {
            broker = broker.with_cas(Arc::clone(cas));
        }

        // TCK-00377: Wire the runtime precondition evaluator so the broker
        // performs real filesystem/git checks (FileExists, FileNotExists,
        // FileHashMatch via BLAKE3, GitCleanWorkingTree, GitRefAtCommit).
        // The evaluator is fail-closed: I/O errors and mismatches both
        // produce PreconditionFailed denials.
        broker = broker.with_precondition_evaluator(Arc::new(RuntimePreconditionEvaluator));

        broker
            .set_policy_from_tool_allowlist(
                &format!("session-role-allowlist-{session_id}"),
                manifest.tool_allowlist(),
            )
            .map_err(|e| format!("policy initialization failed: {e}"))?;

        let broker = Arc::new(broker);

        if let Ok(handle) = tokio::runtime::Handle::try_current() {
            tokio::task::block_in_place(|| {
                handle.block_on(async {
                    broker
                        .initialize_with_manifest(manifest.clone())
                        .await
                        .map_err(|e| format!("broker initialization failed: {e}"))
                })
            })?;
        } else {
            #[cfg(test)]
            {
                let rt = tokio::runtime::Builder::new_current_thread()
                    .enable_all()
                    .build()
                    .map_err(|e| format!("failed to build test runtime for broker init: {e}"))?;
                rt.block_on(async {
                    broker
                        .initialize_with_manifest(manifest.clone())
                        .await
                        .map_err(|e| format!("broker initialization failed: {e}"))
                })?;
            }
            #[cfg(not(test))]
            {
                return Err("broker initialization failed: no async runtime available".to_string());
            }
        }

        Ok(broker)
    }

    /// Rolls back a partially-completed spawn registration.
    ///
    /// Removes the newly-registered session, cleans up its telemetry/stop
    /// conditions, and restores evicted session/telemetry/manifest/stop
    /// entries captured before failure. Optionally removes the manifest if it
    /// was registered.
    ///
    /// Returns `Some(warning)` if any rollback step failed (indicating
    /// partial failure), or `None` if rollback was clean.
    ///
    /// # TCK-00384 BLOCKER 2
    ///
    /// All rollback operations are explicitly error-checked; none are
    /// silently discarded via `let _ = ...`.
    #[allow(clippy::too_many_arguments)]
    fn rollback_spawn(
        &self,
        session_id: &str,
        evicted_sessions: &[SessionState],
        evicted_telemetry: &[(String, std::sync::Arc<crate::session::SessionTelemetry>)],
        evicted_manifests: &[(String, std::sync::Arc<crate::episode::CapabilityManifest>)],
        evicted_brokers: &[(
            String,
            SharedToolBroker<crate::episode::capability::StubManifestLoader>,
        )],
        evicted_stop_conditions: &[(String, crate::episode::envelope::StopConditions)],
        remove_manifest: bool,
    ) -> Option<String> {
        let mut warnings: Vec<String> = Vec::new();

        // 1. Remove the newly-registered session from the registry.
        if let Err(e) = self.session_registry.remove_session(session_id) {
            warn!(
                session_id = %session_id,
                error = %e,
                "Rollback: failed to remove session from registry"
            );
            warnings.push(format!("remove_session({session_id}): {e}"));
        }

        // 2. Clean up telemetry for the new session and restore evicted telemetry
        //    entries.
        if let Some(ref store) = self.telemetry_store {
            store.remove(session_id);
            for (sid, telem) in evicted_telemetry {
                if let Err(e) = store.restore(sid, std::sync::Arc::clone(telem)) {
                    warn!(
                        session_id = %sid,
                        error = %e,
                        "Rollback: failed to restore evicted telemetry"
                    );
                    warnings.push(format!("restore_telemetry({sid}): {e}"));
                }
            }
        }

        // 3. Remove the manifest if it was registered.
        if remove_manifest {
            self.manifest_store.remove(session_id);
        }

        // 3a. TCK-00352 MAJOR 2 fix: Remove V1 manifest on rollback.
        // Without this, a failed spawn leaves a stale V1 manifest that
        // could be matched by a recycled session ID (dangling reference).
        if let Some(ref v1_store) = self.v1_manifest_store {
            v1_store.remove(session_id);
        }

        // 3a-ii. TCK-00351 v4: Remove stop conditions on rollback.
        // Prevents a stale stop conditions entry from persisting for a
        // session that was never fully spawned.
        if let Some(ref store) = self.stop_conditions_store {
            store.remove(session_id);
        }

        // 3a-iii. TCK-00401: Remove per-session broker on rollback.
        if let Some(ref store) = self.session_broker_registry {
            store.remove(session_id);
        }

        // 3b. Restore evicted manifests so capacity is not permanently lost.
        for (sid, manifest) in evicted_manifests {
            self.manifest_store
                .restore(sid, std::sync::Arc::clone(manifest));
        }

        // 3c. Restore evicted per-session brokers.
        if let Some(ref store) = self.session_broker_registry {
            for (sid, broker) in evicted_brokers {
                store.restore(sid, Arc::clone(broker));
            }
        }

        // 4. Re-register evicted sessions to restore capacity.
        for evicted in evicted_sessions {
            if let Err(e) = self.session_registry.register_session(evicted.clone()) {
                warn!(
                    session_id = %evicted.session_id,
                    error = %e,
                    "Rollback: failed to re-register evicted session"
                );
                warnings.push(format!("re-register({}): {e}", evicted.session_id));
            }
        }

        // 5. Restore evicted stop conditions captured before eviction cleanup.
        // This keeps rollback atomic across session/telemetry/manifest/stop stores.
        if let Some(ref store) = self.stop_conditions_store {
            for (sid, conditions) in evicted_stop_conditions {
                if let Err(e) = store.restore(sid, conditions.clone()) {
                    warn!(
                        session_id = %sid,
                        error = %e,
                        "Rollback: failed to restore evicted stop conditions"
                    );
                    warnings.push(format!("restore_stop_conditions({sid}): {e}"));
                }
            }
        }

        if warnings.is_empty() {
            None
        } else {
            Some(warnings.join("; "))
        }
    }

    /// Unified post-start rollback: stops a running episode and then rolls
    /// back the session, telemetry, and manifest stores.
    ///
    /// This helper exists to eliminate duplication across the three post-start
    /// failure paths in `handle_spawn_episode` (`update_episode_id` failure,
    /// peer-credentials missing, and `emit_spawn_lifecycle` failure). Every
    /// post-start error path MUST call this to prevent leaking a running
    /// runtime episode.
    ///
    /// The episode is stopped with `TerminationClass::Crashed` because the
    /// failure occurred after the episode was already started — the episode
    /// never ran to completion, so `Crashed` is the correct classification.
    ///
    /// # Arguments
    ///
    /// * `episode_id_opt` — The episode that was started (if any). When `None`
    ///   (e.g. in unit-test mode), only the store rollback is performed.
    /// * `session_id` — Session ID to remove from stores.
    /// * `evicted_sessions` — Sessions evicted during registration, to be
    ///   restored.
    /// * `evicted_telemetry` — Telemetry captured from evicted sessions, to be
    ///   restored.
    /// * `evicted_manifests` — Manifests captured from evicted sessions, to be
    ///   restored.
    /// * `evicted_stop_conditions` — Stop conditions captured from evicted
    ///   sessions, to be restored.
    /// * `timestamp_ns` — HTF timestamp for the episode stop event.
    /// * `context` — Human-readable label for log messages identifying the
    ///   failure site (e.g. "peer credentials failure").
    #[allow(clippy::too_many_arguments)]
    #[allow(clippy::too_many_arguments)]
    fn rollback_spawn_with_episode_stop(
        &self,
        episode_id_opt: Option<&EpisodeId>,
        session_id: &str,
        evicted_sessions: &[SessionState],
        evicted_telemetry: &[(String, std::sync::Arc<crate::session::SessionTelemetry>)],
        evicted_manifests: &[(String, std::sync::Arc<crate::episode::CapabilityManifest>)],
        evicted_brokers: &[(
            String,
            SharedToolBroker<crate::episode::capability::StubManifestLoader>,
        )],
        evicted_stop_conditions: &[(String, crate::episode::envelope::StopConditions)],
        timestamp_ns: u64,
        context: &str,
    ) -> Option<String> {
        // Step 1: Stop the already-started episode to prevent leak.
        if let Some(episode_id) = episode_id_opt {
            if let Ok(handle) = tokio::runtime::Handle::try_current() {
                let rt = &self.episode_runtime;
                let stop_err = tokio::task::block_in_place(|| {
                    handle.block_on(rt.stop(
                        episode_id,
                        crate::episode::TerminationClass::Crashed,
                        timestamp_ns,
                    ))
                });
                if let Err(stop_e) = stop_err {
                    warn!(
                        error = %stop_e,
                        episode_id = %episode_id,
                        context = %context,
                        "Rollback: failed to stop episode after post-start failure"
                    );
                }
            }
        }

        // Step 2: Rollback session, telemetry, and manifest stores.
        let rollback_warn = self.rollback_spawn(
            session_id,
            evicted_sessions,
            evicted_telemetry,
            evicted_manifests,
            evicted_brokers,
            evicted_stop_conditions,
            true,
        );
        if let Some(ref rw) = rollback_warn {
            warn!(
                rollback_errors = %rw,
                context = %context,
                "Partial rollback failure during post-start error recovery"
            );
        }
        rollback_warn
    }

    /// Resolves the adapter profile hash for `SpawnEpisode`.
    ///
    /// If `requested_hash` is provided, validates it is exactly 32 bytes and
    /// exists in CAS (fail-closed). If omitted, resolves a deterministic
    /// built-in default by `WorkRole` and stores it in CAS so that auditors
    /// reading the ledger can resolve the hash (MAJOR-1 security fix).
    ///
    /// # Policy Binding (BLOCKER security fix)
    ///
    /// When the policy resolution carries an `expected_adapter_profile_hash`,
    /// the resolved hash MUST match exactly. This prevents confused-deputy
    /// attacks where an authorized caller substitutes a different CAS-present
    /// profile to gain access to different command/env configurations.
    ///
    /// CAS existence is NOT authorization. The profile must be bound to the
    /// caller's policy scope.
    ///
    /// # Authorization Trust Chain
    ///
    /// The `requested_hash` originates from the `SpawnEpisodeRequest`, whose
    /// caller has already been authenticated and authorized by the time this
    /// method is invoked:
    ///
    /// 1. `ClaimWork` established a policy resolution for the `work_id`,
    ///    binding the caller identity and lease.
    /// 2. `handle_spawn_episode` verified the `work_id` has a valid claim (line
    ///    ~6306), the role matches the claim, and the `lease_id` matches via
    ///    constant-time comparison (line ~6346).
    /// 3. Therefore the `adapter_profile_hash` was submitted by an authorized
    ///    caller. The CAS-existence check here ensures the hash refers to a
    ///    well-formed, previously stored profile -- not that the caller is
    ///    allowed to use it (that was established upstream).
    /// 4. When `expected_adapter_profile_hash` is set in the policy resolution,
    ///    the resolved hash is validated against it using constant-time
    ///    comparison, closing the confused-deputy gap.
    fn resolve_spawn_adapter_profile_hash(
        &self,
        requested_hash: Option<&[u8]>,
        role: WorkRole,
        claim: &WorkClaim,
    ) -> Result<([u8; 32], Option<SelectionDecision>), String> {
        let (resolved_hash, selection_decision) = if let Some(raw_hash) = requested_hash {
            if raw_hash.len() != 32 {
                return Err(format!(
                    "adapter_profile_hash must be exactly 32 bytes, got {}",
                    raw_hash.len()
                ));
            }

            let mut hash = [0u8; 32];
            hash.copy_from_slice(raw_hash);

            // SECURITY: CAS-existence check. The caller was already authorized
            // via ClaimWork + lease_id verification in handle_spawn_episode
            // (see doc comment above). This check ensures the hash references
            // a valid, previously stored profile -- not that the caller is
            // allowed to use it (that was established upstream).
            let cas = self.adapter_profile_cas.as_ref().ok_or_else(|| {
                "adapter_profile_hash validation requires CAS configuration".to_string()
            })?;
            let exists = cas
                .exists(&hash)
                .map_err(|e| format!("adapter_profile_hash CAS validation failed: {e}"))?;
            if !exists {
                return Err(format!(
                    "adapter_profile_hash not found in CAS: {}",
                    hex::encode(hash)
                ));
            }
            (hash, None)
        } else {
            self.resolve_default_adapter_profile(role, &claim.work_id)?
        };

        // SECURITY (BLOCKER fix): Policy-binding validation.
        //
        // When the policy resolution carries an expected_adapter_profile_hash,
        // the resolved hash MUST match exactly. This prevents confused-deputy
        // attacks where an authorized caller substitutes a CAS-present profile
        // hash that differs from the policy-bound one.
        //
        // Uses constant-time comparison to prevent timing side-channels that
        // could leak information about the expected hash.
        if let Some(expected_hash) = claim.policy_resolution.expected_adapter_profile_hash {
            let binding_matches = bool::from(resolved_hash.ct_eq(&expected_hash));
            if !binding_matches {
                return Err(format!(
                    "adapter_profile_hash policy binding mismatch: \
                     resolved {} but policy expects {}",
                    hex::encode(resolved_hash),
                    hex::encode(expected_hash)
                ));
            }
        } else {
            // WVR-0003: Policy-level adapter profile binding is not yet
            // populated by governance resolution. Once governance populates
            // expected_adapter_profile_hash in PolicyResolution, this branch
            // will be removed and mismatches will be hard-rejected.
            // Accepted risk: CAS-existence serves as the authorization check
            // until governance wiring is complete.
            tracing::debug!(
                work_id = %claim.work_id,
                resolved_hash = %hex::encode(resolved_hash),
                "adapter_profile_hash accepted under rollout waiver WVR-0003 \
                 (no policy binding present)"
            );
        }

        Ok((resolved_hash, selection_decision))
    }

    /// Resolves the role-based default adapter profile hash.
    ///
    /// Stores the default profile in CAS so that auditors reading the ledger
    /// can resolve the hash. If CAS is not configured, falls back to
    /// computing the hash without persistence.
    fn resolve_default_adapter_profile(
        &self,
        role: WorkRole,
        work_id: &str,
    ) -> Result<([u8; 32], Option<SelectionDecision>), String> {
        if let Some(policy_state) = &self.adapter_selection_policy {
            let selection_attempt = self.event_emitter.get_work_transition_count(work_id);
            let decision = policy_state
                .lock()
                .map_err(|e| format!("adapter selection policy lock poisoned: {e}"))?
                .select_profile(work_id, selection_attempt, &self.adapter_available_profiles)
                .map_err(|e| format!("adapter selection failed: {e}"))?;

            let selected_hash = decision.selected_profile_hash;
            let cas = self
                .adapter_profile_cas
                .as_ref()
                .ok_or_else(|| "adapter selection requires CAS configuration".to_string())?;
            let exists = cas
                .exists(&selected_hash)
                .map_err(|e| format!("selected adapter profile CAS validation failed: {e}"))?;
            if !exists {
                return Err(format!(
                    "selected adapter profile hash not found in CAS: {}",
                    hex::encode(selected_hash)
                ));
            }

            return Ok((selected_hash, Some(decision)));
        }

        // TODO(TCK-00397): differentiate per-role profiles post-rollout
        let profile = match role {
            WorkRole::Implementer
            | WorkRole::GateExecutor
            | WorkRole::Reviewer
            | WorkRole::Coordinator
            | WorkRole::Unspecified => builtin_profiles::claude_code_profile(),
        };
        // Store in CAS so the hash is resolvable from the ledger.
        self.adapter_profile_cas.as_ref().map_or_else(
            || {
                profile
                    .compute_cas_hash()
                    .map_err(|e| format!("default adapter profile hash computation failed: {e}"))
                    .map(|hash| (hash, None))
            },
            |cas| {
                profile
                    .store_in_cas(cas.as_ref())
                    .map_err(|e| format!("default adapter profile CAS storage failed: {e}"))
                    .map(|hash| (hash, None))
            },
        )
    }

    /// Validates role/context lineage on the authoritative claim path.
    ///
    /// Returns the validated role spec hash when all lineage checks pass.
    fn validate_claim_authority_context_lineage(
        claim: &WorkClaim,
        cas: &dyn ContentAddressedStore,
    ) -> Result<[u8; 32], (DenyCondition, String)> {
        let role_spec_hash = claim.policy_resolution.role_spec_hash;
        if bool::from(role_spec_hash.ct_eq(&[0u8; 32])) {
            return Err((
                DenyCondition::MissingAuthorityContext,
                "claim role_spec_hash is zero".to_string(),
            ));
        }

        let context_pack_hash = claim.policy_resolution.context_pack_hash;
        if bool::from(context_pack_hash.ct_eq(&[0u8; 32])) {
            return Err((
                DenyCondition::MissingAuthorityContext,
                "claim context_pack_hash is zero".to_string(),
            ));
        }

        let context_pack_recipe_hash = claim.policy_resolution.context_pack_recipe_hash;
        if bool::from(context_pack_recipe_hash.ct_eq(&[0u8; 32])) {
            return Err((
                DenyCondition::MissingAuthorityContext,
                "claim context_pack_recipe_hash is zero".to_string(),
            ));
        }

        let cas_role_spec_hash = resolve_workobject_role_spec_hash(cas).map_err(|e| {
            (
                DenyCondition::UnverifiableContextHash,
                format!("failed to resolve authoritative role spec hash: {e}"),
            )
        })?;
        if !bool::from(role_spec_hash.ct_eq(&cas_role_spec_hash)) {
            return Err((
                DenyCondition::UnknownRoleProfile,
                format!(
                    "claim role_spec_hash {} does not match authoritative hash {}",
                    hex::encode(role_spec_hash),
                    hex::encode(cas_role_spec_hash)
                ),
            ));
        }

        match cas.exists(&role_spec_hash) {
            Ok(true) => {},
            Ok(false) => {
                return Err((
                    DenyCondition::UnverifiableContextHash,
                    format!(
                        "claim role_spec_hash {} is not resolvable in CAS",
                        hex::encode(role_spec_hash)
                    ),
                ));
            },
            Err(e) => {
                return Err((
                    DenyCondition::UnverifiableContextHash,
                    format!("CAS lookup failed for role_spec_hash: {e}"),
                ));
            },
        }

        for (name, hash) in [
            ("context_pack_hash", context_pack_hash),
            ("context_pack_recipe_hash", context_pack_recipe_hash),
        ] {
            match cas.exists(&hash) {
                Ok(true) => {},
                Ok(false) => {
                    return Err((
                        DenyCondition::StaleAuthorityContext,
                        format!("{name} {} is not resolvable in CAS", hex::encode(hash)),
                    ));
                },
                Err(e) => {
                    return Err((
                        DenyCondition::UnverifiableContextHash,
                        format!("CAS lookup failed for {name}: {e}"),
                    ));
                },
            }
        }

        let expected_context_recipe_hash = policy_context_pack_recipe_hash(
            &claim.work_id,
            &claim.actor_id,
            role_spec_hash,
            context_pack_hash,
        )
        .map_err(|e| {
            (
                DenyCondition::UnverifiableContextHash,
                format!("failed to derive authoritative context recipe hash: {e}"),
            )
        })?;
        if !bool::from(context_pack_recipe_hash.ct_eq(&expected_context_recipe_hash)) {
            return Err((
                DenyCondition::StaleAuthorityContext,
                format!(
                    "claim context_pack_recipe_hash {} does not match authoritative hash {}",
                    hex::encode(context_pack_recipe_hash),
                    hex::encode(expected_context_recipe_hash)
                ),
            ));
        }

        Ok(role_spec_hash)
    }

    /// Builds a `HarnessConfig` from an adapter profile and spawn parameters
    /// (TCK-00399).
    ///
    /// # Security
    ///
    /// - Session token is passed via env only (WVR-0002), NEVER in argv
    /// - Security-critical env vars cannot be overridden by the profile
    /// - `workspace_root` path traversal is prevented by
    ///   `HarnessConfig::validate()`
    fn build_harness_config(
        profile: &apm2_core::fac::AgentAdapterProfileV1,
        episode_id: &str,
        workspace_root: &str,
        prompt: &str,
        model: &str,
        session_token: &secrecy::SecretString,
    ) -> Result<crate::episode::HarnessConfig, String> {
        use secrecy::ExposeSecret;

        /// Environment variable names that MUST NOT be overridden by
        /// adapter profiles (privilege escalation / library injection).
        const FORBIDDEN_ENV_KEYS: &[&str] = &[
            "PATH",
            "LD_PRELOAD",
            "LD_LIBRARY_PATH",
            "DYLD_INSERT_LIBRARIES",
            "DYLD_LIBRARY_PATH",
        ];

        /// Known template placeholder tokens. Only these are flagged as
        /// unresolved after expansion; arbitrary `{...}` strings (e.g.,
        /// JSON literals) are accepted.
        const KNOWN_PLACEHOLDERS: &[&str] = &["{workspace}", "{prompt}", "{model}", "{episode_id}"];

        // Expand args_template placeholders.
        // SECURITY: session_token MUST NEVER appear in argv (WVR-0002).
        #[allow(clippy::literal_string_with_formatting_args)]
        let expanded_args: Vec<String> = profile
            .args_template
            .iter()
            .map(|arg| {
                arg.replace("{workspace}", workspace_root)
                    .replace("{prompt}", prompt)
                    .replace("{model}", model)
            })
            .collect();

        // Defense-in-depth: verify session_token is not in any arg.
        let token_str = session_token.expose_secret();
        for (i, arg) in expanded_args.iter().enumerate() {
            if arg.contains(token_str) {
                return Err(format!(
                    "security violation: session_token found in argv[{i}] \
                     after template expansion"
                ));
            }
        }

        // Fail-closed: reject any unresolved KNOWN template placeholders in args.
        // Only flag known placeholder tokens to avoid rejecting legitimate
        // literal braces (e.g., JSON arguments like `{"key": "value"}`).
        for (i, arg) in expanded_args.iter().enumerate() {
            for placeholder in KNOWN_PLACEHOLDERS {
                if arg.contains(placeholder) {
                    return Err(format!(
                        "unresolved template placeholder {placeholder} in argv[{i}]"
                    ));
                }
            }
        }

        let mut config = crate::episode::HarnessConfig::new(&profile.command, episode_id)
            .with_args(expanded_args)
            .with_cwd(workspace_root);

        // Expand and set env vars from profile template.
        for (key, value_template) in &profile.env_template {
            if FORBIDDEN_ENV_KEYS
                .iter()
                .any(|&k| k.eq_ignore_ascii_case(key))
            {
                return Err(format!(
                    "security violation: adapter profile overrides \
                     forbidden env var '{key}'"
                ));
            }
            #[allow(clippy::literal_string_with_formatting_args)]
            let expanded_value = value_template
                .replace("{workspace}", workspace_root)
                .replace("{prompt}", prompt)
                .replace("{model}", model);
            config = config.with_env(key, expanded_value);
        }

        // WVR-0002: Pass session token via environment only.
        config = config.with_secret_env("APM2_SESSION_TOKEN", session_token.clone());

        // SECURITY: After clearenv() the child has no PATH, so non-absolute
        // commands (claude, gemini, codex, ollama) would fail at exec.
        // Inject a daemon-controlled safe default PATH.  This is NOT
        // profile-controlled (FORBIDDEN_ENV_KEYS still blocks profile
        // overrides) -- it is set by the daemon after all profile env
        // expansion.
        config = config.with_env("PATH", "/usr/local/bin:/usr/bin:/bin");

        if profile.requires_pty {
            config = config.with_pty_size(120, 40);
        }

        config
            .validate()
            .map_err(|e| format!("harness config validation failed: {e}"))?;

        Ok(config)
    }

    /// Handles `SpawnEpisode` requests (IPC-PRIV-002).
    ///
    /// # Security Contract (TCK-00257)
    ///
    /// - `GATE_EXECUTOR` role requires a valid `lease_id` that references a
    ///   `GateLeaseIssued` event in the ledger
    /// - The lease must match the `work_id` in the request
    /// - Non-`GATE_EXECUTOR` roles (`IMPLEMENTER`, `REVIEWER`) do not require
    ///   ledger lease validation (they use claim-based validation)
    ///
    /// # TCK-00256 Implementation
    ///
    /// This handler implements the episode spawn flow per DD-001 and DD-002:
    ///
    /// 1. Validate request structure
    /// 2. Query work registry for `PolicyResolvedForChangeSet`
    /// 3. Validate role matches the claimed role
    /// 4. Validate `lease_id` matches the claimed `lease_id` (SEC-SCP-FAC-0020)
    /// 5. Create episode in runtime with policy constraints
    /// 6. Persist session state for subsequent IPC calls
    /// 7. Return session credentials
    ///
    /// # Security
    ///
    /// - Per SEC-SCP-FAC-0020: `lease_id` is validated against the claim to
    ///   prevent authorization bypass. The `lease_id` is redacted from logs to
    ///   prevent capability leakage.
    /// - Per fail-closed semantics: spawn is rejected if policy resolution is
    ///   missing.
    fn handle_spawn_episode(
        &self,
        payload: &[u8],
        ctx: &ConnectionContext,
    ) -> ProtocolResult<PrivilegedResponse> {
        // TCK-00319: Maximum path length constant (declared at function start per
        // clippy)
        const MAX_PATH_LENGTH: usize = 4096;

        let request =
            SpawnEpisodeRequest::decode_bounded(payload, &self.decode_config).map_err(|e| {
                ProtocolError::Serialization {
                    reason: format!("invalid SpawnEpisodeRequest: {e}"),
                }
            })?;

        // SEC-SCP-FAC-0020: lease_id is redacted from logs to prevent capability
        // leakage
        info!(
            work_id = %request.work_id,
            role = ?WorkRole::try_from(request.role).unwrap_or(WorkRole::Unspecified),
            lease_id = "[REDACTED]",
            peer_pid = ?ctx.peer_credentials().map(|c| c.pid),
            "SpawnEpisode request received"
        );

        // Validate required fields
        if request.work_id.is_empty() {
            return Ok(PrivilegedResponse::error(
                PrivilegedErrorCode::CapabilityRequestRejected,
                "work_id is required",
            ));
        }

        // SEC-SCP-FAC-0020: Enforce maximum length on work_id to prevent DoS via OOM
        if request.work_id.len() > MAX_ID_LENGTH {
            warn!(
                work_id_len = request.work_id.len(),
                max_len = MAX_ID_LENGTH,
                "SpawnEpisode rejected: work_id exceeds maximum length"
            );
            return Ok(PrivilegedResponse::error(
                PrivilegedErrorCode::CapabilityRequestRejected,
                format!("work_id exceeds maximum length of {MAX_ID_LENGTH} bytes"),
            ));
        }

        // SEC-SCP-FAC-0020: Enforce maximum length on lease_id to prevent DoS via OOM
        if let Some(ref lease_id) = request.lease_id {
            if lease_id.len() > MAX_ID_LENGTH {
                warn!(
                    lease_id_len = lease_id.len(),
                    max_len = MAX_ID_LENGTH,
                    "SpawnEpisode rejected: lease_id exceeds maximum length"
                );
                return Ok(PrivilegedResponse::error(
                    PrivilegedErrorCode::CapabilityRequestRejected,
                    format!("lease_id exceeds maximum length of {MAX_ID_LENGTH} bytes"),
                ));
            }
        }

        // SECURITY MAJOR-1: bound escalation predicate length before
        // policy validation/persistence to prevent oversized payload DoS.
        if let Some(ref escalation_predicate) = request.escalation_predicate {
            if escalation_predicate.len() > MAX_ESCALATION_PREDICATE_LEN {
                warn!(
                    escalation_predicate_len = escalation_predicate.len(),
                    max_len = MAX_ESCALATION_PREDICATE_LEN,
                    "SpawnEpisode rejected: escalation_predicate exceeds maximum length"
                );
                return Ok(PrivilegedResponse::error(
                    PrivilegedErrorCode::CapabilityRequestRejected,
                    format!(
                        "escalation_predicate exceeds maximum length of \
                         {MAX_ESCALATION_PREDICATE_LEN} bytes"
                    ),
                ));
            }
        }

        // TCK-00319: Validate workspace_root is provided
        if request.workspace_root.is_empty() {
            return Ok(PrivilegedResponse::error(
                PrivilegedErrorCode::CapabilityRequestRejected,
                "workspace_root is required",
            ));
        }

        // TCK-00319: Validate workspace_root path length (prevent DoS via unbounded
        // paths)
        if request.workspace_root.len() > MAX_PATH_LENGTH {
            warn!(
                workspace_root_len = request.workspace_root.len(),
                max_len = MAX_PATH_LENGTH,
                "SpawnEpisode rejected: workspace_root exceeds maximum length"
            );
            return Ok(PrivilegedResponse::error(
                PrivilegedErrorCode::CapabilityRequestRejected,
                format!("workspace_root exceeds maximum length of {MAX_PATH_LENGTH} bytes"),
            ));
        }

        // TCK-00319: Validate workspace_root is an absolute path
        let workspace_path = std::path::Path::new(&request.workspace_root);
        if !workspace_path.is_absolute() {
            warn!(
                workspace_root = %request.workspace_root,
                "SpawnEpisode rejected: workspace_root must be an absolute path"
            );
            return Ok(PrivilegedResponse::error(
                PrivilegedErrorCode::CapabilityRequestRejected,
                "workspace_root must be an absolute path",
            ));
        }

        // TCK-00319: Validate workspace_root exists and is a directory
        if !workspace_path.exists() {
            warn!(
                workspace_root = %request.workspace_root,
                "SpawnEpisode rejected: workspace_root does not exist"
            );
            return Ok(PrivilegedResponse::error(
                PrivilegedErrorCode::CapabilityRequestRejected,
                format!("workspace_root does not exist: {}", request.workspace_root),
            ));
        }

        if !workspace_path.is_dir() {
            warn!(
                workspace_root = %request.workspace_root,
                "SpawnEpisode rejected: workspace_root is not a directory"
            );
            return Ok(PrivilegedResponse::error(
                PrivilegedErrorCode::CapabilityRequestRejected,
                format!(
                    "workspace_root is not a directory: {}",
                    request.workspace_root
                ),
            ));
        }

        if request.role == WorkRole::Unspecified as i32 {
            return Ok(PrivilegedResponse::error(
                PrivilegedErrorCode::CapabilityRequestRejected,
                "role is required",
            ));
        }

        // GATE_EXECUTOR requires lease_id
        if request.role == WorkRole::GateExecutor as i32 && request.lease_id.is_none() {
            return Ok(PrivilegedResponse::error(
                PrivilegedErrorCode::GateLeaseMissing,
                "lease_id is required for GATE_EXECUTOR role",
            ));
        }

        // TCK-00257: GATE_EXECUTOR requires valid lease in ledger
        // Per RFC-0017 IPC-PRIV-002, the lease must exist as a GateLeaseIssued event
        // and the work_id must match. This is a fail-closed check.
        if request.role == WorkRole::GateExecutor as i32 {
            let lease_id = request
                .lease_id
                .as_ref()
                .expect("lease_id presence checked above");

            if let Err(e) = self
                .lease_validator
                .validate_gate_lease(lease_id, &request.work_id)
            {
                warn!(
                    work_id = %request.work_id,
                    error = %e,
                    "SpawnEpisode rejected: gate lease validation failed"
                );
                return Ok(PrivilegedResponse::error(
                    PrivilegedErrorCode::GateLeaseMissing,
                    format!("gate lease validation failed: {e}"),
                ));
            }
        }

        // TCK-00256: Query work registry for PolicyResolvedForChangeSet
        // Fail-closed: spawn is only allowed if a valid policy resolution exists
        // for the work_id. This is established during ClaimWork.
        let Some(claim) = self.work_registry.get_claim(&request.work_id) else {
            // Local precondition failure (no prior ClaimWork / missing local
            // claim state); this is NOT a governance transport failure.
            warn!(
                work_id = %request.work_id,
                "SpawnEpisode rejected: policy resolution not found for work_id"
            );
            return Ok(PrivilegedResponse::error(
                PrivilegedErrorCode::PolicyResolutionMissing,
                format!(
                    "policy resolution not found for work_id={}; ClaimWork must be called first",
                    request.work_id
                ),
            ));
        };

        // TCK-00256: Validate role matches the claimed role
        // Per DD-001, the role in the spawn request should match the claimed role
        let request_role = WorkRole::try_from(request.role).unwrap_or(WorkRole::Unspecified);
        if claim.role != request_role {
            warn!(
                work_id = %request.work_id,
                claimed_role = ?claim.role,
                request_role = ?request_role,
                "SpawnEpisode rejected: role mismatch"
            );
            return Ok(PrivilegedResponse::error(
                PrivilegedErrorCode::CapabilityRequestRejected,
                format!(
                    "role mismatch: work was claimed as {:?} but spawn requested {:?}",
                    claim.role, request_role
                ),
            ));
        }

        // SEC-SCP-FAC-0020: Validate lease_id matches the claimed lease_id
        // This prevents authorization bypass where a caller provides an arbitrary
        // lease_id. All roles must provide the correct lease_id from ClaimWork.
        // NOTE: Uses constant-time comparison to prevent timing side-channel attacks
        // that could leak information about valid lease_id values.
        let provided_lease_id = request.lease_id.as_deref().unwrap_or("");
        let lease_id_matches = provided_lease_id.len() == claim.lease_id.len()
            && bool::from(
                provided_lease_id
                    .as_bytes()
                    .ct_eq(claim.lease_id.as_bytes()),
            );
        if !lease_id_matches {
            warn!(
                work_id = %request.work_id,
                "SpawnEpisode rejected: lease_id mismatch"
            );
            return Ok(PrivilegedResponse::error(
                PrivilegedErrorCode::CapabilityRequestRejected,
                "lease_id does not match the claimed lease_id",
            ));
        }

        // For GateExecutor, the lease_id is required and must match
        // (Redundant but explicit check preserved for clarity per logic)
        // NOTE: Uses constant-time comparison to prevent timing side-channel attacks.
        if request.role == WorkRole::GateExecutor as i32 {
            if let Some(ref lease_id) = request.lease_id {
                let gate_lease_matches = lease_id.len() == claim.lease_id.len()
                    && bool::from(lease_id.as_bytes().ct_eq(claim.lease_id.as_bytes()));
                if !gate_lease_matches {
                    warn!(
                        work_id = %request.work_id,
                        "SpawnEpisode rejected: GateExecutor lease_id mismatch"
                    );
                    return Ok(PrivilegedResponse::error(
                        PrivilegedErrorCode::CapabilityRequestRejected,
                        "lease_id does not match the claimed lease_id for GATE_EXECUTOR",
                    ));
                }
            }
        }

        // =====================================================================
        // TCK-00373: Delegated spawn gate — consumption binding verification
        //
        // BLOCKER 1: Mandatory binding both directions:
        //   - claim has receipt but request omits hash => reject
        //   - request has hash but claim has no receipt => reject
        // This prevents delegated-path bypass by omission.
        //
        // BLOCKER 2: Full ConsumptionContext enforcement:
        //   - actor_id from claim (scope binding)
        //   - policy_root_hash from resolved policy (cross-policy replay)
        //   - authority_ceiling from risk tier (authority bounds)
        //   - parent_chain_commitment for chain commitment verification
        // =====================================================================

        // BLOCKER 1 (reverse direction): If the claim carries a permeability
        // receipt, the request MUST also carry the receipt hash. A delegated
        // claim processed without mandatory permeability_receipt_hash envelope
        // binding violates RFC-0020 section 8.3.3 consumption rules.
        if claim.permeability_receipt.is_some() && request.permeability_receipt_hash.is_none() {
            warn!(
                work_id = %request.work_id,
                "SpawnEpisode rejected: claim has permeability_receipt but request omits permeability_receipt_hash"
            );
            return Ok(PrivilegedResponse::error(
                PrivilegedErrorCode::CapabilityRequestRejected,
                "claim carries permeability_receipt but request omits permeability_receipt_hash \
                 (mandatory binding required, fail-closed)",
            ));
        }

        let mut delegated_gate_inputs: Option<(
            [u8; 32],
            apm2_core::policy::permeability::AuthorityVector,
            u64,
        )> = None;

        if let Some(ref receipt_hash_bytes) = request.permeability_receipt_hash {
            use apm2_core::policy::permeability::ConsumptionContext;

            if receipt_hash_bytes.len() != 32 {
                warn!(
                    work_id = %request.work_id,
                    receipt_hash_len = receipt_hash_bytes.len(),
                    "SpawnEpisode rejected: permeability_receipt_hash must be 32 bytes"
                );
                return Ok(PrivilegedResponse::error(
                    PrivilegedErrorCode::CapabilityRequestRejected,
                    format!(
                        "permeability_receipt_hash must be 32 bytes, got {}",
                        receipt_hash_bytes.len()
                    ),
                ));
            }

            let mut receipt_hash = [0u8; 32];
            receipt_hash.copy_from_slice(receipt_hash_bytes);

            // Fail-closed: receipt hash must be non-zero.
            if bool::from(receipt_hash.ct_eq(&[0u8; 32])) {
                warn!(
                    work_id = %request.work_id,
                    "SpawnEpisode rejected: permeability_receipt_hash is zero"
                );
                return Ok(PrivilegedResponse::error(
                    PrivilegedErrorCode::CapabilityRequestRejected,
                    "permeability_receipt_hash is zero (fail-closed)",
                ));
            }

            // The claim MUST carry the actual receipt for consumption
            // binding verification. Without the receipt object, we cannot
            // verify the hash or authority — fail closed.
            let Some(receipt) = &claim.permeability_receipt else {
                warn!(
                    work_id = %request.work_id,
                    "SpawnEpisode rejected: permeability_receipt_hash present but claim has no receipt"
                );
                return Ok(PrivilegedResponse::error(
                    PrivilegedErrorCode::CapabilityRequestRejected,
                    "permeability_receipt_hash present but claim has no associated receipt (fail-closed)",
                ));
            };

            // Verify the receipt content hash matches the declared hash.
            let actual_hash = receipt.content_hash();
            if !bool::from(actual_hash.ct_eq(&receipt_hash)) {
                warn!(
                    work_id = %request.work_id,
                    expected = %hex::encode(receipt_hash),
                    actual = %hex::encode(actual_hash),
                    "SpawnEpisode rejected: permeability receipt hash mismatch"
                );
                return Ok(PrivilegedResponse::error(
                    PrivilegedErrorCode::CapabilityRequestRejected,
                    format!(
                        "permeability receipt hash mismatch: expected {}, got {}",
                        hex::encode(receipt_hash),
                        hex::encode(actual_hash)
                    ),
                ));
            }

            // BLOCKER 2 (Security): Derive authority ceiling from the
            // resolved risk tier through the SINGLE canonical mapping.
            // Invalid/unknown tier values are **denied** (fail-closed),
            // NOT mapped to `AuthorityVector::top()`.
            let Some(authority_ceiling_vec) =
                apm2_core::policy::permeability::authority_ceiling_for_risk_tier(
                    claim.policy_resolution.resolved_risk_tier,
                )
            else {
                warn!(
                    work_id = %request.work_id,
                    resolved_risk_tier = claim.policy_resolution.resolved_risk_tier,
                    "SpawnEpisode rejected: invalid resolved_risk_tier (fail-closed deny)"
                );
                return Ok(PrivilegedResponse::error(
                    PrivilegedErrorCode::CapabilityRequestRejected,
                    format!(
                        "invalid resolved_risk_tier {} -- fail-closed deny \
                         (valid range 0-4)",
                        claim.policy_resolution.resolved_risk_tier,
                    ),
                ));
            };

            // BLOCKER 1 (Security): The `required_authority` is the
            // risk-tier ceiling itself. The subsequent
            // `validate_consumption_binding` checks that
            // `required_authority.is_subset_of(&receipt.delegated)`,
            // which enforces that the receipt carries AT LEAST the
            // authority demanded by the spawn's risk tier.
            //
            // Using `lattice_meet(ceiling, delegated)` here would be
            // tautological — the meet is always <= delegated, so the
            // subset check would always pass.  By using the ceiling
            // directly, a low-authority receipt attempting a
            // high-risk-tier spawn is correctly rejected.
            let required_authority = authority_ceiling_vec;

            // Use HTF timestamp for consumption time verification.
            let now_ms = match self.get_htf_timestamp_ns() {
                Ok(ts_ns) => ts_ns / 1_000_000, // convert ns -> ms
                Err(e) => {
                    warn!(
                        work_id = %request.work_id,
                        error = %e,
                        "SpawnEpisode rejected: HTF timestamp error for permeability gate"
                    );
                    return Ok(PrivilegedResponse::error(
                        PrivilegedErrorCode::CapabilityRequestRejected,
                        format!("HTF timestamp error for permeability gate: {e}"),
                    ));
                },
            };

            // Construct ConsumptionContext from dispatch state.
            //
            // The context provides:
            //   - actor_id: from the claim's authenticated actor identity
            //   - policy_root_hash: from the claim's resolved policy hash
            //   - authority_ceiling: derived from the canonical risk tier mapping
            //   - parent_chain_commitment: None (root receipts are self-verifiable;
            //     delegated receipts at depth > 0 will fail-closed if parent chain
            //     commitment is unavailable)
            let consumption_ctx = ConsumptionContext {
                actor_id: &claim.actor_id,
                policy_root_hash: &claim.policy_resolution.resolved_policy_hash,
                authority_ceiling: Some(&authority_ceiling_vec),
                parent_chain_commitment: None,
            };

            // Full consumption binding verification. This checks:
            // - Receipt admission (expired, revoked, issuance-time)
            // - Hash binding
            // - Policy root provenance (against claim's policy_root_hash)
            // - Scope binding (against claim's actor_id)
            // - Delegation chain continuity
            // - Chain commitment (cryptographic verification)
            // - Authority ceiling (against risk-tier-derived ceiling)
            // - Authority subset (against ceil-constrained required_authority)
            if let Err(e) = apm2_core::policy::permeability::validate_consumption_binding(
                receipt,
                &receipt_hash,
                &required_authority,
                now_ms,
                Some(&consumption_ctx),
            ) {
                warn!(
                    work_id = %request.work_id,
                    error = %e,
                    "SpawnEpisode rejected: permeability consumption binding failed"
                );
                return Ok(PrivilegedResponse::error(
                    PrivilegedErrorCode::CapabilityRequestRejected,
                    format!("permeability consumption binding failed: {e}"),
                ));
            }

            delegated_gate_inputs = Some((receipt_hash, required_authority, now_ms));

            info!(
                work_id = %request.work_id,
                receipt_hash = %hex::encode(receipt_hash),
                "Delegated spawn gate passed: permeability receipt verified"
            );
        }

        // TCK-00351 BLOCKER 2: Validate caller-supplied stop conditions
        // against an authoritative policy floor before persisting.
        let stop_policy = StopConditionPolicy::fail_closed_default();
        let resolved_stop_conditions = match stop_policy.validate_against_floor(
            request.max_episodes,
            request.escalation_predicate.as_deref(),
        ) {
            Ok(conditions) => conditions,
            Err(e) => {
                warn!(
                    work_id = %request.work_id,
                    max_episodes = ?request.max_episodes,
                    "SpawnEpisode rejected: stop-condition policy floor violation"
                );
                return Ok(PrivilegedResponse::error(
                    PrivilegedErrorCode::CapabilityRequestRejected,
                    format!("invalid stop conditions: {e}"),
                ));
            },
        };

        // =====================================================================
        // TCK-00258: SoD Custody Domain Validation
        //
        // Per REQ-DCP-0006, GATE_EXECUTOR spawns MUST enforce Separation of
        // Duties by rejecting when executor custody domains overlap with author
        // custody domains. This prevents self-review attacks.
        // =====================================================================
        if request.role == WorkRole::GateExecutor as i32 {
            // Convert claim domains to CustodyDomainId for validation
            let executor_domains: Vec<CustodyDomainId> = claim
                .executor_custody_domains
                .iter()
                .filter_map(|d| CustodyDomainId::new(d.clone()).ok())
                .collect();

            let author_domains: Vec<CustodyDomainId> = claim
                .author_custody_domains
                .iter()
                .filter_map(|d| CustodyDomainId::new(d.clone()).ok())
                .collect();

            // TCK-00258: Fail-closed SoD validation for GATE_EXECUTOR.
            // If author domains cannot be resolved (empty), DENY the spawn.
            // The absence of author information MUST block, not allow, to prevent
            // attackers from bypassing SoD by using malformed work_ids.
            if author_domains.is_empty() {
                warn!(
                    work_id = %request.work_id,
                    "SpawnEpisode rejected: cannot resolve author custody domains for SoD validation"
                );
                return Ok(PrivilegedResponse::error(
                    PrivilegedErrorCode::SodViolation,
                    "cannot resolve author custody domains; SoD validation requires author information for GATE_EXECUTOR",
                ));
            }

            // Validate SoD: executor domains must not overlap with author domains
            if !executor_domains.is_empty() {
                if let Err(CustodyDomainError::Overlap {
                    executor_domain,
                    author_domain,
                }) = validate_custody_domain_overlap(&executor_domains, &author_domains)
                {
                    warn!(
                        work_id = %request.work_id,
                        executor_domain = %executor_domain,
                        author_domain = %author_domain,
                        "SpawnEpisode rejected: SoD custody domain violation"
                    );

                    // Emit LeaseIssueDenied event for audit logging.
                    // TCK-00289: Use HTF-compliant timestamp per RFC-0016.
                    // Fail-closed: if clock fails, we still reject the spawn (already
                    // doing that) but log at warning level instead of emitting event.
                    let timestamp_ns = match self.get_htf_timestamp_ns() {
                        Ok(ts) => ts,
                        Err(e) => {
                            warn!(error = %e, "HTF timestamp error for LeaseIssueDenied - skipping event emission");
                            0u64 // Use 0 only for best-effort event, spawn is still denied
                        },
                    };

                    // Best-effort event emission - don't fail spawn on event error.
                    // If no Tokio runtime is available (e.g., in unit tests), skip the
                    // async event emission. This is safe because:
                    // 1. The denial is still returned to the caller
                    // 2. The event is only for audit/diagnostic purposes
                    // 3. Production code always has a Tokio runtime
                    if let Ok(handle) = tokio::runtime::Handle::try_current() {
                        let _ = tokio::task::block_in_place(|| {
                            handle.block_on(async {
                                self.episode_runtime
                                    .emit_lease_issue_denied(
                                        request.work_id.clone(),
                                        LeaseIssueDenialReason::SodViolation {
                                            executor_domain: executor_domain.clone(),
                                            author_domain: author_domain.clone(),
                                        },
                                        timestamp_ns,
                                    )
                                    .await
                            })
                        });
                    }

                    return Ok(PrivilegedResponse::error(
                        PrivilegedErrorCode::SodViolation,
                        format!(
                            "custody domain overlap: executor domain '{executor_domain}' overlaps with author domain '{author_domain}'"
                        ),
                    ));
                }
            }
        }

        // =====================================================================
        // TCK-00416: Lifecycle authority binding validation (SpawnEpisode)
        //
        // Per REQ-HEF-0013, authoritative transitions MUST carry valid
        // authority bindings. Derive bindings using the resolved stop
        // conditions, store CAS artifacts, validate, and fail-closed on
        // any violation. This MUST happen BEFORE state mutation
        // (session registration).
        //
        // Fallback MemoryCas: see ClaimWork block comment.
        // =====================================================================
        {
            let fallback_cas = MemoryCas::default();
            let cas: &dyn ContentAddressedStore = if let Some(ref c) = self.cas {
                c.as_ref()
            } else {
                &fallback_cas
            };

            let bindings = match derive_transition_authority_bindings(
                &claim.work_id,
                &claim.lease_id,
                &claim.actor_id,
                claim.role,
                &claim.policy_resolution.policy_resolved_ref,
                claim.policy_resolution.capability_manifest_hash,
                claim.policy_resolution.context_pack_hash,
                resolved_stop_conditions.clone(),
            ) {
                Ok(b) => b,
                Err(e) => {
                    warn!(
                        work_id = %request.work_id,
                        error = %e,
                        "SpawnEpisode rejected: authority binding derivation failed"
                    );
                    return Ok(PrivilegedResponse::error(
                        PrivilegedErrorCode::CapabilityRequestRejected,
                        format!("authority binding derivation failed: {e}"),
                    ));
                },
            };

            // REQ-HEF-0013: Seed policy-provided CAS artifacts so
            // capability_manifest_hash and context_pack_hash are
            // CAS-resolvable during validation.
            if let Err(e) = seed_policy_artifacts_in_cas(
                &claim.work_id,
                &claim.actor_id,
                claim.role,
                claim.policy_resolution.role_spec_hash,
                claim.policy_resolution.context_pack_recipe_hash,
                cas,
            ) {
                warn!(
                    work_id = %request.work_id,
                    error = %e,
                    "SpawnEpisode rejected: policy CAS artifact seeding failed"
                );
                return Ok(PrivilegedResponse::error(
                    PrivilegedErrorCode::CapabilityRequestRejected,
                    format!("policy CAS artifact seeding failed: {e}"),
                ));
            }

            if let Err(auth_err) =
                validate_and_store_transition_authority(&claim.work_id, &bindings, cas)
            {
                let defect_ts = self.get_htf_timestamp_ns().unwrap_or(0);
                emit_authority_binding_defect(
                    self.event_emitter.as_ref(),
                    &claim.work_id,
                    &auth_err,
                    defect_ts,
                );
                warn!(
                    work_id = %request.work_id,
                    violations = ?auth_err.violations,
                    "SpawnEpisode rejected: authority binding validation failed (fail-closed)"
                );
                return Ok(PrivilegedResponse::error(
                    PrivilegedErrorCode::CapabilityRequestRejected,
                    format!("authority binding validation failed: {auth_err}"),
                ));
            }
        }

        // =====================================================================
        // TCK-00420: Alias reconciliation promotion gate (fail-closed)
        //
        // Verify alias/work_id projection consistency BEFORE state mutation.
        // The gate checks that the work_id's alias binding matches the
        // canonical projection. Infrastructure errors (lock failures,
        // projection rebuild errors) also fail-closed per CTR-2617.
        //
        // Current limitation: ClaimWork registers identity-mapped aliases
        // only (work_id -> work_id). Real TCK-* alias bindings require
        // operator-layer or policy-resolver wire-up.
        // TODO(TCK-00425): Wire real ticket aliases through policy resolution.
        // =====================================================================
        {
            use apm2_core::events::alias_reconcile::TicketAliasBinding;

            // Use HTF clock tick for temporal freshness (never SystemTime::now).
            let current_tick = match self.holonic_clock.now_mono_tick() {
                Ok(tick) => tick.value(),
                Err(e) => {
                    warn!(
                        work_id = %request.work_id,
                        error = %e,
                        "SpawnEpisode rejected: HTF clock tick error for alias reconciliation gate"
                    );
                    return Ok(PrivilegedResponse::error(
                        PrivilegedErrorCode::CapabilityRequestRejected,
                        format!(
                            "alias reconciliation gate: HTF clock tick unavailable (fail-closed): {e}"
                        ),
                    ));
                },
            };

            // Read observation window from the gate's configuration rather
            // than using hardcoded values.
            let obs_window = self.alias_reconciliation_gate.observation_window();

            let binding = TicketAliasBinding {
                // Identity mapping: ticket_alias == work_id for now.
                // TODO(TCK-00425): Use real ticket alias from policy resolution.
                ticket_alias: request.work_id.clone(),
                canonical_work_id: work_id_to_hash(&request.work_id),
                observed_at_tick: current_tick,
                observation_window_start: obs_window.start_tick,
                observation_window_end: obs_window.end_tick,
            };

            match self
                .alias_reconciliation_gate
                .check_promotion(&[binding], current_tick)
            {
                Ok(result) => {
                    // For identity-mapped aliases (ticket_alias == work_id),
                    // NotFound defects are expected when the projection has
                    // not been populated with work events yet (e.g., first
                    // spawn, or when ClaimWork uses a stub registry in tests).
                    // Only Mismatch, Ambiguous, and Stale defects indicate
                    // real reconciliation failures that must block promotion.
                    //
                    // TODO(TCK-00425): When real ticket aliases are wired
                    // through policy resolution, NotFound defects for real
                    // aliases MUST also block promotion.
                    let blocking_defects: Vec<_> = result
                        .unresolved_defects
                        .iter()
                        .filter(|d| {
                            !matches!(
                                d.defect_class,
                                apm2_core::events::alias_reconcile::DefectClass::NotFound
                            )
                        })
                        .collect();

                    if !blocking_defects.is_empty() {
                        let defect_summary: Vec<String> = blocking_defects
                            .iter()
                            .map(|d| format!("{}:{}", d.ticket_alias, d.defect_class))
                            .collect();
                        warn!(
                            work_id = %request.work_id,
                            defect_count = blocking_defects.len(),
                            defects = ?defect_summary,
                            "SpawnEpisode rejected: alias reconciliation promotion gate denied"
                        );
                        return Ok(PrivilegedResponse::error(
                            PrivilegedErrorCode::CapabilityRequestRejected,
                            format!(
                                "alias reconciliation promotion gate denied: {} blocking defect(s)",
                                blocking_defects.len()
                            ),
                        ));
                    }
                    debug!(
                        work_id = %request.work_id,
                        resolved_count = result.resolved_count,
                        not_found_count = result.unresolved_defects.len() - blocking_defects.len(),
                        "Alias reconciliation promotion gate passed"
                    );
                },
                Err(e) => {
                    // Infrastructure errors fail-closed per CTR-2617
                    // (Distributed Capabilities Are Fail-Closed).
                    //
                    // The gate's refresh_projection() handles projection
                    // rebuild errors internally (retaining last good state),
                    // so errors reaching this point are true infrastructure
                    // failures (lock poisoning) where the gate state is
                    // unknown and promotion MUST be rejected.
                    warn!(
                        error = %e,
                        work_id = %request.work_id,
                        "SpawnEpisode rejected: alias reconciliation infrastructure failure (fail-closed)"
                    );
                    return Ok(PrivilegedResponse::error(
                        PrivilegedErrorCode::CapabilityRequestRejected,
                        format!("alias reconciliation gate infrastructure failure: {e}"),
                    ));
                },
            }
        }

        let fallback_cas = MemoryCas::default();
        let cas: &dyn ContentAddressedStore = if let Some(ref c) = self.cas {
            c.as_ref()
        } else {
            &fallback_cas
        };
        let mut role_spec_hash: Option<[u8; 32]> = None;
        let (context_pack_hash_component, context_pack_recipe_hash_component) =
            if claim.role == WorkRole::Implementer {
                if let Err(e) = seed_policy_artifacts_in_cas(
                    &claim.work_id,
                    &claim.actor_id,
                    claim.role,
                    claim.policy_resolution.role_spec_hash,
                    claim.policy_resolution.context_pack_recipe_hash,
                    cas,
                ) {
                    let condition = if e.contains("zero (missing authority context)") {
                        DenyCondition::MissingAuthorityContext
                    } else if e.contains("context_pack_recipe_hash mismatch") {
                        DenyCondition::StaleAuthorityContext
                    } else {
                        DenyCondition::UnverifiableContextHash
                    };
                    warn!(
                        work_id = %request.work_id,
                        deny_condition = %condition,
                        error = %e,
                        "SpawnEpisode rejected: authority lineage CAS seeding failed"
                    );
                    return Ok(deny_response_for_authority_context(
                        condition,
                        format!("spawn authority lineage CAS seeding failed: {e}"),
                    ));
                }
                let validated_role_spec_hash =
                    match Self::validate_claim_authority_context_lineage(&claim, cas) {
                        Ok(hash) => hash,
                        Err((condition, detail)) => {
                            warn!(
                                work_id = %request.work_id,
                                deny_condition = %condition,
                                error = %detail,
                                "SpawnEpisode rejected: authority context lineage validation failed"
                            );
                            return Ok(deny_response_for_authority_context(
                                condition,
                                format!("spawn authority context validation failed: {detail}"),
                            ));
                        },
                    };
                role_spec_hash = Some(validated_role_spec_hash);
                (
                    hex::encode(claim.policy_resolution.context_pack_hash),
                    hex::encode(claim.policy_resolution.context_pack_recipe_hash),
                )
            } else {
                ("WVR-0002".to_string(), "WVR-0002".to_string())
            };

        let (adapter_profile_hash, selection_decision) = match self
            .resolve_spawn_adapter_profile_hash(
                request.adapter_profile_hash.as_deref(),
                request_role,
                &claim,
            ) {
            Ok(resolved) => resolved,
            Err(e) => {
                warn!(
                    work_id = %request.work_id,
                    error = %e,
                    "SpawnEpisode rejected: adapter profile hash resolution failed"
                );
                return Ok(PrivilegedResponse::error(
                    PrivilegedErrorCode::CapabilityRequestRejected,
                    e,
                ));
            },
        };

        let resolved_profile_id = selection_decision
            .as_ref()
            .map(|decision| decision.selected_profile_id.as_str())
            .or_else(|| {
                self.adapter_profile_ids_by_hash
                    .get(&adapter_profile_hash)
                    .map(String::as_str)
            });
        let role_spec_hash_log = role_spec_hash.map_or_else(|| "WVR-0002".to_string(), hex::encode);

        info!(
            work_id = %request.work_id,
            policy_resolved_ref = %claim.policy_resolution.policy_resolved_ref,
            adapter_profile_hash = %hex::encode(adapter_profile_hash),
            selected_profile_id = ?resolved_profile_id,
            role_spec_hash = %role_spec_hash_log,
            context_pack_hash = %context_pack_hash_component,
            context_pack_recipe_hash = %context_pack_recipe_hash_component,
            "SpawnEpisode authorized with policy resolution"
        );

        // Generate session ID and ephemeral handle
        let session_id = format!("S-{}", uuid::Uuid::new_v4());
        let ephemeral_handle = EphemeralHandle::generate();

        // TCK-00406: Build authoritative V1 envelope and enforce spawn gate
        // in the production dispatch path BEFORE any session-state mutation.
        let Some(envelope_risk_tier) =
            crate::episode::RiskTier::from_u8(claim.policy_resolution.resolved_risk_tier)
        else {
            warn!(
                work_id = %request.work_id,
                resolved_risk_tier = claim.policy_resolution.resolved_risk_tier,
                "SpawnEpisode rejected: invalid resolved_risk_tier for envelope gate (fail-closed)"
            );
            return Ok(PrivilegedResponse::error(
                PrivilegedErrorCode::CapabilityRequestRejected,
                format!(
                    "invalid resolved_risk_tier {} for envelope gate (fail-closed)",
                    claim.policy_resolution.resolved_risk_tier
                ),
            ));
        };

        let mut view_commitment_hasher = blake3::Hasher::new();
        view_commitment_hasher.update(b"apm2.spawn.view_commitment.v1");
        view_commitment_hasher.update(b"\0");
        view_commitment_hasher.update(session_id.as_bytes());
        view_commitment_hasher.update(b"\0");
        view_commitment_hasher.update(claim.work_id.as_bytes());
        view_commitment_hasher.update(b"\0");
        view_commitment_hasher.update(claim.lease_id.as_bytes());
        view_commitment_hasher.update(b"\0");
        view_commitment_hasher.update(claim.actor_id.as_bytes());
        view_commitment_hasher.update(b"\0");
        view_commitment_hasher.update(&claim.policy_resolution.resolved_policy_hash);
        view_commitment_hasher.update(b"\0");
        view_commitment_hasher.update(&claim.policy_resolution.capability_manifest_hash);
        view_commitment_hasher.update(b"\0");
        view_commitment_hasher.update(&claim.policy_resolution.context_pack_hash);
        view_commitment_hasher.update(b"\0");
        view_commitment_hasher.update(&claim.policy_resolution.context_pack_recipe_hash);
        view_commitment_hasher.update(b"\0");
        view_commitment_hasher.update(&adapter_profile_hash);
        view_commitment_hasher.update(b"\0");
        if let Some(hash) = role_spec_hash {
            view_commitment_hasher.update(&hash);
            view_commitment_hasher.update(b"\0");
        }
        let view_commitment_hash: [u8; 32] = view_commitment_hasher.finalize().into();

        let mut freshness_pinset_hasher = blake3::Hasher::new();
        freshness_pinset_hasher.update(b"apm2.spawn.freshness_pinset.v1");
        freshness_pinset_hasher.update(b"\0");
        freshness_pinset_hasher.update(claim.work_id.as_bytes());
        freshness_pinset_hasher.update(b"\0");
        freshness_pinset_hasher.update(claim.lease_id.as_bytes());
        freshness_pinset_hasher.update(b"\0");
        freshness_pinset_hasher.update(&claim.policy_resolution.context_pack_hash);
        freshness_pinset_hasher.update(b"\0");
        freshness_pinset_hasher.update(&claim.policy_resolution.context_pack_recipe_hash);
        freshness_pinset_hasher.update(b"\0");
        let freshness_pinset_hash: [u8; 32] = freshness_pinset_hasher.finalize().into();

        let pinned_snapshot = crate::episode::PinnedSnapshot::builder()
            .repo_hash(claim.policy_resolution.context_pack_hash)
            .policy_hash(claim.policy_resolution.resolved_policy_hash)
            .model_profile_hash(adapter_profile_hash)
            .build();

        let context_refs = crate::episode::ContextRefs {
            context_pack_hash: claim.policy_resolution.context_pack_hash.to_vec(),
            dcp_refs: vec![claim.policy_resolution.policy_resolved_ref.clone()],
        };

        let mut envelope_builder = crate::episode::EpisodeEnvelopeV1::builder()
            .episode_id(&session_id)
            .actor_id(&claim.actor_id)
            .work_id(&claim.work_id)
            .lease_id(&claim.lease_id)
            .capability_manifest_hash(claim.policy_resolution.capability_manifest_hash)
            .budget(crate::episode::EpisodeBudget::default())
            .stop_conditions(resolved_stop_conditions.clone())
            .pinned_snapshot(pinned_snapshot)
            .risk_tier(envelope_risk_tier)
            .context_refs(context_refs)
            .view_commitment_hash(view_commitment_hash)
            .freshness_pinset_hash(freshness_pinset_hash);

        if let Some((receipt_hash, _, _)) = delegated_gate_inputs.as_ref() {
            envelope_builder = envelope_builder.permeability_receipt_hash(*receipt_hash);
        }

        let authoritative_envelope = match envelope_builder.build() {
            Ok(envelope) => envelope,
            Err(e) => {
                warn!(
                    work_id = %request.work_id,
                    error = %e,
                    "SpawnEpisode rejected: failed to build authoritative envelope (fail-closed)"
                );
                return Ok(PrivilegedResponse::error(
                    PrivilegedErrorCode::CapabilityRequestRejected,
                    format!("authoritative envelope build failed: {e}"),
                ));
            },
        };

        if let Some((_, required_authority, now_ms)) = delegated_gate_inputs.as_ref() {
            let Some(receipt) = claim.permeability_receipt.as_ref() else {
                warn!(
                    work_id = %request.work_id,
                    "SpawnEpisode rejected: delegated gate inputs present without receipt (fail-closed)"
                );
                return Ok(PrivilegedResponse::error(
                    PrivilegedErrorCode::CapabilityRequestRejected,
                    "delegated gate inputs present without claim permeability receipt (fail-closed)",
                ));
            };

            if let Err(e) = crate::episode::validate_delegated_spawn_gate(
                Some(&authoritative_envelope),
                receipt,
                required_authority,
                *now_ms,
            ) {
                warn!(
                    work_id = %request.work_id,
                    error = %e,
                    "SpawnEpisode rejected: delegated envelope gate failed"
                );
                return Ok(PrivilegedResponse::error(
                    PrivilegedErrorCode::CapabilityRequestRejected,
                    format!("delegated envelope gate failed: {e}"),
                ));
            }
        } else if let Err(e) =
            crate::episode::validate_spawn_gate(Some(&authoritative_envelope), false)
        {
            warn!(
                work_id = %request.work_id,
                error = %e,
                "SpawnEpisode rejected: envelope gate failed"
            );
            return Ok(PrivilegedResponse::error(
                PrivilegedErrorCode::CapabilityRequestRejected,
                format!("envelope gate failed: {e}"),
            ));
        }

        let authoritative_envelope_hash = authoritative_envelope.envelope_hash();

        // TCK-00384 security fix: Transactional session + telemetry registration
        // with guaranteed rollback on any failure path.
        //
        // Registration order:
        //   1. Register session (may evict oldest entries for capacity).
        //   2. Clean up telemetry for any evicted sessions (policy convergence).
        //   3. Register telemetry -- on failure, rollback session.
        //   4. Mint token -- on failure, rollback both session and telemetry.
        //   5. Serialize token -- on failure, rollback both.
        //
        // Session-first ordering is necessary because the session registry
        // uses LRU eviction at capacity while the telemetry store uses
        // fail-closed rejection.  By registering the session first, eviction
        // frees capacity in the telemetry store before we attempt telemetry
        // registration.  Any failure after step 1 rolls back the session via
        // `remove_session` (added to the `SessionRegistry` trait for this
        // purpose).
        //
        // This makes the three stores (session registry, telemetry, token)
        // atomically consistent: either ALL succeed or NONE are committed.

        // Step 1: Persist session state (may evict oldest for capacity).
        // TCK-00256: The episode_runtime can create/start episodes
        // asynchronously when needed.
        let session_state = SessionState {
            session_id: session_id.clone(),
            work_id: request.work_id.clone(),
            role: request_role.into(),
            ephemeral_handle: ephemeral_handle.to_string(),
            lease_id: claim.lease_id.clone(),
            policy_resolved_ref: claim.policy_resolution.policy_resolved_ref.clone(),
            capability_manifest_hash: claim.policy_resolution.capability_manifest_hash.to_vec(),
            episode_id: None,
            pcac_policy: claim.policy_resolution.pcac_policy.clone(),
            pointer_only_waiver: claim.policy_resolution.pointer_only_waiver.clone(),
        };

        let evicted_sessions = match self.session_registry.register_session(session_state) {
            Ok(evicted) => evicted,
            Err(e) => {
                warn!(error = %e, "Session registration failed");
                return Ok(PrivilegedResponse::error(
                    PrivilegedErrorCode::CapabilityRequestRejected,
                    format!("session registration failed: {e}"),
                ));
            },
        };

        // Step 2: Clean up telemetry for evicted sessions to prevent
        // orphaned entries and free capacity (policy convergence fix).
        //
        // TCK-00384 security BLOCKER 1: Use `remove_and_return` to capture
        // the evicted telemetry entries.  If a later spawn step fails, we
        // restore them alongside the session registry entries so that
        // rollback is complete (telemetry + session + manifest).
        let evicted_telemetry: Vec<(String, std::sync::Arc<crate::session::SessionTelemetry>)> =
            self.telemetry_store
                .as_ref()
                .map_or_else(Vec::new, |store| {
                    evicted_sessions
                        .iter()
                        .filter_map(|s| {
                            store
                                .remove_and_return(&s.session_id)
                                .map(|t| (s.session_id.clone(), t))
                        })
                        .collect()
                });

        // Step 2b: Clean up manifest entries for evicted sessions to prevent
        // unbounded manifest store growth.  Captured via `remove_and_return`
        // so they can be restored during rollback.
        let evicted_manifests: Vec<(String, std::sync::Arc<crate::episode::CapabilityManifest>)> =
            evicted_sessions
                .iter()
                .filter_map(|s| {
                    self.manifest_store
                        .remove_and_return(&s.session_id)
                        .map(|m| (s.session_id.clone(), m))
                })
                .collect();

        // Step 2c: Capture and remove per-session brokers for evicted sessions.
        // Restored during rollback to keep broker/session stores converged.
        let evicted_brokers: Vec<(
            String,
            SharedToolBroker<crate::episode::capability::StubManifestLoader>,
        )> = self
            .session_broker_registry
            .as_ref()
            .map_or_else(Vec::new, |store| {
                evicted_sessions
                    .iter()
                    .filter_map(|s| {
                        store
                            .remove_and_return(&s.session_id)
                            .map(|b| (s.session_id.clone(), b))
                    })
                    .collect()
            });

        // Step 2d: Capture and remove stop conditions for evicted sessions.
        // These are restored if a later spawn step fails (atomic rollback).
        let evicted_stop_conditions: Vec<(String, crate::episode::envelope::StopConditions)> = self
            .stop_conditions_store
            .as_ref()
            .map_or_else(Vec::new, |store| {
                evicted_sessions
                    .iter()
                    .filter_map(|s| {
                        store
                            .remove_and_return(&s.session_id)
                            .map(|c| (s.session_id.clone(), c))
                    })
                    .collect()
            });

        // Step 3: Register telemetry with started_at_ns.
        // The wall-clock timestamp is stored as audit metadata only;
        // elapsed duration is computed from a monotonic Instant inside the
        // store (security review: no wall-clock dependency for duration_ms).
        if let Some(ref store) = self.telemetry_store {
            let started_at_ns = SystemTime::now()
                .duration_since(SystemTime::UNIX_EPOCH)
                .map(|d| {
                    #[allow(clippy::cast_possible_truncation)]
                    let ns = d.as_nanos() as u64;
                    ns
                })
                .unwrap_or(0);
            if let Err(e) = store.register(&session_id, started_at_ns) {
                // Rollback session on telemetry failure and restore evicted
                // sessions + telemetry so capacity is not permanently lost.
                let rollback_warn = self.rollback_spawn(
                    &session_id,
                    &evicted_sessions,
                    &evicted_telemetry,
                    &evicted_manifests,
                    &evicted_brokers,
                    &evicted_stop_conditions,
                    false,
                );
                if let Some(ref rw) = rollback_warn {
                    warn!(rollback_errors = %rw, "Partial rollback failure during telemetry error recovery");
                }
                warn!(error = %e, "Telemetry registration rejected (store at capacity)");
                let msg = rollback_warn.map_or_else(
                    || format!("telemetry store at capacity: {e}"),
                    |rw| {
                        format!("telemetry store at capacity: {e} (rollback partial failure: {rw})")
                    },
                );
                return Ok(PrivilegedResponse::error(
                    PrivilegedErrorCode::CapabilityRequestRejected,
                    msg,
                ));
            }
        }

        // Step 3b: Register stop conditions for the session (TCK-00351 v4).
        // The pre-actuation gate reads from this store to enforce
        // max_episodes / escalation_predicate limits.
        //
        // TCK-00351 BLOCKER 1+2 FIX: Persist policy-validated stop
        // conditions derived from authoritative request fields.
        if let Some(ref store) = self.stop_conditions_store {
            if let Err(e) = store.register(&session_id, resolved_stop_conditions) {
                // Rollback session, telemetry, and restore evicted entries.
                let rollback_warn = self.rollback_spawn(
                    &session_id,
                    &evicted_sessions,
                    &evicted_telemetry,
                    &evicted_manifests,
                    &evicted_brokers,
                    &evicted_stop_conditions,
                    false,
                );
                if let Some(ref rw) = rollback_warn {
                    warn!(rollback_errors = %rw, "Partial rollback failure during stop conditions error recovery");
                }
                warn!(error = %e, "Stop conditions registration rejected (store at capacity)");
                let msg = rollback_warn.map_or_else(
                    || format!("stop conditions store at capacity: {e}"),
                    |rw| {
                        format!("stop conditions store at capacity: {e} (rollback partial failure: {rw})")
                    },
                );
                return Ok(PrivilegedResponse::error(
                    PrivilegedErrorCode::CapabilityRequestRejected,
                    msg,
                ));
            }
        }

        debug!(
            session_id = %session_id,
            ephemeral_handle = %ephemeral_handle,
            "Session persisted"
        );

        // Step 4: Generate session token for client authentication.
        // TCK-00287 BLOCKER 2: The token is HMAC-signed and bound to this
        // session's lease_id.
        //
        // NOTE: TokenMinter uses SystemTime for TTL calculation, which is
        // acceptable since token expiry is not a protocol-authoritative event.
        // The HTF clock is used for ledger events that require causal ordering.
        let spawn_time = SystemTime::now();
        let ttl = Duration::from_secs(DEFAULT_SESSION_TOKEN_TTL_SECS);
        let session_token = match self.token_minter.mint(
            &session_id,
            &claim.lease_id,
            spawn_time,
            ttl,
        ) {
            Ok(token) => token,
            Err(e) => {
                // Rollback session, telemetry, and restore evicted
                // sessions + telemetry so capacity is not lost.
                let rollback_warn = self.rollback_spawn(
                    &session_id,
                    &evicted_sessions,
                    &evicted_telemetry,
                    &evicted_manifests,
                    &evicted_brokers,
                    &evicted_stop_conditions,
                    false,
                );
                if let Some(ref rw) = rollback_warn {
                    warn!(rollback_errors = %rw, "Partial rollback failure during token minting error recovery");
                }
                warn!(error = %e, "Session token minting failed");
                let msg = rollback_warn.map_or_else(
                    || format!("session token generation failed: {e}"),
                    |rw| {
                        format!(
                            "session token generation failed: {e} (rollback partial failure: {rw})"
                        )
                    },
                );
                return Ok(PrivilegedResponse::error(
                    PrivilegedErrorCode::CapabilityRequestRejected,
                    msg,
                ));
            },
        };

        // Step 5: Serialize the token to JSON for inclusion in the response.
        let session_token_json = match serde_json::to_string(&session_token) {
            Ok(json) => json,
            Err(e) => {
                // Rollback session, telemetry, and restore evicted
                // sessions + telemetry so capacity is not lost.
                let rollback_warn = self.rollback_spawn(
                    &session_id,
                    &evicted_sessions,
                    &evicted_telemetry,
                    &evicted_manifests,
                    &evicted_brokers,
                    &evicted_stop_conditions,
                    false,
                );
                if let Some(ref rw) = rollback_warn {
                    warn!(rollback_errors = %rw, "Partial rollback failure during token serialization error recovery");
                }
                warn!(error = %e, "Session token serialization failed");
                let msg = rollback_warn.map_or_else(
                    || format!("session token serialization failed: {e}"),
                    |rw| format!("session token serialization failed: {e} (rollback partial failure: {rw})"),
                );
                return Ok(PrivilegedResponse::error(
                    PrivilegedErrorCode::CapabilityRequestRejected,
                    msg,
                ));
            },
        };

        // TCK-00317: Load capability manifest from CAS using hash from
        // PolicyResolution.
        //
        // Per DOD item 1 (CAS Storage & Hash Loading):
        // - Manifests are stored in CAS and referenced by hash
        // - SpawnEpisode loads the manifest using the hash from PolicyResolution
        // - For Reviewer role: Missing manifests result in fail-closed rejection
        // - For other roles: Fall back to minimal manifest (until their manifests are
        //   defined in CAS)
        //
        // Per DOD item 2 (Policy Resolution Bypass fix):
        // - The manifest is NOT selected by role name; it's loaded by hash
        // - StubPolicyResolver returns reviewer_v0_manifest_hash() for Reviewer
        // - This ensures the policy resolution controls which manifest is used
        let manifest_hash: [u8; 32] = claim.policy_resolution.capability_manifest_hash;

        let manifest = match self.manifest_loader.load_manifest(&manifest_hash) {
            Ok(m) => m,
            Err(e) => {
                // TCK-00317: For Reviewer role, fail-closed if manifest not found
                // For other roles, fall back to minimal manifest until their
                // manifests are stored in CAS.
                if request_role == WorkRole::Reviewer {
                    // TCK-00384 security fix: rollback session + telemetry on
                    // manifest-load failure so bounded capacity is not leaked.
                    // Restore evicted sessions + telemetry so capacity is not
                    // lost.
                    let rollback_warn = self.rollback_spawn(
                        &session_id,
                        &evicted_sessions,
                        &evicted_telemetry,
                        &evicted_manifests,
                        &evicted_brokers,
                        &evicted_stop_conditions,
                        false,
                    );
                    if let Some(ref rw) = rollback_warn {
                        warn!(rollback_errors = %rw, "Partial rollback failure during manifest load error recovery");
                    }
                    warn!(
                        work_id = %request.work_id,
                        manifest_hash = %hex::encode(manifest_hash),
                        error = %e,
                        "SpawnEpisode rejected: reviewer manifest not found in CAS"
                    );
                    let msg = rollback_warn.map_or_else(
                        || format!("reviewer capability manifest not found in CAS: {e}"),
                        |rw| format!("reviewer capability manifest not found in CAS: {e} (rollback partial failure: {rw})"),
                    );
                    return Ok(PrivilegedResponse::error(
                        PrivilegedErrorCode::CapabilityRequestRejected,
                        msg,
                    ));
                }

                // Non-Reviewer roles: fall back to minimal manifest
                // This maintains backward compatibility until all role manifests
                // are defined and stored in CAS.
                debug!(
                    work_id = %request.work_id,
                    role = ?request_role,
                    manifest_hash = %hex::encode(manifest_hash),
                    "Manifest not in CAS, using minimal fallback manifest for non-reviewer role"
                );
                CapabilityManifest::from_hash_with_default_allowlist(&manifest_hash)
            },
        };

        self.manifest_store.register(&session_id, manifest.clone());

        // TCK-00401: Build and register per-session broker in production path.
        //
        // This enforces session-scoped capability/policy isolation and ensures
        // broker validator/policy are initialized before session spawn commits.
        if let Some(ref broker_store) = self.session_broker_registry {
            match self.build_session_broker(&session_id, &manifest) {
                Ok(broker) => broker_store.register(&session_id, broker),
                Err(e) => {
                    let rollback_warn = self.rollback_spawn(
                        &session_id,
                        &evicted_sessions,
                        &evicted_telemetry,
                        &evicted_manifests,
                        &evicted_brokers,
                        &evicted_stop_conditions,
                        true,
                    );
                    if let Some(ref rw) = rollback_warn {
                        warn!(rollback_errors = %rw, "Partial rollback failure during broker initialization");
                    }
                    warn!(
                        session_id = %session_id,
                        error = %e,
                        "SpawnEpisode rejected: per-session broker initialization failed"
                    );
                    let msg = rollback_warn.map_or_else(
                        || format!("per-session broker initialization failed: {e}"),
                        |rw| format!(
                            "per-session broker initialization failed: {e} (rollback partial failure: {rw})"
                        ),
                    );
                    return Ok(PrivilegedResponse::error(
                        PrivilegedErrorCode::CapabilityRequestRejected,
                        msg,
                    ));
                },
            }
        }

        // TCK-00352 Security Review MAJOR 2: Mint and register V1 manifest
        // in the shared store so that SessionDispatcher can enforce V1 scope
        // checks (risk tier ceiling, host restrictions, envelope binding)
        // during handle_request_tool.
        if let Some(ref v1_store) = self.v1_manifest_store {
            // TCK-00352 MAJOR 1 v3 fix: The scope baseline MUST come from
            // an authoritative policy source (claim.policy_resolution), NOT
            // from the candidate manifest. Building the baseline from the
            // manifest itself makes the subset check tautological (the
            // manifest is always a subset of itself). Fail closed when no
            // independent baseline is available.
            let scope_baseline = if let Some(ref baseline) =
                claim.policy_resolution.resolved_scope_baseline
            {
                baseline.clone()
            } else {
                // No independent scope baseline available -- fail closed.
                let rollback_warn = self.rollback_spawn(
                    &session_id,
                    &evicted_sessions,
                    &evicted_telemetry,
                    &evicted_manifests,
                    &evicted_brokers,
                    &evicted_stop_conditions,
                    true,
                );
                if let Some(ref rw) = rollback_warn {
                    warn!(
                        rollback_errors = %rw,
                        "Partial rollback failure during scope baseline check"
                    );
                }
                warn!(
                    session_id = %session_id,
                    "SpawnEpisode rejected: no policy-resolved scope baseline (fail-closed)"
                );
                let msg = rollback_warn.map_or_else(
                        || "no policy-resolved scope baseline available; V1 minting denied (fail-closed)".to_string(),
                        |rw| format!("no policy-resolved scope baseline; V1 denied (rollback partial: {rw})"),
                    );
                return Ok(PrivilegedResponse::error(
                    PrivilegedErrorCode::CapabilityRequestRejected,
                    msg,
                ));
            };
            if let Err(e) = crate::episode::capability::validate_manifest_scope_subset(
                &manifest,
                &scope_baseline,
            ) {
                let rollback_warn = self.rollback_spawn(
                    &session_id,
                    &evicted_sessions,
                    &evicted_telemetry,
                    &evicted_manifests,
                    &evicted_brokers,
                    &evicted_stop_conditions,
                    true,
                );
                if let Some(ref rw) = rollback_warn {
                    warn!(
                        rollback_errors = %rw,
                        "Partial rollback failure during scope subset validation"
                    );
                }
                warn!(
                    session_id = %session_id,
                    error = %e,
                    "SpawnEpisode rejected: manifest scope exceeds policy baseline (fail-closed)"
                );
                let msg = rollback_warn.map_or_else(
                    || format!("manifest scope validation failed: {e}"),
                    |rw| {
                        format!(
                            "manifest scope validation failed: {e} (rollback partial failure: {rw})"
                        )
                    },
                );
                return Ok(PrivilegedResponse::error(
                    PrivilegedErrorCode::CapabilityRequestRejected,
                    msg,
                ));
            }

            // TCK-00352 MAJOR 3 fix: Obtain mint token through the
            // governance-resolver authority path, not direct construction.
            // This centralizes minting authority to the policy resolver and
            // ensures the token is obtained through the approved channel.
            let governance = crate::governance::GovernancePolicyResolver::new();
            let mint_token = governance.mint_token();

            // BLOCKER 2 v3 fix: Derive risk tier ceiling from the
            // policy-resolved risk tier (claim.policy_resolution), NOT
            // from the manifest's own capabilities. Using the manifest's
            // max_capability_tier() makes the check tautological because
            // the very manifest being validated determines its own ceiling.
            //
            // SECURITY: Fail closed (Tier4, most restrictive) when the
            // resolved_risk_tier cannot be parsed to a valid RiskTier.
            let risk_tier_ceiling = crate::episode::envelope::RiskTier::from_u8(
                claim.policy_resolution.resolved_risk_tier,
            )
            .unwrap_or(crate::episode::envelope::RiskTier::Tier4);

            // TCK-00352 BLOCKER 2 fix: V1 minting failure is now a HARD
            // DENY (fail-closed). When V1 controls are configured (v1_store
            // is Some), every session MUST have an active V1 manifest.
            // Continuing without one would leave the session without V1
            // scope enforcement, which is fail-open.
            match crate::episode::CapabilityManifestV1::mint(
                mint_token,
                manifest.clone(),
                risk_tier_ceiling,
                Vec::new(), // Host restrictions from policy (empty = fail-closed for network)
            ) {
                Ok(v1_manifest) => {
                    v1_store.register(&session_id, v1_manifest);
                    debug!(
                        session_id = %session_id,
                        risk_tier_ceiling = ?risk_tier_ceiling,
                        "V1 capability manifest minted and registered"
                    );
                },
                Err(e) => {
                    // HARD DENY: Roll back all state and reject the spawn.
                    let rollback_warn = self.rollback_spawn(
                        &session_id,
                        &evicted_sessions,
                        &evicted_telemetry,
                        &evicted_manifests,
                        &evicted_brokers,
                        &evicted_stop_conditions,
                        true,
                    );
                    if let Some(ref rw) = rollback_warn {
                        warn!(
                            rollback_errors = %rw,
                            "Partial rollback failure during V1 mint failure recovery"
                        );
                    }
                    warn!(
                        session_id = %session_id,
                        error = %e,
                        "SpawnEpisode rejected: V1 manifest minting failed (fail-closed)"
                    );
                    let msg = rollback_warn.map_or_else(
                        || format!("V1 manifest minting failed: {e}"),
                        |rw| {
                            format!(
                                "V1 manifest minting failed: {e} (rollback partial failure: {rw})"
                            )
                        },
                    );
                    return Ok(PrivilegedResponse::error(
                        PrivilegedErrorCode::CapabilityRequestRejected,
                        msg,
                    ));
                },
            }
        }

        debug!(
            session_id = %session_id,
            role = ?request_role,
            manifest_id = %manifest.manifest_id,
            tool_allowlist_len = manifest.tool_allowlist.len(),
            "Capability manifest registered in shared store"
        );

        // TCK-00268: Emit session_spawned metric
        if let Some(ref metrics) = self.metrics {
            let role_str = match request_role {
                WorkRole::Implementer => "implementer",
                WorkRole::Reviewer => "reviewer",
                WorkRole::GateExecutor => "gate_executor",
                WorkRole::Coordinator => "coordinator",
                WorkRole::Unspecified => "unspecified",
            };
            metrics.daemon_metrics().session_spawned(role_str);
        }

        // TCK-00289: Emit SessionStarted ledger event for audit trail.
        // Per DOD: "ClaimWork/SpawnEpisode persist state and emit ledger events"
        let timestamp_ns = match self.get_htf_timestamp_ns() {
            Ok(ts) => ts,
            Err(e) => {
                // TCK-00384 security fix: rollback session, telemetry, and
                // manifest on timestamp failure.  Also restore evicted
                // sessions + telemetry so capacity is not permanently lost.
                let rollback_warn = self.rollback_spawn(
                    &session_id,
                    &evicted_sessions,
                    &evicted_telemetry,
                    &evicted_manifests,
                    &evicted_brokers,
                    &evicted_stop_conditions,
                    true,
                );
                if let Some(ref rw) = rollback_warn {
                    warn!(rollback_errors = %rw, "Partial rollback failure during timestamp error recovery");
                }
                // TCK-00289: Fail-closed - do not proceed without valid timestamp
                warn!(error = %e, "HTF timestamp generation failed for SessionStarted - failing closed");
                let msg = rollback_warn.map_or_else(
                    || format!("HTF timestamp error: {e}"),
                    |rw| format!("HTF timestamp error: {e} (rollback partial failure: {rw})"),
                );
                return Ok(PrivilegedResponse::error(
                    PrivilegedErrorCode::CapabilityRequestRejected,
                    msg,
                ));
            },
        };

        // TCK-00319: Create and start episode with workspace root.
        // This ensures that all file/execute operations are confined to the
        // workspace directory. The episode must be started BEFORE returning
        // to the client so that tool handlers are properly initialized.
        //
        let envelope_hash = authoritative_envelope_hash;

        // Try to create and start the episode. This requires a Tokio runtime.
        // In unit tests without a runtime, we skip episode creation but still
        // return a valid session (for backward compatibility with existing tests).
        let episode_id_opt = if let Ok(handle) = tokio::runtime::Handle::try_current() {
            match tokio::task::block_in_place(|| {
                handle.block_on(async {
                    // Create the episode with envelope hash and timestamp
                    let episode_id = self
                        .episode_runtime
                        .create(envelope_hash, timestamp_ns)
                        .await?;

                    // Start with workspace - this initializes rooted handlers
                    let _session_handle = self
                        .episode_runtime
                        .start_with_workspace(
                            &episode_id,
                            &claim.lease_id,
                            timestamp_ns,
                            workspace_path,
                        )
                        .await?;

                    Ok::<_, crate::episode::EpisodeError>(episode_id)
                })
            }) {
                Ok(id) => Some(id),
                Err(e) => {
                    // TCK-00384 security fix: rollback session, telemetry,
                    // and manifest on episode creation failure.  Restore
                    // evicted sessions + telemetry so capacity is not lost.
                    let rollback_warn = self.rollback_spawn(
                        &session_id,
                        &evicted_sessions,
                        &evicted_telemetry,
                        &evicted_manifests,
                        &evicted_brokers,
                        &evicted_stop_conditions,
                        true,
                    );
                    if let Some(ref rw) = rollback_warn {
                        warn!(rollback_errors = %rw, "Partial rollback failure during episode creation error recovery");
                    }
                    warn!(
                        work_id = %request.work_id,
                        error = %e,
                        "SpawnEpisode failed: episode creation/start failed"
                    );
                    let msg = rollback_warn.map_or_else(
                        || format!("episode creation failed: {e}"),
                        |rw| {
                            format!("episode creation failed: {e} (rollback partial failure: {rw})")
                        },
                    );
                    return Ok(PrivilegedResponse::error(
                        PrivilegedErrorCode::CapabilityRequestRejected,
                        msg,
                    ));
                },
            }
        } else {
            // No Tokio runtime available (e.g., in sync unit tests).
            // In production, this should never happen.
            // For testing backward compatibility, we allow session creation
            // without episode creation.
            #[cfg(test)]
            {
                debug!(
                    session_id = %session_id,
                    "No Tokio runtime - skipping episode creation (test mode)"
                );
                None
            }
            #[cfg(not(test))]
            {
                // TCK-00384 security fix: rollback session, telemetry,
                // and manifest when no runtime is available.  Restore
                // evicted sessions + telemetry so capacity is not lost.
                let rollback_warn = self.rollback_spawn(
                    &session_id,
                    &evicted_sessions,
                    &evicted_telemetry,
                    &evicted_manifests,
                    &evicted_brokers,
                    &evicted_stop_conditions,
                    true,
                );
                if let Some(ref rw) = rollback_warn {
                    warn!(rollback_errors = %rw, "Partial rollback failure during no-runtime error recovery");
                }
                warn!("No Tokio runtime available for episode creation");
                let msg = rollback_warn.map_or_else(
                    || "episode creation failed: no async runtime available".to_string(),
                    |rw| format!("episode creation failed: no async runtime available (rollback partial failure: {rw})"),
                );
                return Ok(PrivilegedResponse::error(
                    PrivilegedErrorCode::CapabilityRequestRejected,
                    msg,
                ));
            }
        };

        // TCK-00395 Security BLOCKER 1: Write the episode_id back to the
        // session in the registry. Without this write-back, EndSession cannot
        // resolve the episode binding and will skip runtime stop, allowing
        // the episode to continue running after session termination.
        if let Some(ref episode_id) = episode_id_opt {
            if let Err(e) = self
                .session_registry
                .update_episode_id(&session_id, episode_id.to_string())
            {
                warn!(
                    error = %e,
                    session_id = %session_id,
                    episode_id = %episode_id,
                    "Failed to write episode_id back to session registry - failing closed"
                );

                // TCK-00384 review fix: unified post-start rollback stops
                // the episode and cleans up session/telemetry/manifest.
                let rollback_warn = self.rollback_spawn_with_episode_stop(
                    episode_id_opt.as_ref(),
                    &session_id,
                    &evicted_sessions,
                    &evicted_telemetry,
                    &evicted_manifests,
                    &evicted_brokers,
                    &evicted_stop_conditions,
                    timestamp_ns,
                    "update_episode_id failure",
                );
                let msg = rollback_warn.map_or_else(
                    || format!("session episode_id update failed: {e}"),
                    |rw| {
                        format!(
                            "session episode_id update failed: {e} (rollback partial failure: {rw})"
                        )
                    },
                );
                return Ok(PrivilegedResponse::error(
                    PrivilegedErrorCode::CapabilityRequestRejected,
                    msg,
                ));
            }
            debug!(
                session_id = %session_id,
                episode_id = %episode_id,
                workspace_root = %request.workspace_root,
                "Episode created, started, and bound to session"
            );
        }

        // TCK-00395 Security MAJOR 2: Fail closed when peer credentials are
        // missing. Same pattern as ClaimWork. SpawnEpisode emits authoritative
        // ledger events (SessionStarted + WorkTransitioned), and recording
        // "unknown" as the actor identity would break the accountability chain.
        //
        // SECURITY: Validate peer credentials BEFORE any side-effectful
        // operations (adapter process spawn). Unauthorized requests must be
        // rejected before triggering subprocess spawn.
        //
        // TCK-00384 review fix: Stop the running episode before returning on
        // this failure path.  The original `?` operator would exit without
        // stopping the episode, leaking a running runtime episode.
        let Some(peer_creds) = ctx.peer_credentials() else {
            // TCK-00384 review fix: unified post-start rollback stops the
            // episode and cleans up session/telemetry/manifest.
            let rollback_warn = self.rollback_spawn_with_episode_stop(
                episode_id_opt.as_ref(),
                &session_id,
                &evicted_sessions,
                &evicted_telemetry,
                &evicted_manifests,
                &evicted_brokers,
                &evicted_stop_conditions,
                timestamp_ns,
                "peer credentials failure",
            );
            let msg = rollback_warn.map_or_else(
                || "peer credentials required for episode spawn".to_string(),
                |rw| format!("peer credentials required for episode spawn (rollback partial failure: {rw})"),
            );
            return Ok(PrivilegedResponse::error(
                PrivilegedErrorCode::CapabilityRequestRejected,
                msg,
            ));
        };
        let actor_id = derive_actor_id(peer_creds);

        // TCK-00399: Spawn agent CLI process via adapter registry.
        //
        // After the episode is created and Running, load the adapter profile
        // from CAS, build a HarnessConfig with template expansion, and spawn
        // the agent process. Fail-closed: if the adapter registry is not
        // configured, SpawnEpisode must return an error -- a "successful"
        // response without a spawned agent process is a silent failure.
        if let Some(episode_id) = &episode_id_opt {
            let Some(registry) = &self.adapter_registry else {
                error!(
                    episode_id = %episode_id,
                    "adapter registry not configured; cannot spawn adapter process"
                );
                let rollback_warn = self.rollback_spawn_with_episode_stop(
                    episode_id_opt.as_ref(),
                    &session_id,
                    &evicted_sessions,
                    &evicted_telemetry,
                    &evicted_manifests,
                    &evicted_brokers,
                    &evicted_stop_conditions,
                    timestamp_ns,
                    "missing adapter registry",
                );
                let msg = rollback_warn.map_or_else(
                    || "adapter registry not configured: cannot spawn adapter process".to_string(),
                    |rw| {
                        format!(
                            "adapter registry not configured: cannot spawn adapter process \
                         (rollback partial failure: {rw})"
                        )
                    },
                );
                return Ok(PrivilegedResponse::error(
                    PrivilegedErrorCode::CapabilityRequestRejected,
                    msg,
                ));
            };
            // SpawnFailure carries both the display message and a typed
            // rate-limit classification derived from `AdapterError` variants,
            // avoiding heuristic string matching for routing-state updates.
            #[allow(clippy::items_after_statements)]
            struct SpawnFailure {
                message: String,
                rate_limited: bool,
            }

            #[allow(clippy::items_after_statements)]
            impl std::fmt::Display for SpawnFailure {
                fn fmt(&self, f: &mut std::fmt::Formatter<'_>) -> std::fmt::Result {
                    f.write_str(&self.message)
                }
            }

            #[allow(clippy::items_after_statements)]
            impl SpawnFailure {
                fn config(msg: impl Into<String>) -> Self {
                    Self {
                        message: msg.into(),
                        rate_limited: false,
                    }
                }

                fn from_episode_error(prefix: &str, err: &crate::episode::EpisodeError) -> Self {
                    // Typed classification: use `is_rate_limited()` on the
                    // structured error instead of heuristic string matching.
                    let rate_limited = err.is_rate_limited();
                    Self {
                        message: format!("{prefix}: {err}"),
                        rate_limited,
                    }
                }
            }

            let spawn_result: Result<(), SpawnFailure> = (|| {
                let cas = self.adapter_profile_cas.as_ref().ok_or_else(|| {
                    SpawnFailure::config("adapter spawn requires CAS configuration")
                })?;

                // SECURITY: adapter_profile_hash was resolved by
                // resolve_spawn_adapter_profile_hash which documents the
                // authorization trust chain: the caller was authenticated via
                // ClaimWork + lease_id before the hash was accepted. Loading
                // from CAS here is safe because only authorized callers can
                // reach this point.
                let profile = apm2_core::fac::AgentAdapterProfileV1::load_from_cas(
                    cas.as_ref(),
                    &adapter_profile_hash,
                )
                .map_err(|e| SpawnFailure::config(format!("adapter profile load failed: {e}")))?;

                // SECURITY: Fail-closed adapter mapping. Unknown/unsupported
                // profiles or modes MUST be denied, never silently downgraded.
                let adapter_type = if profile.profile_id == apm2_core::fac::CODEX_CLI_PROFILE_ID {
                    crate::episode::AdapterType::Codex
                } else {
                    match profile.adapter_mode {
                        apm2_core::fac::AdapterMode::StructuredOutput => {
                            crate::episode::AdapterType::ClaudeCode
                        },
                        apm2_core::fac::AdapterMode::BlackBox => crate::episode::AdapterType::Raw,
                        unsupported => {
                            return Err(SpawnFailure::config(format!(
                                "unsupported adapter mode '{unsupported}': \
                                 only BlackBox and StructuredOutput are supported"
                            )));
                        },
                    }
                };

                let adapter = registry.get(adapter_type).ok_or_else(|| {
                    SpawnFailure::config(format!(
                        "adapter type {adapter_type} not registered in registry"
                    ))
                })?;

                let model = if profile.profile_id == apm2_core::fac::CODEX_CLI_PROFILE_ID {
                    "gpt-5.3-codex"
                } else {
                    &profile.profile_id
                };

                // Build HarnessConfig. Prompt is empty -- actual prompt
                // delivery is via stdin/PTY, not in argv.
                let session_token_secret = secrecy::SecretString::from(session_token_json.clone());
                let config = Self::build_harness_config(
                    &profile,
                    episode_id.as_str(),
                    &request.workspace_root,
                    "",
                    model,
                    &session_token_secret,
                )
                .map_err(SpawnFailure::config)?;

                let rt_handle = tokio::runtime::Handle::try_current()
                    .map_err(|_| SpawnFailure::config("adapter spawn requires async runtime"))?;
                tokio::task::block_in_place(|| {
                    rt_handle.block_on(async {
                        self.episode_runtime
                            .spawn_adapter(episode_id, config, adapter)
                            .await
                            .map_err(|e| {
                                SpawnFailure::from_episode_error("adapter spawn failed", &e)
                            })
                    })
                })?;

                Ok(())
            })();

            if let Err(e) = spawn_result {
                // MAJOR fix: Fail-closed on spawn errors.  A successful
                // SpawnEpisode response with no agent process is a silent
                // failure.  Roll back the episode and return an error.
                self.record_adapter_profile_failure_typed(&adapter_profile_hash, e.rate_limited);
                error!(
                    episode_id = %episode_id,
                    error = %e,
                    "adapter process spawn failed; rolling back episode"
                );
                let rollback_warn = self.rollback_spawn_with_episode_stop(
                    Some(episode_id),
                    &session_id,
                    &evicted_sessions,
                    &evicted_telemetry,
                    &evicted_manifests,
                    &evicted_brokers,
                    &evicted_stop_conditions,
                    timestamp_ns,
                    "adapter spawn failure",
                );
                let msg = rollback_warn.map_or_else(
                    || format!("adapter spawn failed: {e}"),
                    |rw| format!("adapter spawn failed: {e} (rollback partial failure: {rw})"),
                );
                return Ok(PrivilegedResponse::error(
                    PrivilegedErrorCode::CapabilityRequestRejected,
                    msg,
                ));
            }

            self.record_adapter_profile_success(&adapter_profile_hash);
        }

        // TCK-00358: Resolve identity proof profile hash for SessionStarted.
        // Primary path: the session-open handler in main.rs sets the profile
        // hash on the ConnectionContext during identity materialization.
        // Defensive fallback: if no context-bound value is set (e.g. direct
        // test usage without session-open wiring), fall back to the baseline
        // SMT-256 10^12 profile hash and log a warning. This ensures audit
        // trail always includes the field per REQ-0012 while alerting
        // operators to a missing production wiring.
        let identity_proof_profile_hash: Option<[u8; 32]> =
            ctx.identity_proof_profile_hash().copied().map_or_else(
                || {
                    warn!(
                        "identity_proof_profile_hash not set on ConnectionContext at spawn time; \
                         falling back to baseline profile hash (session-open wiring may be missing)"
                    );
                    crate::identity::IdentityProofProfileV1::baseline_smt_10e12()
                        .content_hash()
                        .ok()
                },
                Some,
            );

        // TCK-00395: Emit SessionStarted + WorkTransitioned(Claimed->InProgress)
        // atomically via emit_spawn_lifecycle. Both events are persisted as a
        // single atomic operation to prevent partial state commits.
        // TCK-00348: Thread contract binding into SessionStarted.
        if let Err(e) = self.event_emitter.emit_spawn_lifecycle(
            &session_id,
            &request.work_id,
            &claim.lease_id,
            &actor_id,
            &adapter_profile_hash,
            role_spec_hash.as_ref(),
            timestamp_ns,
            ctx.contract_binding(),
            identity_proof_profile_hash.as_ref(),
            selection_decision.as_ref(),
        ) {
            // TCK-00384 review fix: unified post-start rollback stops the
            // episode and cleans up session/telemetry/manifest.
            let rollback_warn = self.rollback_spawn_with_episode_stop(
                episode_id_opt.as_ref(),
                &session_id,
                &evicted_sessions,
                &evicted_telemetry,
                &evicted_manifests,
                &evicted_brokers,
                &evicted_stop_conditions,
                timestamp_ns,
                "event emission failure",
            );
            warn!(error = %e, "SessionStarted event emission failed");
            let msg = rollback_warn.map_or_else(
                || format!("event emission failed: {e}"),
                |rw| format!("event emission failed: {e} (rollback partial failure: {rw})"),
            );
            return Ok(PrivilegedResponse::error(
                PrivilegedErrorCode::CapabilityRequestRejected,
                msg,
            ));
        }

        debug!(
            session_id = %session_id,
            work_id = %request.work_id,
            "Spawn lifecycle events emitted (session_started + work_transitioned)"
        );

        // Successful spawn demonstrates governance-backed policy state is
        // accessible; record fresh governance health.
        self.record_governance_probe_success();

        Ok(PrivilegedResponse::SpawnEpisode(SpawnEpisodeResponse {
            session_id,
            ephemeral_handle: ephemeral_handle.to_string(),
            capability_manifest_hash: claim.policy_resolution.capability_manifest_hash.to_vec(),
            context_pack_sealed: true,
            session_token: session_token_json,
        }))
    }

    /// Handles `IssueCapability` requests (IPC-PRIV-003).
    ///
    /// # TCK-00289 Implementation
    ///
    /// This handler implements capability issuance with:
    /// 1. Session validation (must exist)
    /// 2. Lease validation (session's lease must be valid for its work)
    /// 3. HTF-compliant timestamps via `HolonicClock`
    fn handle_issue_capability(
        &self,
        payload: &[u8],
        ctx: &ConnectionContext,
    ) -> ProtocolResult<PrivilegedResponse> {
        let request = IssueCapabilityRequest::decode_bounded(payload, &self.decode_config)
            .map_err(|e| ProtocolError::Serialization {
                reason: format!("invalid IssueCapabilityRequest: {e}"),
            })?;

        info!(
            session_id = %request.session_id,
            has_capability_request = request.capability_request.is_some(),
            peer_pid = ?ctx.peer_credentials().map(|c| c.pid),
            "IssueCapability request received"
        );

        // Validate required fields
        if request.session_id.is_empty() {
            return Ok(PrivilegedResponse::error(
                PrivilegedErrorCode::CapabilityRequestRejected,
                "session_id is required",
            ));
        }

        if request.capability_request.is_none() {
            return Ok(PrivilegedResponse::error(
                PrivilegedErrorCode::CapabilityRequestRejected,
                "capability_request is required",
            ));
        }

        // 1. Retrieve session state
        let Some(session) = self.session_registry.get_session(&request.session_id) else {
            warn!(session_id = %request.session_id, "IssueCapability rejected: session not found");
            return Ok(PrivilegedResponse::error(
                PrivilegedErrorCode::SessionNotFound,
                format!("session not found: {}", request.session_id),
            ));
        };

        // 2. Validate lease (TCK-00289: "Implement IssueCapability with lease
        //    validation")
        // Ensure the session's lease matches the authoritative work claim.
        // This confirms the session corresponds to a valid, active work item.
        if let Some(claim) = self.work_registry.get_claim(&session.work_id) {
            // Verify lease_id matches
            // Constant-time comparison is good practice for IDs
            let lease_matches = session.lease_id.len() == claim.lease_id.len()
                && bool::from(session.lease_id.as_bytes().ct_eq(claim.lease_id.as_bytes()));

            if !lease_matches {
                warn!(
                    session_id = %request.session_id,
                    expected_lease = "[REDACTED]",
                    actual_lease = "[REDACTED]",
                    "IssueCapability rejected: lease mismatch against work claim"
                );
                return Ok(PrivilegedResponse::error(
                    PrivilegedErrorCode::CapabilityRequestRejected,
                    "lease validation failed: session lease does not match work claim",
                ));
            }
        } else {
            // Local state-precondition failure (missing in-process work claim),
            // not a governance transport/communication failure.
            warn!(
                session_id = %request.session_id,
                work_id = %session.work_id,
                "IssueCapability rejected: work claim not found"
            );
            return Ok(PrivilegedResponse::error(
                PrivilegedErrorCode::CapabilityRequestRejected,
                "lease validation failed: work claim not found",
            ));
        }

        // 3. Generate HTF-compliant timestamps
        let Ok(mono_tick) = self.holonic_clock.now_mono_tick() else {
            warn!("Clock error during IssueCapability");
            return Ok(PrivilegedResponse::error(
                PrivilegedErrorCode::PolicyResolutionFailed,
                "clock error",
            ));
        };
        let _mono_tick = mono_tick.value();

        // For grant/expire times, use HLC Wall Time per RFC-0016.
        // TCK-00289: Fail-closed - do not fall back to SystemTime if HLC disabled.
        let now_wall = match self.holonic_clock.now_hlc() {
            Ok(hlc) => hlc.wall_ns,
            Err(e) => {
                // TCK-00289: Fail-closed - do not use SystemTime fallback
                warn!(error = %e, "HLC clock error during IssueCapability - failing closed");
                return Ok(PrivilegedResponse::error(
                    PrivilegedErrorCode::PolicyResolutionFailed,
                    format!("HTF timestamp error: {e}"),
                ));
            },
        };

        // Duration is in seconds, convert to nanoseconds
        let duration_ns =
            request.capability_request.as_ref().unwrap().duration_secs * 1_000_000_000;
        let expires_at_ns = now_wall + duration_ns;

        // Convert to seconds for response (proto uses u64 seconds)
        let granted_at = now_wall / 1_000_000_000;
        let expires_at = expires_at_ns / 1_000_000_000;

        let capability_id = format!("C-{}", uuid::Uuid::new_v4());

        // TCK-00268: Emit capability_granted metric
        if let Some(ref metrics) = self.metrics {
            let role_str = match WorkRole::try_from(session.role).unwrap_or(WorkRole::Unspecified) {
                WorkRole::Implementer => "implementer",
                WorkRole::Reviewer => "reviewer",
                WorkRole::GateExecutor => "gate_executor",
                WorkRole::Coordinator => "coordinator",
                WorkRole::Unspecified => "unspecified",
            };

            let capability_type = request
                .capability_request
                .as_ref()
                .map_or("unknown", |c| c.tool_class.as_str());

            metrics
                .daemon_metrics()
                .capability_granted(role_str, capability_type);
        }

        info!(
            session_id = %request.session_id,
            capability_id = %capability_id,
            "Capability issued"
        );

        Ok(PrivilegedResponse::IssueCapability(
            IssueCapabilityResponse {
                capability_id,
                granted_at,
                expires_at,
            },
        ))
    }

    /// Handles Shutdown requests (IPC-PRIV-004, TCK-00392).
    ///
    /// Triggers graceful daemon shutdown by setting the atomic shutdown flag
    /// on `SharedState`. The main event loop detects this flag and initiates
    /// the shutdown sequence: stop all processes, clean up sockets, remove
    /// the PID file.
    ///
    /// The shutdown flag is set first, then the response is constructed and
    /// returned. Because the main event loop runs on a separate task, the
    /// caller still receives acknowledgment before the daemon acts on the
    /// flag.
    ///
    /// If `daemon_state` is `None` (test/stub mode), logs a warning and
    /// returns a stub response without triggering shutdown.
    #[allow(clippy::option_if_let_else)] // Both branches have logging side effects; if-let is clearer
    fn handle_shutdown(
        &self,
        payload: &[u8],
        ctx: &ConnectionContext,
    ) -> ProtocolResult<PrivilegedResponse> {
        let request =
            ShutdownRequest::decode_bounded(payload, &self.decode_config).map_err(|e| {
                ProtocolError::Serialization {
                    reason: format!("invalid ShutdownRequest: {e}"),
                }
            })?;

        let reason_display = request.reason.as_deref().unwrap_or("no reason provided");

        if let Some(state) = &self.daemon_state {
            info!(
                reason = %reason_display,
                peer_pid = ?ctx.peer_credentials().map(|c| c.pid),
                "Shutdown request received via IPC, initiating graceful shutdown"
            );

            // Set the atomic shutdown flag. The main event loop polls
            // `is_shutdown_requested()` and will trigger the graceful
            // shutdown sequence (stop processes, cleanup sockets, remove
            // PID file).
            state.request_shutdown();

            Ok(PrivilegedResponse::Shutdown(ShutdownResponse {
                message: format!("Shutdown initiated (reason: {reason_display})"),
            }))
        } else {
            warn!(
                reason = %reason_display,
                peer_pid = ?ctx.peer_credentials().map(|c| c.pid),
                "Shutdown request received but daemon state not configured (test mode)"
            );

            Ok(PrivilegedResponse::Shutdown(ShutdownResponse {
                message: "Shutdown acknowledged (stub — daemon state not configured)".to_string(),
            }))
        }
    }

    /// Handles `UpdateStopFlags` requests (IPC-PRIV-018, TCK-00351).
    ///
    /// Mutates the shared runtime stop flags used by pre-actuation gating.
    ///
    /// # Audit Trail
    ///
    /// After mutation, this emits a `stop_flags_mutated` ledger event on a
    /// best-effort basis. Emission failures are logged but do NOT roll back
    /// the stop mutation. The stop mutation is the safety-critical path and
    /// must take effect even when ledger infrastructure is unavailable.
    fn handle_update_stop_flags(
        &self,
        payload: &[u8],
        ctx: &ConnectionContext,
    ) -> ProtocolResult<PrivilegedResponse> {
        let request = UpdateStopFlagsRequest::decode_bounded(payload, &self.decode_config)
            .map_err(|e| ProtocolError::Serialization {
                reason: format!("invalid UpdateStopFlagsRequest: {e}"),
            })?;

        if request.emergency_stop_active.is_none() && request.governance_stop_active.is_none() {
            return Ok(PrivilegedResponse::error(
                PrivilegedErrorCode::CapabilityRequestRejected,
                "at least one stop flag must be provided",
            ));
        }

        let peer = ctx
            .peer_credentials()
            .ok_or_else(|| ProtocolError::Serialization {
                reason: "peer credentials required for UpdateStopFlags".to_string(),
            })?;
        let actor_id = derive_actor_id(peer);

        let Some(authority) = self.stop_authority.as_ref() else {
            return Ok(PrivilegedResponse::error(
                PrivilegedErrorCode::PermissionDenied,
                "stop authority is not configured",
            ));
        };

        let prev_emergency = authority.emergency_stop_active();
        let prev_governance = authority.governance_stop_active();

        if let Some(active) = request.emergency_stop_active {
            authority.set_emergency_stop(active);
        }
        if let Some(active) = request.governance_stop_active {
            authority.set_governance_stop(active);
        }

        let emergency_stop_active = authority.emergency_stop_active();
        let governance_stop_active = authority.governance_stop_active();

        info!(
            actor_id = %actor_id,
            emergency_stop_previous = prev_emergency,
            emergency_stop_current = emergency_stop_active,
            governance_stop_previous = prev_governance,
            governance_stop_current = governance_stop_active,
            "UpdateStopFlags applied"
        );

        // Best-effort audit evidence emission. Never roll back the stop
        // mutation based on ledger availability.
        if !self.event_emitter.has_durable_storage() {
            warn!(
                actor_id = %actor_id,
                "UpdateStopFlags audit trail is in-memory only; event durability unavailable"
            );
        }

        match self.get_htf_timestamp_ns() {
            Ok(timestamp_ns) => {
                let request_context = serde_json::json!({
                    "endpoint": "UpdateStopFlags",
                    "connection_id": ctx.connection_id(),
                    "connection_phase": format!("{:?}", ctx.phase()),
                    "peer_uid": peer.uid,
                    "peer_gid": peer.gid,
                    "peer_pid": peer.pid,
                    "requested_updates": {
                        "emergency_stop_active": request.emergency_stop_active,
                        "governance_stop_active": request.governance_stop_active,
                    },
                });

                let mutation = StopFlagsMutation {
                    actor_id: actor_id.as_str(),
                    emergency_stop_previous: prev_emergency,
                    emergency_stop_current: emergency_stop_active,
                    governance_stop_previous: prev_governance,
                    governance_stop_current: governance_stop_active,
                    timestamp_ns,
                    request_context: &request_context,
                };

                if let Err(error) = self.event_emitter.emit_stop_flags_mutated(&mutation) {
                    warn!(
                        actor_id = %actor_id,
                        error = %error,
                        emergency_stop_current = emergency_stop_active,
                        governance_stop_current = governance_stop_active,
                        "UpdateStopFlags mutation applied but ledger audit emission failed"
                    );
                }
            },
            Err(error) => {
                warn!(
                    actor_id = %actor_id,
                    error = %error,
                    emergency_stop_current = emergency_stop_active,
                    governance_stop_current = governance_stop_active,
                    "UpdateStopFlags mutation applied but audit timestamp acquisition failed"
                );
            },
        }

        Ok(PrivilegedResponse::UpdateStopFlags(
            UpdateStopFlagsResponse {
                emergency_stop_active,
                governance_stop_active,
            },
        ))
    }

    /// Handles `WorkStatus` requests (IPC-PRIV-015, TCK-00344/TCK-00415).
    ///
    /// Authority is projection-backed: lifecycle state is derived from ledger
    /// projection only. Session/claim registries provide supplementary runtime
    /// metadata (`session_id`, `lease_id`) and never determine lifecycle truth.
    fn handle_work_status(
        &self,
        payload: &[u8],
        _ctx: &ConnectionContext,
    ) -> ProtocolResult<PrivilegedResponse> {
        let request =
            WorkStatusRequest::decode_bounded(payload, &self.decode_config).map_err(|e| {
                ProtocolError::Serialization {
                    reason: format!("invalid WorkStatusRequest: {e}"),
                }
            })?;

        // CTR-1603: Validate work_id length to prevent DoS
        if request.work_id.len() > MAX_ID_LENGTH {
            return Ok(PrivilegedResponse::error(
                PrivilegedErrorCode::CapabilityRequestRejected,
                format!("work_id exceeds maximum length of {MAX_ID_LENGTH} bytes"),
            ));
        }

        if request.work_id.is_empty() {
            return Ok(PrivilegedResponse::error(
                PrivilegedErrorCode::CapabilityRequestRejected,
                "work_id cannot be empty",
            ));
        }

        debug!(
            work_id = %request.work_id,
            "Processing WorkStatus request via projection authority"
        );

        let authority = self.projection_work_authority();
        match authority.get_work_status(&request.work_id) {
            Ok(status) => Ok(PrivilegedResponse::WorkStatus(
                self.authority_status_to_work_status_response(&status),
            )),
            Err(error) => Ok(Self::map_work_authority_error(error)),
        }
    }

    /// Handles `WorkList` requests (IPC-PRIV-019, TCK-00415).
    ///
    /// Lists work items known to projection authority with bounded pagination.
    /// Server enforces `MAX_WORK_LIST_ROWS` regardless of client `limit`.
    fn handle_work_list(
        &self,
        payload: &[u8],
        _ctx: &ConnectionContext,
    ) -> ProtocolResult<PrivilegedResponse> {
        use crate::work::authority::MAX_WORK_LIST_ROWS;

        let request =
            WorkListRequest::decode_bounded(payload, &self.decode_config).map_err(|e| {
                ProtocolError::Serialization {
                    reason: format!("invalid WorkListRequest: {e}"),
                }
            })?;

        let limit = if request.limit == 0 {
            MAX_WORK_LIST_ROWS
        } else {
            (request.limit as usize).min(MAX_WORK_LIST_ROWS)
        };

        debug!(
            claimable_only = request.claimable_only,
            limit = limit,
            cursor = %request.cursor,
            "Processing WorkList request via projection authority"
        );

        let authority = self.projection_work_authority();
        let statuses = if request.claimable_only {
            authority.list_claimable(limit, &request.cursor)
        } else {
            authority.list_all(limit, &request.cursor)
        };

        match statuses {
            Ok(statuses) => {
                let work_items = statuses
                    .iter()
                    .map(|status| self.authority_status_to_work_status_response(status))
                    .collect();

                Ok(PrivilegedResponse::WorkList(WorkListResponse {
                    work_items,
                }))
            },
            Err(error) => Ok(Self::map_work_authority_error(error)),
        }
    }

    /// Handles `AuditorLaunchProjection` requests (IPC-PRIV-020, TCK-00452).
    ///
    /// Returns launch-lineage completeness and boundary-conformance indicators
    /// backed by authoritative receipt events.
    fn handle_auditor_launch_projection(
        &self,
        payload: &[u8],
        _ctx: &ConnectionContext,
    ) -> ProtocolResult<PrivilegedResponse> {
        AuditorLaunchProjectionRequest::decode_bounded(payload, &self.decode_config).map_err(
            |error| ProtocolError::Serialization {
                reason: format!("invalid AuditorLaunchProjectionRequest: {error}"),
            },
        )?;

        let projection = self.build_auditor_launch_projection();
        let envelope = match digest_first_projection(&projection) {
            Ok(envelope) => envelope,
            Err(error) => {
                warn!(error = %error, "auditor projection digest generation failed closed");
                return Ok(PrivilegedResponse::error(
                    PrivilegedErrorCode::CapabilityRequestRejected,
                    format!("auditor projection digest generation failed: {error}"),
                ));
            },
        };

        Ok(PrivilegedResponse::AuditorLaunchProjection(
            AuditorLaunchProjectionResponse {
                projection_digest: envelope.projection_digest.to_vec(),
                canonical_projection_json: envelope.canonical_projection_json,
                lineage_complete: projection.lineage_complete,
                boundary_conformant: projection.boundary_conformant,
                authoritative_receipt_count: projection.authoritative_receipt_count,
                complete_lineage_receipt_count: projection.complete_lineage_receipt_count,
                boundary_conformant_receipt_count: projection.boundary_conformant_receipt_count,
                uncertainty_flags: projection
                    .uncertainty_flags
                    .into_iter()
                    .map(Self::projection_uncertainty_to_proto)
                    .collect(),
                admissible: projection.admissible,
            },
        ))
    }

    /// Handles `OrchestratorLaunchProjection` requests (IPC-PRIV-027,
    /// TCK-00452).
    ///
    /// Returns active-run/liveness projection fields for orchestrator control
    /// loops.
    fn handle_orchestrator_launch_projection(
        &self,
        payload: &[u8],
        _ctx: &ConnectionContext,
    ) -> ProtocolResult<PrivilegedResponse> {
        OrchestratorLaunchProjectionRequest::decode_bounded(payload, &self.decode_config).map_err(
            |error| ProtocolError::Serialization {
                reason: format!("invalid OrchestratorLaunchProjectionRequest: {error}"),
            },
        )?;

        let projection = self.build_orchestrator_launch_projection();
        let envelope = match digest_first_projection(&projection) {
            Ok(envelope) => envelope,
            Err(error) => {
                warn!(error = %error, "orchestrator projection digest generation failed closed");
                return Ok(PrivilegedResponse::error(
                    PrivilegedErrorCode::CapabilityRequestRejected,
                    format!("orchestrator projection digest generation failed: {error}"),
                ));
            },
        };

        Ok(PrivilegedResponse::OrchestratorLaunchProjection(
            OrchestratorLaunchProjectionResponse {
                projection_digest: envelope.projection_digest.to_vec(),
                canonical_projection_json: envelope.canonical_projection_json,
                active_runs: projection.active_runs,
                last_authoritative_receipt_tick: projection.last_authoritative_receipt_tick,
                restart_count: projection.restart_count,
                uncertainty_flags: projection
                    .uncertainty_flags
                    .into_iter()
                    .map(Self::projection_uncertainty_to_proto)
                    .collect(),
                admissible: projection.admissible,
            },
        ))
    }

    fn build_auditor_launch_projection(&self) -> AuditorLaunchProjectionV1 {
        let total_receipts = self.event_emitter.get_authoritative_receipt_event_count();
        let receipt_events = self.event_emitter.get_authoritative_receipt_events();
        let considered_receipts = receipt_events.len();
        let mut complete_lineage_receipt_count = 0usize;
        let mut boundary_conformant_receipt_count = 0usize;
        let mut uncertainty_flags = Vec::new();

        for event in &receipt_events {
            let Some(payload) = Self::parse_event_payload_json(event) else {
                uncertainty_flags.push(ProjectionUncertainty::MissingLineageEvidence);
                uncertainty_flags.push(ProjectionUncertainty::BoundaryConformanceUnverifiable);
                continue;
            };

            if Self::payload_has_complete_lineage(&payload) {
                complete_lineage_receipt_count += 1;
            } else {
                uncertainty_flags.push(ProjectionUncertainty::MissingLineageEvidence);
            }

            if Self::payload_has_boundary_conformance(&payload) {
                boundary_conformant_receipt_count += 1;
            } else {
                uncertainty_flags.push(ProjectionUncertainty::BoundaryConformanceUnverifiable);
            }
        }

        if total_receipts > considered_receipts {
            uncertainty_flags.push(ProjectionUncertainty::TruncatedHistory);
        }

        if considered_receipts == 0 {
            uncertainty_flags.push(ProjectionUncertainty::MissingLineageEvidence);
            uncertainty_flags.push(ProjectionUncertainty::BoundaryConformanceUnverifiable);
        }

        let lineage_complete =
            considered_receipts > 0 && complete_lineage_receipt_count == considered_receipts;
        let boundary_conformant =
            considered_receipts > 0 && boundary_conformant_receipt_count == considered_receipts;

        AuditorLaunchProjectionV1::new(
            saturating_u32(considered_receipts),
            saturating_u32(complete_lineage_receipt_count),
            saturating_u32(boundary_conformant_receipt_count),
            lineage_complete,
            boundary_conformant,
            uncertainty_flags,
        )
    }

    fn build_orchestrator_launch_projection(&self) -> OrchestratorLaunchProjectionV1 {
        let total_events = self
            .event_emitter
            .get_launch_liveness_projection_event_count();
        let events = self.event_emitter.get_launch_liveness_projection_events();
        let mut uncertainty_flags = Vec::new();
        let mut active_sessions = BTreeSet::new();
        let mut restart_count = 0u32;
        let mut has_liveness_evidence = false;
        let mut has_receipt_event = false;
        let mut last_authoritative_receipt_tick: Option<u64> = None;

        for event in &events {
            match event.event_type.as_str() {
                "session_started" => {
                    let Some(payload) = Self::parse_event_payload_json(event) else {
                        uncertainty_flags.push(ProjectionUncertainty::MissingLivenessEvidence);
                        continue;
                    };
                    if let Some(session_id) = Self::payload_nonempty_string(&payload, "session_id")
                    {
                        has_liveness_evidence = true;
                        active_sessions.insert(session_id.to_string());
                    } else {
                        uncertainty_flags.push(ProjectionUncertainty::MissingLivenessEvidence);
                    }
                },
                "session_terminated" => {
                    let Some(payload) = Self::parse_event_payload_json(event) else {
                        uncertainty_flags.push(ProjectionUncertainty::MissingLivenessEvidence);
                        continue;
                    };

                    if let Some(session_id) = Self::payload_nonempty_string(&payload, "session_id")
                    {
                        has_liveness_evidence = true;
                        active_sessions.remove(session_id);
                    } else {
                        uncertainty_flags.push(ProjectionUncertainty::MissingLivenessEvidence);
                    }

                    if payload
                        .get("exit_code")
                        .and_then(serde_json::Value::as_i64)
                        .is_some_and(|exit_code| exit_code != 0)
                    {
                        restart_count = restart_count.saturating_add(1);
                    }
                },
                "review_receipt_recorded" | "review_blocked_recorded" => {
                    has_receipt_event = true;

                    let Some(payload) = Self::parse_event_payload_json(event) else {
                        uncertainty_flags
                            .push(ProjectionUncertainty::MissingAuthoritativeReceiptTick);
                        continue;
                    };

                    if let Some(tick) = payload
                        .get("consume_tick")
                        .and_then(serde_json::Value::as_u64)
                    {
                        last_authoritative_receipt_tick = Some(
                            last_authoritative_receipt_tick
                                .map_or(tick, |current| current.max(tick)),
                        );
                    }
                },
                _ => {},
            }
        }

        if total_events > events.len() {
            uncertainty_flags.push(ProjectionUncertainty::TruncatedHistory);
        }
        if !has_liveness_evidence {
            uncertainty_flags.push(ProjectionUncertainty::MissingLivenessEvidence);
        }
        if !has_receipt_event || last_authoritative_receipt_tick.is_none() {
            uncertainty_flags.push(ProjectionUncertainty::MissingAuthoritativeReceiptTick);
        }

        OrchestratorLaunchProjectionV1::new(
            saturating_u32(active_sessions.len()),
            last_authoritative_receipt_tick,
            restart_count,
            uncertainty_flags,
        )
    }

    fn parse_event_payload_json(event: &SignedLedgerEvent) -> Option<serde_json::Value> {
        serde_json::from_slice::<serde_json::Value>(&event.payload).ok()
    }

    fn payload_has_complete_lineage(payload: &serde_json::Value) -> bool {
        const REQUIRED_HASH_FIELDS: [&str; 5] = [
            "changeset_digest",
            "artifact_bundle_hash",
            "capability_manifest_hash",
            "context_pack_hash",
            "role_spec_hash",
        ];

        REQUIRED_HASH_FIELDS
            .iter()
            .all(|field| Self::payload_has_hash32(payload, field))
            && Self::payload_nonempty_string(payload, "receipt_id").is_some()
    }

    fn payload_has_boundary_conformance(payload: &serde_json::Value) -> bool {
        Self::payload_has_hash32(payload, "identity_proof_hash")
            && Self::payload_nonempty_string(payload, "time_envelope_ref").is_some()
            && Self::payload_nonempty_string(payload, "lease_id").is_some()
    }

    fn payload_has_hash32(payload: &serde_json::Value, field: &str) -> bool {
        payload
            .get(field)
            .and_then(serde_json::Value::as_str)
            .is_some_and(|value| decode_hash32_hex(field, value).is_ok())
    }

    fn payload_nonempty_string<'a>(payload: &'a serde_json::Value, field: &str) -> Option<&'a str> {
        payload
            .get(field)
            .and_then(serde_json::Value::as_str)
            .filter(|value| !value.trim().is_empty())
    }

    const fn projection_uncertainty_to_proto(flag: ProjectionUncertainty) -> i32 {
        match flag {
            ProjectionUncertainty::MissingLineageEvidence => {
                ProjectionUncertaintyFlag::MissingLineageEvidence as i32
            },
            ProjectionUncertainty::BoundaryConformanceUnverifiable => {
                ProjectionUncertaintyFlag::BoundaryConformanceUnverifiable as i32
            },
            ProjectionUncertainty::MissingLivenessEvidence => {
                ProjectionUncertaintyFlag::MissingLivenessEvidence as i32
            },
            ProjectionUncertainty::MissingAuthoritativeReceiptTick => {
                ProjectionUncertaintyFlag::MissingAuthoritativeReceiptTick as i32
            },
            ProjectionUncertainty::TruncatedHistory => {
                ProjectionUncertaintyFlag::TruncatedHistory as i32
            },
        }
    }

    /// Returns the shared projection-backed work authority (TCK-00415).
    ///
    /// The authority is instantiated once during dispatcher construction and
    /// reused across requests. Its internal event-count cache avoids
    /// redundant O(N) full-ledger replays.
    fn projection_work_authority(&self) -> &ProjectionWorkAuthority {
        &self.work_authority
    }

    fn authority_status_to_work_status_response(
        &self,
        authority_status: &WorkAuthorityStatus,
    ) -> WorkStatusResponse {
        // Projection state is authoritative for lifecycle.
        let mut response = WorkStatusResponse {
            work_id: authority_status.work_id.clone(),
            status: authority_status.state.as_str().to_string(),
            actor_id: None,
            role: None,
            session_id: None,
            lease_id: None,
            created_at_ns: authority_status.created_at_ns,
            claimed_at_ns: authority_status.claimed_at_ns,
        };

        // Supplement with claim metadata when available.
        if let Some(claim) = self.work_registry.get_claim(&authority_status.work_id) {
            response.actor_id = Some(claim.actor_id);
            response.role = Some(claim.role.into());
            response.lease_id = Some(claim.lease_id);
        }

        // Supplement with active session metadata when available.
        if let Some(session) = self.find_session_by_work_id(&authority_status.work_id) {
            response.session_id = Some(session.session_id);
            response.role = Some(session.role);
        }

        response
    }

    fn map_work_authority_error(error: WorkAuthorityError) -> PrivilegedResponse {
        match error {
            WorkAuthorityError::WorkNotFound { work_id } => PrivilegedResponse::error(
                PrivilegedErrorCode::WorkNotFound,
                format!("work item not found: {work_id}"),
            ),
            other => {
                warn!(error = %other, "projection-backed work authority failed closed");
                PrivilegedResponse::error(
                    PrivilegedErrorCode::CapabilityRequestRejected,
                    format!("projection-backed work authority unavailable: {other}"),
                )
            },
        }
    }

    /// Finds a session by `work_id`.
    ///
    /// Delegates to `SessionRegistry::get_session_by_work_id` which performs
    /// an O(n) scan. This is acceptable for status queries which are not
    /// performance-critical.
    fn find_session_by_work_id(&self, work_id: &str) -> Option<SessionState> {
        self.session_registry.get_session_by_work_id(work_id)
    }

    // ========================================================================
    // Session Termination Handler (TCK-00395)
    // ========================================================================

    /// Handles `EndSession` requests (IPC-PRIV-016, TCK-00395).
    ///
    /// Terminates an active session and emits a `SessionTerminated` ledger
    /// event. This is the authoritative path for session termination that
    /// ensures the ledger records all session lifecycle events.
    ///
    /// # Security Contract
    ///
    /// - Session must exist in the session registry
    /// - Actor ID is derived from peer credentials (not user input)
    /// - `SessionTerminated` event is signed and persisted atomically
    /// - Fail-closed: returns error if timestamp generation, runtime stop, or
    ///   event emission fails
    /// - Session is removed from registry POST-COMMIT (after runtime stop +
    ///   ledger succeed), ensuring the session is re-findable for retry on
    ///   failure
    /// - `WorkTransitioned` emission failure is propagated (fail-closed)
    /// - Exit code derived from typed `TerminationOutcome` enum (not string
    ///   matching)
    /// - Reason field bounded by `MAX_REASON_LENGTH` to prevent OOM/bloated
    ///   ledger entries
    ///
    /// # Ordering (Security BLOCKER 2 / Quality BLOCKER 1)
    ///
    /// 1. Read session state (no removal)
    /// 2. Stop runtime via `stop_with_session_context` (fail-closed)
    /// 3. Derive exit code from typed `TerminationOutcome` enum
    /// 4. Emit ledger events (`SessionTerminated`, `WorkTransitioned`)
    /// 5. Remove session from registry (POST-COMMIT)
    fn handle_end_session(
        &self,
        payload: &[u8],
        ctx: &ConnectionContext,
    ) -> ProtocolResult<PrivilegedResponse> {
        let request =
            EndSessionRequest::decode_bounded(payload, &self.decode_config).map_err(|e| {
                ProtocolError::Serialization {
                    reason: format!("invalid EndSessionRequest: {e}"),
                }
            })?;

        info!(
            session_id = %request.session_id,
            reason = %request.reason,
            outcome = %request.outcome,
            peer_pid = ?ctx.peer_credentials().map(|c| c.pid),
            "EndSession request received"
        );

        // Validate required fields
        if request.session_id.is_empty() {
            return Ok(PrivilegedResponse::error(
                PrivilegedErrorCode::CapabilityRequestRejected,
                "session_id is required",
            ));
        }

        // SEC-SCP-FAC-0020: Enforce maximum length on session_id
        if request.session_id.len() > MAX_ID_LENGTH {
            return Ok(PrivilegedResponse::error(
                PrivilegedErrorCode::CapabilityRequestRejected,
                format!("session_id exceeds maximum length of {MAX_ID_LENGTH} bytes"),
            ));
        }

        // SEC-SCP-FAC-0020 / Security MAJOR 1: Enforce maximum length on
        // reason to prevent OOM and bloated signed ledger payloads.
        if request.reason.len() > MAX_REASON_LENGTH {
            return Ok(PrivilegedResponse::error(
                PrivilegedErrorCode::CapabilityRequestRejected,
                format!(
                    "reason exceeds maximum length of {MAX_REASON_LENGTH} bytes (got {})",
                    request.reason.len()
                ),
            ));
        }

        // ---- Phase 1: Read session state (no removal) ----
        // Look up session WITHOUT removing it. The session is only removed
        // after all fallible operations succeed (POST-COMMIT), ensuring the
        // session is re-findable for retry if any step fails.
        let Some(session) = self.session_registry.get_session(&request.session_id) else {
            return Ok(PrivilegedResponse::error(
                PrivilegedErrorCode::CapabilityRequestRejected,
                format!("session not found: {}", request.session_id),
            ));
        };

        // TCK-00395 Security MAJOR 2: Fail closed when peer credentials are
        // missing. Same pattern as ClaimWork (line 3631). EndSession emits
        // authoritative ledger events (SessionTerminated), and recording
        // "unknown" as the actor identity would break the accountability
        // chain for a privileged termination action.
        let peer_creds = ctx
            .peer_credentials()
            .ok_or_else(|| ProtocolError::Serialization {
                reason: "peer credentials required for session termination".to_string(),
            })?;
        let actor_id = derive_actor_id(peer_creds);

        // Get HTF-compliant timestamp
        let timestamp_ns = match self.get_htf_timestamp_ns() {
            Ok(ts) => ts,
            Err(e) => {
                warn!(error = %e, "HTF timestamp generation failed - failing closed");
                return Ok(PrivilegedResponse::error(
                    PrivilegedErrorCode::CapabilityRequestRejected,
                    format!("HTF timestamp error: {e}"),
                ));
            },
        };

        // TCK-00395 Quality v3 MAJOR: Use typed TerminationOutcome enum to
        // determine exit code instead of free-form string matching.
        // When `outcome` is set (non-zero), it takes precedence over the
        // legacy `reason` string. Unspecified (0) falls back to string
        // matching for backward compatibility.
        let exit_code = match TerminationOutcome::try_from(request.outcome) {
            Ok(TerminationOutcome::Success) => 0,
            Ok(
                TerminationOutcome::Failure
                | TerminationOutcome::Cancelled
                | TerminationOutcome::Timeout,
            ) => 1,
            // Unspecified or unknown: fall back to legacy string matching
            _ => {
                let is_failure =
                    request.reason != "completed_normally" && request.reason != "success";
                i32::from(is_failure)
            },
        };

        // ---- Phase 2: Stop runtime BEFORE emitting ledger events ----
        // Security BLOCKER 1: Fail closed if the session has no resolvable
        // episode binding. SpawnEpisode now writes back the episode_id
        // (see update_episode_id call above). If the session still lacks
        // an episode_id, it means the episode was never created or the
        // write-back failed, so we must not emit termination facts.
        //
        // In test mode (no tokio runtime), episode_id may legitimately be
        // None. Production sessions always have an episode_id after
        // SpawnEpisode succeeds.
        let episode_id_str = match session.episode_id {
            Some(ref id) => id.clone(),
            None => {
                // In cfg(test), allow sessions without episode_id for
                // backward compatibility with sync unit tests.
                #[cfg(test)]
                {
                    debug!(
                        session_id = %request.session_id,
                        "EndSession: session lacks episode_id (test mode) - skipping runtime stop"
                    );
                    String::new()
                }
                #[cfg(not(test))]
                {
                    warn!(
                        session_id = %request.session_id,
                        "EndSession: session has no episode_id binding - failing closed"
                    );
                    return Ok(PrivilegedResponse::error(
                        PrivilegedErrorCode::CapabilityRequestRejected,
                        "session has no episode binding: cannot stop runtime",
                    ));
                }
            },
        };

        // Stop the runtime episode if we have a valid episode_id.
        // TCK-00395 Security MAJOR 1: Fail closed on EpisodeId parse failure.
        // Previously, a malformed/corrupted episode_id would silently skip
        // the runtime stop but still emit termination facts and clean up
        // registry state, leaving the runtime running without accountability.
        if !episode_id_str.is_empty() {
            let episode_id_parsed = match EpisodeId::new(episode_id_str.clone()) {
                Ok(id) => id,
                Err(e) => {
                    warn!(
                        error = %e,
                        episode_id = %episode_id_str,
                        session_id = %request.session_id,
                        "EndSession: EpisodeId parse failed - failing closed before ledger emission"
                    );
                    return Ok(PrivilegedResponse::error(
                        PrivilegedErrorCode::CapabilityRequestRejected,
                        format!("malformed episode_id '{episode_id_str}': {e}"),
                    ));
                },
            };

            let termination_class = if exit_code == 0 {
                TerminationClass::Success
            } else {
                TerminationClass::Failure
            };
            let stop_result = if let Ok(handle) = tokio::runtime::Handle::try_current() {
                let rt = &self.episode_runtime;
                tokio::task::block_in_place(|| {
                    handle.block_on(rt.stop_with_session_context(
                        &episode_id_parsed,
                        termination_class,
                        timestamp_ns,
                        &request.session_id,
                        &session.work_id,
                        &actor_id,
                    ))
                })
            } else {
                // No tokio runtime available; fail closed rather than
                // skipping the runtime stop.
                warn!("No tokio runtime handle available for runtime stop - failing closed");
                return Ok(PrivilegedResponse::error(
                    PrivilegedErrorCode::CapabilityRequestRejected,
                    "runtime stop failed: no async runtime available",
                ));
            };

            if let Err(e) = stop_result {
                warn!(
                    error = %e,
                    episode_id = %episode_id_str,
                    "Runtime stop failed - failing closed without writing success facts"
                );
                return Ok(PrivilegedResponse::error(
                    PrivilegedErrorCode::CapabilityRequestRejected,
                    format!("runtime stop failed: {e}"),
                ));
            }
        }

        // ---- Phase 3: Emit ledger events (after successful runtime stop) ----
        let termination_reason = if request.reason.is_empty() {
            "session_ended_via_ipc"
        } else {
            &request.reason
        };

        // TCK-00395: Emit SessionTerminated event to ledger.
        // This is the authoritative path for recording session termination.
        // Only emitted AFTER runtime stop succeeds (Security BLOCKER 2).
        if let Err(e) = self.event_emitter.emit_session_terminated(
            &request.session_id,
            &session.work_id,
            exit_code,
            termination_reason,
            &actor_id,
            timestamp_ns,
        ) {
            warn!(error = %e, "SessionTerminated event emission failed");
            return Ok(PrivilegedResponse::error(
                PrivilegedErrorCode::CapabilityRequestRejected,
                format!("session terminated event emission failed: {e}"),
            ));
        }

        // TCK-00395 Quality BLOCKER 2: DO NOT emit
        // WorkTransitioned(InProgress -> Completed) on EndSession.
        // The InProgress -> Completed transition violates core work-state
        // transition rules (state.rs only allows InProgress -> Review,
        // CiPending, NeedsInput, NeedsAdjudication, or Aborted).
        // Work state completion belongs to a separate workflow path
        // (gate orchestration), not EndSession.

        // ---- Phase 4: POST-COMMIT session + manifest + telemetry removal ----
        // Remove session from registry only after ALL fallible operations
        // (runtime stop + ledger emission) have succeeded. This prevents
        // the session from being orphaned if a downstream step fails.
        //
        // TCK-00395 Security BLOCKER 2: Also remove the manifest entry
        // to prevent retained session tokens from passing manifest lookup
        // in RequestTool after EndSession.
        self.manifest_store.remove(&request.session_id);

        // TCK-00352 MAJOR 2 fix: Remove V1 manifest on EndSession.
        // Without this, a terminated session's V1 manifest remains in the
        // store. If a new session reuses the same ID (or a retained token
        // is replayed), the stale V1 manifest could incorrectly match,
        // either granting or denying based on an expired session's policy.
        if let Some(ref v1_store) = self.v1_manifest_store {
            v1_store.remove(&request.session_id);
        }

        // TCK-00401: Remove per-session broker on EndSession to prevent stale
        // capability/policy state from being reused by retained tokens.
        if let Some(ref broker_store) = self.session_broker_registry {
            broker_store.remove(&request.session_id);
        }

        // TCK-00351 BLOCKER-2: `episode_count` is defined as completed
        // episodes. Increment on termination (not spawn) to avoid
        // off-by-one self-deny on the first RequestTool.
        if let Some(ref store) = self.telemetry_store {
            if let Some(telemetry) = store.get(&request.session_id) {
                telemetry.increment_episode_count();
            }
        }

        // TCK-00384 review fix: Clean up telemetry on EndSession to free
        // capacity in the bounded store. Without this, repeated
        // spawn/end cycles exhaust MAX_TELEMETRY_SESSIONS and block new
        // spawns (DoS). Mirrors the cleanup in session_dispatch.rs:1421.
        if let Some(ref store) = self.telemetry_store {
            store.remove(&request.session_id);
        }

        // TCK-00351 BLOCKER 3 FIX: Clean up stop conditions on EndSession
        // to free capacity in the bounded store.  Without this, stop
        // conditions entries accumulate on every spawn/end cycle and are
        // never reclaimed, causing the store to reach capacity and reject
        // new registrations (DoS).  Same lifecycle as telemetry cleanup
        // above.
        if let Some(ref store) = self.stop_conditions_store {
            store.remove(&request.session_id);
        }

        if let Err(e) = self.session_registry.remove_session(&request.session_id) {
            warn!(
                error = %e,
                session_id = %request.session_id,
                "Session removal persistence failed after successful ledger commit"
            );
            return Ok(PrivilegedResponse::error(
                PrivilegedErrorCode::CapabilityRequestRejected,
                format!("session removal persistence failed: {e}"),
            ));
        }

        debug!(
            session_id = %request.session_id,
            work_id = %session.work_id,
            "EndSession completed - SessionTerminated event emitted"
        );

        Ok(PrivilegedResponse::EndSession(EndSessionResponse {
            session_id: request.session_id,
            message: "session terminated".to_string(),
        }))
    }

    // ========================================================================
    // TCK-00389: IngestReviewReceipt Handler
    // ========================================================================

    /// Handles `IngestReviewReceipt` requests (IPC-PRIV-017, TCK-00389).
    ///
    /// Ingests a review receipt from an external reviewer into the FAC ledger.
    /// Validates reviewer identity against the gate lease, verifies request
    /// integrity, and emits either `ReviewReceiptRecorded` or
    /// `ReviewBlockedRecorded` event.
    ///
    /// # Security Invariants
    ///
    /// - **Reviewer identity validation**: The `reviewer_actor_id` MUST match
    ///   the `executor_actor_id` bound in the gate lease (constant-time
    ///   comparison to prevent timing side-channels).
    /// - **Lease existence validation**: The `lease_id` MUST reference a valid
    ///   gate lease (fail-closed on missing lease).
    /// - **Idempotency**: Duplicate `receipt_id` values do not create duplicate
    ///   events (checked via ledger query).
    /// - **Fail-closed**: Any validation failure rejects the request with a
    ///   clear error.
    fn handle_ingest_review_receipt(
        &self,
        payload: &[u8],
        ctx: &ConnectionContext,
    ) -> ProtocolResult<PrivilegedResponse> {
        let request = IngestReviewReceiptRequest::decode_bounded(payload, &self.decode_config)
            .map_err(|e| ProtocolError::Serialization {
                reason: format!("invalid IngestReviewReceiptRequest: {e}"),
            })?;

        // ---- Phase -1 (v6 Finding 1): Bind reviewer identity to authenticated caller
        // ----
        //
        // SECURITY: The reviewer identity MUST be derived from peer credentials,
        // not trusted from the caller-supplied `reviewer_actor_id`. Without this
        // binding, any operator-socket process could submit receipts as any
        // reviewer. The authenticated identity is used for:
        //   - Reviewer-vs-executor comparison (Phase 1)
        //   - Attestation signer identity (Phase 1b)
        //   - Event actor attribution (Phase 4)
        let Some(peer_creds) = ctx.peer_credentials() else {
            return Ok(PrivilegedResponse::error(
                PrivilegedErrorCode::PermissionDenied,
                "peer credentials required for review receipt ingestion",
            ));
        };
        let authenticated_reviewer_id = derive_actor_id(peer_creds);

        info!(
            lease_id = %request.lease_id,
            receipt_id = %request.receipt_id,
            claimed_reviewer_actor_id = %request.reviewer_actor_id,
            authenticated_reviewer_id = %authenticated_reviewer_id,
            verdict = %request.verdict,
            peer_pid = ?ctx.peer_credentials().map(|c| c.pid),
            "IngestReviewReceipt request received"
        );

        // ---- Phase 0: Validate required fields (admission checks) ----

        if request.lease_id.is_empty() {
            return Ok(PrivilegedResponse::error(
                PrivilegedErrorCode::CapabilityRequestRejected,
                "lease_id is required",
            ));
        }
        if request.lease_id.len() > MAX_ID_LENGTH {
            return Ok(PrivilegedResponse::error(
                PrivilegedErrorCode::CapabilityRequestRejected,
                format!("lease_id exceeds maximum length of {MAX_ID_LENGTH} bytes"),
            ));
        }

        if request.receipt_id.is_empty() {
            return Ok(PrivilegedResponse::error(
                PrivilegedErrorCode::CapabilityRequestRejected,
                "receipt_id is required",
            ));
        }
        if request.receipt_id.len() > MAX_ID_LENGTH {
            return Ok(PrivilegedResponse::error(
                PrivilegedErrorCode::CapabilityRequestRejected,
                format!("receipt_id exceeds maximum length of {MAX_ID_LENGTH} bytes"),
            ));
        }

        if request.reviewer_actor_id.is_empty() {
            return Ok(PrivilegedResponse::error(
                PrivilegedErrorCode::CapabilityRequestRejected,
                "reviewer_actor_id is required",
            ));
        }
        if request.reviewer_actor_id.len() > MAX_ID_LENGTH {
            return Ok(PrivilegedResponse::error(
                PrivilegedErrorCode::CapabilityRequestRejected,
                format!("reviewer_actor_id exceeds maximum length of {MAX_ID_LENGTH} bytes"),
            ));
        }

        // REQ-0010: Identity-bearing authoritative requests MUST carry
        // proof-carrying pointers.
        //
        // SECURITY (TCK-00356 Fix 1): Validate the identity proof hash using
        // the centralized validator which enforces non-zero commitment and
        // correct length. Phase 1 (pre-CAS transport) validates the hash as
        // a binding commitment; full proof dereference requires CAS
        // integration (TCK-00359).
        if let Err(e) = crate::identity::validate_identity_proof_hash(&request.identity_proof_hash)
        {
            return Ok(PrivilegedResponse::error(
                PrivilegedErrorCode::CapabilityRequestRejected,
                format!("identity_proof_hash validation failed: {e}"),
            ));
        }
        // WVR-0103: Log once that identity proof hash is validated as
        // shape-only commitment (Phase 1 / pre-CAS transport).
        {
            static PROOF_WAIVER_WARN: std::sync::Once = std::sync::Once::new();
            PROOF_WAIVER_WARN.call_once(|| {
                warn!(
                    waiver = "WVR-0103",
                    "identity proof hash validated as shape-only commitment; \
                     full CAS dereference + IdentityProofV1::verify() deferred (WVR-0103)"
                );
            });
        }

        // Validate changeset_digest is exactly 32 bytes
        if request.changeset_digest.len() != 32 {
            return Ok(PrivilegedResponse::error(
                PrivilegedErrorCode::CapabilityRequestRejected,
                format!(
                    "changeset_digest must be exactly 32 bytes, got {}",
                    request.changeset_digest.len()
                ),
            ));
        }

        // Validate artifact_bundle_hash is exactly 32 bytes
        if request.artifact_bundle_hash.len() != 32 {
            return Ok(PrivilegedResponse::error(
                PrivilegedErrorCode::CapabilityRequestRejected,
                format!(
                    "artifact_bundle_hash must be exactly 32 bytes, got {}",
                    request.artifact_bundle_hash.len()
                ),
            ));
        }

        // Validate verdict is not unspecified (fail-closed)
        let verdict = ReviewReceiptVerdict::try_from(request.verdict)
            .unwrap_or(ReviewReceiptVerdict::Unspecified);
        if verdict == ReviewReceiptVerdict::Unspecified {
            return Ok(PrivilegedResponse::error(
                PrivilegedErrorCode::CapabilityRequestRejected,
                "verdict must be APPROVE or BLOCKED, not UNSPECIFIED",
            ));
        }

        // For BLOCKED verdict, validate blocked_log_hash is present
        if verdict == ReviewReceiptVerdict::Blocked {
            if request.blocked_log_hash.len() != 32 {
                return Ok(PrivilegedResponse::error(
                    PrivilegedErrorCode::CapabilityRequestRejected,
                    format!(
                        "blocked_log_hash must be exactly 32 bytes for BLOCKED verdict, got {}",
                        request.blocked_log_hash.len()
                    ),
                ));
            }
            if request.blocked_reason_code == 0 {
                return Ok(PrivilegedResponse::error(
                    PrivilegedErrorCode::CapabilityRequestRejected,
                    "blocked_reason_code must be non-zero for BLOCKED verdict",
                ));
            }
        }

        // ---- Phase 1: Lease existence + reviewer identity validation ----
        // Security-critical: reviewer identity MUST match lease executor.
        // This prevents unauthorized actors from submitting review results.

        let expected_actor_id = self
            .lease_validator
            .get_lease_executor_actor_id(&request.lease_id);

        let Some(expected_actor_id) = expected_actor_id else {
            return Ok(PrivilegedResponse::error(
                PrivilegedErrorCode::GateLeaseMissing,
                format!("gate lease not found: {}", request.lease_id),
            ));
        };

        // SEC-TIMING-001: Constant-time comparison to prevent timing
        // side-channel attacks on reviewer identity.
        //
        // SECURITY (v6 Finding 1): Compare the AUTHENTICATED reviewer identity
        // (derived from peer credentials) against the lease executor — NOT the
        // caller-supplied `request.reviewer_actor_id`. This prevents any
        // operator-socket process from submitting receipts as an arbitrary
        // reviewer.
        let identity_matches = expected_actor_id.len() == authenticated_reviewer_id.len()
            && bool::from(
                expected_actor_id
                    .as_bytes()
                    .ct_eq(authenticated_reviewer_id.as_bytes()),
            );

        if !identity_matches {
            warn!(
                lease_id = %request.lease_id,
                authenticated_reviewer = %authenticated_reviewer_id,
                claimed_reviewer = %request.reviewer_actor_id,
                "Authenticated reviewer identity mismatch - rejecting review receipt"
            );
            return Ok(PrivilegedResponse::error(
                PrivilegedErrorCode::PermissionDenied,
                "authenticated caller identity does not match gate lease executor",
            ));
        }

        // ---- Phase 1a (TCK-00408): CAS existence validation ----
        // CAS is a hard requirement for review receipt ingestion (fail-closed).
        // Without CAS, artifact_bundle_hash cannot be verified, so we reject
        // the request rather than silently skipping validation.
        let Some(cas) = &self.cas else {
            warn!(
                receipt_id = %request.receipt_id,
                "CAS not configured — rejecting review receipt ingestion (fail-closed). \
                 CAS is required to verify artifact_bundle_hash."
            );
            return Ok(PrivilegedResponse::error(
                PrivilegedErrorCode::CapabilityRequestRejected,
                "CAS is not configured; review receipt ingestion requires CAS \
                 for artifact_bundle_hash verification (fail-closed)",
            ));
        };
        {
            let bundle_hash: [u8; 32] = request
                .artifact_bundle_hash
                .as_slice()
                .try_into()
                .expect("validated to be 32 bytes above");
            match cas.exists(&bundle_hash) {
                Ok(true) => {
                    // Bundle exists in CAS, proceed.
                },
                Ok(false) => {
                    warn!(
                        receipt_id = %request.receipt_id,
                        artifact_bundle_hash = %hex::encode(bundle_hash),
                        "artifact_bundle_hash not found in CAS — rejecting review receipt (fail-closed)"
                    );
                    return Ok(PrivilegedResponse::error(
                        PrivilegedErrorCode::CapabilityRequestRejected,
                        format!(
                            "artifact_bundle_hash {} not found in CAS; \
                             the referenced bundle must be published before review receipt ingestion",
                            hex::encode(bundle_hash)
                        ),
                    ));
                },
                Err(e) => {
                    warn!(
                        receipt_id = %request.receipt_id,
                        error = %e,
                        "CAS existence check failed — rejecting review receipt (fail-closed)"
                    );
                    return Ok(PrivilegedResponse::error(
                        PrivilegedErrorCode::CapabilityRequestRejected,
                        format!("CAS existence check failed for artifact_bundle_hash: {e}"),
                    ));
                },
            }
        }

        // ---- Phase 1b: Resolve risk tier + enforce HTF envelope/profile authority
        // ----
        let changeset_digest: [u8; 32] = request
            .changeset_digest
            .as_slice()
            .try_into()
            .expect("validated to be 32 bytes above");
        let (risk_tier, resolved_policy_hash) =
            self.resolve_risk_tier_for_lease(&request.lease_id, changeset_digest);

        // Authoritative terminal receipts MUST bind an authoritative lease
        // envelope. Fail closed if the lease cannot be resolved with full HTF
        // bindings.
        let Some(lease_for_receipt) = self.lease_validator.get_gate_lease(&request.lease_id) else {
            return Ok(PrivilegedResponse::error(
                PrivilegedErrorCode::CapabilityRequestRejected,
                format!(
                    "gate lease '{}' is missing authoritative time_envelope_ref binding",
                    request.lease_id
                ),
            ));
        };

        if let Err(e) = self.validate_lease_time_authority(&lease_for_receipt, risk_tier) {
            warn!(
                lease_id = %request.lease_id,
                risk_tier = ?risk_tier,
                error = %e,
                "HTF lease time authority validation failed for review receipt ingestion"
            );
            return Ok(PrivilegedResponse::error(
                PrivilegedErrorCode::CapabilityRequestRejected,
                format!("HTF authority validation failed: {e}"),
            ));
        }
        let lease_time_envelope_ref = lease_for_receipt.time_envelope_ref;
        let authoritative_work_id = lease_for_receipt.work_id.clone();
        let Some(claim_for_receipt) = self.work_registry.get_claim(&lease_for_receipt.work_id)
        else {
            return Ok(deny_response_for_authority_context(
                DenyCondition::MissingAuthorityContext,
                format!(
                    "work claim not found for lease work_id={}",
                    lease_for_receipt.work_id
                ),
            ));
        };
        if claim_for_receipt.work_id != authoritative_work_id {
            return Ok(deny_response_for_authority_context(
                DenyCondition::MissingAuthorityContext,
                format!(
                    "lease work_id '{}' does not match authoritative claim work_id '{}'",
                    authoritative_work_id, claim_for_receipt.work_id
                ),
            ));
        }
        let claim_role_spec_hash = match Self::validate_claim_authority_context_lineage(
            &claim_for_receipt,
            cas.as_ref(),
        ) {
            Ok(hash) => hash,
            Err((condition, detail)) => {
                return Ok(deny_response_for_authority_context(
                    condition,
                    format!("review receipt authority lineage validation failed: {detail}"),
                ));
            },
        };
        let claim_capability_manifest_hash =
            claim_for_receipt.policy_resolution.capability_manifest_hash;
        let claim_context_pack_hash = claim_for_receipt.policy_resolution.context_pack_hash;

        // ---- Phase 1c (TCK-00340): Attestation ratchet validation ----
        //
        // SECURITY (v5 Finding 4): Instead of hard-rejecting all non-Tier0
        // requests, consult the ratchet table first. The ratchet table
        // explicitly allows Tier1 with SelfSigned attestation. Only reject
        // if the tier's required attestation level exceeds what this
        // endpoint can provide (SelfSigned). This restores Tier1 throughput
        // while maintaining fail-closed semantics for higher tiers.
        let requirements = AttestationRequirements::new();
        let required_level = requirements.required_level(ReceiptKind::Review, risk_tier);
        if !AttestationLevel::SelfSigned.satisfies(required_level) {
            warn!(
                receipt_id = %request.receipt_id,
                risk_tier = ?risk_tier,
                required_level = %required_level,
                "Risk tier requires attestation stronger than SelfSigned — \
                 rejecting (fail-closed)"
            );
            return Ok(PrivilegedResponse::error(
                PrivilegedErrorCode::CapabilityRequestRejected,
                format!(
                    "review receipt endpoint provides SelfSigned attestation; \
                     {risk_tier:?} requires {required_level} which this endpoint cannot produce"
                ),
            ));
        }

        // SECURITY (v6 Finding 1): Use authenticated reviewer identity
        // for attestation signer, not the caller-supplied value.
        let attestation = ReceiptAttestation {
            kind: ReceiptKind::Review,
            level: AttestationLevel::SelfSigned,
            policy_hash: resolved_policy_hash,
            signer_identity: authenticated_reviewer_id.clone(),
            counter_signer_identity: None,
            threshold_signer_count: None,
        };

        // Validate attestation against requirements using the resolved
        // policy hash from the work item's policy resolution (not the
        // raw changeset_digest). This binds the attestation to the
        // governance-resolved policy state.
        if let Err(e) = validate_receipt_attestation(
            &attestation,
            risk_tier,
            &resolved_policy_hash,
            &requirements,
        ) {
            warn!(
                receipt_id = %request.receipt_id,
                error = %e,
                "Receipt attestation validation failed - rejecting (fail-closed)"
            );
            return Ok(PrivilegedResponse::error(
                PrivilegedErrorCode::CapabilityRequestRejected,
                format!("attestation validation failed: {e}"),
            ));
        }

        let request_changeset_digest_arr: [u8; 32] = request
            .changeset_digest
            .as_slice()
            .try_into()
            .expect("validated to be 32 bytes above");
        let request_identity_proof_hash_arr: [u8; 32] = request
            .identity_proof_hash
            .as_slice()
            .try_into()
            .expect("validated to be 32 bytes by validate_identity_proof_hash above");
        let request_artifact_bundle_hash_arr: [u8; 32] = request
            .artifact_bundle_hash
            .as_slice()
            .try_into()
            .expect("validated to be 32 bytes above");
        let request_changeset_digest_hex = hex::encode(request_changeset_digest_arr);
        let request_verdict = match verdict {
            ReviewReceiptVerdict::Approve => "APPROVE",
            ReviewReceiptVerdict::Blocked => "BLOCKED",
            ReviewReceiptVerdict::Unspecified => {
                return Ok(PrivilegedResponse::error(
                    PrivilegedErrorCode::CapabilityRequestRejected,
                    "verdict must be APPROVE or BLOCKED",
                ));
            },
        };
        let request_blocked_log_hash_arr: Option<[u8; 32]> =
            if verdict == ReviewReceiptVerdict::Blocked {
                Some(
                    request
                        .blocked_log_hash
                        .as_slice()
                        .try_into()
                        .expect("validated to be 32 bytes above"),
                )
            } else {
                None
            };
        let request_blocked_reason_code =
            (verdict == ReviewReceiptVerdict::Blocked).then_some(request.blocked_reason_code);

        let to_response_event_type = |event_type: &str| -> String {
            match event_type {
                "review_receipt_recorded" => "ReviewReceiptRecorded".to_string(),
                "review_blocked_recorded" => "ReviewBlockedRecorded".to_string(),
                other => other.to_string(),
            }
        };

        // =====================================================================
        // TCK-00416: Review outcome binding validation (IngestReviewReceipt)
        //
        // Per REQ-HEF-0013, review outcomes MUST carry valid evidence-index
        // bindings. All three commitment hashes must be non-zero AND
        // CAS-resolvable. This uses the full `validate_review_outcome_bindings`
        // helper for complete validation (not just non-zero checks).
        //
        // SECURITY: Outcome bindings use domain-tagged derivation to ensure
        // each hash is independent. Raw aliasing (e.g. view_commitment_hash
        // = changeset_digest) is forbidden per security review MAJOR finding.
        //
        // This MUST happen BEFORE state mutation (Phase 4).
        // =====================================================================
        // outcome_bindings used for validation only; emit methods re-derive
        // the same bindings deterministically from the same inputs.
        let _outcome_bindings = {
            // Derive proper independent outcome bindings via domain-tagged hashing
            let bindings = derive_review_outcome_bindings(
                &request_changeset_digest_arr,
                &request_artifact_bundle_hash_arr,
                &request.receipt_id,
            );

            // Store outcome binding preimages in CAS before validation
            if let Err(e) = store_review_outcome_artifacts(
                &request_changeset_digest_arr,
                &request_artifact_bundle_hash_arr,
                &request.receipt_id,
                cas.as_ref(),
            ) {
                warn!(
                    receipt_id = %request.receipt_id,
                    error = %e,
                    "IngestReviewReceipt rejected: outcome binding CAS storage failed (fail-closed)"
                );
                return Ok(PrivilegedResponse::error(
                    PrivilegedErrorCode::CapabilityRequestRejected,
                    format!("review outcome binding CAS storage failed: {e}"),
                ));
            }

            // Full validation: non-zero + CAS resolvability
            if let Err(auth_err) = validate_review_outcome_bindings(&bindings, cas.as_ref()) {
                let defect_ts = self.get_htf_timestamp_ns().unwrap_or(0);
                emit_authority_binding_defect(
                    self.event_emitter.as_ref(),
                    &authoritative_work_id,
                    &auth_err,
                    defect_ts,
                );
                warn!(
                    receipt_id = %request.receipt_id,
                    violations = ?auth_err.violations,
                    "IngestReviewReceipt rejected: review outcome binding validation failed (fail-closed)"
                );
                return Ok(PrivilegedResponse::error(
                    PrivilegedErrorCode::CapabilityRequestRejected,
                    format!("review outcome binding validation failed: {auth_err}"),
                ));
            }

            bindings
        };

        // ---- Phase 2: Semantic idempotency check ----
        if let Some(existing) = self.event_emitter.get_event_by_receipt_identity(
            &request.receipt_id,
            &request.lease_id,
            &authoritative_work_id,
            &request_changeset_digest_hex,
        ) {
            if let Err(message) = validate_receipt_replay_bindings(
                &existing,
                &request.lease_id,
                authoritative_work_id.as_str(),
                &request_identity_proof_hash_arr,
                &request_changeset_digest_arr,
                request_verdict,
                &request_artifact_bundle_hash_arr,
                request_blocked_reason_code,
                request_blocked_log_hash_arr,
            ) {
                return Ok(PrivilegedResponse::error(
                    PrivilegedErrorCode::CapabilityRequestRejected,
                    message,
                ));
            }

            info!(
                receipt_id = %request.receipt_id,
                lease_id = %request.lease_id,
                work_id = %authoritative_work_id,
                existing_event_id = %existing.event_id,
                "Duplicate semantic review identity detected - returning existing event"
            );

            return Ok(PrivilegedResponse::IngestReviewReceipt(
                IngestReviewReceiptResponse {
                    receipt_id: request.receipt_id.clone(),
                    event_type: to_response_event_type(&existing.event_type),
                    event_id: existing.event_id,
                },
            ));
        }

        // If the transport-level receipt_id already exists under a different
        // semantic tuple, fail closed with a conflict.
        if self
            .event_emitter
            .get_event_by_receipt_id(&request.receipt_id)
            .is_some()
        {
            return Ok(PrivilegedResponse::error(
                PrivilegedErrorCode::CapabilityRequestRejected,
                format!(
                    "receipt_id '{}' already exists with a different semantic identity tuple; \
                     PCAC linear consumption prevents reuse",
                    request.receipt_id
                ),
            ));
        }

        let Some(pcac_gate) = self.pcac_lifecycle_gate.as_deref() else {
            return Ok(PrivilegedResponse::error(
                PrivilegedErrorCode::CapabilityRequestRejected,
                "PCAC authority gate not wired for IngestReviewReceipt (fail-closed)",
            ));
        };

        let (join_freshness_tick, join_time_envelope_ref, join_ledger_anchor, join_revocation_head) =
            match self.derive_privileged_pcac_revalidation_inputs(&request.lease_id) {
                Ok(values) => values,
                Err(error) => {
                    return Ok(PrivilegedResponse::error(
                        PrivilegedErrorCode::CapabilityRequestRejected,
                        format!(
                            "PCAC authority denied for IngestReviewReceipt: \
                             authoritative revalidation unavailable: {error}"
                        ),
                    ));
                },
            };

        let pcac_risk_tier = Self::map_fac_risk_tier_to_pcac(risk_tier);
        let pcac_builder =
            PrivilegedPcacInputBuilder::new(PrivilegedHandlerClass::IngestReviewReceipt)
                .session_id(request.receipt_id.clone())
                .lease_id(request.lease_id.clone())
                .boundary_intent_class(apm2_core::pcac::BoundaryIntentClass::Assert)
                .identity_proof_hash(request_identity_proof_hash_arr)
                .identity_evidence_level(IdentityEvidenceLevel::PointerOnly)
                .risk_tier(pcac_risk_tier);

        let capability_manifest_hash = pcac_builder.hash(
            "capability",
            &[
                request.lease_id.as_bytes(),
                lease_for_receipt.gate_id.as_bytes(),
                &resolved_policy_hash,
                &request_changeset_digest_arr,
            ],
        );

        let scope_witness_hash = {
            let mut hasher = blake3::Hasher::new();
            let tag = format!(
                "{}-scope-v1",
                PrivilegedHandlerClass::IngestReviewReceipt.tag_prefix()
            );
            hasher.update(tag.as_bytes());
            hasher.update(request.receipt_id.as_bytes());
            hasher.update(&request_changeset_digest_arr);
            hasher.update(&request_artifact_bundle_hash_arr);
            hasher.update(&request.verdict.to_le_bytes());
            hasher.update(authenticated_reviewer_id.as_bytes());
            hasher.update(expected_actor_id.as_bytes());
            if verdict == ReviewReceiptVerdict::Blocked {
                hasher.update(&request.blocked_reason_code.to_le_bytes());
                let blocked_log_hash = request_blocked_log_hash_arr
                    .expect("validated blocked_log_hash for BLOCKED verdict");
                hasher.update(&blocked_log_hash);
            }
            *hasher.finalize().as_bytes()
        };

        let freshness_policy_hash = pcac_builder.hash(
            "freshness-policy",
            &[&resolved_policy_hash, request.lease_id.as_bytes()],
        );

        let stop_budget_profile_digest = pcac_builder.hash(
            "stop-budget",
            &[
                &[u8::from(self.stop_authority.as_ref().is_some_and(
                    |authority| authority.emergency_stop_active(),
                ))],
                &[u8::from(self.stop_authority.as_ref().is_some_and(
                    |authority| authority.governance_stop_active(),
                ))],
                request.receipt_id.as_bytes(),
                &request.verdict.to_le_bytes(),
                &request.blocked_reason_code.to_le_bytes(),
            ],
        );

        let effect_intent_digest = {
            let verdict_bytes = request.verdict.to_le_bytes();
            let blocked_reason_code_bytes = request.blocked_reason_code.to_le_bytes();
            let mut intent_data: Vec<&[u8]> = vec![
                request.lease_id.as_bytes(),
                request.receipt_id.as_bytes(),
                authenticated_reviewer_id.as_bytes(),
                expected_actor_id.as_bytes(),
                &request_changeset_digest_arr,
                &request_artifact_bundle_hash_arr,
                &request_identity_proof_hash_arr,
                &verdict_bytes,
                &blocked_reason_code_bytes,
            ];
            if let Some(ref blocked_log_hash) = request_blocked_log_hash_arr {
                intent_data.push(blocked_log_hash);
            }
            domain_tagged_hash(
                PrivilegedHandlerClass::IngestReviewReceipt,
                "intent",
                &intent_data,
            )
        };

        let pcac_builder = pcac_builder
            .capability_manifest_hash(capability_manifest_hash)
            .scope_witness_hash(scope_witness_hash)
            .freshness_policy_hash(freshness_policy_hash)
            .stop_budget_profile_digest(stop_budget_profile_digest)
            .effect_intent_digest(effect_intent_digest);

        let pcac_input = pcac_builder.build(
            join_freshness_tick,
            join_time_envelope_ref,
            join_ledger_anchor,
            join_revocation_head,
        );

        let pcac_lifecycle_artifacts = match self.enforce_privileged_pcac_lifecycle(
            PrivilegedHandlerClass::IngestReviewReceipt.operation_name(),
            pcac_gate,
            &pcac_input,
            &request.lease_id,
            join_freshness_tick,
            join_time_envelope_ref,
            join_ledger_anchor,
            join_revocation_head,
            effect_intent_digest,
        ) {
            Ok(artifacts) => {
                if artifacts.is_none() {
                    return Ok(PrivilegedResponse::error(
                        PrivilegedErrorCode::CapabilityRequestRejected,
                        "IngestReviewReceipt requires PCAC lifecycle evidence (mandatory cutover); \
                         lifecycle_enforcement is disabled in claim policy",
                    ));
                }
                artifacts
            },
            Err(response) => return Ok(response),
        };

        // ---- Phase 3: Get HTF timestamp ----
        let timestamp_ns = match self.get_htf_timestamp_ns() {
            Ok(ts) => ts,
            Err(e) => {
                warn!(error = %e, "HTF timestamp generation failed - failing closed");
                return Ok(PrivilegedResponse::error(
                    PrivilegedErrorCode::CapabilityRequestRejected,
                    format!("HTF timestamp error: {e}"),
                ));
            },
        };

        // ---- Phase 4: Emit event based on verdict ----
        let (event_type, emit_result, emit_error_prefix) = match verdict {
            ReviewReceiptVerdict::Approve => (
                "ReviewReceiptRecorded".to_string(),
                self.event_emitter.emit_review_receipt(
                    &request.lease_id,
                    &authoritative_work_id,
                    &request.receipt_id,
                    &request_changeset_digest_arr,
                    &request_artifact_bundle_hash_arr,
                    &claim_capability_manifest_hash,
                    &claim_context_pack_hash,
                    &claim_role_spec_hash,
                    &authenticated_reviewer_id,
                    timestamp_ns,
                    &request_identity_proof_hash_arr,
                    &lease_time_envelope_ref,
                    pcac_lifecycle_artifacts.as_ref(),
                ),
                "review receipt emission failed",
            ),
            ReviewReceiptVerdict::Blocked => {
                let blocked_log_hash_arr = request_blocked_log_hash_arr
                    .expect("validated to be present for BLOCKED verdict");
                (
                    "ReviewBlockedRecorded".to_string(),
                    self.event_emitter.emit_review_blocked_receipt(
                        &request.lease_id,
                        &authoritative_work_id,
                        &request.receipt_id,
                        &request_changeset_digest_arr,
                        &request_artifact_bundle_hash_arr,
                        &claim_capability_manifest_hash,
                        &claim_context_pack_hash,
                        &claim_role_spec_hash,
                        request.blocked_reason_code,
                        &blocked_log_hash_arr,
                        &authenticated_reviewer_id,
                        timestamp_ns,
                        &request_identity_proof_hash_arr,
                        &lease_time_envelope_ref,
                        pcac_lifecycle_artifacts.as_ref(),
                    ),
                    "review blocked emission failed",
                )
            },
            ReviewReceiptVerdict::Unspecified => unreachable!("validated above"),
        };

        let signed_event = match emit_result {
            Ok(event) => event,
            Err(e) if e.to_string().contains("UNIQUE constraint") => {
                warn!(
                    receipt_id = %request.receipt_id,
                    lease_id = %request.lease_id,
                    work_id = %authoritative_work_id,
                    verdict = %request_verdict,
                    error = %e,
                    "Concurrent duplicate review receipt detected by UNIQUE constraint"
                );

                // First try: exact semantic tuple match (true idempotent replay)
                if let Some(existing) = self.event_emitter.get_event_by_receipt_identity(
                    &request.receipt_id,
                    &request.lease_id,
                    &authoritative_work_id,
                    &request_changeset_digest_hex,
                ) {
                    if let Err(message) = validate_receipt_replay_bindings(
                        &existing,
                        &request.lease_id,
                        authoritative_work_id.as_str(),
                        &request_identity_proof_hash_arr,
                        &request_changeset_digest_arr,
                        request_verdict,
                        &request_artifact_bundle_hash_arr,
                        request_blocked_reason_code,
                        request_blocked_log_hash_arr,
                    ) {
                        return Ok(PrivilegedResponse::error(
                            PrivilegedErrorCode::CapabilityRequestRejected,
                            message,
                        ));
                    }

                    info!(
                        receipt_id = %request.receipt_id,
                        existing_event_id = %existing.event_id,
                        "Concurrent duplicate receipt resolved as idempotent replay (all fields validated)"
                    );

                    return Ok(PrivilegedResponse::IngestReviewReceipt(
                        IngestReviewReceiptResponse {
                            receipt_id: request.receipt_id.clone(),
                            event_type: to_response_event_type(&existing.event_type),
                            event_id: existing.event_id,
                        },
                    ));
                }

                // Fallback: receipt_id exists but semantic tuple does NOT match — deny
                // fail-closed. This prevents a conflicting concurrent request
                // from being misclassified as idempotent replay.
                return Ok(PrivilegedResponse::error(
                    PrivilegedErrorCode::CapabilityRequestRejected,
                    format!(
                        "receipt_id '{}' already committed under a different semantic identity tuple; \
                         concurrent conflict denied (fail-closed)",
                        request.receipt_id,
                    ),
                ));
            },
            Err(e) => {
                return Err(ProtocolError::Serialization {
                    reason: format!("{emit_error_prefix}: {e}"),
                });
            },
        };

        info!(
            receipt_id = %request.receipt_id,
            event_type = %event_type,
            event_id = %signed_event.event_id,
            reviewer = %authenticated_reviewer_id,
            "Review receipt ingested successfully"
        );

        Ok(PrivilegedResponse::IngestReviewReceipt(
            IngestReviewReceiptResponse {
                receipt_id: request.receipt_id,
                event_type,
                event_id: signed_event.event_id,
            },
        ))
    }

    // ========================================================================
    // Process Management Handlers (TCK-00342)
    // ========================================================================

    /// Converts a `ProcessState` to the corresponding proto `ProcessStateEnum`
    /// i32 value.
    fn process_state_to_proto(state: &ProcessState) -> i32 {
        match state {
            ProcessState::Starting => ProcessStateEnum::ProcessStateStarting.into(),
            ProcessState::Running => ProcessStateEnum::ProcessStateRunning.into(),
            ProcessState::Unhealthy => ProcessStateEnum::ProcessStateUnhealthy.into(),
            ProcessState::Stopping => ProcessStateEnum::ProcessStateStopping.into(),
            ProcessState::Stopped { .. } => ProcessStateEnum::ProcessStateStopped.into(),
            ProcessState::Crashed { .. } => ProcessStateEnum::ProcessStateCrashed.into(),
            ProcessState::Terminated => ProcessStateEnum::ProcessStateTerminated.into(),
        }
    }

    /// Builds a `ProcessInfo` proto message from supervisor data.
    ///
    /// Collects state information across all instances of a process,
    /// using the first running instance's PID and uptime.
    #[allow(clippy::cast_possible_truncation)] // Instance count bounded by ProcessSpec.instances (u32)
    fn build_process_info(
        name: &str,
        spec_instances: u32,
        handles: &[&apm2_core::process::ProcessHandle],
    ) -> ProcessInfo {
        let running_instances = handles.iter().filter(|h| h.state.is_running()).count() as u32;

        // Use first running instance's PID
        let pid = handles.iter().find(|h| h.pid.is_some()).and_then(|h| h.pid);

        // Use first running instance's uptime
        #[allow(clippy::cast_sign_loss)] // .max(0) guarantees non-negative
        let uptime_secs = handles.iter().find_map(|h| {
            h.started_at.map(|started| {
                let elapsed = chrono::Utc::now().signed_duration_since(started);
                elapsed.num_seconds().max(0) as u64
            })
        });

        // Determine aggregate state: if any running, report first running
        // instance's state; otherwise use first handle's state.
        let state = handles
            .iter()
            .find(|h| h.state.is_running())
            .or_else(|| handles.first())
            .map_or_else(
                || ProcessStateEnum::ProcessStateUnspecified.into(),
                |h| Self::process_state_to_proto(&h.state),
            );

        // Collect exit code from first stopped/crashed handle.
        let exit_code = handles.iter().find_map(|h| match &h.state {
            ProcessState::Stopped { exit_code } | ProcessState::Crashed { exit_code } => *exit_code,
            _ => None,
        });

        ProcessInfo {
            name: name.to_string(),
            state,
            instances: spec_instances,
            running_instances,
            pid,
            uptime_secs,
            exit_code,
        }
    }

    /// Tries to acquire a read lock on daemon state.
    ///
    /// Returns an error response if daemon state is not configured (test mode)
    /// or if the lock is currently held for writing.
    #[allow(clippy::result_large_err)] // PrivilegedResponse is large by design; boxing would be a breaking change
    fn try_read_daemon_state(
        &self,
    ) -> Result<tokio::sync::RwLockReadGuard<'_, crate::state::DaemonState>, PrivilegedResponse>
    {
        let state = self.daemon_state.as_ref().ok_or_else(|| {
            PrivilegedResponse::error(
                PrivilegedErrorCode::PrivilegedErrorUnspecified,
                "process management not available (daemon state not configured)",
            )
        })?;

        state.try_read().ok_or_else(|| {
            PrivilegedResponse::error(
                PrivilegedErrorCode::PrivilegedErrorUnspecified,
                "daemon state temporarily unavailable (write lock held)",
            )
        })
    }

    /// Tries to acquire a write lock on daemon state.
    ///
    /// Returns an error response if daemon state is not configured (test mode)
    /// or if any lock is currently held.
    #[allow(clippy::result_large_err)] // PrivilegedResponse is large by design; boxing would be a breaking change
    fn try_write_daemon_state(
        &self,
    ) -> Result<tokio::sync::RwLockWriteGuard<'_, crate::state::DaemonState>, PrivilegedResponse>
    {
        let state = self.daemon_state.as_ref().ok_or_else(|| {
            PrivilegedResponse::error(
                PrivilegedErrorCode::PrivilegedErrorUnspecified,
                "process management not available (daemon state not configured)",
            )
        })?;

        state.try_write().ok_or_else(|| {
            PrivilegedResponse::error(
                PrivilegedErrorCode::PrivilegedErrorUnspecified,
                "daemon state temporarily unavailable (lock held)",
            )
        })
    }

    /// Handles `ListProcesses` requests (IPC-PRIV-005).
    ///
    /// Returns a list of all configured processes with their current state
    /// by querying the `Supervisor` in `DaemonState`.
    fn handle_list_processes(&self, payload: &[u8]) -> ProtocolResult<PrivilegedResponse> {
        // Decode request (empty, but validate format)
        let _request =
            ListProcessesRequest::decode_bounded(payload, &self.decode_config).map_err(|e| {
                ProtocolError::Serialization {
                    reason: format!("invalid ListProcessesRequest: {e}"),
                }
            })?;

        debug!("ListProcesses request received");

        let daemon_state = match self.try_read_daemon_state() {
            Ok(state) => state,
            Err(err_resp) => return Ok(err_resp),
        };

        let supervisor = daemon_state.supervisor();
        let names = supervisor.list_names();

        let mut processes = Vec::with_capacity(names.len());
        for name in &names {
            if let Some(spec) = supervisor.get_spec(name) {
                let handles = supervisor.get_handles(name);
                processes.push(Self::build_process_info(name, spec.instances, &handles));
            }
        }

        Ok(PrivilegedResponse::ListProcesses(ListProcessesResponse {
            processes,
        }))
    }

    /// Handles `ProcessStatus` requests (IPC-PRIV-006).
    ///
    /// Returns detailed status for a specific process by name, including
    /// restart count, CPU usage, memory usage, and command information.
    fn handle_process_status(&self, payload: &[u8]) -> ProtocolResult<PrivilegedResponse> {
        let request =
            ProcessStatusRequest::decode_bounded(payload, &self.decode_config).map_err(|e| {
                ProtocolError::Serialization {
                    reason: format!("invalid ProcessStatusRequest: {e}"),
                }
            })?;

        // Validate process name length (CTR-1303: bounded inputs)
        if request.name.len() > MAX_ID_LENGTH {
            return Ok(PrivilegedResponse::error(
                PrivilegedErrorCode::CapabilityRequestRejected,
                format!(
                    "process name too long: {} > {}",
                    request.name.len(),
                    MAX_ID_LENGTH
                ),
            ));
        }

        debug!(name = %request.name, "ProcessStatus request received");

        let daemon_state = match self.try_read_daemon_state() {
            Ok(state) => state,
            Err(err_resp) => return Ok(err_resp),
        };

        let supervisor = daemon_state.supervisor();
        let Some(spec) = supervisor.get_spec(&request.name) else {
            return Ok(PrivilegedResponse::error(
                PrivilegedErrorCode::CapabilityRequestRejected,
                format!("process not found: {}", request.name),
            ));
        };

        let handles = supervisor.get_handles(&request.name);
        let info = Self::build_process_info(&request.name, spec.instances, &handles);

        // Aggregate restart count across all instances
        let restart_count: u32 = handles.iter().map(|h| h.restart_count).sum();

        Ok(PrivilegedResponse::ProcessStatus(ProcessStatusResponse {
            info: Some(info),
            restart_count,
            cpu_percent: None,
            memory_bytes: None,
            command: spec.command.clone(),
            cwd: spec.cwd.as_ref().map(|p| p.display().to_string()),
        }))
    }

    /// Handles `StartProcess` requests (IPC-PRIV-007).
    ///
    /// Marks all stopped/crashed instances of a configured process as
    /// starting. Transitions instance states via the `Supervisor` so the
    /// daemon's run loop can pick them up for actual OS process spawning.
    fn handle_start_process(&self, payload: &[u8]) -> ProtocolResult<PrivilegedResponse> {
        let request =
            StartProcessRequest::decode_bounded(payload, &self.decode_config).map_err(|e| {
                ProtocolError::Serialization {
                    reason: format!("invalid StartProcessRequest: {e}"),
                }
            })?;

        // Validate process name length
        if request.name.len() > MAX_ID_LENGTH {
            return Ok(PrivilegedResponse::error(
                PrivilegedErrorCode::CapabilityRequestRejected,
                format!(
                    "process name too long: {} > {}",
                    request.name.len(),
                    MAX_ID_LENGTH
                ),
            ));
        }

        info!(name = %request.name, "StartProcess request received");

        let mut daemon_state = match self.try_write_daemon_state() {
            Ok(state) => state,
            Err(err_resp) => return Ok(err_resp),
        };

        let supervisor = daemon_state.supervisor();
        let Some(spec) = supervisor.get_spec(&request.name) else {
            return Ok(PrivilegedResponse::error(
                PrivilegedErrorCode::CapabilityRequestRejected,
                format!("process not found: {}", request.name),
            ));
        };
        let spec_instances = spec.instances;

        // Collect instances that are not already running
        let handles = supervisor.get_handles(&request.name);
        let startable_indices: Vec<u32> = handles
            .iter()
            .filter(|h| !h.state.is_running())
            .map(|h| h.instance)
            .collect();
        #[allow(clippy::cast_possible_truncation)] // bounded by spec.instances (u32)
        let startable_count = startable_indices.len() as u32;

        // Transition each startable instance to Starting state
        let supervisor = daemon_state.supervisor_mut();
        for idx in &startable_indices {
            supervisor.update_state(&request.name, *idx, ProcessState::Starting);
        }

        Ok(PrivilegedResponse::StartProcess(StartProcessResponse {
            name: request.name.clone(),
            instances_started: startable_count,
            message: format!(
                "scheduled {} instance(s) of '{}' for start (total configured: {})",
                startable_count, request.name, spec_instances
            ),
        }))
    }

    /// Handles `StopProcess` requests (IPC-PRIV-008).
    ///
    /// Marks all running instances of a process as stopping. Transitions
    /// instance states via the `Supervisor` for the daemon's run loop to
    /// perform actual shutdown.
    fn handle_stop_process(&self, payload: &[u8]) -> ProtocolResult<PrivilegedResponse> {
        let request =
            StopProcessRequest::decode_bounded(payload, &self.decode_config).map_err(|e| {
                ProtocolError::Serialization {
                    reason: format!("invalid StopProcessRequest: {e}"),
                }
            })?;

        // Validate process name length
        if request.name.len() > MAX_ID_LENGTH {
            return Ok(PrivilegedResponse::error(
                PrivilegedErrorCode::CapabilityRequestRejected,
                format!(
                    "process name too long: {} > {}",
                    request.name.len(),
                    MAX_ID_LENGTH
                ),
            ));
        }

        info!(name = %request.name, "StopProcess request received");

        let mut daemon_state = match self.try_write_daemon_state() {
            Ok(state) => state,
            Err(err_resp) => return Ok(err_resp),
        };

        let supervisor = daemon_state.supervisor();
        if supervisor.get_spec(&request.name).is_none() {
            return Ok(PrivilegedResponse::error(
                PrivilegedErrorCode::CapabilityRequestRejected,
                format!("process not found: {}", request.name),
            ));
        }

        // Collect running instance indices
        let handles = supervisor.get_handles(&request.name);
        let running_indices: Vec<u32> = handles
            .iter()
            .filter(|h| h.state.is_running())
            .map(|h| h.instance)
            .collect();
        #[allow(clippy::cast_possible_truncation)] // bounded by spec.instances (u32)
        let running_count = running_indices.len() as u32;

        // Transition each running instance to Stopping state
        let supervisor = daemon_state.supervisor_mut();
        for idx in &running_indices {
            supervisor.update_state(&request.name, *idx, ProcessState::Stopping);
        }

        Ok(PrivilegedResponse::StopProcess(StopProcessResponse {
            name: request.name.clone(),
            instances_stopped: running_count,
            message: format!(
                "scheduled {} running instance(s) of '{}' for stop",
                running_count, request.name
            ),
        }))
    }

    /// Handles `RestartProcess` requests (IPC-PRIV-009).
    ///
    /// Transitions all instances through a stop-then-start cycle. Running
    /// instances are marked as stopping first; stopped/crashed instances
    /// are marked as starting directly.
    fn handle_restart_process(&self, payload: &[u8]) -> ProtocolResult<PrivilegedResponse> {
        let request =
            RestartProcessRequest::decode_bounded(payload, &self.decode_config).map_err(|e| {
                ProtocolError::Serialization {
                    reason: format!("invalid RestartProcessRequest: {e}"),
                }
            })?;

        // Validate process name length
        if request.name.len() > MAX_ID_LENGTH {
            return Ok(PrivilegedResponse::error(
                PrivilegedErrorCode::CapabilityRequestRejected,
                format!(
                    "process name too long: {} > {}",
                    request.name.len(),
                    MAX_ID_LENGTH
                ),
            ));
        }

        info!(name = %request.name, "RestartProcess request received");

        let mut daemon_state = match self.try_write_daemon_state() {
            Ok(state) => state,
            Err(err_resp) => return Ok(err_resp),
        };

        let supervisor = daemon_state.supervisor();
        let Some(spec) = supervisor.get_spec(&request.name) else {
            return Ok(PrivilegedResponse::error(
                PrivilegedErrorCode::CapabilityRequestRejected,
                format!("process not found: {}", request.name),
            ));
        };
        let spec_instances = spec.instances;

        // Collect all instance indices and their current running status
        let handles = supervisor.get_handles(&request.name);
        let instance_transitions: Vec<(u32, bool)> = handles
            .iter()
            .map(|h| (h.instance, h.state.is_running()))
            .collect();

        // Transition: running -> Stopping, stopped/crashed -> Starting
        let supervisor = daemon_state.supervisor_mut();
        for (idx, is_running) in &instance_transitions {
            if *is_running {
                supervisor.update_state(&request.name, *idx, ProcessState::Stopping);
            } else {
                supervisor.update_state(&request.name, *idx, ProcessState::Starting);
            }
        }

        Ok(PrivilegedResponse::RestartProcess(RestartProcessResponse {
            name: request.name.clone(),
            instances_restarted: spec_instances,
            message: format!(
                "scheduled {} instance(s) of '{}' for restart",
                spec_instances, request.name
            ),
        }))
    }

    /// Handles `ReloadProcess` requests (IPC-PRIV-010).
    ///
    /// Performs a rolling restart (graceful reload) by marking the first
    /// running instance as stopping. The daemon's run loop handles the
    /// sequential restart of remaining instances to maintain availability.
    fn handle_reload_process(&self, payload: &[u8]) -> ProtocolResult<PrivilegedResponse> {
        let request =
            ReloadProcessRequest::decode_bounded(payload, &self.decode_config).map_err(|e| {
                ProtocolError::Serialization {
                    reason: format!("invalid ReloadProcessRequest: {e}"),
                }
            })?;

        // Validate process name length
        if request.name.len() > MAX_ID_LENGTH {
            return Ok(PrivilegedResponse::error(
                PrivilegedErrorCode::CapabilityRequestRejected,
                format!(
                    "process name too long: {} > {}",
                    request.name.len(),
                    MAX_ID_LENGTH
                ),
            ));
        }

        info!(name = %request.name, "ReloadProcess request received");

        let mut daemon_state = match self.try_write_daemon_state() {
            Ok(state) => state,
            Err(err_resp) => return Ok(err_resp),
        };

        let supervisor = daemon_state.supervisor();
        if supervisor.get_spec(&request.name).is_none() {
            return Ok(PrivilegedResponse::error(
                PrivilegedErrorCode::CapabilityRequestRejected,
                format!("process not found: {}", request.name),
            ));
        }

        // For rolling restart, mark the first running instance as stopping.
        // The daemon run loop will restart it and proceed to the next instance
        // sequentially.
        let handles = supervisor.get_handles(&request.name);
        let first_running = handles
            .iter()
            .find(|h| h.state.is_running())
            .map(|h| h.instance);

        if let Some(idx) = first_running {
            let supervisor = daemon_state.supervisor_mut();
            supervisor.update_state(&request.name, idx, ProcessState::Stopping);
        }

        Ok(PrivilegedResponse::ReloadProcess(ReloadProcessResponse {
            name: request.name.clone(),
            success: true,
            message: format!("rolling restart scheduled for '{}'", request.name),
        }))
    }

    // =========================================================================
    // Credential Management Handlers (CTR-PROTO-011, TCK-00343)
    // =========================================================================

    /// Converts a protobuf `CredentialProvider` enum to a core `Provider`.
    fn proto_provider_to_core(proto: i32) -> Provider {
        match ProtoProvider::try_from(proto) {
            Ok(ProtoProvider::Anthropic) => Provider::Claude,
            Ok(ProtoProvider::Openai) => Provider::OpenAI,
            // Github, ApiKey, Unspecified, and unknown values all map to Custom
            _ => Provider::Custom,
        }
    }

    /// Converts a core `Provider` to a protobuf `CredentialProvider` enum.
    const fn core_provider_to_proto(provider: Provider) -> i32 {
        match provider {
            Provider::Claude => ProtoProvider::Anthropic as i32,
            Provider::OpenAI => ProtoProvider::Openai as i32,
            // Gemini and Custom both map to the generic ApiKey provider
            Provider::Gemini | Provider::Custom => ProtoProvider::ApiKey as i32,
        }
    }

    /// Converts a protobuf auth method enum value and secret bytes into a core
    /// `AuthMethod`.
    ///
    /// # Errors
    ///
    /// Returns `ProtocolError::Serialization` if `secret` is not valid UTF-8.
    fn proto_auth_to_core(auth_method: i32, secret: &[u8]) -> Result<AuthMethod, ProtocolError> {
        let secret_str =
            String::from_utf8(secret.to_vec()).map_err(|_| ProtocolError::Serialization {
                reason: "credential_secret is not valid UTF-8".to_string(),
            })?;
        let auth = match ProtoAuthMethod::try_from(auth_method) {
            Ok(ProtoAuthMethod::Oauth) => AuthMethod::OAuth {
                access_token: SecretString::from(secret_str),
                refresh_token: None,
                expires_at: None,
                scopes: vec![],
            },
            Ok(ProtoAuthMethod::Ssh) => AuthMethod::SessionToken {
                token: SecretString::from(secret_str),
                cookie_jar: None,
                expires_at: None,
            },
            // Pat, ApiKey, Unspecified, and unknown values all map to ApiKey
            _ => AuthMethod::ApiKey {
                key: SecretString::from(secret_str),
            },
        };
        Ok(auth)
    }

    /// Converts a core `AuthMethod` to a protobuf auth method enum value.
    const fn core_auth_method_to_proto(auth: &AuthMethod) -> i32 {
        match auth {
            AuthMethod::OAuth { .. } => ProtoAuthMethod::Oauth as i32,
            AuthMethod::ApiKey { .. } => ProtoAuthMethod::ApiKey as i32,
            AuthMethod::SessionToken { .. } => ProtoAuthMethod::Pat as i32,
        }
    }

    /// Converts a core `CredentialProfile` to a protobuf `CredentialProfile`
    /// message (without secrets).
    fn core_profile_to_proto(
        profile: &CoreCredentialProfile,
        display_name: &str,
    ) -> super::messages::CredentialProfile {
        let expires_at = match &profile.auth {
            AuthMethod::OAuth { expires_at, .. } | AuthMethod::SessionToken { expires_at, .. } => {
                expires_at
                    .map(|dt| dt.timestamp().try_into().unwrap_or(0u64))
                    .unwrap_or(0)
            },
            AuthMethod::ApiKey { .. } => 0,
        };

        super::messages::CredentialProfile {
            profile_id: profile.id.as_str().to_string(),
            provider: Self::core_provider_to_proto(profile.provider),
            auth_method: Self::core_auth_method_to_proto(&profile.auth),
            created_at: profile.created_at.timestamp().try_into().unwrap_or(0u64),
            expires_at,
            is_active: !profile.is_expired(),
            display_name: display_name.to_string(),
        }
    }

    // =========================================================================
    // TCK-00394: ChangeSet Publishing (RFC-0018)
    // =========================================================================

    /// Handles `PublishChangeSet` requests (IPC-PRIV-017).
    ///
    /// Accepts a `ChangeSetBundleV1` payload, validates it, stores it in CAS,
    /// emits a `ChangeSetPublished` ledger event, and returns the
    /// `changeset_digest` and `cas_hash` for subsequent gate lease binding.
    ///
    /// # Security
    ///
    /// - `work_id` is validated against the work registry (fail-closed)
    /// - Bundle bytes are validated before CAS storage
    /// - Idempotent: re-publishing the same bundle returns the same digest
    ///   without duplicate ledger events
    /// - Domain-separated signing prevents cross-context replay
    /// - Actor ID is derived from peer credentials, not user input
    fn handle_publish_changeset(
        &self,
        payload: &[u8],
        ctx: &ConnectionContext,
    ) -> ProtocolResult<PrivilegedResponse> {
        let request = PublishChangeSetRequest::decode_bounded(payload, &self.decode_config)
            .map_err(|e| ProtocolError::Serialization {
                reason: format!("invalid PublishChangeSetRequest: {e}"),
            })?;

        info!(
            work_id = %request.work_id,
            bundle_size = request.bundle_bytes.len(),
            peer_pid = ?ctx.peer_credentials().map(|c| c.pid),
            "PublishChangeSet request received"
        );

        // --- Validation Phase (all checks BEFORE any state mutation) ---

        // Validate required fields
        if request.work_id.is_empty() {
            return Ok(PrivilegedResponse::error(
                PrivilegedErrorCode::CapabilityRequestRejected,
                "work_id is required",
            ));
        }

        if request.work_id.len() > MAX_ID_LENGTH {
            return Ok(PrivilegedResponse::error(
                PrivilegedErrorCode::CapabilityRequestRejected,
                format!("work_id exceeds maximum length of {MAX_ID_LENGTH} bytes"),
            ));
        }

        if request.bundle_bytes.is_empty() {
            return Ok(PrivilegedResponse::error(
                PrivilegedErrorCode::CapabilityRequestRejected,
                "bundle_bytes is required (empty payload)",
            ));
        }

        // Derive actor_id from peer credentials (not user input)
        let peer_creds = ctx
            .peer_credentials()
            .ok_or_else(|| ProtocolError::Serialization {
                reason: "peer credentials required for changeset publishing".to_string(),
            })?;
        let actor_id = derive_actor_id(peer_creds);

        // Validate work_id exists in registry (fail-closed: reject orphaned changesets)
        let Some(claim) = self.work_registry.get_claim(&request.work_id) else {
            return Ok(PrivilegedResponse::error(
                PrivilegedErrorCode::CapabilityRequestRejected,
                format!("work_id not found in registry: {}", request.work_id),
            ));
        };

        // TCK-00408: Actor ownership check — the authenticated caller must be
        // the actor that owns the work claim. This prevents unauthorized actors
        // from publishing changesets against work items they don't own.
        if claim.actor_id != actor_id {
            warn!(
                work_id = %request.work_id,
                claim_actor = %claim.actor_id,
                caller_actor = %actor_id,
                "PublishChangeSet rejected: caller does not own work claim"
            );
            return Ok(PrivilegedResponse::error(
                PrivilegedErrorCode::PermissionDenied,
                format!(
                    "authenticated caller '{}' does not own work claim for '{}' (owned by '{}')",
                    actor_id, request.work_id, claim.actor_id
                ),
            ));
        }

        let bundle: apm2_core::fac::ChangeSetBundleV1 =
            match serde_json::from_slice(&request.bundle_bytes) {
                Ok(bundle) => bundle,
                Err(e) => {
                    return Ok(PrivilegedResponse::error(
                        PrivilegedErrorCode::CapabilityRequestRejected,
                        format!("invalid ChangeSetBundleV1 JSON: {e}"),
                    ));
                },
            };

        // Recompute digest from canonical bundle fields and reject caller-provided
        // digest mismatches before any side effects.
        let computed_changeset_digest = bundle.compute_digest();
        let provided_changeset_digest = bundle.changeset_digest();
        if !bool::from(computed_changeset_digest.ct_eq(&provided_changeset_digest)) {
            return Ok(PrivilegedResponse::error(
                PrivilegedErrorCode::CapabilityRequestRejected,
                format!(
                    "changeset_digest mismatch: expected {}, got {}",
                    hex::encode(computed_changeset_digest),
                    hex::encode(provided_changeset_digest)
                ),
            ));
        }

        // Full bundle validation is mandatory and fail-closed.
        if let Err(e) = bundle.validate() {
            return Ok(PrivilegedResponse::error(
                PrivilegedErrorCode::CapabilityRequestRejected,
                format!("invalid ChangeSetBundleV1: {e}"),
            ));
        }

        // Require CAS to be configured (fail-closed)
        let Some(cas) = &self.cas else {
            return Ok(PrivilegedResponse::error(
                PrivilegedErrorCode::CapabilityRequestRejected,
                "content-addressed store not configured on daemon",
            ));
        };

        let changeset_digest_hex = hex::encode(computed_changeset_digest);
        if let Some((event_id, persisted_cas_hash)) =
            self.find_changeset_published_replay(&request.work_id, &changeset_digest_hex)
        {
            debug!(
                work_id = %request.work_id,
                changeset_digest = %changeset_digest_hex,
                event_id = %event_id,
                "Idempotent: returning existing ChangeSetPublished event"
            );
            return Ok(PrivilegedResponse::PublishChangeSet(
                PublishChangeSetResponse {
                    changeset_digest: changeset_digest_hex,
                    cas_hash: persisted_cas_hash,
                    work_id: request.work_id,
                    event_id,
                },
            ));
        }

        // --- State Mutation Phase ---

        // Store bundle bytes in CAS
        let store_result = match cas.store(&request.bundle_bytes) {
            Ok(result) => result,
            Err(e) => {
                warn!(error = %e, "CAS store failed for changeset bundle");
                return Ok(PrivilegedResponse::error(
                    PrivilegedErrorCode::CapabilityRequestRejected,
                    format!("CAS storage failed: {e}"),
                ));
            },
        };

        let cas_hash = store_result.hash;
        let cas_hash_hex = hex::encode(cas_hash);

        // Get HTF-compliant timestamp
        let timestamp_ns = match self.get_htf_timestamp_ns() {
            Ok(ts) => ts,
            Err(e) => {
                warn!(error = %e, "HTF timestamp generation failed - failing closed");
                return Ok(PrivilegedResponse::error(
                    PrivilegedErrorCode::CapabilityRequestRejected,
                    format!("HTF timestamp error: {e}"),
                ));
            },
        };

        // Emit ChangeSetPublished ledger event
        let signed_event = match self.event_emitter.emit_changeset_published(
            &request.work_id,
            &computed_changeset_digest,
            &cas_hash,
            &actor_id,
            timestamp_ns,
        ) {
            Ok(event) => event,
            Err(e) => {
                // Race-safe idempotency fallback: if another writer persisted the
                // same semantic event concurrently, replay the persisted binding.
                if let Some((event_id, persisted_cas_hash)) =
                    self.find_changeset_published_replay(&request.work_id, &changeset_digest_hex)
                {
                    return Ok(PrivilegedResponse::PublishChangeSet(
                        PublishChangeSetResponse {
                            changeset_digest: changeset_digest_hex,
                            cas_hash: persisted_cas_hash,
                            work_id: request.work_id,
                            event_id,
                        },
                    ));
                }
                warn!(error = %e, "ChangeSetPublished event emission failed");
                return Ok(PrivilegedResponse::error(
                    PrivilegedErrorCode::CapabilityRequestRejected,
                    format!("changeset published event emission failed: {e}"),
                ));
            },
        };

        info!(
            event_id = %signed_event.event_id,
            work_id = %request.work_id,
            changeset_digest = %changeset_digest_hex,
            cas_hash = %cas_hash_hex,
            "ChangeSetPublished: bundle stored in CAS and event emitted to ledger"
        );

        Ok(PrivilegedResponse::PublishChangeSet(
            PublishChangeSetResponse {
                changeset_digest: changeset_digest_hex,
                cas_hash: cas_hash_hex,
                work_id: request.work_id,
                event_id: signed_event.event_id,
            },
        ))
    }

    /// Returns persisted replay bindings for a semantically matching
    /// `changeset_published` event.
    ///
    /// Matching key: `(work_id, changeset_digest)`. Response values are the
    /// authoritative persisted `event_id` and `cas_hash`.
    fn find_changeset_published_replay(
        &self,
        work_id: &str,
        changeset_digest_hex: &str,
    ) -> Option<(String, String)> {
        let event = self
            .event_emitter
            .get_event_by_changeset_identity(work_id, changeset_digest_hex)?;
        let payload = serde_json::from_slice::<serde_json::Value>(&event.payload).ok()?;
        let persisted_cas_hash = payload.get("cas_hash")?.as_str()?.to_string();
        Some((event.event_id, persisted_cas_hash))
    }

    // =========================================================================
    // TCK-00340: DelegateSublease Handler
    // =========================================================================

    /// Handles `DelegateSublease` requests (IPC-PRIV-072, TCK-00340).
    ///
    /// Delegates a sublease from a parent gate lease to a child holon,
    /// validating strict-subset constraints before signing the sublease.
    ///
    /// # Security Invariants
    ///
    /// - **Admission before mutation**: Parent lease existence and validity are
    ///   checked before any sublease issuance.
    /// - **Fail-closed**: Any validation failure rejects the request.
    /// - **Gate-scope enforcement**: Sublease inherits the parent's `gate_id`.
    /// - **Strict-subset**: Sublease constraints cannot exceed parent bounds.
    fn handle_delegate_sublease(
        &self,
        payload: &[u8],
        ctx: &ConnectionContext,
    ) -> ProtocolResult<PrivilegedResponse> {
        let request = DelegateSubleaseRequest::decode_bounded(payload, &self.decode_config)
            .map_err(|e| ProtocolError::Serialization {
                reason: format!("invalid DelegateSubleaseRequest: {e}"),
            })?;

        info!(
            parent_lease_id = %request.parent_lease_id,
            delegatee_actor_id = %request.delegatee_actor_id,
            sublease_id = %request.sublease_id,
            requested_expiry_ns = request.requested_expiry_ns,
            "DelegateSublease request received"
        );

        // ---- Phase 0a: Caller identity extraction (fail-closed) ----
        //
        // SECURITY: Caller identity is derived from peer credentials and bound
        // into the PCAC effect-intent digest.
        let Some(peer_creds) = ctx.peer_credentials() else {
            return Ok(PrivilegedResponse::error(
                PrivilegedErrorCode::PermissionDenied,
                "peer credentials required for sublease delegation",
            ));
        };
        let caller_actor_id = derive_actor_id(peer_creds);

        // ---- Phase 0b: Validate required fields (admission checks) ----

        if request.parent_lease_id.is_empty() {
            return Ok(PrivilegedResponse::error(
                PrivilegedErrorCode::CapabilityRequestRejected,
                "parent_lease_id is required",
            ));
        }
        if request.parent_lease_id.len() > MAX_ID_LENGTH {
            return Ok(PrivilegedResponse::error(
                PrivilegedErrorCode::CapabilityRequestRejected,
                format!("parent_lease_id exceeds maximum length of {MAX_ID_LENGTH} bytes"),
            ));
        }

        if request.delegatee_actor_id.is_empty() {
            return Ok(PrivilegedResponse::error(
                PrivilegedErrorCode::CapabilityRequestRejected,
                "delegatee_actor_id is required",
            ));
        }
        if request.delegatee_actor_id.len() > MAX_ID_LENGTH {
            return Ok(PrivilegedResponse::error(
                PrivilegedErrorCode::CapabilityRequestRejected,
                format!("delegatee_actor_id exceeds maximum length of {MAX_ID_LENGTH} bytes"),
            ));
        }

        // REQ-0010: Identity-bearing authoritative requests MUST carry
        // proof-carrying pointers.
        //
        // SECURITY (TCK-00356 Fix 1): Validate the identity proof hash using
        // the centralized validator which enforces non-zero commitment and
        // correct length. Phase 1 (pre-CAS transport) validates the hash as
        // a binding commitment; full proof dereference requires CAS
        // integration (TCK-00359).
        if let Err(e) = crate::identity::validate_identity_proof_hash(&request.identity_proof_hash)
        {
            return Ok(PrivilegedResponse::error(
                PrivilegedErrorCode::CapabilityRequestRejected,
                format!("identity_proof_hash validation failed: {e}"),
            ));
        }
        let request_identity_proof_hash: [u8; 32] = request
            .identity_proof_hash
            .as_slice()
            .try_into()
            .expect("validated to be 32 bytes by validate_identity_proof_hash above");

        // WVR-0103: Log once that identity proof hash is validated as
        // shape-only commitment (Phase 1 / pre-CAS transport).
        {
            static SUBLEASE_PROOF_WAIVER_WARN: std::sync::Once = std::sync::Once::new();
            SUBLEASE_PROOF_WAIVER_WARN.call_once(|| {
                warn!(
                    waiver = "WVR-0103",
                    "identity proof hash validated as shape-only commitment; \
                     full CAS dereference + IdentityProofV1::verify() deferred (WVR-0103)"
                );
            });
        }

        if request.sublease_id.is_empty() {
            return Ok(PrivilegedResponse::error(
                PrivilegedErrorCode::CapabilityRequestRejected,
                "sublease_id is required",
            ));
        }
        if request.sublease_id.len() > MAX_ID_LENGTH {
            return Ok(PrivilegedResponse::error(
                PrivilegedErrorCode::CapabilityRequestRejected,
                format!("sublease_id exceeds maximum length of {MAX_ID_LENGTH} bytes"),
            ));
        }

        if request.requested_expiry_ns == 0 {
            return Ok(PrivilegedResponse::error(
                PrivilegedErrorCode::CapabilityRequestRejected,
                "requested_expiry_ns must be non-zero",
            ));
        }

        // ---- Phase 1: Gate orchestrator availability ----
        let Some(gate_orchestrator) = &self.gate_orchestrator else {
            return Ok(PrivilegedResponse::error(
                PrivilegedErrorCode::CapabilityRequestRejected,
                "gate orchestrator not configured; \
                 deny_trace={\"reason_code\":\"missing_gate_orchestrator\",\"path\":\"delegate_sublease\"}",
            ));
        };

        // ---- Phase 2: Retrieve parent lease (admission check) ----
        //
        // SECURITY (v7 Finding 1 — Trust Model for Persisted Leases):
        //
        // The parent lease is deserialized from the daemon-owned SQLite ledger.
        // We do NOT call `GateLease::validate_signature()` here because the
        // ledger is a **same-process trusted store**: only the daemon writes
        // to it (via authenticated IPC handlers that enforce admission checks
        // before persisting). The SQLite file is not externally writable.
        //
        // The GateLease's `issuer_signature` (Ed25519 over canonical bytes)
        // IS preserved in the serialized `full_lease` field and can be
        // verified by downstream consumers that need cryptographic proof of
        // issuance (e.g., cross-node replication). Within the daemon process,
        // the ledger's integrity is guaranteed by the process trust boundary.
        //
        // See `SqliteLeaseValidator::register_full_lease()` in ledger.rs for
        // the full trust model documentation (v6 Finding 2).
        //
        // TODO(RFC-0019): When cross-node lease federation is implemented,
        // add signature verification here for leases received from external
        // nodes (where the process trust boundary does not apply).
        let Some(parent_lease) = self
            .lease_validator
            .get_gate_lease(&request.parent_lease_id)
        else {
            return Ok(PrivilegedResponse::error(
                PrivilegedErrorCode::GateLeaseMissing,
                format!("parent gate lease not found: {}", request.parent_lease_id),
            ));
        };

        // ---- Phase 2a: Caller authorization against parent lease ----
        //
        // SECURITY: The caller must be the executor authorized by the parent
        // lease, OR the issuer of the parent lease. Only the lease holder
        // (executor) or the lease issuer should be able to delegate subleases.
        // This prevents confused-deputy / capability laundering attacks where
        // an unauthorized caller delegates from another entity's lease.
        //
        // NOTE: This is a pre-PCAC admission gate — PCAC binds the
        // caller_actor_id into the effect_intent_digest for integrity, but
        // cannot enforce caller-to-lease ownership semantics. Authorization
        // MUST be checked before the PCAC lifecycle is invoked.
        if parent_lease.executor_actor_id != caller_actor_id
            && parent_lease.issuer_actor_id != caller_actor_id
        {
            warn!(
                parent_lease_id = %request.parent_lease_id,
                caller = %caller_actor_id,
                executor = %parent_lease.executor_actor_id,
                issuer = %parent_lease.issuer_actor_id,
                "Caller not authorized to delegate from parent lease"
            );
            return Ok(PrivilegedResponse::error(
                PrivilegedErrorCode::PermissionDenied,
                format!(
                    "caller is not authorized to delegate from lease '{}'",
                    request.parent_lease_id
                ),
            ));
        }

        // Resolve parent risk tier for delegated PCAC join input construction.
        let (parent_risk_tier, _) =
            self.resolve_risk_tier_for_lease(&request.parent_lease_id, parent_lease.policy_hash);

        // ---- Phase 2b: Parent lease HTF authority validation ----
        //
        // Authoritative sublease delegation is denied unless the parent lease
        // has a resolvable CAS-hosted time envelope with an admissible pinned
        // clock profile for the lease's risk tier.
        //
        // NOTE: This is a pre-PCAC admission gate. PCAC revalidation checks
        // freshness/expiry but does not verify CAS envelope admissibility
        // per risk tier.
        if let Err(e) = self.validate_lease_time_authority(&parent_lease, parent_risk_tier) {
            warn!(
                parent_lease_id = %request.parent_lease_id,
                risk_tier = ?parent_risk_tier,
                error = %e,
                "Parent lease HTF authority validation failed"
            );
            return Ok(PrivilegedResponse::error(
                PrivilegedErrorCode::CapabilityRequestRejected,
                format!("parent lease HTF authority validation failed: {e}"),
            ));
        }

        // ---- Phase 2c: Sublease ID uniqueness check (admission before mutation) ----
        //
        // SECURITY (v10 — Defense-in-Depth Uniqueness):
        //
        // This application-level check provides the idempotent fast-path for
        // duplicate sublease requests. However, since `dispatch()` takes
        // `&self` (not `&mut self`), concurrent dispatch of two requests
        // with the same `sublease_id` could theoretically race past this
        // check. Two database-level constraints enforce authoritative
        // uniqueness (see `SqliteLedgerEventEmitter::init_schema_for_test` /
        // `init_schema_with_signing_key`):
        //
        // 1. `idx_unique_full_lease_id`: Partial unique index on `json_extract(payload,
        //    '$.lease_id')` for `gate_lease_issued` events with `full_lease`. This
        //    prevents duplicate lease persistence via `register_full_lease`.
        //
        // 2. `idx_unique_sublease_issued`: Partial unique index on `(event_type,
        //    work_id)` for `SubleaseIssued` events. For these events, `work_id` =
        //    `sublease_id`, so the index enforces at-most-once event emission per
        //    sublease.
        //
        // If a duplicate races past the application check, either
        // `register_full_lease` or `emit_session_event` will fail with a
        // UNIQUE constraint violation, and the handler will fail-closed or
        // resolve via the idempotent duplicate path.
        let mut existing_sublease_matches_request = false;
        if let Some(existing) = self.lease_validator.get_gate_lease(&request.sublease_id) {
            // Idempotent return: if the existing sublease has identical
            // parameters, return it. Otherwise reject as a conflict.
            // Compare executor_actor_id of existing sublease against the
            // requested delegatee — these are intentionally different field
            // names (the sublease's executor IS the delegatee).
            //
            // SECURITY (v6 Finding 3): The idempotent equality check MUST
            // cover the FULL request tuple, not just (work_id, gate_id,
            // delegatee). Omitting parent_lease_id or requested_expiry_ns
            // would allow a request with a different parent lineage or
            // different expiry to silently receive the cached sublease.
            //
            // Since GateLease does not store parent_lease_id directly, we
            // verify parent lineage by comparing the sublease's inherited
            // changeset_digest and policy_hash against the current parent
            // lease's values. A different parent_lease_id will have different
            // policy/changeset hashes (or fail the v5 revalidation below).
            //
            // For requested_expiry_ns, convert to ms and compare against
            // the existing sublease's expires_at (which is stored in ms).
            let delegatee_matches = existing.executor_actor_id == request.delegatee_actor_id;
            let expiry_ms = request.requested_expiry_ns / 1_000_000;
            let expiry_matches = existing.expires_at == expiry_ms;
            let lineage_matches =
                bool::from(
                    existing
                        .changeset_digest
                        .ct_eq(&parent_lease.changeset_digest),
                ) && bool::from(existing.policy_hash.ct_eq(&parent_lease.policy_hash));
            if existing.work_id == parent_lease.work_id
                && existing.gate_id == parent_lease.gate_id
                && delegatee_matches
                && expiry_matches
                && lineage_matches
            {
                // SECURITY (v5 Finding 1): Revalidate the existing sublease
                // against the PROVIDED parent lease. A stale sublease issued
                // under a previous parent boundary must not be returned if it
                // violates the current parent's constraints (time bounds,
                // policy hash, changeset digest, AAT extension).
                if let Err(e) = crate::gate::GateOrchestrator::validate_sublease_delegation(
                    &parent_lease,
                    &existing,
                ) {
                    warn!(
                        sublease_id = %request.sublease_id,
                        parent_lease_id = %request.parent_lease_id,
                        error = %e,
                        "Idempotent sublease failed revalidation against current parent lease"
                    );
                    return Ok(PrivilegedResponse::error(
                        PrivilegedErrorCode::CapabilityRequestRejected,
                        format!(
                            "existing sublease '{}' is incompatible with parent lease '{}': {e}",
                            request.sublease_id, request.parent_lease_id
                        ),
                    ));
                }

                // Look up the original SubleaseIssued event from the
                // ledger. The emit_session_event call stored the event with
                // work_id = sublease.lease_id, so we query by that key and
                // filter for event_type == "SubleaseIssued".
                //
                // SECURITY (v7 Finding 1 — Fail-Closed Idempotent Replay):
                //
                // The response contract defines event_id as the ledger event
                // identity for SubleaseIssued. If the original event cannot be
                // found, we MUST fail-closed rather than returning an empty
                // event_id. An empty event_id would violate the response
                // contract and prevent callers from verifying the ledger trail
                // of the sublease. This can happen if the ledger was truncated
                // or if there's a storage inconsistency.
                let Some(original_event) = self
                    .event_emitter
                    .get_events_by_work_id(&existing.lease_id)
                    .into_iter()
                    .find(|e| e.event_type == "SubleaseIssued")
                else {
                    warn!(
                        sublease_id = %request.sublease_id,
                        "Idempotent sublease replay failed: original SubleaseIssued event not found in ledger"
                    );
                    return Ok(PrivilegedResponse::error(
                        PrivilegedErrorCode::CapabilityRequestRejected,
                        format!(
                            "sublease '{}' exists but its original SubleaseIssued ledger event \
                             is missing — cannot verify ledger trail",
                            request.sublease_id
                        ),
                    ));
                };

                // SECURITY (v12): Idempotent replay must verify both lineage
                // (`parent_lease_id`) and proof pointer (`identity_proof_hash`)
                // extracted from the persisted `SubleaseIssued` payload.
                let (original_parent_id, original_identity_proof_hash) =
                    match extract_sublease_replay_bindings(&original_event.payload) {
                        Ok(values) => values,
                        Err(e) => {
                            warn!(
                                sublease_id = %request.sublease_id,
                                error = %e,
                                "Idempotent sublease replay rejected: cannot extract replay bindings"
                            );
                            return Ok(PrivilegedResponse::error(
                                PrivilegedErrorCode::CapabilityRequestRejected,
                                format!(
                                    "sublease '{}' exists but replay bindings could not be extracted \
                                     from its event payload: {e}",
                                    request.sublease_id
                                ),
                            ));
                        },
                    };

                if original_parent_id != request.parent_lease_id {
                    warn!(
                        sublease_id = %request.sublease_id,
                        original_parent = %original_parent_id,
                        requested_parent = %request.parent_lease_id,
                        "Idempotent sublease replay rejected: parent_lease_id mismatch"
                    );
                    return Ok(PrivilegedResponse::error(
                        PrivilegedErrorCode::CapabilityRequestRejected,
                        format!(
                            "sublease '{}' was originally delegated from parent '{}', \
                             not '{}'",
                            request.sublease_id, original_parent_id, request.parent_lease_id
                        ),
                    ));
                }

                if !bool::from(original_identity_proof_hash.ct_eq(&request_identity_proof_hash)) {
                    warn!(
                        sublease_id = %request.sublease_id,
                        original_identity_proof_hash = %hex::encode(original_identity_proof_hash),
                        requested_identity_proof_hash = %hex::encode(request_identity_proof_hash),
                        "Idempotent sublease replay rejected: identity_proof_hash mismatch"
                    );
                    return Ok(PrivilegedResponse::error(
                        PrivilegedErrorCode::CapabilityRequestRejected,
                        format!(
                            "sublease '{}' was originally delegated with identity_proof_hash '{}', \
                             not '{}'",
                            request.sublease_id,
                            hex::encode(original_identity_proof_hash),
                            hex::encode(request_identity_proof_hash)
                        ),
                    ));
                }

                info!(
                    sublease_id = %request.sublease_id,
                    event_id = %original_event.event_id,
                    "Sublease already exists with identical parameters; continuing through \
                     mandatory PCAC consume and persistence replay checks"
                );
                existing_sublease_matches_request = true;
            } else {
                warn!(
                    sublease_id = %request.sublease_id,
                    "Sublease ID conflict: existing sublease has different parameters"
                );
                return Ok(PrivilegedResponse::error(
                    PrivilegedErrorCode::CapabilityRequestRejected,
                    format!(
                        "sublease_id '{}' already exists with different parameters",
                        request.sublease_id
                    ),
                ));
            }
        }
        // Also check if a lease (non-sublease) with this ID already exists
        // via the executor lookup (covers leases registered through
        // register_lease_with_executor).
        if !existing_sublease_matches_request
            && self
                .lease_validator
                .get_lease_executor_actor_id(&request.sublease_id)
                .is_some()
        {
            return Ok(PrivilegedResponse::error(
                PrivilegedErrorCode::CapabilityRequestRejected,
                format!(
                    "sublease_id '{}' conflicts with an existing lease",
                    request.sublease_id
                ),
            ));
        }

        // ---- Phase 2d: Deterministic delegation depth + budget checks ----
        let (parent_depth, depth_eval_ticks) = match compute_sublease_parent_depth_and_ticks(
            self.event_emitter.as_ref(),
            self.lease_validator.as_ref(),
            &request.parent_lease_id,
        ) {
            Ok(values) => values,
            Err(error) => {
                let deny_class = AuthorityDenyClass::InvalidDelegationChain;
                return Ok(PrivilegedResponse::error(
                    PrivilegedErrorCode::CapabilityRequestRejected,
                    format!("sublease delegation denied: {deny_class} ({error})"),
                ));
            },
        };

        let Some(requested_depth) = parent_depth.checked_add(1) else {
            let deny_class = AuthorityDenyClass::InvalidDelegationChain;
            return Ok(PrivilegedResponse::error(
                PrivilegedErrorCode::CapabilityRequestRejected,
                format!(
                    "sublease delegation denied: {deny_class} \
                     (delegation depth overflow from parent_depth={parent_depth})"
                ),
            ));
        };

        if requested_depth > MAX_SUBLEASE_DELEGATION_DEPTH {
            let deny_class = AuthorityDenyClass::InvalidDelegationChain;
            return Ok(PrivilegedResponse::error(
                PrivilegedErrorCode::CapabilityRequestRejected,
                format!(
                    "sublease delegation denied: {deny_class} \
                     (requested_depth={requested_depth}, max_depth={MAX_SUBLEASE_DELEGATION_DEPTH})"
                ),
            ));
        }

        if depth_eval_ticks > SUBLEASE_DELEGATION_SATISFIABILITY_BUDGET_TICKS {
            let deny_class = AuthorityDenyClass::InvalidDelegationChain;
            return Ok(PrivilegedResponse::error(
                PrivilegedErrorCode::CapabilityRequestRejected,
                format!(
                    "sublease delegation denied: {deny_class} \
                     (delegation_satisfiability_budget_ticks exhausted: ticks_used={depth_eval_ticks}, \
                     budget_ticks={SUBLEASE_DELEGATION_SATISFIABILITY_BUDGET_TICKS})"
                ),
            ));
        }

        let delegation_satisfiability_artifact =
            match build_delegate_sublease_satisfiability_artifact(
                &request.parent_lease_id,
                &request.sublease_id,
                &parent_lease.changeset_digest,
                &parent_lease.policy_hash,
                requested_depth,
                depth_eval_ticks,
            ) {
                Ok(artifact) => artifact,
                Err(error) => {
                    let deny_class = AuthorityDenyClass::InvalidDelegationChain;
                    return Ok(PrivilegedResponse::error(
                        PrivilegedErrorCode::CapabilityRequestRejected,
                        format!("sublease delegation denied: {deny_class} ({error})"),
                    ));
                },
            };

        let delegation_satisfiability_artifact_digest =
            match canonical_json_bytes(&delegation_satisfiability_artifact) {
                Ok(bytes) => hash_bytes(&bytes),
                Err(error) => {
                    let deny_class = AuthorityDenyClass::InvalidDelegationChain;
                    return Ok(PrivilegedResponse::error(
                        PrivilegedErrorCode::CapabilityRequestRejected,
                        format!("sublease delegation denied: {deny_class} ({error})"),
                    ));
                },
            };

        let delegation_satisfiability_artifact_cas_hash =
            match persist_delegate_sublease_satisfiability_artifact(
                self.cas.as_deref(),
                &delegation_satisfiability_artifact,
            ) {
                Ok(hash) => hash,
                Err(error) => {
                    let deny_class = AuthorityDenyClass::InvalidDelegationChain;
                    return Ok(PrivilegedResponse::error(
                        PrivilegedErrorCode::CapabilityRequestRejected,
                        format!("sublease delegation denied: {deny_class} ({error})"),
                    ));
                },
            };

        // ---- Phase 2e: Delegation narrowing and lineage binding checks ----
        // `requested_expiry_ns` must be carried into both PCAC scope witnesses
        // and delegated sublease issuance.
        let expiry_millis = request.requested_expiry_ns / 1_000_000;
        if expiry_millis >= parent_lease.expires_at {
            let deny_class = AuthorityDenyClass::InvalidDelegationChain;
            return Ok(PrivilegedResponse::error(
                PrivilegedErrorCode::CapabilityRequestRejected,
                format!(
                    "sublease delegation denied: {deny_class} \
                     (strict expiry narrowing violated: requested_expiry_ms={expiry_millis}, \
                     parent_expires_at_ms={})",
                    parent_lease.expires_at
                ),
            ));
        }

        // Decode parent time envelope hash for delegated lineage witnesses.
        let parent_time_envelope_hash =
            match decode_hash32_hex("time_envelope_ref", &parent_lease.time_envelope_ref) {
                Ok(hash) => hash,
                Err(error) => {
                    let deny_class = AuthorityDenyClass::InvalidDelegationChain;
                    return Ok(PrivilegedResponse::error(
                        PrivilegedErrorCode::CapabilityRequestRejected,
                        format!("sublease delegation denied: {deny_class} ({error})"),
                    ));
                },
            };

        // ---- Phase 2d: Lineage binding prerequisites ----
        //
        // SECURITY: Delegation from a parent with missing/zero lineage bindings
        // is denied fail-closed. Zero changeset_digest or policy_hash indicates
        // an uninitialized or corrupted parent lease that must not propagate
        // authority.
        if bool::from(parent_lease.changeset_digest.ct_eq(&[0u8; 32]))
            || bool::from(parent_lease.policy_hash.ct_eq(&[0u8; 32]))
        {
            let deny_class = AuthorityDenyClass::InvalidDelegationChain;
            return Ok(PrivilegedResponse::error(
                PrivilegedErrorCode::CapabilityRequestRejected,
                format!(
                    "sublease delegation denied: {deny_class} \
                     (parent lease lineage bindings are missing)"
                ),
            ));
        }

        let Some(pcac_gate) = self.pcac_lifecycle_gate.as_deref() else {
            return Ok(PrivilegedResponse::error(
                PrivilegedErrorCode::CapabilityRequestRejected,
                "PCAC authority gate not wired for DelegateSublease (fail-closed)",
            ));
        };

        let (join_freshness_tick, join_time_envelope_ref, join_ledger_anchor, join_revocation_head) =
            match self.derive_privileged_pcac_revalidation_inputs(&request.parent_lease_id) {
                Ok(values) => values,
                Err(error) => {
                    return Ok(PrivilegedResponse::error(
                        PrivilegedErrorCode::CapabilityRequestRejected,
                        format!(
                            "PCAC authority denied for DelegateSublease: \
                         authoritative revalidation unavailable: {error}"
                        ),
                    ));
                },
            };

        let pcac_risk_tier = Self::map_fac_risk_tier_to_pcac(parent_risk_tier);
        let pcac_builder =
            PrivilegedPcacInputBuilder::new(PrivilegedHandlerClass::DelegateSublease)
                .session_id(request.sublease_id.clone())
                .lease_id(request.parent_lease_id.clone())
                .boundary_intent_class(apm2_core::pcac::BoundaryIntentClass::Delegate)
                .identity_proof_hash(request_identity_proof_hash)
                .identity_evidence_level(IdentityEvidenceLevel::PointerOnly)
                .risk_tier(pcac_risk_tier);

        let lineage_receipt_hash = pcac_builder.hash(
            "lineage",
            &[
                request.parent_lease_id.as_bytes(),
                parent_lease.work_id.as_bytes(),
                parent_lease.gate_id.as_bytes(),
                &parent_lease.changeset_digest,
                &parent_lease.policy_hash,
                &parent_time_envelope_hash,
            ],
        );

        let capability_manifest_hash = pcac_builder.hash(
            "capability",
            &[
                parent_lease.work_id.as_bytes(),
                parent_lease.gate_id.as_bytes(),
                &parent_lease.policy_hash,
                &parent_lease.changeset_digest,
                &parent_time_envelope_hash,
            ],
        );

        let scope_witness_hash = pcac_builder.hash(
            "scope",
            &[
                request.parent_lease_id.as_bytes(),
                request.sublease_id.as_bytes(),
                request.delegatee_actor_id.as_bytes(),
                &expiry_millis.to_le_bytes(),
                parent_lease.work_id.as_bytes(),
                parent_lease.gate_id.as_bytes(),
            ],
        );

        let freshness_policy_hash = pcac_builder.hash(
            "freshness-policy",
            &[&parent_lease.policy_hash, parent_lease.gate_id.as_bytes()],
        );

        let stop_budget_profile_digest = pcac_builder.hash(
            "stop-budget",
            &[
                &[u8::from(self.stop_authority.as_ref().is_some_and(
                    |authority| authority.emergency_stop_active(),
                ))],
                &[u8::from(self.stop_authority.as_ref().is_some_and(
                    |authority| authority.governance_stop_active(),
                ))],
                parent_lease.gate_id.as_bytes(),
                request.delegatee_actor_id.as_bytes(),
            ],
        );

        let effect_intent_digest = pcac_builder.hash(
            "intent",
            &[
                request.parent_lease_id.as_bytes(),
                request.sublease_id.as_bytes(),
                request.delegatee_actor_id.as_bytes(),
                caller_actor_id.as_bytes(),
                &request_identity_proof_hash,
                &expiry_millis.to_le_bytes(),
                &lineage_receipt_hash,
            ],
        );

        let pcac_builder = pcac_builder
            .lineage_receipt_hash(lineage_receipt_hash)
            .capability_manifest_hash(capability_manifest_hash)
            .scope_witness_hash(scope_witness_hash)
            .freshness_policy_hash(freshness_policy_hash)
            .stop_budget_profile_digest(stop_budget_profile_digest)
            .effect_intent_digest(effect_intent_digest);

        let pcac_input = pcac_builder.build(
            join_freshness_tick,
            join_time_envelope_ref,
            join_ledger_anchor,
            join_revocation_head,
        );

        let pcac_lifecycle_artifacts = match self.enforce_privileged_pcac_lifecycle(
            PrivilegedHandlerClass::DelegateSublease.operation_name(),
            pcac_gate,
            &pcac_input,
            &request.parent_lease_id,
            join_freshness_tick,
            join_time_envelope_ref,
            join_ledger_anchor,
            join_revocation_head,
            effect_intent_digest,
        ) {
            Ok(artifacts) => {
                if artifacts.is_none() {
                    return Ok(PrivilegedResponse::error(
                        PrivilegedErrorCode::CapabilityRequestRejected,
                        "DelegateSublease requires PCAC lifecycle evidence (mandatory cutover); \
                         lifecycle_enforcement is disabled in claim policy",
                    ));
                }
                artifacts
            },
            Err(response) => return Ok(response),
        };

        // ---- Phase 3: Get HTF timestamp ----
        let timestamp_ns = match self.get_htf_timestamp_ns() {
            Ok(ts) => ts,
            Err(e) => {
                warn!(error = %e, "HTF timestamp generation failed - failing closed");
                return Ok(PrivilegedResponse::error(
                    PrivilegedErrorCode::CapabilityRequestRejected,
                    format!("HTF timestamp error: {e}"),
                ));
            },
        };

        // ---- Phase 4: Issue sublease via orchestrator (validates strict-subset) ----
        //
        // SECURITY (v5 Finding 2): Convert nanosecond values from the proto
        // wire format to milliseconds at the handler boundary. GateLease uses
        // milliseconds internally (issued_at, expires_at), but the proto
        // fields are `requested_expiry_ns` / `expires_at_ns`. Passing raw
        // nanosecond values through would cause temporal comparison mismatches
        // against parent lease bounds (which are in milliseconds).
        let issued_at_millis = timestamp_ns / 1_000_000;
        let parent_issued_at_ms = parent_lease.issued_at;
        if expiry_millis <= parent_issued_at_ms {
            let deny_class = AuthorityDenyClass::InvalidDelegationChain;
            return Ok(PrivilegedResponse::error(
                PrivilegedErrorCode::CapabilityRequestRejected,
                format!(
                    "sublease delegation denied: {deny_class} \
                     (vacuous delegation window: requested_expiry_ms={expiry_millis}, \
                     parent_issued_at_ms={parent_issued_at_ms})"
                ),
            ));
        }
        let sublease = match gate_orchestrator.issue_delegated_sublease(
            &parent_lease,
            &request.sublease_id,
            &request.delegatee_actor_id,
            issued_at_millis,
            expiry_millis,
        ) {
            Ok(lease) => lease,
            Err(e) => {
                warn!(
                    parent_lease_id = %request.parent_lease_id,
                    error = %e,
                    "Sublease delegation rejected"
                );
                return Ok(PrivilegedResponse::error(
                    PrivilegedErrorCode::CapabilityRequestRejected,
                    format!("sublease delegation failed: {e}"),
                ));
            },
        };

        // Lineage continuity: delegated lease MUST preserve authoritative
        // parent bindings.
        let lineage_hashes_match =
            bool::from(
                sublease
                    .changeset_digest
                    .ct_eq(&parent_lease.changeset_digest),
            ) && bool::from(sublease.policy_hash.ct_eq(&parent_lease.policy_hash));

        if sublease.work_id != parent_lease.work_id
            || sublease.gate_id != parent_lease.gate_id
            || !lineage_hashes_match
            || sublease.time_envelope_ref != parent_lease.time_envelope_ref
        {
            let deny_class = AuthorityDenyClass::InvalidDelegationChain;
            return Ok(PrivilegedResponse::error(
                PrivilegedErrorCode::CapabilityRequestRejected,
                format!(
                    "sublease delegation denied: {deny_class} \
                     (issued sublease does not preserve parent lineage bindings)"
                ),
            ));
        }

        // ---- Phase 5: Build staged delegation ledger events in memory ----
        //
        // PRE-COMMITMENT: Build every delegation event payload first. Any
        // failure here discards the staged vector and performs zero writes.
        //
        // SECURITY (v5 Finding 3): The event actor MUST be the authenticated
        // caller (peer credentials), not caller-supplied delegatee identity.
        //
        // SECURITY (TCK-00356 Fix 1): identity_proof_hash is included in the
        // signed payload so proof-carrying bindings are audit-stable.
        let mut event_payload = serde_json::json!({
            "parent_lease_id": request.parent_lease_id,
            "sublease_id": sublease.lease_id,
            "delegator_actor_id": caller_actor_id,
            "delegatee_actor_id": request.delegatee_actor_id,
            "gate_id": sublease.gate_id,
            "work_id": sublease.work_id,
            "expires_at": sublease.expires_at,
            "issued_at": sublease.issued_at,
            "identity_proof_hash": hex::encode(&request.identity_proof_hash),
            "delegation_satisfiability_artifact": delegation_satisfiability_artifact,
            "delegation_satisfiability_artifact_digest": hex::encode(
                delegation_satisfiability_artifact_digest
            ),
        });
        if let (Some(cas_hash), Some(payload_object)) = (
            delegation_satisfiability_artifact_cas_hash,
            event_payload.as_object_mut(),
        ) {
            payload_object.insert(
                "delegation_satisfiability_artifact_cas_hash".to_string(),
                serde_json::Value::String(hex::encode(cas_hash)),
            );
        }
        if let (Some(artifacts), Some(payload_object)) = (
            pcac_lifecycle_artifacts.as_ref(),
            event_payload.as_object_mut(),
        ) {
            append_privileged_pcac_lifecycle_fields(payload_object, artifacts);
        }

        let event_payload_bytes = match serde_json::to_vec(&event_payload) {
            Ok(bytes) => bytes,
            Err(e) => {
                warn!(
                    sublease_id = %sublease.lease_id,
                    error = %e,
                    "SubleaseIssued event payload serialization failed before commit"
                );
                return Ok(PrivilegedResponse::error(
                    PrivilegedErrorCode::CapabilityRequestRejected,
                    format!(
                        "sublease '{}' event payload serialization failed: {e} \
                         -- no ledger writes committed",
                        sublease.lease_id
                    ),
                ));
            },
        };

        let staged_ledger_events = vec![
            DelegationCommitEvent::PersistDelegatedLease,
            DelegationCommitEvent::EmitSubleaseIssued(event_payload_bytes),
        ];

        // ---- Phase 6: Flush staged events (append-only, no delete rollback) ----
        let mut signed_event: Option<SignedLedgerEvent> = None;
        for staged_event in staged_ledger_events {
            match staged_event {
                DelegationCommitEvent::PersistDelegatedLease => {
                    // Persist delegated parent lineage metadata so depth/satisfiability
                    // checks fail closed if SubleaseIssued evidence is unavailable.
                    if let Err(e) = self
                        .lease_validator
                        .register_delegated_full_lease(&sublease, &request.parent_lease_id)
                    {
                        // Check if this is a duplicate: the lease_id already exists.
                        // If it does, treat as idempotent if the existing lease matches
                        // ALL fields including parent lineage. Otherwise reject as conflict.
                        if let Some(existing) =
                            self.lease_validator.get_gate_lease(&sublease.lease_id)
                        {
                            // SECURITY (v11 BLOCKER 2 -- Full-Field Duplicate Validation):
                            //
                            // The duplicate resolution after register_full_lease failure
                            // MUST verify the SAME fields as the Phase 2b pre-check path:
                            // work_id, gate_id, executor_actor_id, expires_at, AND parent
                            // lineage (changeset_digest, policy_hash, parent_lease_id).
                            //
                            // Without parent lineage verification, a concurrent request
                            // with a different parent_lease_id could silently receive an
                            // idempotent response for a sublease delegated from a different
                            // parent, enabling confused-deputy / capability laundering.
                            let fields_match = existing.work_id == sublease.work_id
                                && existing.gate_id == sublease.gate_id
                                && existing.executor_actor_id == sublease.executor_actor_id
                                && existing.expires_at == sublease.expires_at
                                && bool::from(
                                    existing.changeset_digest.ct_eq(&sublease.changeset_digest),
                                )
                                && bool::from(existing.policy_hash.ct_eq(&sublease.policy_hash))
                                && existing.issuer_actor_id == sublease.issuer_actor_id;

                            if !fields_match {
                                warn!(
                                    sublease_id = %sublease.lease_id,
                                    error = %e,
                                    "Lease persistence duplicate with field mismatch -- conflict"
                                );
                                return Ok(PrivilegedResponse::error(
                                    PrivilegedErrorCode::CapabilityRequestRejected,
                                    format!(
                                        "sublease_id '{}' already exists with different parameters",
                                        sublease.lease_id
                                    ),
                                ));
                            }

                            // SECURITY (v11 BLOCKER 2 -- Parent Lineage Binding):
                            //
                            // Look up the original SubleaseIssued event and verify its
                            // parent_lease_id matches the current request. This mirrors
                            // the Phase 2b lineage verification logic: even if
                            // changeset_digest/policy_hash match, two different parent
                            // leases could share those inherited cryptographic fields.
                            let Some(original_event) = self
                                .event_emitter
                                .get_events_by_work_id(&existing.lease_id)
                                .into_iter()
                                .find(|ev| ev.event_type == "SubleaseIssued")
                            else {
                                // SECURITY (v11 MAJOR 1 -- Fail-Closed Event Lookup):
                                //
                                // If the original SubleaseIssued event cannot be found,
                                // we MUST fail-closed. Returning an empty event_id would
                                // violate the response contract and prevent callers from
                                // verifying the ledger trail.
                                warn!(
                                    sublease_id = %sublease.lease_id,
                                    "Concurrent duplicate sublease: original SubleaseIssued \
                                     event not found in ledger -- failing closed"
                                );
                                return Ok(PrivilegedResponse::error(
                                    PrivilegedErrorCode::CapabilityRequestRejected,
                                    format!(
                                        "sublease '{}' exists but its original SubleaseIssued \
                                         ledger event is missing -- cannot verify ledger trail",
                                        sublease.lease_id
                                    ),
                                ));
                            };

                            // SECURITY (v12): Re-check persisted replay bindings
                            // (`parent_lease_id` + `identity_proof_hash`) for the
                            // duplicate detected at persistence time.
                            let (original_parent_id, original_identity_proof_hash) =
                                match extract_sublease_replay_bindings(&original_event.payload) {
                                    Ok(values) => values,
                                    Err(extract_err) => {
                                        warn!(
                                            sublease_id = %sublease.lease_id,
                                            error = %extract_err,
                                            "Concurrent duplicate sublease: cannot extract replay bindings -- failing closed"
                                        );
                                        return Ok(PrivilegedResponse::error(
                                            PrivilegedErrorCode::CapabilityRequestRejected,
                                            format!(
                                                "sublease '{}' exists but replay bindings could not be \
                                                 extracted from its event payload: {extract_err}",
                                                sublease.lease_id
                                            ),
                                        ));
                                    },
                                };

                            if original_parent_id != request.parent_lease_id {
                                warn!(
                                    sublease_id = %sublease.lease_id,
                                    original_parent = %original_parent_id,
                                    requested_parent = %request.parent_lease_id,
                                    "Concurrent duplicate sublease: parent_lease_id mismatch"
                                );
                                return Ok(PrivilegedResponse::error(
                                    PrivilegedErrorCode::CapabilityRequestRejected,
                                    format!(
                                        "sublease '{}' was originally delegated from parent \
                                         '{}', not '{}'",
                                        sublease.lease_id,
                                        original_parent_id,
                                        request.parent_lease_id
                                    ),
                                ));
                            }

                            if !bool::from(
                                original_identity_proof_hash.ct_eq(&request_identity_proof_hash),
                            ) {
                                warn!(
                                    sublease_id = %sublease.lease_id,
                                    original_identity_proof_hash = %hex::encode(original_identity_proof_hash),
                                    requested_identity_proof_hash = %hex::encode(request_identity_proof_hash),
                                    "Concurrent duplicate sublease: identity_proof_hash mismatch"
                                );
                                return Ok(PrivilegedResponse::error(
                                    PrivilegedErrorCode::CapabilityRequestRejected,
                                    format!(
                                        "sublease '{}' was originally delegated with identity_proof_hash \
                                         '{}', not '{}'",
                                        sublease.lease_id,
                                        hex::encode(original_identity_proof_hash),
                                        hex::encode(request_identity_proof_hash)
                                    ),
                                ));
                            }

                            info!(
                                sublease_id = %sublease.lease_id,
                                event_id = %original_event.event_id,
                                "Concurrent duplicate sublease persisted -- idempotent return"
                            );
                            return Ok(PrivilegedResponse::DelegateSublease(
                                DelegateSubleaseResponse {
                                    sublease_id: existing.lease_id,
                                    parent_lease_id: request.parent_lease_id,
                                    delegatee_actor_id: request.delegatee_actor_id,
                                    gate_id: existing.gate_id,
                                    expires_at_ns: existing.expires_at.saturating_mul(1_000_000),
                                    event_id: original_event.event_id,
                                },
                            ));
                        }
                        warn!(
                            sublease_id = %sublease.lease_id,
                            error = %e,
                            "Lease persistence failed -- failing closed without emitting event"
                        );
                        return Ok(PrivilegedResponse::error(
                            PrivilegedErrorCode::CapabilityRequestRejected,
                            format!(
                                "sublease '{}' lease persistence failed: {e} \
                                 -- no event emitted (fail-closed)",
                                sublease.lease_id
                            ),
                        ));
                    }
                },
                DelegationCommitEvent::EmitSubleaseIssued(payload) => {
                    let emitted_event = match self.event_emitter.emit_session_event(
                        &sublease.lease_id,
                        "SubleaseIssued",
                        &payload,
                        &caller_actor_id,
                        timestamp_ns,
                    ) {
                        Ok(evt) => evt,
                        Err(e) => {
                            warn!(
                                sublease_id = %sublease.lease_id,
                                error = %e,
                                "SubleaseIssued emission failed after delegated lease append"
                            );
                            return Ok(PrivilegedResponse::error(
                                PrivilegedErrorCode::CapabilityRequestRejected,
                                format!(
                                    "sublease '{}' SubleaseIssued emission failed: {e} \
                                     -- delegated lease remains append-only and lineage checks fail closed",
                                    sublease.lease_id
                                ),
                            ));
                        },
                    };
                    signed_event = Some(emitted_event);
                },
            }
        }
        let Some(signed_event) = signed_event else {
            warn!(
                sublease_id = %sublease.lease_id,
                "staged delegation commit completed without SubleaseIssued append"
            );
            return Ok(PrivilegedResponse::error(
                PrivilegedErrorCode::CapabilityRequestRejected,
                format!(
                    "sublease '{}' delegation commit incomplete: missing SubleaseIssued append",
                    sublease.lease_id
                ),
            ));
        };

        info!(
            parent_lease_id = %request.parent_lease_id,
            sublease_id = %sublease.lease_id,
            gate_id = %sublease.gate_id,
            event_id = %signed_event.event_id,
            "Sublease delegation completed successfully"
        );

        // Convert ms -> ns for the proto response wire format.
        Ok(PrivilegedResponse::DelegateSublease(
            DelegateSubleaseResponse {
                sublease_id: sublease.lease_id,
                parent_lease_id: request.parent_lease_id,
                delegatee_actor_id: request.delegatee_actor_id,
                gate_id: sublease.gate_id,
                expires_at_ns: sublease.expires_at.saturating_mul(1_000_000),
                event_id: signed_event.event_id,
            },
        ))
    }

    #[allow(clippy::result_large_err)] // Error variant is PrivilegedResponse, matching dispatch pattern
    fn require_credential_store(&self) -> Result<&CredentialStore, PrivilegedResponse> {
        self.credential_store.as_deref().ok_or_else(|| {
            PrivilegedResponse::error(
                PrivilegedErrorCode::CredentialInvalidConfig,
                "credential store not configured on daemon",
            )
        })
    }

    /// Handles `ListCredentials` requests (IPC-PRIV-021).
    ///
    /// Lists all credential profiles. Secrets are never included in responses.
    fn handle_list_credentials(
        &self,
        payload: &[u8],
        _ctx: &ConnectionContext,
    ) -> ProtocolResult<PrivilegedResponse> {
        let request = ListCredentialsRequest::decode_bounded(payload, &self.decode_config)
            .map_err(|e| ProtocolError::Serialization {
                reason: format!("invalid ListCredentialsRequest: {e}"),
            })?;

        debug!("ListCredentials handler invoked");

        let store = match self.require_credential_store() {
            Ok(s) => s,
            Err(resp) => return Ok(resp),
        };

        let profile_ids = match store.list() {
            Ok(ids) => ids,
            Err(e) => {
                warn!(error = %e, "failed to list credential profiles");
                return Ok(PrivilegedResponse::error(
                    PrivilegedErrorCode::CredentialInvalidConfig,
                    format!("failed to list credentials: {e}"),
                ));
            },
        };

        let mut profiles = Vec::new();
        for pid in &profile_ids {
            match store.get(pid) {
                Ok(profile) => {
                    // Apply provider filter if specified
                    if let Some(filter) = request.provider_filter {
                        let profile_provider = Self::core_provider_to_proto(profile.provider);
                        if profile_provider != filter {
                            continue;
                        }
                    }
                    let display_name = profile.label.clone().unwrap_or_default();
                    profiles.push(Self::core_profile_to_proto(&profile, &display_name));
                },
                Err(e) => {
                    debug!(
                        profile_id = %pid,
                        error = %e,
                        "skipping profile that could not be loaded"
                    );
                },
            }
        }

        let total_count: u32 = profiles.len().try_into().unwrap_or(u32::MAX);

        Ok(PrivilegedResponse::ListCredentials(
            ListCredentialsResponse {
                profiles,
                total_count,
            },
        ))
    }

    /// Handles `AddCredential` requests (IPC-PRIV-022).
    ///
    /// Adds a new credential profile. The secret is stored securely and never
    /// logged.
    fn handle_add_credential(
        &self,
        payload: &[u8],
        _ctx: &ConnectionContext,
    ) -> ProtocolResult<PrivilegedResponse> {
        let request =
            AddCredentialRequest::decode_bounded(payload, &self.decode_config).map_err(|e| {
                ProtocolError::Serialization {
                    reason: format!("invalid AddCredentialRequest: {e}"),
                }
            })?;

        // Validate profile_id length (MAX_ID_LENGTH)
        if request.profile_id.len() > MAX_ID_LENGTH {
            return Ok(PrivilegedResponse::error(
                PrivilegedErrorCode::CredentialInvalidConfig,
                format!(
                    "profile_id too long: {} bytes (max {})",
                    request.profile_id.len(),
                    MAX_ID_LENGTH
                ),
            ));
        }

        // Security: Never log credential_secret
        debug!(
            profile_id = %request.profile_id,
            provider = request.provider,
            auth_method = request.auth_method,
            "AddCredential handler invoked"
        );

        let store = match self.require_credential_store() {
            Ok(s) => s,
            Err(resp) => return Ok(resp),
        };

        // Convert protobuf types to core types
        let provider = Self::proto_provider_to_core(request.provider);
        let Ok(auth) = Self::proto_auth_to_core(request.auth_method, &request.credential_secret)
        else {
            return Ok(PrivilegedResponse::error(
                PrivilegedErrorCode::CredentialInvalidConfig,
                "credential_secret is not valid UTF-8".to_string(),
            ));
        };
        let profile_id = ProfileId::new(&request.profile_id);

        // Build the core credential profile
        let mut core_profile = CoreCredentialProfile::new(profile_id, provider, auth);
        if !request.display_name.is_empty() {
            core_profile = core_profile.with_label(&request.display_name);
        }

        // Store the credential
        if let Err(e) = store.store(core_profile.clone()) {
            warn!(
                profile_id = %request.profile_id,
                error = %e,
                "failed to store credential"
            );
            return Ok(PrivilegedResponse::error(
                PrivilegedErrorCode::CredentialInvalidConfig,
                format!("failed to store credential: {e}"),
            ));
        }

        info!(
            profile_id = %request.profile_id,
            "credential profile stored successfully"
        );

        let proto_profile = Self::core_profile_to_proto(&core_profile, &request.display_name);

        Ok(PrivilegedResponse::AddCredential(AddCredentialResponse {
            profile: Some(proto_profile),
        }))
    }

    /// Handles `RemoveCredential` requests (IPC-PRIV-023).
    ///
    /// Removes a credential profile from storage.
    fn handle_remove_credential(
        &self,
        payload: &[u8],
        _ctx: &ConnectionContext,
    ) -> ProtocolResult<PrivilegedResponse> {
        let request = RemoveCredentialRequest::decode_bounded(payload, &self.decode_config)
            .map_err(|e| ProtocolError::Serialization {
                reason: format!("invalid RemoveCredentialRequest: {e}"),
            })?;

        debug!(
            profile_id = %request.profile_id,
            "RemoveCredential handler invoked"
        );

        let store = match self.require_credential_store() {
            Ok(s) => s,
            Err(resp) => return Ok(resp),
        };

        let profile_id = ProfileId::new(&request.profile_id);

        // Check if profile exists before removal
        let exists = match store.exists(&profile_id) {
            Ok(e) => e,
            Err(e) => {
                warn!(
                    profile_id = %request.profile_id,
                    error = %e,
                    "failed to check credential existence"
                );
                return Ok(PrivilegedResponse::RemoveCredential(
                    RemoveCredentialResponse { removed: false },
                ));
            },
        };

        if !exists {
            return Ok(PrivilegedResponse::RemoveCredential(
                RemoveCredentialResponse { removed: false },
            ));
        }

        match store.remove(&profile_id) {
            Ok(()) => {
                info!(
                    profile_id = %request.profile_id,
                    "credential profile removed successfully"
                );
                Ok(PrivilegedResponse::RemoveCredential(
                    RemoveCredentialResponse { removed: true },
                ))
            },
            Err(e) => {
                warn!(
                    profile_id = %request.profile_id,
                    error = %e,
                    "failed to remove credential"
                );
                Ok(PrivilegedResponse::RemoveCredential(
                    RemoveCredentialResponse { removed: false },
                ))
            },
        }
    }

    /// Handles `RefreshCredential` requests (IPC-PRIV-024).
    ///
    /// Refreshes an OAuth credential by requesting a new token.
    /// Note: Automated OAuth refresh requires an external token endpoint,
    /// which is out of scope for TCK-00343. This handler verifies the
    /// profile exists in the store and returns an appropriate error.
    fn handle_refresh_credential(
        &self,
        payload: &[u8],
        _ctx: &ConnectionContext,
    ) -> ProtocolResult<PrivilegedResponse> {
        let request = RefreshCredentialRequest::decode_bounded(payload, &self.decode_config)
            .map_err(|e| ProtocolError::Serialization {
                reason: format!("invalid RefreshCredentialRequest: {e}"),
            })?;

        debug!(
            profile_id = %request.profile_id,
            "RefreshCredential handler invoked"
        );

        let store = match self.require_credential_store() {
            Ok(s) => s,
            Err(resp) => return Ok(resp),
        };

        let profile_id = ProfileId::new(&request.profile_id);

        // Verify the profile exists before attempting refresh
        match store.get(&profile_id) {
            Ok(profile) => {
                // Check that the credential is an OAuth type (only OAuth supports
                // refresh)
                if !matches!(profile.auth, AuthMethod::OAuth { .. }) {
                    return Ok(PrivilegedResponse::error(
                        PrivilegedErrorCode::CredentialRefreshNotSupported,
                        format!(
                            "profile '{}' uses {} auth, which does not support refresh",
                            request.profile_id,
                            profile.auth.method_type()
                        ),
                    ));
                }

                // OAuth token refresh requires an external token endpoint, which
                // is out of scope for TCK-00343. Return an informative error.
                Ok(PrivilegedResponse::error(
                    PrivilegedErrorCode::CredentialRefreshNotSupported,
                    "OAuth token refresh requires an external token endpoint (not yet implemented)",
                ))
            },
            Err(e) => Ok(PrivilegedResponse::error(
                PrivilegedErrorCode::CredentialInvalidConfig,
                format!("credential profile not found: {e}"),
            )),
        }
    }

    /// Handles `SwitchCredential` requests (IPC-PRIV-025).
    ///
    /// Switches the active credential for a process. Validates that the
    /// target profile exists in the credential store before reporting
    /// success.
    fn handle_switch_credential(
        &self,
        payload: &[u8],
        _ctx: &ConnectionContext,
    ) -> ProtocolResult<PrivilegedResponse> {
        let request = SwitchCredentialRequest::decode_bounded(payload, &self.decode_config)
            .map_err(|e| ProtocolError::Serialization {
                reason: format!("invalid SwitchCredentialRequest: {e}"),
            })?;

        debug!(
            process_name = %request.process_name,
            profile_id = %request.profile_id,
            "SwitchCredential handler invoked"
        );

        let store = match self.require_credential_store() {
            Ok(s) => s,
            Err(resp) => return Ok(resp),
        };

        let profile_id = ProfileId::new(&request.profile_id);

        // Verify the target profile exists before switching
        match store.exists(&profile_id) {
            Ok(true) => {
                info!(
                    process_name = %request.process_name,
                    profile_id = %request.profile_id,
                    "credential switch validated"
                );

                // Note: Actual process credential binding is managed at the
                // process supervision layer. This handler validates that the
                // target profile exists. The previous_profile_id is empty
                // because per-process credential binding tracking is managed
                // by the supervisor (TCK-FUTURE).
                Ok(PrivilegedResponse::SwitchCredential(
                    SwitchCredentialResponse {
                        previous_profile_id: String::new(),
                        success: true,
                    },
                ))
            },
            Ok(false) => Ok(PrivilegedResponse::error(
                PrivilegedErrorCode::CredentialInvalidConfig,
                format!("credential profile not found: {}", request.profile_id),
            )),
            Err(e) => {
                warn!(
                    profile_id = %request.profile_id,
                    error = %e,
                    "failed to check credential existence"
                );
                Ok(PrivilegedResponse::SwitchCredential(
                    SwitchCredentialResponse {
                        previous_profile_id: String::new(),
                        success: false,
                    },
                ))
            },
        }
    }

    /// Handles `LoginCredential` requests (IPC-PRIV-026).
    ///
    /// Initiates an interactive login for a provider. For OAuth flows, this
    /// would generate an authorization URL. For API key flows, the key is
    /// provided directly in a subsequent `AddCredential` call.
    ///
    /// Note: Full interactive OAuth flow is out of scope for TCK-00343.
    /// This handler returns a stub response indicating the flow type.
    fn handle_login_credential(
        &self,
        payload: &[u8],
        _ctx: &ConnectionContext,
    ) -> ProtocolResult<PrivilegedResponse> {
        let request = LoginCredentialRequest::decode_bounded(payload, &self.decode_config)
            .map_err(|e| ProtocolError::Serialization {
                reason: format!("invalid LoginCredentialRequest: {e}"),
            })?;

        debug!(
            provider = request.provider,
            profile_id = ?request.profile_id,
            "LoginCredential handler invoked"
        );

        // LoginCredential is an interactive flow that requires browser-based
        // OAuth or terminal prompts. For TCK-00343, the credential store is
        // wired but interactive login flows remain as future work.
        let profile_id = request
            .profile_id
            .unwrap_or_else(|| format!("auto-{}", uuid::Uuid::new_v4()));

        let profile = super::messages::CredentialProfile {
            profile_id,
            provider: request.provider,
            auth_method: ProtoAuthMethod::Oauth as i32,
            created_at: std::time::SystemTime::now()
                .duration_since(std::time::UNIX_EPOCH)
                .map(|d| d.as_secs())
                .unwrap_or(0),
            expires_at: 0,
            is_active: false, // Not active until login completes
            display_name: request.display_name,
        };

        Ok(PrivilegedResponse::LoginCredential(
            LoginCredentialResponse {
                profile: Some(profile),
                login_url: String::new(), // Would contain OAuth URL for browser-based login
                completed: false,         // Login not yet complete
            },
        ))
    }

    // ========================================================================
    // Consensus Query Handlers (TCK-00345)
    // ========================================================================

    /// Maximum number of Byzantine evidence entries to return.
    /// Per `consensus.rs::limits::MAX_BYZANTINE_EVIDENCE_ENTRIES`.
    const MAX_BYZANTINE_EVIDENCE_ENTRIES: u32 = 1000;

    /// Handles `ConsensusStatus` requests (IPC-PRIV-011).
    ///
    /// # TCK-00345: Consensus Status Query
    ///
    /// Returns current consensus cluster status. If the consensus subsystem
    /// is not configured (single-node mode), returns `CONSENSUS_NOT_CONFIGURED`
    /// error instead of mock data.
    fn handle_consensus_status(&self, payload: &[u8]) -> ProtocolResult<PrivilegedResponse> {
        let request = ConsensusStatusRequest::decode_bounded(payload, &self.decode_config)
            .map_err(|e| ProtocolError::Serialization {
                reason: format!("invalid ConsensusStatusRequest: {e}"),
            })?;

        debug!(
            verbose = request.verbose,
            "ConsensusStatus request received"
        );

        // Check if consensus subsystem is configured
        // For now, return "not configured" since consensus state integration
        // requires additional daemon wiring (future work)
        if self.consensus_state.is_none() {
            return Ok(PrivilegedResponse::Error(PrivilegedError {
                code: ConsensusErrorCode::ConsensusNotConfigured.into(),
                message: "consensus subsystem is not configured".to_string(),
            }));
        }

        // TODO: Wire to actual consensus state when available
        // For now, return a response indicating the subsystem exists but has no data
        let response = ConsensusStatusResponse {
            node_id: self.node_id.clone(),
            epoch: 0,
            round: 0,
            leader_id: String::new(),
            is_leader: false,
            validator_count: 0,
            active_validators: 0,
            quorum_threshold: 0,
            quorum_met: false,
            health: "unknown".to_string(),
            high_qc_round: if request.verbose { Some(0) } else { None },
            locked_qc_round: None,
            committed_blocks: if request.verbose { Some(0) } else { None },
            last_committed_hash: None,
        };

        Ok(PrivilegedResponse::ConsensusStatus(response))
    }

    /// Handles `ConsensusValidators` requests (IPC-PRIV-012).
    ///
    /// # TCK-00345: Validator List Query
    ///
    /// Returns list of validators in the consensus cluster. If the consensus
    /// subsystem is not configured, returns `CONSENSUS_NOT_CONFIGURED` error.
    fn handle_consensus_validators(&self, payload: &[u8]) -> ProtocolResult<PrivilegedResponse> {
        let request = ConsensusValidatorsRequest::decode_bounded(payload, &self.decode_config)
            .map_err(|e| ProtocolError::Serialization {
                reason: format!("invalid ConsensusValidatorsRequest: {e}"),
            })?;

        debug!(
            active_only = request.active_only,
            "ConsensusValidators request received"
        );

        // Check if consensus subsystem is configured
        if self.consensus_state.is_none() {
            return Ok(PrivilegedResponse::Error(PrivilegedError {
                code: ConsensusErrorCode::ConsensusNotConfigured.into(),
                message: "consensus subsystem is not configured".to_string(),
            }));
        }

        // TODO: Wire to actual consensus state when available
        let response = ConsensusValidatorsResponse {
            validators: Vec::new(),
            total: 0,
            active: 0,
        };

        Ok(PrivilegedResponse::ConsensusValidators(response))
    }

    /// Handles `ConsensusByzantineEvidence` requests (IPC-PRIV-013).
    ///
    /// # TCK-00345: Byzantine Evidence Query
    ///
    /// Returns list of detected Byzantine fault evidence. If the consensus
    /// subsystem is not configured, returns `CONSENSUS_NOT_CONFIGURED` error.
    fn handle_consensus_byzantine_evidence(
        &self,
        payload: &[u8],
    ) -> ProtocolResult<PrivilegedResponse> {
        let request =
            ConsensusByzantineEvidenceRequest::decode_bounded(payload, &self.decode_config)
                .map_err(|e| ProtocolError::Serialization {
                    reason: format!("invalid ConsensusByzantineEvidenceRequest: {e}"),
                })?;

        // Cap limit to prevent DoS
        let effective_limit = request.limit.min(Self::MAX_BYZANTINE_EVIDENCE_ENTRIES);

        debug!(
            fault_type = ?request.fault_type,
            limit = effective_limit,
            "ConsensusByzantineEvidence request received"
        );

        // Check if consensus subsystem is configured
        if self.consensus_state.is_none() {
            return Ok(PrivilegedResponse::Error(PrivilegedError {
                code: ConsensusErrorCode::ConsensusNotConfigured.into(),
                message: "consensus subsystem is not configured".to_string(),
            }));
        }

        // TODO: Wire to actual consensus state when available
        let response = ConsensusByzantineEvidenceResponse {
            evidence: Vec::new(),
            total: 0,
        };

        Ok(PrivilegedResponse::ConsensusByzantineEvidence(response))
    }

    /// Handles `ConsensusMetrics` requests (IPC-PRIV-014).
    ///
    /// # TCK-00345: Consensus Metrics Query
    ///
    /// Returns consensus metrics summary. If the consensus subsystem
    /// is not configured, returns `CONSENSUS_NOT_CONFIGURED` error.
    fn handle_consensus_metrics(&self, payload: &[u8]) -> ProtocolResult<PrivilegedResponse> {
        let request = ConsensusMetricsRequest::decode_bounded(payload, &self.decode_config)
            .map_err(|e| ProtocolError::Serialization {
                reason: format!("invalid ConsensusMetricsRequest: {e}"),
            })?;

        debug!(
            period_secs = request.period_secs,
            "ConsensusMetrics request received"
        );

        // Check if consensus subsystem is configured
        if self.consensus_state.is_none() {
            return Ok(PrivilegedResponse::Error(PrivilegedError {
                code: ConsensusErrorCode::ConsensusNotConfigured.into(),
                message: "consensus subsystem is not configured".to_string(),
            }));
        }

        // TODO: Wire to actual consensus state when available
        let response = ConsensusMetricsResponse {
            node_id: self.node_id.clone(),
            proposals_committed: 0,
            proposals_rejected: 0,
            proposals_timeout: 0,
            leader_elections: 0,
            sync_events: 0,
            conflicts: 0,
            byzantine_evidence: 0,
            latency_p50_ms: 0.0,
            latency_p99_ms: 0.0,
        };

        Ok(PrivilegedResponse::ConsensusMetrics(response))
    }

    // ========================================================================
    // HEF Pulse Plane Handlers (TCK-00302)
    // ========================================================================

    /// Handles `SubscribePulse` requests from operator sockets (IPC-HEF-001).
    ///
    /// # TCK-00302: Operator Full Taxonomy Access
    ///
    /// Per DD-HEF-0004: "Operator connections may subscribe broadly."
    /// Operators can subscribe to any valid pattern including wildcards.
    ///
    /// This handler:
    /// 1. Validates request structure and field lengths
    /// 2. Validates topic patterns using `pulse_topic` grammar
    /// 3. Allows all valid patterns (no ACL restrictions for operators)
    /// 4. Returns accepted patterns and any rejected invalid patterns
    ///
    /// # Note: Subscription Registry
    ///
    /// Actual subscription registration and pulse delivery are handled by
    /// TCK-00303 (resource governance) and TCK-00304 (outbox + publisher).
    fn handle_subscribe_pulse(
        &self,
        payload: &[u8],
        ctx: &ConnectionContext,
    ) -> ProtocolResult<PrivilegedResponse> {
        // Max patterns per request per RFC-0018 (must be declared before statements)
        const MAX_PATTERNS_PER_REQUEST: usize = 16;

        let request =
            SubscribePulseRequest::decode_bounded(payload, &self.decode_config).map_err(|e| {
                ProtocolError::Serialization {
                    reason: format!("invalid SubscribePulseRequest: {e}"),
                }
            })?;

        info!(
            client_sub_id = %request.client_sub_id,
            pattern_count = request.topic_patterns.len(),
            since_cursor = request.since_ledger_cursor,
            peer_pid = ?ctx.peer_credentials().map(|c| c.pid),
            "SubscribePulse (operator) request received"
        );

        // Validate client_sub_id length
        if let Err(e) = validate_client_sub_id(&request.client_sub_id) {
            warn!(error = %e, "Invalid client_sub_id");
            return Ok(PrivilegedResponse::error(
                PrivilegedErrorCode::PermissionDenied,
                e.to_string(),
            ));
        }

        // Validate topic_patterns count
        if request.topic_patterns.len() > MAX_PATTERNS_PER_REQUEST {
            warn!(
                pattern_count = request.topic_patterns.len(),
                "Too many patterns in subscribe request"
            );
            return Ok(PrivilegedResponse::error(
                PrivilegedErrorCode::PermissionDenied,
                format!(
                    "too many patterns: {} exceeds maximum {}",
                    request.topic_patterns.len(),
                    MAX_PATTERNS_PER_REQUEST
                ),
            ));
        }

        // Create ACL evaluator for operator subscriptions
        // Per DD-HEF-0004: "Operator connections may subscribe broadly"
        let evaluator = PulseAclEvaluator::for_operator();

        // Evaluate each pattern
        let mut accepted_patterns = Vec::new();
        let mut rejected_patterns = Vec::new();

        for pattern in &request.topic_patterns {
            match evaluator.check_subscribe(pattern) {
                AclDecision::Allow => {
                    accepted_patterns.push(pattern.clone());
                },
                AclDecision::Deny(err) => {
                    let reason_code = Self::acl_error_to_reason_code(&err);
                    rejected_patterns.push(PatternRejection {
                        pattern: pattern.clone(),
                        reason_code,
                    });
                    debug!(
                        pattern = %pattern,
                        reason = %err,
                        "Pattern rejected (invalid syntax)"
                    );
                },
            }
        }

        // Generate subscription ID; use connection ID from context (TCK-00303)
        let subscription_id = format!("SUB-{}", uuid::Uuid::new_v4());
        // TCK-00303: Use connection_id from context for consistent tracking
        // across the connection lifecycle. The connection handler will call
        // unregister_connection with this ID when the connection closes.
        let connection_id = ctx.connection_id();

        // TCK-00303: Wire resource governance - register connection if not exists
        // and add subscription with limit checks
        if !accepted_patterns.is_empty() {
            // Parse accepted patterns into TopicPattern
            let mut parsed_patterns = Vec::new();
            for pattern_str in &accepted_patterns {
                match super::pulse_topic::TopicPattern::parse(pattern_str) {
                    Ok(pattern) => parsed_patterns.push(pattern),
                    Err(e) => {
                        // Should not happen since ACL already validated, but be defensive
                        warn!(
                            pattern = %pattern_str,
                            error = %e,
                            "Pattern parse failed after ACL validation"
                        );
                        rejected_patterns.push(PatternRejection {
                            pattern: pattern_str.clone(),
                            reason_code: "INVALID_PATTERN".to_string(),
                        });
                    },
                }
            }

            // Register connection if it doesn't exist (idempotent)
            if let Err(e) = self
                .subscription_registry
                .register_connection(connection_id)
            {
                // Only TooManyConnections is a real error; ignore if connection already exists
                if matches!(
                    e,
                    super::resource_governance::ResourceError::TooManyConnections { .. }
                ) {
                    warn!(
                        connection_id = %connection_id,
                        error = %e,
                        "Connection registration failed: resource limit exceeded"
                    );
                    return Ok(PrivilegedResponse::error(
                        PrivilegedErrorCode::PermissionDenied,
                        format!("resource limit exceeded: {e}"),
                    ));
                }
                // Connection already exists - this is fine
            }

            // Create subscription state and add to registry
            let subscription = SubscriptionState::new(
                &subscription_id,
                &request.client_sub_id,
                parsed_patterns,
                request.since_ledger_cursor,
            );

            if let Err(e) = self
                .subscription_registry
                .add_subscription(connection_id, subscription)
            {
                warn!(
                    connection_id = %connection_id,
                    subscription_id = %subscription_id,
                    error = %e,
                    "Subscription registration failed: resource limit exceeded"
                );
                return Ok(PrivilegedResponse::error(
                    PrivilegedErrorCode::PermissionDenied,
                    format!("resource limit exceeded: {e}"),
                ));
            }
        }

        // Log outcome
        if rejected_patterns.is_empty() {
            info!(
                subscription_id = %subscription_id,
                connection_id = %connection_id,
                accepted_count = accepted_patterns.len(),
                "All patterns accepted (operator)"
            );
        } else {
            warn!(
                subscription_id = %subscription_id,
                connection_id = %connection_id,
                accepted_count = accepted_patterns.len(),
                rejected_count = rejected_patterns.len(),
                "Some patterns rejected (operator)"
            );
        }

        Ok(PrivilegedResponse::SubscribePulse(SubscribePulseResponse {
            subscription_id,
            effective_since_cursor: request.since_ledger_cursor,
            accepted_patterns,
            rejected_patterns,
        }))
    }

    /// Converts an `AclError` to a reason code string for `PatternRejection`.
    fn acl_error_to_reason_code(err: &AclError) -> String {
        match err {
            AclError::TopicNotAllowed { .. } => "ACL_DENY".to_string(),
            AclError::WildcardNotAllowed { .. } => "WILDCARD_NOT_ALLOWED".to_string(),
            AclError::PublishNotAllowed => "PUBLISH_DENIED".to_string(),
            AclError::InvalidPattern { .. } | AclError::InvalidTopic { .. } => {
                "INVALID_PATTERN".to_string()
            },
            AclError::AllowlistTooLarge { .. } | AclError::SubscriptionIdTooLong { .. } => {
                "LIMIT_EXCEEDED".to_string()
            },
        }
    }

    /// Handles `UnsubscribePulse` requests from operator sockets (IPC-HEF-002).
    ///
    /// # TCK-00302: Unsubscribe Handling
    ///
    /// This handler validates the unsubscribe request and returns success.
    ///
    /// # Note: Subscription Registry
    ///
    /// Actual subscription removal is handled by TCK-00303 (resource
    /// governance).
    fn handle_unsubscribe_pulse(
        &self,
        payload: &[u8],
        ctx: &ConnectionContext,
    ) -> ProtocolResult<PrivilegedResponse> {
        let request = UnsubscribePulseRequest::decode_bounded(payload, &self.decode_config)
            .map_err(|e| ProtocolError::Serialization {
                reason: format!("invalid UnsubscribePulseRequest: {e}"),
            })?;

        info!(
            subscription_id = %request.subscription_id,
            peer_pid = ?ctx.peer_credentials().map(|c| c.pid),
            "UnsubscribePulse (operator) request received"
        );

        // Validate subscription_id length
        if let Err(e) = validate_subscription_id(&request.subscription_id) {
            warn!(error = %e, "Invalid subscription_id");
            return Ok(PrivilegedResponse::error(
                PrivilegedErrorCode::PermissionDenied,
                e.to_string(),
            ));
        }

        // TCK-00303: Wire resource governance - remove subscription from registry
        // Use connection_id from context for consistent tracking
        let connection_id = ctx.connection_id();

        let removed = match self
            .subscription_registry
            .remove_subscription(connection_id, &request.subscription_id)
        {
            Ok(_) => {
                info!(
                    subscription_id = %request.subscription_id,
                    connection_id = %connection_id,
                    "Unsubscribe (operator) processed successfully"
                );
                true
            },
            Err(e) => {
                // Log but don't fail - subscription may already be removed or never existed
                debug!(
                    subscription_id = %request.subscription_id,
                    connection_id = %connection_id,
                    error = %e,
                    "Unsubscribe (operator) - subscription not found (may already be removed)"
                );
                false
            },
        };

        Ok(PrivilegedResponse::UnsubscribePulse(
            UnsubscribePulseResponse { removed },
        ))
    }
}

// ============================================================================
// Request Encoding Helpers
// ============================================================================

/// Encodes a `ClaimWork` request to bytes for sending.
///
/// The format is: `[tag: u8][payload: protobuf]`
#[must_use]
pub fn encode_claim_work_request(request: &ClaimWorkRequest) -> Bytes {
    let mut buf = vec![PrivilegedMessageType::ClaimWork.tag()];
    request.encode(&mut buf).expect("encode cannot fail");
    Bytes::from(buf)
}

/// Encodes a `SpawnEpisode` request to bytes for sending.
#[must_use]
pub fn encode_spawn_episode_request(request: &SpawnEpisodeRequest) -> Bytes {
    let mut buf = vec![PrivilegedMessageType::SpawnEpisode.tag()];
    request.encode(&mut buf).expect("encode cannot fail");
    Bytes::from(buf)
}

/// Encodes an `IssueCapability` request to bytes for sending.
#[must_use]
pub fn encode_issue_capability_request(request: &IssueCapabilityRequest) -> Bytes {
    let mut buf = vec![PrivilegedMessageType::IssueCapability.tag()];
    request.encode(&mut buf).expect("encode cannot fail");
    Bytes::from(buf)
}

/// Encodes a `Shutdown` request to bytes for sending.
#[must_use]
pub fn encode_shutdown_request(request: &ShutdownRequest) -> Bytes {
    let mut buf = vec![PrivilegedMessageType::Shutdown.tag()];
    request.encode(&mut buf).expect("encode cannot fail");
    Bytes::from(buf)
}

/// Encodes an `UpdateStopFlags` request to bytes for sending (TCK-00351).
#[must_use]
pub fn encode_update_stop_flags_request(request: &UpdateStopFlagsRequest) -> Bytes {
    let mut buf = vec![PrivilegedMessageType::UpdateStopFlags.tag()];
    request.encode(&mut buf).expect("encode cannot fail");
    Bytes::from(buf)
}

/// Encodes an `EndSession` request to bytes for sending (TCK-00395).
#[must_use]
pub fn encode_end_session_request(request: &EndSessionRequest) -> Bytes {
    let mut buf = vec![PrivilegedMessageType::EndSession.tag()];
    request.encode(&mut buf).expect("encode cannot fail");
    Bytes::from(buf)
}

/// Encodes an `IngestReviewReceipt` request to bytes for sending (TCK-00389).
#[must_use]
pub fn encode_ingest_review_receipt_request(request: &IngestReviewReceiptRequest) -> Bytes {
    let mut buf = vec![PrivilegedMessageType::IngestReviewReceipt.tag()];
    request.encode(&mut buf).expect("encode cannot fail");
    Bytes::from(buf)
}

/// Encodes a `DelegateSublease` request to bytes for sending (TCK-00340).
#[must_use]
pub fn encode_delegate_sublease_request(request: &DelegateSubleaseRequest) -> Bytes {
    let mut buf = vec![PrivilegedMessageType::DelegateSublease.tag()];
    request.encode(&mut buf).expect("encode cannot fail");
    Bytes::from(buf)
}

/// Encodes a `VerifyLedgerChainRequest` into a wire frame.
#[must_use]
pub fn encode_verify_ledger_chain_request(request: &VerifyLedgerChainRequest) -> Bytes {
    let mut buf = vec![PrivilegedMessageType::VerifyLedgerChain.tag()];
    request.encode(&mut buf).expect("encode cannot fail");
    Bytes::from(buf)
}

// ============================================================================
// Process Management Request Encoding (TCK-00342)
// ============================================================================

/// Encodes a `ListProcesses` request to bytes for sending.
#[must_use]
pub fn encode_list_processes_request(request: &ListProcessesRequest) -> Bytes {
    let mut buf = vec![PrivilegedMessageType::ListProcesses.tag()];
    request.encode(&mut buf).expect("encode cannot fail");
    Bytes::from(buf)
}

/// Encodes a `ProcessStatus` request to bytes for sending.
#[must_use]
pub fn encode_process_status_request(request: &ProcessStatusRequest) -> Bytes {
    let mut buf = vec![PrivilegedMessageType::ProcessStatus.tag()];
    request.encode(&mut buf).expect("encode cannot fail");
    Bytes::from(buf)
}

/// Encodes a `StartProcess` request to bytes for sending.
#[must_use]
pub fn encode_start_process_request(request: &StartProcessRequest) -> Bytes {
    let mut buf = vec![PrivilegedMessageType::StartProcess.tag()];
    request.encode(&mut buf).expect("encode cannot fail");
    Bytes::from(buf)
}

/// Encodes a `StopProcess` request to bytes for sending.
#[must_use]
pub fn encode_stop_process_request(request: &StopProcessRequest) -> Bytes {
    let mut buf = vec![PrivilegedMessageType::StopProcess.tag()];
    request.encode(&mut buf).expect("encode cannot fail");
    Bytes::from(buf)
}

/// Encodes a `RestartProcess` request to bytes for sending.
#[must_use]
pub fn encode_restart_process_request(request: &RestartProcessRequest) -> Bytes {
    let mut buf = vec![PrivilegedMessageType::RestartProcess.tag()];
    request.encode(&mut buf).expect("encode cannot fail");
    Bytes::from(buf)
}

/// Encodes a `ReloadProcess` request to bytes for sending.
#[must_use]
pub fn encode_reload_process_request(request: &ReloadProcessRequest) -> Bytes {
    let mut buf = vec![PrivilegedMessageType::ReloadProcess.tag()];
    request.encode(&mut buf).expect("encode cannot fail");
    Bytes::from(buf)
}

// ============================================================================
// CTR-PROTO-011: Consensus Query Request Encoding (RFC-0014, TCK-00345)
// ============================================================================

/// Encodes a `ConsensusStatus` request to bytes for sending.
///
/// # Wire Format
/// ```text
/// +------+---------------------------+
/// | 0x0B | ConsensusStatusRequest    |
/// +------+---------------------------+
/// ```
#[must_use]
pub fn encode_consensus_status_request(request: &ConsensusStatusRequest) -> Bytes {
    let mut buf = vec![PrivilegedMessageType::ConsensusStatus.tag()];
    request.encode(&mut buf).expect("encode cannot fail");
    Bytes::from(buf)
}

/// Encodes a `ConsensusValidators` request to bytes for sending.
///
/// # Wire Format
/// ```text
/// +------+------------------------------+
/// | 0x0C | ConsensusValidatorsRequest   |
/// +------+------------------------------+
/// ```
#[must_use]
pub fn encode_consensus_validators_request(request: &ConsensusValidatorsRequest) -> Bytes {
    let mut buf = vec![PrivilegedMessageType::ConsensusValidators.tag()];
    request.encode(&mut buf).expect("encode cannot fail");
    Bytes::from(buf)
}

/// Encodes a `ConsensusByzantineEvidence` request to bytes for sending.
///
/// # Wire Format
/// ```text
/// +------+------------------------------------+
/// | 0x0D | ConsensusByzantineEvidenceRequest  |
/// +------+------------------------------------+
/// ```
#[must_use]
pub fn encode_consensus_byzantine_evidence_request(
    request: &ConsensusByzantineEvidenceRequest,
) -> Bytes {
    let mut buf = vec![PrivilegedMessageType::ConsensusByzantineEvidence.tag()];
    request.encode(&mut buf).expect("encode cannot fail");
    Bytes::from(buf)
}

/// Encodes a `ConsensusMetrics` request to bytes for sending.
///
/// # Wire Format
/// ```text
/// +------+---------------------------+
/// | 0x0E | ConsensusMetricsRequest   |
/// +------+---------------------------+
/// ```
#[must_use]
pub fn encode_consensus_metrics_request(request: &ConsensusMetricsRequest) -> Bytes {
    let mut buf = vec![PrivilegedMessageType::ConsensusMetrics.tag()];
    request.encode(&mut buf).expect("encode cannot fail");
    Bytes::from(buf)
}

/// Encodes a `WorkStatus` request to bytes for sending (TCK-00344).
#[must_use]
pub fn encode_work_status_request(request: &WorkStatusRequest) -> Bytes {
    let mut buf = vec![PrivilegedMessageType::WorkStatus.tag()];
    request.encode(&mut buf).expect("encode cannot fail");
    Bytes::from(buf)
}

/// Encodes a `WorkList` request to bytes for sending (TCK-00415).
#[must_use]
pub fn encode_work_list_request(request: &WorkListRequest) -> Bytes {
    let mut buf = vec![PrivilegedMessageType::WorkList.tag()];
    request.encode(&mut buf).expect("encode cannot fail");
    Bytes::from(buf)
}

/// Encodes an `AuditorLaunchProjection` request to bytes for sending
/// (TCK-00452).
#[must_use]
pub fn encode_auditor_launch_projection_request(request: &AuditorLaunchProjectionRequest) -> Bytes {
    let mut buf = vec![PrivilegedMessageType::AuditorLaunchProjection.tag()];
    request.encode(&mut buf).expect("encode cannot fail");
    Bytes::from(buf)
}

/// Encodes an `OrchestratorLaunchProjection` request to bytes for sending
/// (TCK-00452).
#[must_use]
pub fn encode_orchestrator_launch_projection_request(
    request: &OrchestratorLaunchProjectionRequest,
) -> Bytes {
    let mut buf = vec![PrivilegedMessageType::OrchestratorLaunchProjection.tag()];
    request.encode(&mut buf).expect("encode cannot fail");
    Bytes::from(buf)
}

// =============================================================================
// TCK-00394: ChangeSet Publishing Encoding (RFC-0018)
// =============================================================================

/// Encodes a `PublishChangeSet` request to bytes for sending (TCK-00394).
#[must_use]
pub fn encode_publish_changeset_request(request: &PublishChangeSetRequest) -> Bytes {
    let mut buf = vec![PrivilegedMessageType::PublishChangeSet.tag()];
    request.encode(&mut buf).expect("encode cannot fail");
    Bytes::from(buf)
}

// =============================================================================
// CTR-PROTO-012: Credential Management Encoding (RFC-0018, TCK-00343)
// =============================================================================

/// Encodes a `ListCredentials` request to bytes for sending.
#[must_use]
pub fn encode_list_credentials_request(request: &ListCredentialsRequest) -> Bytes {
    let mut buf = vec![PrivilegedMessageType::ListCredentials.tag()];
    request.encode(&mut buf).expect("encode cannot fail");
    Bytes::from(buf)
}

/// Encodes an `AddCredential` request to bytes for sending.
#[must_use]
pub fn encode_add_credential_request(request: &AddCredentialRequest) -> Bytes {
    let mut buf = vec![PrivilegedMessageType::AddCredential.tag()];
    request.encode(&mut buf).expect("encode cannot fail");
    Bytes::from(buf)
}

/// Encodes a `RemoveCredential` request to bytes for sending.
#[must_use]
pub fn encode_remove_credential_request(request: &RemoveCredentialRequest) -> Bytes {
    let mut buf = vec![PrivilegedMessageType::RemoveCredential.tag()];
    request.encode(&mut buf).expect("encode cannot fail");
    Bytes::from(buf)
}

/// Encodes a `RefreshCredential` request to bytes for sending.
#[must_use]
pub fn encode_refresh_credential_request(request: &RefreshCredentialRequest) -> Bytes {
    let mut buf = vec![PrivilegedMessageType::RefreshCredential.tag()];
    request.encode(&mut buf).expect("encode cannot fail");
    Bytes::from(buf)
}

/// Encodes a `SwitchCredential` request to bytes for sending.
#[must_use]
pub fn encode_switch_credential_request(request: &SwitchCredentialRequest) -> Bytes {
    let mut buf = vec![PrivilegedMessageType::SwitchCredential.tag()];
    request.encode(&mut buf).expect("encode cannot fail");
    Bytes::from(buf)
}

/// Encodes a `LoginCredential` request to bytes for sending.
#[must_use]
pub fn encode_login_credential_request(request: &LoginCredentialRequest) -> Bytes {
    let mut buf = vec![PrivilegedMessageType::LoginCredential.tag()];
    request.encode(&mut buf).expect("encode cannot fail");
    Bytes::from(buf)
}

#[cfg(test)]
mod tests {
    use super::*;

    /// TCK-00319: Helper function to get a test workspace root.
    /// Uses /tmp which exists on all Unix systems.
    fn test_workspace_root() -> String {
        "/tmp".to_string()
    }

    #[test]
    fn test_domain_tagged_hash_produces_expected_output() {
        let data = b"test-data";
        let expected = {
            let mut hasher = blake3::Hasher::new();
            hasher.update(b"pcac-privileged-ingest-review-capability-v1");
            hasher.update(data);
            *hasher.finalize().as_bytes()
        };
        let actual = domain_tagged_hash(
            PrivilegedHandlerClass::IngestReviewReceipt,
            "capability",
            &[data],
        );
        assert_eq!(expected, actual);
    }

    #[test]
    fn test_domain_tagged_hash_handler_class_separation() {
        let data = b"same-data";
        let ingest = domain_tagged_hash(
            PrivilegedHandlerClass::IngestReviewReceipt,
            "capability",
            &[data],
        );
        let delegate = domain_tagged_hash(
            PrivilegedHandlerClass::DelegateSublease,
            "capability",
            &[data],
        );
        assert_ne!(ingest, delegate);
    }

    #[test]
    fn test_pcac_input_builder_ingest_review() {
        let input = PrivilegedPcacInputBuilder::new(PrivilegedHandlerClass::IngestReviewReceipt)
            .session_id("receipt-123".to_string())
            .lease_id("lease-456".to_string())
            .boundary_intent_class(apm2_core::pcac::BoundaryIntentClass::Assert)
            .identity_proof_hash([0xAA; 32])
            .identity_evidence_level(IdentityEvidenceLevel::PointerOnly)
            .risk_tier(PcacRiskTier::Tier0)
            .capability_manifest_hash([0xBB; 32])
            .scope_witness_hash([0xCC; 32])
            .freshness_policy_hash([0xDD; 32])
            .stop_budget_profile_digest([0xEE; 32])
            .effect_intent_digest([0xFF; 32])
            .build(42, [0x11; 32], [0x22; 32], [0x33; 32]);

        assert_eq!(input.session_id, "receipt-123");
        assert_eq!(input.lease_id, "lease-456");
        assert_eq!(
            input.boundary_intent_class,
            apm2_core::pcac::BoundaryIntentClass::Assert
        );
        assert_eq!(input.capability_manifest_hash, [0xBB; 32]);
        assert_eq!(input.scope_witness_hashes, vec![[0xCC; 32]]);
        assert_eq!(input.intent_digest, [0xFF; 32]);
        assert_eq!(input.permeability_receipt_hash, None);
        assert_eq!(input.freshness_witness_tick, 42);
        assert_eq!(
            input.identity_evidence_level,
            IdentityEvidenceLevel::PointerOnly
        );
        assert!(input.pointer_only_waiver_hash.is_none());
        assert!(input.holon_id.is_none());
        assert_eq!(input.determinism_class, PcacDeterminismClass::Deterministic);
    }

    #[test]
    fn test_pcac_input_builder_delegate_sublease() {
        let lineage = [0x44; 32];
        let input = PrivilegedPcacInputBuilder::new(PrivilegedHandlerClass::DelegateSublease)
            .session_id("sub-789".to_string())
            .lease_id("parent-123".to_string())
            .boundary_intent_class(apm2_core::pcac::BoundaryIntentClass::Delegate)
            .identity_proof_hash([0xAA; 32])
            .identity_evidence_level(IdentityEvidenceLevel::PointerOnly)
            .risk_tier(PcacRiskTier::Tier2Plus)
            .lineage_receipt_hash(lineage)
            .capability_manifest_hash([0xBB; 32])
            .scope_witness_hash([0xCC; 32])
            .freshness_policy_hash([0xDD; 32])
            .stop_budget_profile_digest([0xEE; 32])
            .effect_intent_digest([0xFF; 32])
            .build(100, [0x11; 32], [0x22; 32], [0x33; 32]);

        assert_eq!(input.permeability_receipt_hash, Some(lineage));
        assert_eq!(
            input.boundary_intent_class,
            apm2_core::pcac::BoundaryIntentClass::Delegate
        );
        assert_eq!(input.risk_tier, PcacRiskTier::Tier2Plus);
    }

    mod channel_boundary_integration {
        use apm2_core::channel::{
            ChannelSource, ChannelViolationClass, decode_channel_context_token,
            validate_channel_boundary,
        };

        use super::*;

        #[test]
        fn test_daemon_classifies_tool_request_as_typed() {
            let dispatcher = PrivilegedDispatcher::new();
            let check =
                dispatcher.build_channel_boundary_check(&ToolClass::Read, true, true, true, true);

            assert_eq!(check.source, ChannelSource::TypedToolIntent);
            assert!(
                check.channel_source_witness.is_some(),
                "typed tool request must carry channel source witness"
            );

            let defects = validate_channel_boundary(&check);
            assert!(
                defects.is_empty(),
                "fully verified daemon-built check should pass boundary validation"
            );
        }

        #[test]
        fn test_daemon_denies_unverified_policy() {
            let dispatcher = PrivilegedDispatcher::new();
            let check =
                dispatcher.build_channel_boundary_check(&ToolClass::Read, false, true, true, true);

            let defects = validate_channel_boundary(&check);
            assert!(
                defects.iter().any(|defect| defect.violation_class
                    == ChannelViolationClass::PolicyNotLedgerVerified),
                "unverified policy must emit PolicyNotLedgerVerified defect"
            );
        }

        #[test]
        fn test_daemon_denies_unverified_broker() {
            let dispatcher = PrivilegedDispatcher::new();
            let check =
                dispatcher.build_channel_boundary_check(&ToolClass::Read, true, false, true, true);

            let defects = validate_channel_boundary(&check);
            assert!(
                defects
                    .iter()
                    .any(|defect| defect.violation_class
                        == ChannelViolationClass::BrokerBypassDetected),
                "unverified broker must emit BrokerBypassDetected defect"
            );
        }

        #[test]
        fn test_daemon_denies_unverified_capability() {
            let dispatcher = PrivilegedDispatcher::new();
            let check =
                dispatcher.build_channel_boundary_check(&ToolClass::Read, true, true, false, true);

            let defects = validate_channel_boundary(&check);
            assert!(
                defects
                    .iter()
                    .any(|defect| defect.violation_class
                        == ChannelViolationClass::CapabilityNotVerified),
                "unverified capability must emit CapabilityNotVerified defect"
            );
        }

        #[test]
        fn test_daemon_denies_unverified_context_firewall() {
            let dispatcher = PrivilegedDispatcher::new();
            let check =
                dispatcher.build_channel_boundary_check(&ToolClass::Read, true, true, true, false);

            let defects = validate_channel_boundary(&check);
            assert!(
                defects.iter().any(|defect| defect.violation_class
                    == ChannelViolationClass::ContextFirewallNotVerified),
                "unverified context firewall must emit ContextFirewallNotVerified defect"
            );
        }

        #[test]
        fn test_channel_context_token_roundtrip() {
            let dispatcher = PrivilegedDispatcher::new();
            // INV-BRK-HEALTH-GATE-001: Open the health gate for this test.
            dispatcher.set_admission_health_gate(true);
            let signer = apm2_core::crypto::Signer::generate();
            let issued_at_secs = std::time::UNIX_EPOCH
                .elapsed()
                .expect("current time should be after unix epoch")
                .as_secs();
            #[allow(deprecated)]
            let token = dispatcher
                .validate_channel_boundary_and_issue_context_token(
                    &signer,
                    "lease-1",
                    "REQ-1",
                    issued_at_secs,
                    &ToolClass::Execute,
                    true,
                    true,
                    true,
                    true,
                )
                .expect("validated boundary should issue token");

            let decoded = decode_channel_context_token(
                &token,
                &signer.verifying_key(),
                "lease-1",
                std::time::SystemTime::now()
                    .duration_since(std::time::UNIX_EPOCH)
                    .expect("system clock should be after unix epoch")
                    .as_secs(),
                "REQ-1",
            )
            .expect("token should decode");
            assert_eq!(decoded.source, ChannelSource::TypedToolIntent);
            assert!(decoded.broker_verified);
            assert!(decoded.capability_verified);
            assert!(decoded.context_firewall_verified);
            assert!(decoded.policy_ledger_verified);
        }

        /// INV-BRK-HEALTH-GATE-001: Token issuance MUST be denied when the
        /// admission health gate is closed (default state).
        #[test]
        fn test_health_gate_closed_denies_token_issuance() {
            let dispatcher = PrivilegedDispatcher::new();
            // Gate is closed by default (fail-closed).
            assert!(
                !dispatcher.admission_health_gate_passed(),
                "health gate must start closed"
            );

            let signer = apm2_core::crypto::Signer::generate();
            let issued_at_secs = std::time::UNIX_EPOCH
                .elapsed()
                .expect("current time should be after unix epoch")
                .as_secs();
            let result = dispatcher.validate_channel_boundary_and_issue_context_token_with_flow(
                &signer,
                "lease-health",
                "REQ-HEALTH",
                issued_at_secs,
                &ToolClass::Execute,
                true,
                true,
                true,
                true,
                BoundaryFlowRuntimeState::allow_all(true),
            );
            let defects = result.expect_err("closed health gate must deny token issuance");
            assert!(
                defects
                    .iter()
                    .any(|d| d.detail.contains("INV-BRK-HEALTH-GATE-001")),
                "denial must cite INV-BRK-HEALTH-GATE-001: {defects:?}"
            );
        }

        /// INV-BRK-HEALTH-GATE-001: Token issuance succeeds after the gate
        /// is opened.
        #[test]
        fn test_health_gate_open_allows_token_issuance() {
            let dispatcher = PrivilegedDispatcher::new();
            dispatcher.set_admission_health_gate(true);
            assert!(
                dispatcher.admission_health_gate_passed(),
                "health gate must be open after set_admission_health_gate(true)"
            );

            let signer = apm2_core::crypto::Signer::generate();
            let issued_at_secs = std::time::UNIX_EPOCH
                .elapsed()
                .expect("current time should be after unix epoch")
                .as_secs();
            let token = dispatcher
                .validate_channel_boundary_and_issue_context_token_with_flow(
                    &signer,
                    "lease-health-open",
                    "REQ-HEALTH-OPEN",
                    issued_at_secs,
                    &ToolClass::Execute,
                    true,
                    true,
                    true,
                    true,
                    BoundaryFlowRuntimeState::allow_all(true),
                )
                .expect("open health gate must allow token issuance");
            assert!(!token.is_empty(), "issued token must be non-empty");
        }

        /// INV-BRK-HEALTH-GATE-001: Gate can be re-closed after opening,
        /// denying subsequent token issuance.
        #[test]
        fn test_health_gate_reclose_denies_token_issuance() {
            let dispatcher = PrivilegedDispatcher::new();
            dispatcher.set_admission_health_gate(true);
            assert!(dispatcher.admission_health_gate_passed());

            // Re-close the gate.
            dispatcher.set_admission_health_gate(false);
            assert!(!dispatcher.admission_health_gate_passed());

            let signer = apm2_core::crypto::Signer::generate();
            let issued_at_secs = std::time::UNIX_EPOCH
                .elapsed()
                .expect("current time should be after unix epoch")
                .as_secs();
            let result = dispatcher.validate_channel_boundary_and_issue_context_token_with_flow(
                &signer,
                "lease-reclose",
                "REQ-RECLOSE",
                issued_at_secs,
                &ToolClass::Execute,
                true,
                true,
                true,
                true,
                BoundaryFlowRuntimeState::allow_all(true),
            );
            assert!(
                result.is_err(),
                "re-closed health gate must deny token issuance"
            );
        }
    }

    mod governance_probe_failure_classification {
        use super::*;
        use crate::episode::preactuation::StopAuthority;
        use crate::governance::GovernanceFreshnessConfig;

        #[test]
        fn spawn_missing_claim_does_not_record_governance_failure() {
            let authority = Arc::new(StopAuthority::new());
            let monitor = Arc::new(GovernanceFreshnessMonitor::new(
                Arc::clone(&authority),
                GovernanceFreshnessConfig::default(),
                false,
            ));
            monitor.record_success();

            let dispatcher =
                PrivilegedDispatcher::new().with_governance_freshness_monitor(Arc::clone(&monitor));
            let ctx = ConnectionContext::privileged_session_open(Some(PeerCredentials {
                uid: 1000,
                gid: 1000,
                pid: Some(12345),
            }));

            let request = SpawnEpisodeRequest {
                workspace_root: test_workspace_root(),
                work_id: "W-NO-CLAIM".to_string(),
                role: WorkRole::Implementer.into(),
                lease_id: Some("L-NO-CLAIM".to_string()),
                adapter_profile_hash: None,
                max_episodes: None,
                escalation_predicate: None,
                permeability_receipt_hash: None,
            };
            let frame = encode_spawn_episode_request(&request);

            let response = dispatcher.dispatch(&frame, &ctx).unwrap();
            match response {
                PrivilegedResponse::Error(err) => {
                    assert_eq!(
                        err.code,
                        PrivilegedErrorCode::PolicyResolutionMissing as i32
                    );
                },
                other => panic!("expected PolicyResolutionMissing error, got: {other:?}"),
            }

            assert!(
                !authority.governance_uncertain(),
                "local missing-claim path must not set governance uncertainty"
            );
        }

        #[test]
        fn issue_capability_missing_claim_does_not_record_governance_failure() {
            let authority = Arc::new(StopAuthority::new());
            let monitor = Arc::new(GovernanceFreshnessMonitor::new(
                Arc::clone(&authority),
                GovernanceFreshnessConfig::default(),
                false,
            ));
            monitor.record_success();

            let dispatcher =
                PrivilegedDispatcher::new().with_governance_freshness_monitor(Arc::clone(&monitor));
            let ctx = ConnectionContext::privileged_session_open(Some(PeerCredentials {
                uid: 1000,
                gid: 1000,
                pid: Some(12345),
            }));

            dispatcher
                .session_registry
                .register_session(crate::session::SessionState {
                    session_id: "S-NO-CLAIM".to_string(),
                    work_id: "W-NO-CLAIM".to_string(),
                    role: WorkRole::Implementer.into(),
                    lease_id: "L-NO-CLAIM".to_string(),
                    ephemeral_handle: "EH-NO-CLAIM".to_string(),
                    policy_resolved_ref: String::new(),
                    capability_manifest_hash: vec![],
                    episode_id: None,
                    pcac_policy: None,
                    pointer_only_waiver: None,
                })
                .expect("session registration should succeed");

            let request = IssueCapabilityRequest {
                session_id: "S-NO-CLAIM".to_string(),
                capability_request: Some(super::super::super::messages::CapabilityRequest {
                    tool_class: "read".to_string(),
                    read_patterns: vec!["**/*".to_string()],
                    write_patterns: vec![],
                    duration_secs: 60,
                }),
            };
            let frame = encode_issue_capability_request(&request);

            let response = dispatcher.dispatch(&frame, &ctx).unwrap();
            match response {
                PrivilegedResponse::Error(err) => {
                    assert_eq!(
                        err.code,
                        PrivilegedErrorCode::CapabilityRequestRejected as i32
                    );
                },
                other => panic!("expected CapabilityRequestRejected error, got: {other:?}"),
            }

            assert!(
                !authority.governance_uncertain(),
                "local missing-claim path must not set governance uncertainty"
            );
        }
    }

    // ========================================================================
    // INT-001: Privileged endpoint routing (TCK-00251)
    // Test name matches verification command: cargo test -p apm2-daemon
    // privileged_routing
    // ========================================================================
    mod privileged_routing {
        use super::*;
        use crate::episode::preactuation::StopAuthority;

        #[test]
        fn test_claim_work_routing() {
            let dispatcher = PrivilegedDispatcher::new();
            let ctx = ConnectionContext::privileged_session_open(Some(PeerCredentials {
                uid: 1000,
                gid: 1000,
                pid: Some(12345),
            }));

            let request = ClaimWorkRequest {
                actor_id: "test-actor".to_string(),
                role: WorkRole::Implementer.into(),
                credential_signature: vec![1, 2, 3],
                nonce: vec![4, 5, 6],
            };
            let frame = encode_claim_work_request(&request);

            let response = dispatcher.dispatch(&frame, &ctx).unwrap();
            assert!(matches!(response, PrivilegedResponse::ClaimWork(_)));
        }

        #[test]
        fn test_update_stop_flags_tag_and_routing() {
            let authority = Arc::new(StopAuthority::new());
            let dispatcher =
                PrivilegedDispatcher::new().with_stop_authority(Arc::clone(&authority));
            let ctx = ConnectionContext::privileged_session_open(Some(PeerCredentials {
                uid: 1000,
                gid: 1000,
                pid: Some(12345),
            }));

            assert_eq!(PrivilegedMessageType::UpdateStopFlags.tag(), 18);
            assert_eq!(
                PrivilegedMessageType::from_tag(18),
                Some(PrivilegedMessageType::UpdateStopFlags)
            );

            let request = UpdateStopFlagsRequest {
                emergency_stop_active: Some(true),
                governance_stop_active: None,
            };
            let frame = encode_update_stop_flags_request(&request);
            let response = dispatcher.dispatch(&frame, &ctx).unwrap();

            match response {
                PrivilegedResponse::UpdateStopFlags(resp) => {
                    assert!(resp.emergency_stop_active);
                    assert!(!resp.governance_stop_active);
                },
                other => panic!("expected UpdateStopFlags response, got {other:?}"),
            }
            assert!(authority.emergency_stop_active());

            let events = dispatcher
                .event_emitter()
                .get_events_by_work_id(STOP_FLAGS_MUTATED_WORK_ID);
            assert_eq!(events.len(), 1, "expected one stop-flags audit event");
            let event = &events[0];
            assert_eq!(event.event_type, "stop_flags_mutated");

            let payload: serde_json::Value =
                serde_json::from_slice(&event.payload).expect("payload should be valid JSON");
            assert_eq!(
                payload["actor_id"],
                derive_actor_id(&PeerCredentials {
                    uid: 1000,
                    gid: 1000,
                    pid: Some(12345),
                })
            );
            assert_eq!(payload["emergency_stop_previous"], false);
            assert_eq!(payload["emergency_stop_current"], true);
            assert_eq!(payload["governance_stop_previous"], false);
            assert_eq!(payload["governance_stop_current"], false);
            assert_eq!(
                payload["request_context"]["endpoint"], "UpdateStopFlags",
                "request context must include endpoint"
            );
            assert_eq!(
                payload["request_context"]["requested_updates"]["emergency_stop_active"],
                true
            );
        }

        #[test]
        fn test_verify_ledger_chain_tag_and_routing() {
            let dispatcher = PrivilegedDispatcher::new();
            let ctx = ConnectionContext::privileged_session_open(Some(PeerCredentials {
                uid: 1000,
                gid: 1000,
                pid: Some(12345),
            }));

            assert_eq!(PrivilegedMessageType::VerifyLedgerChain.tag(), 73);
            assert_eq!(
                PrivilegedMessageType::from_tag(73),
                Some(PrivilegedMessageType::VerifyLedgerChain)
            );

            let frame = encode_verify_ledger_chain_request(&VerifyLedgerChainRequest {});
            let response = dispatcher.dispatch(&frame, &ctx).unwrap();
            match response {
                PrivilegedResponse::VerifyLedgerChain(resp) => {
                    assert!(resp.verified, "empty stub ledger should verify");
                    assert_eq!(resp.rows_validated, 0);
                    assert!(!resp.message.is_empty(), "response should include details");
                },
                other => panic!("expected VerifyLedgerChain response, got: {other:?}"),
            }
        }

        #[test]
        fn test_spawn_episode_routing() {
            let dispatcher = PrivilegedDispatcher::new();
            let ctx = ConnectionContext::privileged_session_open(Some(PeerCredentials {
                uid: 1000,
                gid: 1000,
                pid: Some(12345),
            }));

            // TCK-00256: First claim work to establish policy resolution
            let claim_request = ClaimWorkRequest {
                actor_id: "test-actor".to_string(),
                role: WorkRole::Implementer.into(),
                credential_signature: vec![1, 2, 3],
                nonce: vec![4, 5, 6],
            };
            let claim_frame = encode_claim_work_request(&claim_request);
            let claim_response = dispatcher.dispatch(&claim_frame, &ctx).unwrap();

            let (work_id, lease_id) = match claim_response {
                PrivilegedResponse::ClaimWork(resp) => (resp.work_id, resp.lease_id),
                _ => panic!("Expected ClaimWork response"),
            };

            // Now spawn with the claimed work_id
            let request = SpawnEpisodeRequest {
                workspace_root: test_workspace_root(),
                work_id,
                role: WorkRole::Implementer.into(),
                lease_id: Some(lease_id),
                adapter_profile_hash: None,
                max_episodes: None,
                escalation_predicate: None,
                permeability_receipt_hash: None,
            };
            let frame = encode_spawn_episode_request(&request);

            let response = dispatcher.dispatch(&frame, &ctx).unwrap();
            assert!(matches!(response, PrivilegedResponse::SpawnEpisode(_)));
        }

        #[test]
        fn test_issue_capability_routing() {
            use crate::session::SessionState;

            let dispatcher = PrivilegedDispatcher::new();
            let ctx = ConnectionContext::privileged_session_open(Some(PeerCredentials {
                uid: 1000,
                gid: 1000,
                pid: Some(12345),
            }));

            // TCK-00289: Register a session and work claim for IssueCapability validation
            let work_id = "W-TEST-001";
            let lease_id = "L-TEST-001";
            let session_id = "S-001";

            // Register work claim
            let claim = WorkClaim {
                work_id: work_id.to_string(),
                lease_id: lease_id.to_string(),
                actor_id: "test-actor".to_string(),
                role: WorkRole::Implementer,
                policy_resolution: PolicyResolution {
                    policy_resolved_ref: format!("resolved-for-{work_id}"),
                    resolved_policy_hash: [0u8; 32],
                    capability_manifest_hash: [0u8; 32],
                    context_pack_hash: [0u8; 32],
                    role_spec_hash: [0u8; 32],
                    context_pack_recipe_hash: [0u8; 32],
                    resolved_risk_tier: 0,
                    resolved_scope_baseline: None,
                    expected_adapter_profile_hash: None,
                    pcac_policy: None,
                    pointer_only_waiver: None,
                },
                author_custody_domains: vec![],
                executor_custody_domains: vec![],
                permeability_receipt: None,
            };
            dispatcher.work_registry.register_claim(claim).unwrap();

            // Register session
            let session_state = SessionState {
                session_id: session_id.to_string(),
                work_id: work_id.to_string(),
                role: WorkRole::Implementer.into(),
                lease_id: lease_id.to_string(),
                ephemeral_handle: String::new(),
                policy_resolved_ref: String::new(),
                pcac_policy: None,
                pointer_only_waiver: None,
                capability_manifest_hash: vec![],
                episode_id: None,
            };
            dispatcher
                .session_registry
                .register_session(session_state)
                .unwrap();

            let request = IssueCapabilityRequest {
                session_id: session_id.to_string(),
                capability_request: Some(super::super::super::messages::CapabilityRequest {
                    tool_class: "file_read".to_string(),
                    read_patterns: vec!["**/*.rs".to_string()],
                    write_patterns: vec![],
                    duration_secs: 3600,
                }),
            };
            let frame = encode_issue_capability_request(&request);

            let response = dispatcher.dispatch(&frame, &ctx).unwrap();
            assert!(matches!(response, PrivilegedResponse::IssueCapability(_)));
        }

        #[test]
        fn test_shutdown_routing() {
            let dispatcher = PrivilegedDispatcher::new();
            let ctx = ConnectionContext::privileged_session_open(Some(PeerCredentials {
                uid: 1000,
                gid: 1000,
                pid: Some(12345),
            }));

            let request = ShutdownRequest {
                reason: Some("test".to_string()),
            };
            let frame = encode_shutdown_request(&request);

            let response = dispatcher.dispatch(&frame, &ctx).unwrap();
            assert!(matches!(response, PrivilegedResponse::Shutdown(_)));
        }

        #[test]
        fn test_session_socket_returns_permission_denied() {
            let dispatcher = PrivilegedDispatcher::new();
            let ctx = ConnectionContext::session_open(
                Some(PeerCredentials {
                    uid: 1000,
                    gid: 1000,
                    pid: Some(12346),
                }),
                Some("test-session".to_string()),
            );

            // All 4 endpoints should return PERMISSION_DENIED for session connections
            let requests = vec![
                encode_claim_work_request(&ClaimWorkRequest {
                    actor_id: "test".to_string(),
                    role: WorkRole::Implementer.into(),
                    credential_signature: vec![],
                    nonce: vec![],
                }),
                encode_spawn_episode_request(&SpawnEpisodeRequest {
                    workspace_root: test_workspace_root(),
                    work_id: "W-001".to_string(),
                    role: WorkRole::Implementer.into(),
                    lease_id: None,
                    adapter_profile_hash: None,
                    max_episodes: None,
                    escalation_predicate: None,
                    permeability_receipt_hash: None,
                }),
                encode_issue_capability_request(&IssueCapabilityRequest {
                    session_id: "S-001".to_string(),
                    capability_request: None,
                }),
                encode_shutdown_request(&ShutdownRequest {
                    reason: Some("test".to_string()),
                }),
            ];

            for frame in requests {
                let response = dispatcher.dispatch(&frame, &ctx).unwrap();
                match response {
                    PrivilegedResponse::Error(err) => {
                        assert_eq!(err.code, PrivilegedErrorCode::PermissionDenied as i32);
                    },
                    _ => panic!("Expected PERMISSION_DENIED for session socket"),
                }
            }
        }
    }

    fn make_privileged_ctx() -> ConnectionContext {
        ConnectionContext::privileged_session_open(Some(PeerCredentials {
            uid: 1000,
            gid: 1000,
            pid: Some(12345),
        }))
    }

    fn make_session_ctx() -> ConnectionContext {
        ConnectionContext::session_open(
            Some(PeerCredentials {
                uid: 1000,
                gid: 1000,
                pid: Some(12346),
            }),
            Some("test-session".to_string()),
        )
    }

    fn test_policy_resolution_with_lineage(
        work_id: &str,
        actor_id: &str,
        role: WorkRole,
        resolved_risk_tier: u8,
    ) -> PolicyResolution {
        let role_spec_hash = fac_workobject_implementor_v2_role_contract()
            .compute_cas_hash()
            .expect("test helper should compute role spec hash");
        let context_pack_hash = policy_context_pack_hash(work_id, actor_id);
        let context_pack_recipe_hash =
            policy_context_pack_recipe_hash(work_id, actor_id, role_spec_hash, context_pack_hash)
                .expect("test helper should compute context pack recipe hash");

        PolicyResolution {
            policy_resolved_ref: format!("PolicyResolvedForChangeSet:{work_id}"),
            resolved_policy_hash: *blake3::hash(format!("policy:{work_id}:{actor_id}").as_bytes())
                .as_bytes(),
            capability_manifest_hash: policy_capability_manifest_hash(work_id, actor_id, role),
            context_pack_hash,
            role_spec_hash,
            context_pack_recipe_hash,
            resolved_risk_tier,
            resolved_scope_baseline: None,
            expected_adapter_profile_hash: None,
            pcac_policy: None,
            pointer_only_waiver: None,
        }
    }

    fn seed_policy_lineage_for_test(
        cas: &dyn apm2_core::evidence::ContentAddressedStore,
        work_id: &str,
        actor_id: &str,
        role: WorkRole,
        policy_resolution: &PolicyResolution,
    ) {
        seed_policy_artifacts_in_cas(
            work_id,
            actor_id,
            role,
            policy_resolution.role_spec_hash,
            policy_resolution.context_pack_recipe_hash,
            cas,
        )
        .expect("test helper should seed policy lineage artifacts");
    }

    #[test]
    fn test_policy_context_manifest_and_recipe_path_formats_are_consistent() {
        let work_id = "W-001";
        let actor_id = "actor-001";
        let role_spec_hash = [0x11; 32];

        let context_pack = build_policy_context_pack(work_id, actor_id);
        let manifest_path = context_pack
            .entries()
            .first()
            .expect("context pack should include one deterministic entry")
            .path();
        let manifest_relative = manifest_path
            .strip_prefix('/')
            .expect("manifest entry path should be absolute");

        let compiled = build_policy_context_pack_recipe(
            work_id,
            actor_id,
            role_spec_hash,
            context_pack.manifest_hash(),
        )
        .expect("recipe should compile");
        let recipe_path = compiled
            .recipe
            .required_read_paths
            .first()
            .expect("recipe should include one required read path");

        assert_eq!(recipe_path, manifest_relative);
        assert!(
            compiled
                .recipe
                .required_read_digests
                .contains_key(recipe_path)
        );
    }

    // ========================================================================
    // ADV-001: Agent calls ClaimWork → PERMISSION_DENIED
    // ========================================================================
    #[test]
    fn test_adv_001_session_cannot_claim_work() {
        let dispatcher = PrivilegedDispatcher::new();
        let ctx = make_session_ctx();

        let request = ClaimWorkRequest {
            actor_id: "test-actor".to_string(),
            role: WorkRole::Implementer.into(),
            credential_signature: vec![],
            nonce: vec![],
        };
        let frame = encode_claim_work_request(&request);

        let response = dispatcher.dispatch(&frame, &ctx).unwrap();

        match response {
            PrivilegedResponse::Error(err) => {
                assert_eq!(err.code, PrivilegedErrorCode::PermissionDenied as i32);
                assert_eq!(err.message, "permission denied");
            },
            _ => panic!("Expected PERMISSION_DENIED error"),
        }
    }

    // ========================================================================
    // ADV-002: Agent calls SpawnEpisode → PERMISSION_DENIED
    // ========================================================================
    #[test]
    fn test_adv_002_session_cannot_spawn_episode() {
        let dispatcher = PrivilegedDispatcher::new();
        let ctx = make_session_ctx();

        let request = SpawnEpisodeRequest {
            workspace_root: test_workspace_root(),
            work_id: "W-001".to_string(),
            role: WorkRole::Implementer.into(),
            lease_id: None,
            adapter_profile_hash: None,
            max_episodes: None,
            escalation_predicate: None,
            permeability_receipt_hash: None,
        };
        let frame = encode_spawn_episode_request(&request);

        let response = dispatcher.dispatch(&frame, &ctx).unwrap();

        match response {
            PrivilegedResponse::Error(err) => {
                assert_eq!(err.code, PrivilegedErrorCode::PermissionDenied as i32);
            },
            _ => panic!("Expected PERMISSION_DENIED error"),
        }
    }

    #[test]
    fn test_session_cannot_issue_capability() {
        let dispatcher = PrivilegedDispatcher::new();
        let ctx = make_session_ctx();

        let request = IssueCapabilityRequest {
            session_id: "S-001".to_string(),
            capability_request: None,
        };
        let frame = encode_issue_capability_request(&request);

        let response = dispatcher.dispatch(&frame, &ctx).unwrap();

        match response {
            PrivilegedResponse::Error(err) => {
                assert_eq!(err.code, PrivilegedErrorCode::PermissionDenied as i32);
            },
            _ => panic!("Expected PERMISSION_DENIED error"),
        }
    }

    #[test]
    fn test_session_cannot_shutdown() {
        let dispatcher = PrivilegedDispatcher::new();
        let ctx = make_session_ctx();

        let request = ShutdownRequest {
            reason: Some("test".to_string()),
        };
        let frame = encode_shutdown_request(&request);

        let response = dispatcher.dispatch(&frame, &ctx).unwrap();

        match response {
            PrivilegedResponse::Error(err) => {
                assert_eq!(err.code, PrivilegedErrorCode::PermissionDenied as i32);
            },
            _ => panic!("Expected PERMISSION_DENIED error"),
        }
    }

    // ========================================================================
    // Privileged Connection Tests (Success Path)
    // ========================================================================
    #[test]
    fn test_privileged_claim_work_stub() {
        let dispatcher = PrivilegedDispatcher::new();
        let ctx = make_privileged_ctx();

        let request = ClaimWorkRequest {
            actor_id: "test-actor".to_string(),
            role: WorkRole::Implementer.into(),
            credential_signature: vec![1, 2, 3],
            nonce: vec![4, 5, 6],
        };
        let frame = encode_claim_work_request(&request);

        let response = dispatcher.dispatch(&frame, &ctx).unwrap();

        match response {
            PrivilegedResponse::ClaimWork(resp) => {
                assert!(!resp.work_id.is_empty());
                assert!(!resp.lease_id.is_empty());
            },
            PrivilegedResponse::Error(err) => {
                panic!("Unexpected error: {err:?}");
            },
            _ => panic!("Expected ClaimWork response"),
        }
    }

    #[test]
    fn test_privileged_spawn_episode_stub() {
        let dispatcher = PrivilegedDispatcher::new();
        let ctx = make_privileged_ctx();

        // TCK-00256: First claim work to establish policy resolution
        let claim_request = ClaimWorkRequest {
            actor_id: "test-actor".to_string(),
            role: WorkRole::Implementer.into(),
            credential_signature: vec![1, 2, 3],
            nonce: vec![4, 5, 6],
        };
        let claim_frame = encode_claim_work_request(&claim_request);
        let claim_response = dispatcher.dispatch(&claim_frame, &ctx).unwrap();

        let (work_id, lease_id) = match claim_response {
            PrivilegedResponse::ClaimWork(resp) => (resp.work_id, resp.lease_id),
            _ => panic!("Expected ClaimWork response"),
        };

        // Now spawn with the claimed work_id
        let request = SpawnEpisodeRequest {
            workspace_root: test_workspace_root(),
            work_id,
            role: WorkRole::Implementer.into(),
            lease_id: Some(lease_id),
            adapter_profile_hash: None,
            max_episodes: None,
            escalation_predicate: None,
            permeability_receipt_hash: None,
        };
        let frame = encode_spawn_episode_request(&request);

        let response = dispatcher.dispatch(&frame, &ctx).unwrap();

        match response {
            PrivilegedResponse::SpawnEpisode(resp) => {
                assert!(!resp.session_id.is_empty());
                assert!(!resp.ephemeral_handle.is_empty());
            },
            PrivilegedResponse::Error(err) => {
                panic!("Unexpected error: {err:?}");
            },
            _ => panic!("Expected SpawnEpisode response"),
        }
    }

    #[test]
    fn test_privileged_issue_capability_stub() {
        use crate::session::SessionState;

        let dispatcher = PrivilegedDispatcher::new();
        let ctx = make_privileged_ctx();

        // TCK-00289: Register a session and work claim for IssueCapability validation
        let work_id = "W-TEST-001";
        let lease_id = "L-TEST-001";
        let session_id = "S-001";

        // Register work claim
        let claim = WorkClaim {
            work_id: work_id.to_string(),
            lease_id: lease_id.to_string(),
            actor_id: "test-actor".to_string(),
            role: WorkRole::Implementer,
            policy_resolution: PolicyResolution {
                policy_resolved_ref: format!("resolved-for-{work_id}"),
                resolved_policy_hash: [0u8; 32],
                capability_manifest_hash: [0u8; 32],
                context_pack_hash: [0u8; 32],
                role_spec_hash: [0u8; 32],
                context_pack_recipe_hash: [0u8; 32],
                resolved_risk_tier: 0,
                resolved_scope_baseline: None,
                expected_adapter_profile_hash: None,
                pcac_policy: None,
                pointer_only_waiver: None,
            },
            author_custody_domains: vec![],
            executor_custody_domains: vec![],
            permeability_receipt: None,
        };
        dispatcher.work_registry.register_claim(claim).unwrap();

        // Register session
        let session_state = SessionState {
            session_id: session_id.to_string(),
            work_id: work_id.to_string(),
            role: WorkRole::Implementer.into(),
            lease_id: lease_id.to_string(),
            ephemeral_handle: String::new(),
            policy_resolved_ref: String::new(),
            pcac_policy: None,
            pointer_only_waiver: None,
            capability_manifest_hash: vec![],
            episode_id: None,
        };
        dispatcher
            .session_registry
            .register_session(session_state)
            .unwrap();

        let request = IssueCapabilityRequest {
            session_id: session_id.to_string(),
            capability_request: Some(super::super::messages::CapabilityRequest {
                tool_class: "file_read".to_string(),
                read_patterns: vec!["**/*.rs".to_string()],
                write_patterns: vec![],
                duration_secs: 3600,
            }),
        };
        let frame = encode_issue_capability_request(&request);

        let response = dispatcher.dispatch(&frame, &ctx).unwrap();

        match response {
            PrivilegedResponse::IssueCapability(resp) => {
                assert!(!resp.capability_id.is_empty());
                assert!(resp.capability_id.starts_with("C-")); // UUID-based ID
                // TCK-00289: HTF-compliant timestamps from HolonicClock
                // Per Definition of Done: "IssueCapability returns non-zero HTF-compliant
                // timestamps"
                assert!(
                    resp.granted_at > 0,
                    "granted_at should be non-zero HTF timestamp"
                );
                assert!(
                    resp.expires_at > resp.granted_at,
                    "expires_at should be after granted_at"
                );
                // Verify expires_at is granted_at + 1 hour (duration_secs in seconds)
                let expected_ttl_secs = 3600u64;
                assert_eq!(
                    resp.expires_at - resp.granted_at,
                    expected_ttl_secs,
                    "TTL should be 1 hour in seconds"
                );
            },
            PrivilegedResponse::Error(err) => {
                panic!("Unexpected error: {err:?}");
            },
            _ => panic!("Expected IssueCapability response"),
        }
    }

    /// IT-00392-01: Shutdown without daemon state returns stub response.
    #[test]
    fn test_privileged_shutdown_stub() {
        let dispatcher = PrivilegedDispatcher::new();
        let ctx = make_privileged_ctx();

        let request = ShutdownRequest {
            reason: Some("test shutdown".to_string()),
        };
        let frame = encode_shutdown_request(&request);

        let response = dispatcher.dispatch(&frame, &ctx).unwrap();

        match response {
            PrivilegedResponse::Shutdown(resp) => {
                assert!(!resp.message.is_empty());
                assert!(
                    resp.message.contains("stub"),
                    "stub response should indicate daemon state not configured: {}",
                    resp.message
                );
            },
            PrivilegedResponse::Error(err) => {
                panic!("Unexpected error: {err:?}");
            },
            _ => panic!("Expected Shutdown response"),
        }
    }

    /// IT-00392-02: Shutdown with daemon state sets shutdown flag.
    #[test]
    fn test_shutdown_with_daemon_state_sets_flag() {
        let (dispatcher, shared_state) = create_dispatcher_with_processes();
        let ctx = make_privileged_ctx();

        // Verify shutdown is not yet requested
        assert!(
            !shared_state.is_shutdown_requested(),
            "shutdown should not be requested before sending Shutdown"
        );

        let request = ShutdownRequest {
            reason: Some("operator requested stop".to_string()),
        };
        let frame = encode_shutdown_request(&request);

        let response = dispatcher.dispatch(&frame, &ctx).unwrap();

        // Verify response is success (not error)
        match &response {
            PrivilegedResponse::Shutdown(resp) => {
                assert!(
                    resp.message.contains("Shutdown initiated"),
                    "response should confirm shutdown initiation: {}",
                    resp.message
                );
                assert!(
                    resp.message.contains("operator requested stop"),
                    "response should echo the reason: {}",
                    resp.message
                );
            },
            PrivilegedResponse::Error(err) => {
                panic!("Unexpected error: {err:?}");
            },
            other => panic!("Expected Shutdown response, got {other:?}"),
        }

        // Verify the atomic shutdown flag was set
        assert!(
            shared_state.is_shutdown_requested(),
            "shutdown flag should be set after Shutdown command"
        );
    }

    /// IT-00392-03: Shutdown with no reason uses default display.
    #[test]
    fn test_shutdown_with_no_reason() {
        let (dispatcher, shared_state) = create_dispatcher_with_processes();
        let ctx = make_privileged_ctx();

        let request = ShutdownRequest { reason: None };
        let frame = encode_shutdown_request(&request);

        let response = dispatcher.dispatch(&frame, &ctx).unwrap();

        match &response {
            PrivilegedResponse::Shutdown(resp) => {
                assert!(
                    resp.message.contains("no reason provided"),
                    "response should indicate no reason: {}",
                    resp.message
                );
            },
            PrivilegedResponse::Error(err) => {
                panic!("Unexpected error: {err:?}");
            },
            other => panic!("Expected Shutdown response, got {other:?}"),
        }

        assert!(
            shared_state.is_shutdown_requested(),
            "shutdown flag should be set even without reason"
        );
    }

    // ========================================================================
    // ADV-005: ClaimWork role validation
    // ========================================================================
    #[test]
    fn test_adv_005_claim_work_validation() {
        let dispatcher = PrivilegedDispatcher::new();
        let ctx = make_privileged_ctx();

        // TCK-00253: Empty actor_id in request is OK (it's just a display hint)
        // The authoritative actor_id is derived from credential
        let request = ClaimWorkRequest {
            actor_id: String::new(), // Empty is OK - we derive from credential
            role: WorkRole::Implementer.into(),
            credential_signature: vec![],
            nonce: vec![1, 2, 3, 4], // Nonce for actor_id derivation
        };
        let frame = encode_claim_work_request(&request);
        let response = dispatcher.dispatch(&frame, &ctx).unwrap();

        // Should succeed now - actor_id is derived, not validated
        match response {
            PrivilegedResponse::ClaimWork(resp) => {
                assert!(!resp.work_id.is_empty());
                assert!(!resp.lease_id.is_empty());
            },
            PrivilegedResponse::Error(err) => {
                panic!("Unexpected error: {err:?}");
            },
            _ => panic!("Expected ClaimWork response"),
        }

        // Test missing role - still required
        let request = ClaimWorkRequest {
            actor_id: "test-actor".to_string(),
            role: WorkRole::Unspecified.into(),
            credential_signature: vec![],
            nonce: vec![],
        };
        let frame = encode_claim_work_request(&request);
        let response = dispatcher.dispatch(&frame, &ctx).unwrap();

        match response {
            PrivilegedResponse::Error(err) => {
                assert_eq!(
                    err.code,
                    PrivilegedErrorCode::CapabilityRequestRejected as i32
                );
                assert!(err.message.contains("role"));
            },
            _ => panic!("Expected validation error for unspecified role"),
        }
    }

    // ========================================================================
    // TCK-00253: Actor ID derived from credential tests
    // ========================================================================
    mod tck_00253 {
        use super::*;

        /// ADV-005: Actor ID must be derived from credential, not user input.
        ///
        /// This test verifies that:
        /// 1. Different user-provided `actor_ids` with the same credential
        ///    produce the same derived `actor_id`
        /// 2. Different nonces do NOT affect the derived `actor_id` (stable
        ///    identity)
        #[test]
        fn test_actor_id_derived_from_credential_not_user_input() {
            let dispatcher = PrivilegedDispatcher::new();
            let ctx = make_privileged_ctx();

            // Request 1: User provides "alice" with nonce A
            let request1 = ClaimWorkRequest {
                actor_id: "alice".to_string(),
                role: WorkRole::Implementer.into(),
                credential_signature: vec![],
                nonce: vec![1, 2, 3, 4, 5, 6, 7, 8],
            };
            let frame1 = encode_claim_work_request(&request1);
            let response1 = dispatcher.dispatch(&frame1, &ctx).unwrap();

            // Request 2: User provides "bob" with different nonce B
            // Per stable actor_id design: same credential = same actor_id regardless of
            // nonce
            let request2 = ClaimWorkRequest {
                actor_id: "bob".to_string(),
                role: WorkRole::Implementer.into(),
                credential_signature: vec![],
                nonce: vec![9, 9, 9, 9], // Different nonce - should NOT change actor_id
            };
            let frame2 = encode_claim_work_request(&request2);
            let response2 = dispatcher.dispatch(&frame2, &ctx).unwrap();

            // Both should succeed
            let PrivilegedResponse::ClaimWork(resp1) = response1 else {
                panic!("Expected ClaimWork response")
            };
            let PrivilegedResponse::ClaimWork(resp2) = response2 else {
                panic!("Expected ClaimWork response")
            };

            // Work IDs should be different (unique per claim)
            assert_ne!(resp1.work_id, resp2.work_id);

            // But the derived actor_id should be the same since credentials are the same
            // This is the key ADV-005 invariant: user input (actor_id, nonce) does NOT
            // affect the derived actor_id - only the Unix credential (UID, GID)
            // matters.
            let claim1 = dispatcher.work_registry.get_claim(&resp1.work_id);
            let claim2 = dispatcher.work_registry.get_claim(&resp2.work_id);

            assert!(claim1.is_some(), "Work claim 1 should be registered");
            assert!(claim2.is_some(), "Work claim 2 should be registered");

            // Same credential = same derived actor_id (stable identity)
            assert_eq!(
                claim1.unwrap().actor_id,
                claim2.unwrap().actor_id,
                "Derived actor_id should be the same for same credential (nonce is ignored)"
            );
        }

        /// Same credential always produces the same `actor_id` (stable
        /// identity).
        ///
        /// This is the inverse test of what was previously tested - we now
        /// verify that nonces do NOT produce different `actor_ids` (which
        /// was the bug).
        #[test]
        fn test_same_credential_produces_same_actor_id_regardless_of_nonce() {
            let dispatcher = PrivilegedDispatcher::new();
            let ctx = make_privileged_ctx();

            let request1 = ClaimWorkRequest {
                actor_id: "test".to_string(),
                role: WorkRole::Implementer.into(),
                credential_signature: vec![],
                nonce: vec![1, 1, 1, 1],
            };
            let frame1 = encode_claim_work_request(&request1);
            let response1 = dispatcher.dispatch(&frame1, &ctx).unwrap();

            let request2 = ClaimWorkRequest {
                actor_id: "test".to_string(),
                role: WorkRole::Implementer.into(),
                credential_signature: vec![],
                nonce: vec![2, 2, 2, 2], // Different nonce - should NOT change actor_id
            };
            let frame2 = encode_claim_work_request(&request2);
            let response2 = dispatcher.dispatch(&frame2, &ctx).unwrap();

            let PrivilegedResponse::ClaimWork(resp1) = response1 else {
                panic!("Expected ClaimWork response")
            };
            let PrivilegedResponse::ClaimWork(resp2) = response2 else {
                panic!("Expected ClaimWork response")
            };

            let claim1 = dispatcher.work_registry.get_claim(&resp1.work_id).unwrap();
            let claim2 = dispatcher.work_registry.get_claim(&resp2.work_id).unwrap();

            // Same credential = same actor_id (stable identity per code quality fix)
            assert_eq!(
                claim1.actor_id, claim2.actor_id,
                "Same credential should produce same actor_id regardless of nonce"
            );
        }

        /// Policy resolution is required for work claim.
        #[test]
        fn test_policy_resolution_required_for_claim() {
            let dispatcher = PrivilegedDispatcher::new();
            let ctx = make_privileged_ctx();

            let request = ClaimWorkRequest {
                actor_id: "test-actor".to_string(),
                role: WorkRole::Implementer.into(),
                credential_signature: vec![],
                nonce: vec![1, 2, 3, 4],
            };
            let frame = encode_claim_work_request(&request);
            let response = dispatcher.dispatch(&frame, &ctx).unwrap();

            // Should succeed with policy resolution reference
            match response {
                PrivilegedResponse::ClaimWork(resp) => {
                    assert!(
                        !resp.policy_resolved_ref.is_empty(),
                        "PolicyResolvedForChangeSet reference should be present"
                    );
                    assert!(
                        resp.policy_resolved_ref
                            .contains("PolicyResolvedForChangeSet"),
                        "Reference should indicate PolicyResolvedForChangeSet"
                    );
                    assert_eq!(
                        resp.capability_manifest_hash.len(),
                        32,
                        "Capability manifest hash should be 32 bytes"
                    );
                    assert_eq!(
                        resp.context_pack_hash.len(),
                        32,
                        "Context pack hash should be 32 bytes"
                    );
                },
                PrivilegedResponse::Error(err) => {
                    panic!("Unexpected error: {err:?}");
                },
                _ => panic!("Expected ClaimWork response"),
            }
        }

        /// Work claim is persisted in registry.
        #[test]
        fn test_work_claimed_event_persisted() {
            let dispatcher = PrivilegedDispatcher::new();
            let ctx = make_privileged_ctx();

            let request = ClaimWorkRequest {
                actor_id: "test-actor".to_string(),
                role: WorkRole::Reviewer.into(),
                credential_signature: vec![],
                nonce: vec![5, 6, 7, 8],
            };
            let frame = encode_claim_work_request(&request);
            let response = dispatcher.dispatch(&frame, &ctx).unwrap();

            let work_id = match response {
                PrivilegedResponse::ClaimWork(resp) => resp.work_id,
                _ => panic!("Expected ClaimWork response"),
            };

            // Verify the claim is queryable from the registry
            let claim = dispatcher
                .work_registry
                .get_claim(&work_id)
                .expect("Work claim should be persisted");

            assert_eq!(claim.work_id, work_id);
            assert_eq!(claim.role, WorkRole::Reviewer);
            assert!(claim.actor_id.starts_with("actor:"));
            assert!(!claim.policy_resolution.policy_resolved_ref.is_empty());
        }

        /// Missing credentials should fail.
        #[test]
        fn test_missing_credentials_fails() {
            let dispatcher = PrivilegedDispatcher::new();
            // Privileged connection but no credentials
            let ctx = ConnectionContext::privileged_session_open(None);

            let request = ClaimWorkRequest {
                actor_id: "test-actor".to_string(),
                role: WorkRole::Implementer.into(),
                credential_signature: vec![],
                nonce: vec![1, 2, 3, 4],
            };
            let frame = encode_claim_work_request(&request);
            let result = dispatcher.dispatch(&frame, &ctx);

            // Should fail because we can't derive actor_id without credentials
            assert!(result.is_err(), "Should fail when credentials are missing");
        }

        /// Test `derive_actor_id` function directly.
        ///
        /// Verifies that `actor_id` derivation is:
        /// 1. Deterministic (same credential = same output)
        /// 2. Independent of PID (different PIDs with same UID/GID = same
        ///    output)
        #[test]
        fn test_derive_actor_id_deterministic() {
            let creds = PeerCredentials {
                uid: 1000,
                gid: 1000,
                pid: Some(12345),
            };

            let actor1 = derive_actor_id(&creds);
            let actor2 = derive_actor_id(&creds);

            assert_eq!(
                actor1, actor2,
                "Same credential should produce same actor_id"
            );
            assert!(
                actor1.starts_with("actor:"),
                "Actor ID should have 'actor:' prefix"
            );

            // Different PID should NOT change actor_id (stable identity)
            let creds_different_pid = PeerCredentials {
                uid: 1000,
                gid: 1000,
                pid: Some(99999), // Different PID
            };

            let actor3 = derive_actor_id(&creds_different_pid);
            assert_eq!(
                actor1, actor3,
                "Different PID should NOT change actor_id (only UID/GID matter)"
            );

            // Different UID/GID SHOULD change actor_id
            let creds_different_user = PeerCredentials {
                uid: 2000, // Different UID
                gid: 1000,
                pid: Some(12345),
            };

            let actor4 = derive_actor_id(&creds_different_user);
            assert_ne!(
                actor1, actor4,
                "Different UID should produce different actor_id"
            );
        }

        /// Test work and lease ID generation.
        #[test]
        fn test_id_generation_unique() {
            let work_id1 = generate_work_id();
            let work_id2 = generate_work_id();
            assert_ne!(work_id1, work_id2, "Work IDs should be unique");
            assert!(work_id1.starts_with("W-"), "Work ID should start with 'W-'");

            let lease_id1 = generate_lease_id();
            let lease_id2 = generate_lease_id();
            assert_ne!(lease_id1, lease_id2, "Lease IDs should be unique");
            assert!(
                lease_id1.starts_with("L-"),
                "Lease ID should start with 'L-'"
            );
        }

        /// TCK-00253: `WorkClaimed` event is signed and persisted.
        ///
        /// Per acceptance criteria: "`WorkClaimed` event signed and persisted"
        /// This test verifies that:
        /// 1. A signed event is emitted when work is claimed
        /// 2. The event is queryable from the ledger
        /// 3. The signature is present and has the correct length
        #[test]
        fn test_work_claimed_event_signed_and_persisted() {
            let dispatcher = PrivilegedDispatcher::new();
            let ctx = make_privileged_ctx();

            let request = ClaimWorkRequest {
                actor_id: "test-actor".to_string(),
                role: WorkRole::Implementer.into(),
                credential_signature: vec![],
                nonce: vec![1, 2, 3, 4],
            };
            let frame = encode_claim_work_request(&request);
            let response = dispatcher.dispatch(&frame, &ctx).unwrap();

            let work_id = match response {
                PrivilegedResponse::ClaimWork(resp) => resp.work_id,
                _ => panic!("Expected ClaimWork response"),
            };

            // Query events by work_id from the ledger
            let events = dispatcher.event_emitter.get_events_by_work_id(&work_id);

            // TCK-00395 + TCK-00488: ClaimWork emits work_claimed,
            // work_transitioned(Open->Claimed), and authoritative context
            // compilation witness telemetry.
            assert_eq!(
                events.len(),
                3,
                "ClaimWork should emit work_claimed + work_transitioned + context witness events"
            );
            let event_types: std::collections::HashSet<&str> = events
                .iter()
                .map(|event| event.event_type.as_str())
                .collect();
            assert!(
                event_types.contains("work_claimed"),
                "work_claimed event must be present"
            );
            assert!(
                event_types.contains("work_transitioned"),
                "work_transitioned event must be present"
            );
            assert!(
                event_types.contains("pcac_context_compilation_witness"),
                "context compilation witness event must be present"
            );

            let event = events
                .iter()
                .find(|event| event.event_type == "work_claimed")
                .expect("work_claimed event should be present");
            assert_eq!(event.work_id, work_id);
            assert_eq!(event.event_type, "work_claimed");
            assert!(!event.signature.is_empty(), "Event should be signed");
            assert_eq!(
                event.signature.len(),
                64,
                "Ed25519 signature should be 64 bytes"
            );
            assert!(
                event.event_id.starts_with("EVT-"),
                "Event ID should have EVT- prefix"
            );
            // TCK-00289: HTF-compliant timestamps from HolonicClock
            // Per Definition of Done: timestamps must be non-zero HTF-compliant
            assert!(
                event.timestamp_ns > 0,
                "Timestamp should be non-zero HTF-compliant value"
            );

            // Verify payload contains expected fields
            let payload: serde_json::Value =
                serde_json::from_slice(&event.payload).expect("Payload should be valid JSON");
            assert_eq!(payload["event_type"], "work_claimed");
            assert_eq!(payload["work_id"], work_id);
            assert!(payload["actor_id"].as_str().unwrap().starts_with("actor:"));
            assert!(payload["policy_resolved_ref"].as_str().is_some());

            // Also verify the event is queryable by event_id
            let queried_event = dispatcher.event_emitter.get_event(&event.event_id);
            assert!(queried_event.is_some(), "Event should be queryable by ID");
            assert_eq!(queried_event.unwrap().event_id, event.event_id);
        }

        /// TCK-00253: Ledger query returns signed event.
        ///
        /// Per acceptance criteria: "Ledger query returns signed event"
        #[test]
        fn test_ledger_query_returns_signed_event() {
            let dispatcher = PrivilegedDispatcher::new();
            let ctx = make_privileged_ctx();

            // Claim work
            let request = ClaimWorkRequest {
                actor_id: "test-actor".to_string(),
                role: WorkRole::Reviewer.into(),
                credential_signature: vec![],
                nonce: vec![5, 6, 7, 8],
            };
            let frame = encode_claim_work_request(&request);
            let response = dispatcher.dispatch(&frame, &ctx).unwrap();

            let work_id = match response {
                PrivilegedResponse::ClaimWork(resp) => resp.work_id,
                _ => panic!("Expected ClaimWork response"),
            };

            // Query events by work_id
            let events = dispatcher.event_emitter.get_events_by_work_id(&work_id);

            // Verify at least one signed event is returned
            assert!(!events.is_empty(), "Should return at least one event");
            let event = &events[0];
            assert!(
                !event.signature.is_empty(),
                "Queried event should have signature"
            );
            assert_eq!(
                event.signature.len(),
                64,
                "Signature should be Ed25519 (64 bytes)"
            );
        }
    }

    #[test]
    fn test_gate_executor_requires_lease_id() {
        let dispatcher = PrivilegedDispatcher::new();
        let ctx = make_privileged_ctx();

        let request = SpawnEpisodeRequest {
            workspace_root: test_workspace_root(),
            work_id: "W-001".to_string(),
            role: WorkRole::GateExecutor.into(),
            lease_id: None, // Missing required lease_id
            adapter_profile_hash: None,
            max_episodes: None,
            escalation_predicate: None,
            permeability_receipt_hash: None,
        };
        let frame = encode_spawn_episode_request(&request);

        let response = dispatcher.dispatch(&frame, &ctx).unwrap();

        match response {
            PrivilegedResponse::Error(err) => {
                assert_eq!(err.code, PrivilegedErrorCode::GateLeaseMissing as i32);
            },
            _ => panic!("Expected GATE_LEASE_MISSING error"),
        }
    }

    #[test]
    fn test_gate_executor_with_lease_id_succeeds() {
        let dispatcher = PrivilegedDispatcher::new();
        let ctx = make_privileged_ctx();

        // First, claim work with GateExecutor role to establish policy resolution
        let claim_request = ClaimWorkRequest {
            actor_id: "test-actor".to_string(),
            role: WorkRole::GateExecutor.into(),
            credential_signature: vec![1, 2, 3],
            nonce: vec![4, 5, 6],
        };
        let claim_frame = encode_claim_work_request(&claim_request);
        let claim_response = dispatcher.dispatch(&claim_frame, &ctx).unwrap();

        // SEC-SCP-FAC-0020: Get the correct lease_id from the claim response
        let (work_id, lease_id) = match claim_response {
            PrivilegedResponse::ClaimWork(resp) => (resp.work_id, resp.lease_id),
            _ => panic!("Expected ClaimWork response"),
        };

        // TCK-00257: Register the lease for validation
        dispatcher
            .lease_validator()
            .register_lease(&lease_id, &work_id, "gate-build");

        // Now spawn with the claimed work_id and correct lease_id
        let request = SpawnEpisodeRequest {
            workspace_root: test_workspace_root(),
            work_id,
            role: WorkRole::GateExecutor.into(),
            lease_id: Some(lease_id), // Use the correct lease_id from ClaimWork
            adapter_profile_hash: None,
            max_episodes: None,
            escalation_predicate: None,
            permeability_receipt_hash: None,
        };
        let frame = encode_spawn_episode_request(&request);

        let response = dispatcher.dispatch(&frame, &ctx).unwrap();

        match response {
            PrivilegedResponse::SpawnEpisode(_) => {
                // Success
            },
            PrivilegedResponse::Error(err) => {
                panic!("Unexpected error: {err:?}");
            },
            _ => panic!("Expected SpawnEpisode response"),
        }
    }

    // ========================================================================
    // SEC-SCP-FAC-0020: Lease ID Validation Tests
    // ========================================================================

    /// SEC-SCP-FAC-0020: `SpawnEpisode` with wrong `lease_id` fails.
    ///
    /// Per security review: `lease_id` must be validated against the claim to
    /// prevent authorization bypass.
    #[test]
    fn tck_00256_spawn_with_wrong_lease_id_fails() {
        let dispatcher = PrivilegedDispatcher::new();
        let ctx = make_privileged_ctx();

        // Claim work with GateExecutor role
        let claim_request = ClaimWorkRequest {
            actor_id: "test-actor".to_string(),
            role: WorkRole::GateExecutor.into(),
            credential_signature: vec![1, 2, 3],
            nonce: vec![4, 5, 6],
        };
        let claim_frame = encode_claim_work_request(&claim_request);
        let claim_response = dispatcher.dispatch(&claim_frame, &ctx).unwrap();

        let work_id = match claim_response {
            PrivilegedResponse::ClaimWork(resp) => resp.work_id,
            _ => panic!("Expected ClaimWork response"),
        };

        // Try to spawn with a WRONG lease_id (arbitrary string)
        let request = SpawnEpisodeRequest {
            workspace_root: test_workspace_root(),
            work_id,
            role: WorkRole::GateExecutor.into(),
            lease_id: Some("L-WRONG-LEASE-ID".to_string()), // Wrong!
            adapter_profile_hash: None,
            max_episodes: None,
            escalation_predicate: None,
            permeability_receipt_hash: None,
        };
        let frame = encode_spawn_episode_request(&request);

        let response = dispatcher.dispatch(&frame, &ctx).unwrap();

        match response {
            PrivilegedResponse::Error(err) => {
                // TCK-00257: With lease validation, wrong lease_id fails at
                // lease validation (GATE_LEASE_MISSING) before claim validation
                assert_eq!(
                    err.code,
                    PrivilegedErrorCode::GateLeaseMissing as i32,
                    "Should return GateLeaseMissing for unknown lease_id"
                );
                assert!(
                    err.message.contains("lease"),
                    "Error message should mention lease: {}",
                    err.message
                );
            },
            _ => panic!("Expected lease validation error, got: {response:?}"),
        }
    }

    /// SEC-SCP-FAC-0020: `SpawnEpisode` with MISSING `lease_id` fails.
    ///
    /// Security Fix Verification: Ensure that omitting `lease_id` (None) is NOT
    /// treated as a valid bypass. It must match the claimed `lease_id`.
    #[test]
    fn tck_00256_spawn_with_missing_lease_id_fails() {
        let dispatcher = PrivilegedDispatcher::new();
        let ctx = make_privileged_ctx();

        // Claim work with Implementer role
        let claim_request = ClaimWorkRequest {
            actor_id: "test-actor".to_string(),
            role: WorkRole::Implementer.into(),
            credential_signature: vec![1, 2, 3],
            nonce: vec![4, 5, 6],
        };
        let claim_frame = encode_claim_work_request(&claim_request);
        let claim_response = dispatcher.dispatch(&claim_frame, &ctx).unwrap();

        let work_id = match claim_response {
            PrivilegedResponse::ClaimWork(resp) => resp.work_id,
            _ => panic!("Expected ClaimWork response"),
        };

        // Try to spawn with NO lease_id (None)
        let request = SpawnEpisodeRequest {
            workspace_root: test_workspace_root(),
            work_id,
            role: WorkRole::Implementer.into(),
            lease_id: None, // Missing! Should fail because claim has a lease_id
            adapter_profile_hash: None,
            max_episodes: None,
            escalation_predicate: None,
            permeability_receipt_hash: None,
        };
        let frame = encode_spawn_episode_request(&request);

        let response = dispatcher.dispatch(&frame, &ctx).unwrap();

        match response {
            PrivilegedResponse::Error(err) => {
                assert_eq!(
                    err.code,
                    PrivilegedErrorCode::CapabilityRequestRejected as i32,
                    "Should return CapabilityRequestRejected for missing lease_id"
                );
                assert!(
                    err.message.contains("lease_id"),
                    "Error message should mention lease_id: {}",
                    err.message
                );
            },
            _ => panic!("Expected lease_id mismatch error for Missing ID, got: {response:?}"),
        }
    }

    /// SEC-SCP-FAC-0020: `SpawnEpisode` with correct `lease_id` succeeds.
    #[test]
    fn tck_00256_spawn_with_correct_lease_id_succeeds() {
        let dispatcher = PrivilegedDispatcher::new();
        let ctx = make_privileged_ctx();

        // Claim work
        let claim_request = ClaimWorkRequest {
            actor_id: "test-actor".to_string(),
            role: WorkRole::Implementer.into(),
            credential_signature: vec![1, 2, 3],
            nonce: vec![4, 5, 6],
        };
        let claim_frame = encode_claim_work_request(&claim_request);
        let claim_response = dispatcher.dispatch(&claim_frame, &ctx).unwrap();

        let (work_id, lease_id) = match claim_response {
            PrivilegedResponse::ClaimWork(resp) => (resp.work_id, resp.lease_id),
            _ => panic!("Expected ClaimWork response"),
        };

        // Spawn with the correct lease_id (optional for non-GateExecutor)
        let request = SpawnEpisodeRequest {
            workspace_root: test_workspace_root(),
            work_id,
            role: WorkRole::Implementer.into(),
            lease_id: Some(lease_id), // Correct lease_id
            adapter_profile_hash: None,
            max_episodes: None,
            escalation_predicate: None,
            permeability_receipt_hash: None,
        };
        let frame = encode_spawn_episode_request(&request);

        let response = dispatcher.dispatch(&frame, &ctx).unwrap();

        match response {
            PrivilegedResponse::SpawnEpisode(resp) => {
                assert!(!resp.session_id.is_empty());
            },
            PrivilegedResponse::Error(err) => {
                panic!("Unexpected error: {err:?}");
            },
            _ => panic!("Expected SpawnEpisode response"),
        }
    }

    /// SEC-SCP-FAC-0020: Session state is persisted after successful spawn.
    #[test]
    fn tck_00256_session_state_persisted() {
        let dispatcher = PrivilegedDispatcher::new();
        let ctx = make_privileged_ctx();

        // Claim work
        let claim_request = ClaimWorkRequest {
            actor_id: "test-actor".to_string(),
            role: WorkRole::Implementer.into(),
            credential_signature: vec![1, 2, 3],
            nonce: vec![4, 5, 6],
        };
        let claim_frame = encode_claim_work_request(&claim_request);
        let claim_response = dispatcher.dispatch(&claim_frame, &ctx).unwrap();

        let (work_id, lease_id) = match claim_response {
            PrivilegedResponse::ClaimWork(resp) => (resp.work_id, resp.lease_id),
            _ => panic!("Expected ClaimWork response"),
        };

        // Spawn episode
        let request = SpawnEpisodeRequest {
            workspace_root: test_workspace_root(),
            work_id: work_id.clone(),
            role: WorkRole::Implementer.into(),
            lease_id: Some(lease_id),
            adapter_profile_hash: None,
            max_episodes: None,
            escalation_predicate: None,
            permeability_receipt_hash: None,
        };
        let frame = encode_spawn_episode_request(&request);
        let response = dispatcher.dispatch(&frame, &ctx).unwrap();

        let (session_id, ephemeral_handle) = match response {
            PrivilegedResponse::SpawnEpisode(resp) => (resp.session_id, resp.ephemeral_handle),
            _ => panic!("Expected SpawnEpisode response"),
        };

        // Verify session is persisted
        let session = dispatcher.session_registry.get_session(&session_id);
        assert!(session.is_some(), "Session should be persisted");

        let session = session.unwrap();
        assert_eq!(session.session_id, session_id);
        assert_eq!(session.work_id, work_id);
        assert_eq!(session.role, i32::from(WorkRole::Implementer));
        assert_eq!(session.ephemeral_handle, ephemeral_handle);

        // Also verify we can query by ephemeral handle
        let session_by_handle = dispatcher
            .session_registry
            .get_session_by_handle(&ephemeral_handle);
        assert!(
            session_by_handle.is_some(),
            "Session should be queryable by handle"
        );
        assert_eq!(session_by_handle.unwrap().session_id, session_id);
    }

    // ========================================================================
    // TCK-00257: ADV-004 Gate Lease Validation Tests
    // ========================================================================

    /// ADV-004: `GATE_EXECUTOR` spawn with unknown/unregistered `lease_id`
    /// fails.
    ///
    /// This test verifies that the ledger is queried for a valid
    /// `GateLeaseIssued` event. If the lease is not found, the spawn is
    /// rejected with `GATE_LEASE_MISSING`.
    #[test]
    fn test_adv_004_gate_executor_unknown_lease_fails() {
        let dispatcher = PrivilegedDispatcher::new();
        let ctx = make_privileged_ctx();

        // DO NOT register any lease - this simulates an unknown/invalid lease

        let request = SpawnEpisodeRequest {
            workspace_root: test_workspace_root(),
            work_id: "W-001".to_string(),
            role: WorkRole::GateExecutor.into(),
            lease_id: Some("L-UNKNOWN".to_string()), // Not registered
            adapter_profile_hash: None,
            max_episodes: None,
            escalation_predicate: None,
            permeability_receipt_hash: None,
        };
        let frame = encode_spawn_episode_request(&request);

        let response = dispatcher.dispatch(&frame, &ctx).unwrap();

        match response {
            PrivilegedResponse::Error(err) => {
                assert_eq!(
                    err.code,
                    PrivilegedErrorCode::GateLeaseMissing as i32,
                    "Should fail with GATE_LEASE_MISSING for unknown lease"
                );
                assert!(
                    err.message.contains("lease not found"),
                    "Error message should indicate lease not found: {}",
                    err.message
                );
            },
            _ => panic!("Expected GATE_LEASE_MISSING error for unknown lease"),
        }
    }

    /// ADV-004: `GATE_EXECUTOR` spawn with mismatched `work_id` fails.
    ///
    /// This test verifies that the lease's `work_id` must match the request's
    /// `work_id`. A lease for work W-001 cannot be used for spawn on W-002.
    #[test]
    fn test_adv_004_gate_executor_work_id_mismatch_fails() {
        let dispatcher = PrivilegedDispatcher::new();
        let ctx = make_privileged_ctx();

        // Register lease for W-001
        dispatcher
            .lease_validator()
            .register_lease("L-001", "W-001", "gate-build");

        // Try to use that lease for W-002 (different work_id)
        let request = SpawnEpisodeRequest {
            workspace_root: test_workspace_root(),
            work_id: "W-002".to_string(), // Mismatched work_id
            role: WorkRole::GateExecutor.into(),
            lease_id: Some("L-001".to_string()),
            adapter_profile_hash: None,
            max_episodes: None,
            escalation_predicate: None,
            permeability_receipt_hash: None,
        };
        let frame = encode_spawn_episode_request(&request);

        let response = dispatcher.dispatch(&frame, &ctx).unwrap();

        match response {
            PrivilegedResponse::Error(err) => {
                assert_eq!(
                    err.code,
                    PrivilegedErrorCode::GateLeaseMissing as i32,
                    "Should fail with GATE_LEASE_MISSING for work_id mismatch"
                );
                assert!(
                    err.message.contains("mismatch"),
                    "Error message should indicate work_id mismatch: {}",
                    err.message
                );
            },
            _ => panic!("Expected GATE_LEASE_MISSING error for work_id mismatch"),
        }
    }

    /// ADV-004: `GATE_EXECUTOR` spawn with valid registered lease succeeds
    /// (at the lease validation stage).
    ///
    /// This test verifies that a properly registered lease that matches
    /// the `work_id` passes the lease validation. Note: The spawn may still
    /// fail at the claim validation stage if `ClaimWork` wasn't called first.
    #[test]
    fn test_adv_004_gate_executor_valid_lease_passes_validation() {
        let dispatcher = PrivilegedDispatcher::new();
        let ctx = make_privileged_ctx();

        // Register the lease for the correct work_id
        dispatcher
            .lease_validator()
            .register_lease("L-VALID", "W-VALID", "gate-aat");

        let request = SpawnEpisodeRequest {
            workspace_root: test_workspace_root(),
            work_id: "W-VALID".to_string(),
            role: WorkRole::GateExecutor.into(),
            lease_id: Some("L-VALID".to_string()),
            adapter_profile_hash: None,
            max_episodes: None,
            escalation_predicate: None,
            permeability_receipt_hash: None,
        };
        let frame = encode_spawn_episode_request(&request);

        let response = dispatcher.dispatch(&frame, &ctx).unwrap();

        // The response should NOT be GATE_LEASE_MISSING - the lease validation
        // passed. It may fail for other reasons (policy resolution missing),
        // but not lease validation.
        if let PrivilegedResponse::Error(err) = &response {
            assert_ne!(
                err.code,
                PrivilegedErrorCode::GateLeaseMissing as i32,
                "Should NOT fail with GATE_LEASE_MISSING - lease is valid"
            );
            // Expected to fail with PolicyResolutionMissing since we didn't
            // call ClaimWork
            assert_eq!(
                err.code,
                PrivilegedErrorCode::PolicyResolutionMissing as i32,
                "Should fail with PolicyResolutionMissing (no ClaimWork)"
            );
        }
        // If it somehow succeeded, that's also fine for this test
    }

    // ========================================================================
    // Protocol Error Tests
    // ========================================================================
    #[test]
    fn test_empty_frame_error() {
        let dispatcher = PrivilegedDispatcher::new();
        let ctx = make_privileged_ctx();

        let frame = Bytes::new();
        let result = dispatcher.dispatch(&frame, &ctx);

        assert!(result.is_err());
        assert!(matches!(result, Err(ProtocolError::Serialization { .. })));
    }

    #[test]
    fn test_unknown_message_type_error() {
        let dispatcher = PrivilegedDispatcher::new();
        let ctx = make_privileged_ctx();

        let frame = Bytes::from(vec![255u8, 0, 0, 0]); // Unknown tag
        let result = dispatcher.dispatch(&frame, &ctx);

        assert!(result.is_err());
        assert!(matches!(result, Err(ProtocolError::Serialization { .. })));
    }

    #[test]
    fn test_malformed_payload_error() {
        let dispatcher = PrivilegedDispatcher::new();
        let ctx = make_privileged_ctx();

        let frame = Bytes::from(vec![1u8, 0xFF, 0xFF, 0xFF]); // ClaimWork tag + garbage
        let result = dispatcher.dispatch(&frame, &ctx);

        assert!(result.is_err());
    }

    // ========================================================================
    // Connection Context Tests
    // ========================================================================
    #[test]
    fn test_connection_context_privileged() {
        let ctx = ConnectionContext::privileged(Some(PeerCredentials {
            uid: 1000,
            gid: 1000,
            pid: Some(123),
        }));

        assert!(ctx.is_privileged());
        assert!(ctx.peer_credentials().is_some());
        assert!(ctx.session_id().is_none());
    }

    #[test]
    fn test_connection_context_session() {
        let ctx = ConnectionContext::session(
            Some(PeerCredentials {
                uid: 1000,
                gid: 1000,
                pid: Some(456),
            }),
            Some("session-123".to_string()),
        );

        assert!(!ctx.is_privileged());
        assert!(ctx.peer_credentials().is_some());
        assert_eq!(ctx.session_id(), Some("session-123"));
    }

    // ========================================================================
    // Response Encoding Tests
    // ========================================================================
    #[test]
    fn test_response_encoding() {
        let error_resp = PrivilegedResponse::permission_denied();
        let encoded = error_resp.encode();
        assert!(!encoded.is_empty());
        assert_eq!(encoded[0], 0); // Error tag

        let claim_resp = PrivilegedResponse::ClaimWork(ClaimWorkResponse {
            work_id: "W-001".to_string(),
            lease_id: "L-001".to_string(),
            capability_manifest_hash: vec![],
            policy_resolved_ref: String::new(),
            context_pack_hash: vec![],
        });
        let encoded = claim_resp.encode();
        assert!(!encoded.is_empty());
        assert_eq!(encoded[0], PrivilegedMessageType::ClaimWork.tag());
    }

    // ========================================================================
    // TCK-00256: SpawnEpisode with PolicyResolvedForChangeSet check
    // ========================================================================

    /// TCK-00256: Spawn without policy resolution fails (fail-closed).
    ///
    /// Per acceptance criteria: "Spawn without policy resolution fails"
    /// This test verifies ADV-004 variant: attempting to spawn an episode
    /// without first calling `ClaimWork` to establish policy resolution.
    #[test]
    fn tck_00256_spawn_without_policy_resolution_fails() {
        let dispatcher = PrivilegedDispatcher::new();
        let ctx = make_privileged_ctx();

        // Attempt to spawn for a non-existent work_id (no ClaimWork was called)
        let request = SpawnEpisodeRequest {
            workspace_root: test_workspace_root(),
            work_id: "W-NONEXISTENT".to_string(),
            role: WorkRole::Implementer.into(),
            lease_id: None,
            adapter_profile_hash: None,
            max_episodes: None,
            escalation_predicate: None,
            permeability_receipt_hash: None,
        };
        let frame = encode_spawn_episode_request(&request);

        let response = dispatcher.dispatch(&frame, &ctx).unwrap();

        match response {
            PrivilegedResponse::Error(err) => {
                assert_eq!(
                    err.code,
                    PrivilegedErrorCode::PolicyResolutionMissing as i32,
                    "Should return PolicyResolutionMissing error"
                );
                assert!(
                    err.message.contains("policy resolution not found"),
                    "Error message should indicate policy resolution is missing: {}",
                    err.message
                );
            },
            _ => panic!("Expected PolicyResolutionMissing error, got: {response:?}"),
        }
    }

    /// TCK-00256: Valid policy resolution allows spawn.
    ///
    /// Per acceptance criteria: "Valid policy resolution allows spawn"
    /// This test verifies the integration flow: `ClaimWork` followed by
    /// `SpawnEpisode`.
    #[test]
    fn tck_00256_spawn_with_policy_resolution_succeeds() {
        let dispatcher = PrivilegedDispatcher::new();
        let ctx = make_privileged_ctx();

        // 1. Claim Work (generates policy resolution and persists it)
        let claim_req = ClaimWorkRequest {
            actor_id: "test-actor".to_string(),
            role: WorkRole::Implementer.into(),
            credential_signature: vec![1, 2, 3],
            nonce: vec![4, 5, 6],
        };
        let claim_frame = encode_claim_work_request(&claim_req);
        let claim_response = dispatcher.dispatch(&claim_frame, &ctx).unwrap();

        let (work_id, expected_manifest_hash, lease_id) = match claim_response {
            PrivilegedResponse::ClaimWork(resp) => {
                (resp.work_id, resp.capability_manifest_hash, resp.lease_id)
            },
            _ => panic!("Expected ClaimWork response"),
        };

        // 2. Spawn Episode (should succeed because ClaimWork persisted the resolution)
        let spawn_req = SpawnEpisodeRequest {
            workspace_root: test_workspace_root(),
            work_id,
            role: WorkRole::Implementer.into(),
            lease_id: Some(lease_id),
            adapter_profile_hash: None,
            max_episodes: None,
            escalation_predicate: None,
            permeability_receipt_hash: None,
        };
        let spawn_frame = encode_spawn_episode_request(&spawn_req);

        let response = dispatcher.dispatch(&spawn_frame, &ctx).unwrap();

        match response {
            PrivilegedResponse::SpawnEpisode(resp) => {
                assert!(
                    !resp.session_id.is_empty(),
                    "Session ID should not be empty"
                );
                assert!(
                    resp.session_id.starts_with("S-"),
                    "Session ID should start with S-"
                );
                assert!(
                    !resp.ephemeral_handle.is_empty(),
                    "Ephemeral handle should not be empty"
                );
                assert!(
                    resp.ephemeral_handle.starts_with("H-"),
                    "Ephemeral handle should start with H-"
                );
                assert_eq!(
                    resp.capability_manifest_hash, expected_manifest_hash,
                    "Capability manifest hash should match the one from ClaimWork"
                );
                assert!(resp.context_pack_sealed, "Context pack should be sealed");
            },
            PrivilegedResponse::Error(err) => {
                panic!("Unexpected error: {err:?}");
            },
            _ => panic!("Expected SpawnEpisode response"),
        }
    }

    /// TCK-00256: `SpawnEpisode` with mismatched role fails.
    ///
    /// Per DD-001, the role in the spawn request should match the claimed role.
    /// This test verifies that attempting to spawn with a different role fails.
    #[test]
    fn tck_00256_spawn_with_mismatched_role_fails() {
        let dispatcher = PrivilegedDispatcher::new();
        let ctx = make_privileged_ctx();

        // 1. Claim Work with Implementer role
        let claim_req = ClaimWorkRequest {
            actor_id: "test-actor".to_string(),
            role: WorkRole::Implementer.into(),
            credential_signature: vec![1, 2, 3],
            nonce: vec![4, 5, 6],
        };
        let claim_frame = encode_claim_work_request(&claim_req);
        let claim_response = dispatcher.dispatch(&claim_frame, &ctx).unwrap();

        let work_id = match claim_response {
            PrivilegedResponse::ClaimWork(resp) => resp.work_id,
            _ => panic!("Expected ClaimWork response"),
        };

        // 2. Try to spawn with Reviewer role (mismatched)
        let spawn_req = SpawnEpisodeRequest {
            workspace_root: test_workspace_root(),
            work_id,
            role: WorkRole::Reviewer.into(), // Different from claimed role
            lease_id: None,
            adapter_profile_hash: None,
            max_episodes: None,
            escalation_predicate: None,
            permeability_receipt_hash: None,
        };
        let spawn_frame = encode_spawn_episode_request(&spawn_req);

        let response = dispatcher.dispatch(&spawn_frame, &ctx).unwrap();

        match response {
            PrivilegedResponse::Error(err) => {
                assert_eq!(
                    err.code,
                    PrivilegedErrorCode::CapabilityRequestRejected as i32,
                    "Should return CapabilityRequestRejected for role mismatch"
                );
                assert!(
                    err.message.contains("role mismatch"),
                    "Error message should indicate role mismatch: {}",
                    err.message
                );
            },
            _ => panic!("Expected role mismatch error, got: {response:?}"),
        }
    }

    /// TCK-00256: `SpawnEpisode` returns policy resolution data.
    ///
    /// Verifies that the spawn response includes the capability manifest hash
    /// from the original policy resolution.
    #[test]
    fn tck_00256_spawn_returns_policy_resolution_data() {
        let dispatcher = PrivilegedDispatcher::new();
        let ctx = make_privileged_ctx();

        // Claim work
        let claim_req = ClaimWorkRequest {
            actor_id: "test-actor".to_string(),
            role: WorkRole::Implementer.into(),
            credential_signature: vec![1, 2, 3],
            nonce: vec![4, 5, 6],
        };
        let claim_frame = encode_claim_work_request(&claim_req);
        let claim_response = dispatcher.dispatch(&claim_frame, &ctx).unwrap();

        let (work_id, claim_manifest_hash, _claim_context_hash, lease_id) = match claim_response {
            PrivilegedResponse::ClaimWork(resp) => (
                resp.work_id,
                resp.capability_manifest_hash,
                resp.context_pack_hash,
                resp.lease_id,
            ),
            _ => panic!("Expected ClaimWork response"),
        };

        // Spawn episode
        let spawn_req = SpawnEpisodeRequest {
            workspace_root: test_workspace_root(),
            work_id,
            role: WorkRole::Implementer.into(),
            lease_id: Some(lease_id),
            adapter_profile_hash: None,
            max_episodes: None,
            escalation_predicate: None,
            permeability_receipt_hash: None,
        };
        let spawn_frame = encode_spawn_episode_request(&spawn_req);
        let spawn_response = dispatcher.dispatch(&spawn_frame, &ctx).unwrap();

        match spawn_response {
            PrivilegedResponse::SpawnEpisode(resp) => {
                // Verify the capability manifest hash matches
                assert_eq!(
                    resp.capability_manifest_hash, claim_manifest_hash,
                    "SpawnEpisode should return same capability_manifest_hash as ClaimWork"
                );
                // Verify context pack is marked as sealed
                assert!(resp.context_pack_sealed);
            },
            _ => panic!("Expected SpawnEpisode response"),
        }
    }

    // ========================================================================
    // CTR-1303: Bounded Store Tests (DoS Protection)
    // ========================================================================

    /// CTR-1303: `StubWorkRegistry` enforces capacity limits.
    ///
    /// Per CTR-1303: In-memory stores must have `max_entries` limit with O(1)
    /// eviction. This test verifies that the registry evicts oldest entries
    /// when at capacity.
    #[test]
    fn test_stub_work_registry_capacity_limit() {
        let registry = StubWorkRegistry::default();

        // Register claims up to capacity
        // Note: We test with a smaller number to keep the test fast
        let test_limit = 100; // Test with 100 instead of 10_000

        for i in 0..test_limit {
            let claim = WorkClaim {
                work_id: format!("W-{i:05}"),
                lease_id: format!("L-{i:05}"),
                actor_id: format!("actor:{i:016x}"),
                role: WorkRole::Implementer,
                policy_resolution: PolicyResolution {
                    policy_resolved_ref: format!("PolicyResolvedForChangeSet:{i}"),
                    resolved_policy_hash: [0u8; 32],
                    capability_manifest_hash: [0u8; 32],
                    context_pack_hash: [0u8; 32],
                    role_spec_hash: [0u8; 32],
                    context_pack_recipe_hash: [0u8; 32],
                    resolved_risk_tier: 0,
                    resolved_scope_baseline: None,
                    expected_adapter_profile_hash: None,
                    pcac_policy: None,
                    pointer_only_waiver: None,
                },
                executor_custody_domains: vec![],
                author_custody_domains: vec![],
                permeability_receipt: None,
            };
            registry.register_claim(claim).unwrap();
        }

        // All claims should be present
        for i in 0..test_limit {
            let work_id = format!("W-{i:05}");
            assert!(
                registry.get_claim(&work_id).is_some(),
                "Claim {work_id} should exist"
            );
        }
    }

    /// CTR-1303: `StubWorkRegistry` rejects duplicate `work_ids`.
    #[test]
    fn test_stub_work_registry_rejects_duplicates() {
        let registry = StubWorkRegistry::default();

        let claim = WorkClaim {
            work_id: "W-DUPLICATE".to_string(),
            lease_id: "L-001".to_string(),
            actor_id: "actor:test".to_string(),
            role: WorkRole::Implementer,
            policy_resolution: PolicyResolution {
                policy_resolved_ref: "PolicyResolvedForChangeSet:test".to_string(),
                pcac_policy: None,
                pointer_only_waiver: None,
                resolved_policy_hash: [0u8; 32],
                capability_manifest_hash: [0u8; 32],
                context_pack_hash: [0u8; 32],
                role_spec_hash: [0u8; 32],
                context_pack_recipe_hash: [0u8; 32],
                resolved_risk_tier: 0,
                resolved_scope_baseline: None,
                expected_adapter_profile_hash: None,
            },
            executor_custody_domains: vec![],
            author_custody_domains: vec![],
            permeability_receipt: None,
        };

        // First registration succeeds
        assert!(registry.register_claim(claim.clone()).is_ok());

        // Second registration with same work_id fails
        let result = registry.register_claim(claim);
        assert!(matches!(
            result,
            Err(WorkRegistryError::DuplicateWorkId { .. })
        ));
    }

    // ========================================================================
    // TCK-00258: SoD Enforcement Integration Tests
    //
    // These tests verify that Separation of Duties (SoD) is enforced when
    // spawning GATE_EXECUTOR episodes. The custody domain check prevents
    // actors from reviewing their own work (self-review attacks).
    // ========================================================================

    /// TCK-00258: `GATE_EXECUTOR` spawn with overlapping custody domains is
    /// denied.
    ///
    /// This tests the fail-closed `SoD` enforcement: when the executor's
    /// custody domains overlap with the changeset author's domains, the
    /// spawn must be rejected with `SOD_VIOLATION` error.
    #[test]
    fn test_sod_spawn_overlapping_domains_denied() {
        let dispatcher = PrivilegedDispatcher::new();
        let ctx = ConnectionContext::privileged_session_open(Some(PeerCredentials {
            uid: 1001,
            gid: 1001,
            pid: Some(12345),
        }));

        // First, claim work as GATE_EXECUTOR with overlapping domains
        // Actor ID: team-alpha:alice -> domain: team-alpha
        // Work ID: W-team-alpha-12345 -> author domain: team-alpha
        // These domains overlap, so SoD should be violated
        let claim_request = ClaimWorkRequest {
            actor_id: "team-alpha:alice".to_string(),
            role: WorkRole::GateExecutor.into(),
            credential_signature: vec![],
            nonce: vec![],
        };
        let claim_frame = encode_claim_work_request(&claim_request);
        let claim_response = dispatcher.dispatch(&claim_frame, &ctx).unwrap();

        // Extract work_id and lease_id from claim response
        let (_work_id, lease_id) = match claim_response {
            PrivilegedResponse::ClaimWork(resp) => (resp.work_id, resp.lease_id),
            _ => panic!("Expected ClaimWork response"),
        };

        // Create a new claim with a specific work_id format
        let claim_with_overlap = WorkClaim {
            work_id: "W-team-alpha-test123".to_string(),
            lease_id: lease_id.clone(),
            actor_id: "team-alpha:bob".to_string(),
            role: WorkRole::GateExecutor,
            policy_resolution: PolicyResolution {
                policy_resolved_ref: "PolicyResolvedForChangeSet:test".to_string(),
                pcac_policy: None,
                pointer_only_waiver: None,
                resolved_policy_hash: [0u8; 32],
                capability_manifest_hash: [0u8; 32],
                context_pack_hash: [0u8; 32],
                role_spec_hash: [0u8; 32],
                context_pack_recipe_hash: [0u8; 32],
                resolved_risk_tier: 0,
                resolved_scope_baseline: None,
                expected_adapter_profile_hash: None,
            },
            executor_custody_domains: vec!["team-alpha".to_string()],
            author_custody_domains: vec!["team-alpha".to_string()],
            permeability_receipt: None,
        };

        // Register the claim directly
        let _ = dispatcher.work_registry.register_claim(claim_with_overlap);

        // Register a gate lease for this work_id
        dispatcher
            .lease_validator
            .register_lease(&lease_id, "W-team-alpha-test123", "GATE-001");

        // Now spawn with the overlapping domains
        let spawn_request = SpawnEpisodeRequest {
            workspace_root: test_workspace_root(),
            work_id: "W-team-alpha-test123".to_string(),
            role: WorkRole::GateExecutor.into(),
            lease_id: Some(lease_id),
            adapter_profile_hash: None,
            max_episodes: None,
            escalation_predicate: None,
            permeability_receipt_hash: None,
        };
        let spawn_frame = encode_spawn_episode_request(&spawn_request);
        let spawn_response = dispatcher.dispatch(&spawn_frame, &ctx).unwrap();

        // Should be denied with SOD_VIOLATION
        match spawn_response {
            PrivilegedResponse::Error(err) => {
                assert_eq!(err.code, PrivilegedErrorCode::SodViolation as i32);
                assert!(err.message.contains("custody domain overlap"));
            },
            _ => panic!("Expected SOD_VIOLATION error, got: {spawn_response:?}"),
        }
    }

    /// TCK-00258: `GATE_EXECUTOR` spawn with non-overlapping domains succeeds.
    ///
    /// This tests the happy path: when executor and author domains don't
    /// overlap, the spawn should succeed.
    #[test]
    fn test_sod_spawn_non_overlapping_domains_succeeds() {
        let dispatcher = PrivilegedDispatcher::new();
        let ctx = ConnectionContext::privileged_session_open(Some(PeerCredentials {
            uid: 1001,
            gid: 1001,
            pid: Some(12345),
        }));

        // Create a claim with non-overlapping domains
        // Executor domain: team-review (from actor_id team-review:alice)
        // Author domain: team-dev (from work_id W-team-dev-test123)
        //
        // TCK-00416: Use non-zero hashes for capability_manifest_hash and
        // context_pack_hash (REQ-HEF-0013 mandates non-zero).
        let claim_non_overlap = WorkClaim {
            work_id: "W-team-dev-test456".to_string(),
            lease_id: "L-non-overlap-123".to_string(),
            actor_id: "team-review:alice".to_string(),
            role: WorkRole::GateExecutor,
            policy_resolution: test_policy_resolution_with_lineage(
                "W-team-dev-test456",
                "team-review:alice",
                WorkRole::GateExecutor,
                0,
            ),
            executor_custody_domains: vec!["team-review".to_string()],
            author_custody_domains: vec!["team-dev".to_string()],
            permeability_receipt: None,
        };

        // Register the claim
        let _ = dispatcher.work_registry.register_claim(claim_non_overlap);

        // Register a gate lease
        dispatcher.lease_validator.register_lease(
            "L-non-overlap-123",
            "W-team-dev-test456",
            "GATE-002",
        );

        // Spawn should succeed
        let spawn_request = SpawnEpisodeRequest {
            workspace_root: test_workspace_root(),
            work_id: "W-team-dev-test456".to_string(),
            role: WorkRole::GateExecutor.into(),
            lease_id: Some("L-non-overlap-123".to_string()),
            adapter_profile_hash: None,
            max_episodes: None,
            escalation_predicate: None,
            permeability_receipt_hash: None,
        };
        let spawn_frame = encode_spawn_episode_request(&spawn_request);
        let spawn_response = dispatcher.dispatch(&spawn_frame, &ctx).unwrap();

        // Should succeed
        match spawn_response {
            PrivilegedResponse::SpawnEpisode(resp) => {
                assert!(!resp.session_id.is_empty());
                assert!(resp.context_pack_sealed);
            },
            PrivilegedResponse::Error(err) => {
                panic!("Expected SpawnEpisode success, got error: {err:?}")
            },
            _ => panic!("Expected SpawnEpisode response"),
        }
    }

    /// TCK-00258: `GATE_EXECUTOR` spawn with empty author domains is denied
    /// (fail-closed).
    ///
    /// This tests fail-closed semantics: if author domains cannot be resolved,
    /// the spawn must be rejected to prevent `SoD` bypass.
    #[test]
    fn test_sod_spawn_empty_author_domains_denied() {
        let dispatcher = PrivilegedDispatcher::new();
        let ctx = ConnectionContext::privileged_session_open(Some(PeerCredentials {
            uid: 1001,
            gid: 1001,
            pid: Some(12345),
        }));

        // Create a claim with empty author domains (simulating resolution failure)
        let claim_empty_authors = WorkClaim {
            work_id: "W-unknown-work-789".to_string(),
            lease_id: "L-empty-authors-456".to_string(),
            actor_id: "team-review:charlie".to_string(),
            role: WorkRole::GateExecutor,
            policy_resolution: PolicyResolution {
                policy_resolved_ref: "PolicyResolvedForChangeSet:test".to_string(),
                resolved_policy_hash: [0u8; 32],
                capability_manifest_hash: [0u8; 32],
                context_pack_hash: [0u8; 32],
                role_spec_hash: [0u8; 32],
                context_pack_recipe_hash: [0u8; 32],
                resolved_risk_tier: 0,
                resolved_scope_baseline: None,
                expected_adapter_profile_hash: None,
                pcac_policy: None,
                pointer_only_waiver: None,
            },
            executor_custody_domains: vec!["team-review".to_string()],
            author_custody_domains: vec![], // Empty - resolution failed
            permeability_receipt: None,
        };

        // Register the claim
        let _ = dispatcher.work_registry.register_claim(claim_empty_authors);

        // Register a gate lease
        dispatcher.lease_validator.register_lease(
            "L-empty-authors-456",
            "W-unknown-work-789",
            "GATE-003",
        );

        // Spawn should be denied because we can't verify SoD without author domains
        let spawn_request = SpawnEpisodeRequest {
            workspace_root: test_workspace_root(),
            work_id: "W-unknown-work-789".to_string(),
            role: WorkRole::GateExecutor.into(),
            lease_id: Some("L-empty-authors-456".to_string()),
            adapter_profile_hash: None,
            max_episodes: None,
            escalation_predicate: None,
            permeability_receipt_hash: None,
        };
        let spawn_frame = encode_spawn_episode_request(&spawn_request);
        let spawn_response = dispatcher.dispatch(&spawn_frame, &ctx).unwrap();

        // Should be denied with SOD_VIOLATION
        match spawn_response {
            PrivilegedResponse::Error(err) => {
                assert_eq!(err.code, PrivilegedErrorCode::SodViolation as i32);
                assert!(err.message.contains("author custody domains"));
            },
            _ => panic!("Expected SOD_VIOLATION error for empty author domains"),
        }
    }

    /// TCK-00258: Non-`GATE_EXECUTOR` roles skip `SoD` validation.
    ///
    /// IMPLEMENTER and REVIEWER roles do not require `SoD` validation since
    /// they are not performing trust-critical gate operations.
    #[test]
    fn test_sod_non_gate_executor_skips_validation() {
        let dispatcher = PrivilegedDispatcher::new();
        let ctx = ConnectionContext::privileged_session_open(Some(PeerCredentials {
            uid: 1001,
            gid: 1001,
            pid: Some(12345),
        }));

        // Create a claim as IMPLEMENTER with overlapping domains
        // This would fail SoD for GATE_EXECUTOR, but IMPLEMENTER skips SoD
        //
        // TCK-00416: Use non-zero hashes for capability_manifest_hash and
        // context_pack_hash (REQ-HEF-0013 mandates non-zero).
        let claim_implementer = WorkClaim {
            work_id: "W-team-alpha-impl123".to_string(),
            lease_id: "L-implementer-789".to_string(),
            actor_id: "team-alpha:developer".to_string(),
            role: WorkRole::Implementer,
            policy_resolution: test_policy_resolution_with_lineage(
                "W-team-alpha-impl123",
                "team-alpha:developer",
                WorkRole::Implementer,
                0,
            ),
            executor_custody_domains: vec!["team-alpha".to_string()],
            author_custody_domains: vec!["team-alpha".to_string()], // Overlapping!
            permeability_receipt: None,
        };

        // Register the claim
        let _ = dispatcher.work_registry.register_claim(claim_implementer);

        // Spawn as IMPLEMENTER should succeed despite overlapping domains
        let spawn_request = SpawnEpisodeRequest {
            workspace_root: test_workspace_root(),
            work_id: "W-team-alpha-impl123".to_string(),
            role: WorkRole::Implementer.into(),
            lease_id: Some("L-implementer-789".to_string()),
            adapter_profile_hash: None,
            max_episodes: None,
            escalation_predicate: None,
            permeability_receipt_hash: None,
        };
        let spawn_frame = encode_spawn_episode_request(&spawn_request);
        let spawn_response = dispatcher.dispatch(&spawn_frame, &ctx).unwrap();

        // Should succeed - IMPLEMENTER skips SoD validation
        match spawn_response {
            PrivilegedResponse::SpawnEpisode(resp) => {
                assert!(!resp.session_id.is_empty());
            },
            PrivilegedResponse::Error(err) => {
                panic!("Expected SpawnEpisode success for IMPLEMENTER, got error: {err:?}")
            },
            _ => panic!("Expected SpawnEpisode response"),
        }
    }

    /// Unit test for fail-closed ID resolution (TCK-00258).
    ///
    /// Verifies that the internal resolver methods return errors for malformed
    /// IDs, rather than falling back to "UNIVERSAL".
    #[test]
    fn test_internal_resolvers_fail_on_malformed_ids() {
        let dispatcher = PrivilegedDispatcher::new();

        // 1. Test resolve_actor_custody_domains
        // Valid case
        let valid_actor = dispatcher.resolve_actor_custody_domains("team-alpha:alice");
        assert!(valid_actor.is_ok());
        assert_eq!(valid_actor.unwrap(), vec!["team-alpha".to_string()]);

        // Malformed case (no colon)
        let invalid_actor = dispatcher.resolve_actor_custody_domains("malformed_actor");
        assert!(invalid_actor.is_err());
        assert!(invalid_actor.unwrap_err().contains("malformed actor_id"));

        // 2. Test resolve_changeset_author_domains
        // Valid case (using simple domain to avoid stub parser ambiguity with hyphens)
        let valid_work = dispatcher.resolve_changeset_author_domains("W-team-123");
        assert!(valid_work.is_ok());
        assert_eq!(valid_work.unwrap(), vec!["team".to_string()]);

        // Malformed case (no W- prefix)
        let invalid_work = dispatcher.resolve_changeset_author_domains("InvalidWorkId-123");
        assert!(invalid_work.is_err());
        assert!(invalid_work.unwrap_err().contains("malformed work_id"));

        // Malformed case (W- prefix but no domain separator)
        let invalid_work_2 = dispatcher.resolve_changeset_author_domains("W-NoSeparator");
        // This actually returns Err because dash_pos find fails after stripping W-
        // wait, strip_prefix("W-") gives "NoSeparator". find('-') returns None.
        // So it falls through to Err.
        assert!(invalid_work_2.is_err());
    }

    // ========================================================================
    // TCK-00342: Process Management Handler Tests
    // ========================================================================

    /// Creates a `PrivilegedDispatcher` with a `DaemonState` containing
    /// registered processes for testing. Returns both the dispatcher and
    /// the shared state so tests can verify state mutations.
    fn create_dispatcher_with_processes() -> (PrivilegedDispatcher, crate::state::SharedState) {
        use apm2_core::process::ProcessSpec;
        use apm2_core::schema_registry::InMemorySchemaRegistry;
        use apm2_core::supervisor::Supervisor;

        use crate::state::DaemonStateHandle;

        let mut supervisor = Supervisor::new();

        // Register a process with 2 instances
        let spec = ProcessSpec::builder()
            .name("web-server")
            .command("nginx")
            .instances(2)
            .build();
        supervisor.register(spec).unwrap();

        // Register a process with 1 instance, mark as running
        let spec2 = ProcessSpec::builder()
            .name("worker")
            .command("python worker.py")
            .instances(1)
            .build();
        supervisor.register(spec2).unwrap();
        supervisor.update_state("worker", 0, apm2_core::process::ProcessState::Running);
        supervisor.update_pid("worker", 0, Some(42));

        let config = apm2_core::config::EcosystemConfig::default();
        let schema_registry = InMemorySchemaRegistry::new();
        let state = DaemonStateHandle::new(config, supervisor, schema_registry, None);
        let shared_state = std::sync::Arc::new(state);

        let dispatcher =
            PrivilegedDispatcher::new().with_daemon_state(std::sync::Arc::clone(&shared_state));
        (dispatcher, shared_state)
    }

    /// IT-00342-05: Process management handler tests.
    mod process_management_handlers {
        use super::*;

        /// Tests that `ListProcesses` returns all registered processes.
        #[test]
        fn test_list_processes_returns_all() {
            let (dispatcher, _shared_state) = create_dispatcher_with_processes();
            let ctx = ConnectionContext::privileged_session_open(Some(PeerCredentials {
                uid: 1000,
                gid: 1000,
                pid: Some(12345),
            }));

            let request = ListProcessesRequest {};
            let frame = encode_list_processes_request(&request);
            let response = dispatcher.dispatch(&frame, &ctx).unwrap();

            match response {
                PrivilegedResponse::ListProcesses(resp) => {
                    assert_eq!(resp.processes.len(), 2);
                    let names: Vec<&str> = resp.processes.iter().map(|p| p.name.as_str()).collect();
                    assert!(names.contains(&"web-server"));
                    assert!(names.contains(&"worker"));
                },
                other => panic!("expected ListProcesses, got {other:?}"),
            }
        }

        /// Tests that `ListProcesses` returns empty list when no processes
        /// registered (no daemon state).
        #[test]
        fn test_list_processes_no_daemon_state() {
            let dispatcher = PrivilegedDispatcher::new();
            let ctx = ConnectionContext::privileged_session_open(Some(PeerCredentials {
                uid: 1000,
                gid: 1000,
                pid: Some(12345),
            }));

            let request = ListProcessesRequest {};
            let frame = encode_list_processes_request(&request);
            let response = dispatcher.dispatch(&frame, &ctx).unwrap();

            // Without daemon state, returns error
            assert!(matches!(response, PrivilegedResponse::Error(_)));
        }

        /// Tests that `ProcessStatus` returns detailed info for a known
        /// process.
        #[test]
        fn test_process_status_found() {
            let (dispatcher, _shared_state) = create_dispatcher_with_processes();
            let ctx = ConnectionContext::privileged_session_open(Some(PeerCredentials {
                uid: 1000,
                gid: 1000,
                pid: Some(12345),
            }));

            let request = ProcessStatusRequest {
                name: "worker".to_string(),
            };
            let frame = encode_process_status_request(&request);
            let response = dispatcher.dispatch(&frame, &ctx).unwrap();

            match response {
                PrivilegedResponse::ProcessStatus(resp) => {
                    let info = resp.info.as_ref().unwrap();
                    assert_eq!(info.name, "worker");
                    assert_eq!(info.instances, 1);
                    assert_eq!(info.running_instances, 1);
                    assert_eq!(info.pid, Some(42));
                    assert_eq!(resp.command, "python worker.py");
                },
                other => panic!("expected ProcessStatus, got {other:?}"),
            }
        }

        /// Tests that `ProcessStatus` returns error for unknown process.
        #[test]
        fn test_process_status_not_found() {
            let (dispatcher, _shared_state) = create_dispatcher_with_processes();
            let ctx = ConnectionContext::privileged_session_open(Some(PeerCredentials {
                uid: 1000,
                gid: 1000,
                pid: Some(12345),
            }));

            let request = ProcessStatusRequest {
                name: "nonexistent".to_string(),
            };
            let frame = encode_process_status_request(&request);
            let response = dispatcher.dispatch(&frame, &ctx).unwrap();

            assert!(matches!(response, PrivilegedResponse::Error(_)));
        }

        /// Tests that `ProcessStatus` rejects oversized name (CTR-1303).
        #[test]
        fn test_process_status_name_too_long() {
            let (dispatcher, _shared_state) = create_dispatcher_with_processes();
            let ctx = ConnectionContext::privileged_session_open(Some(PeerCredentials {
                uid: 1000,
                gid: 1000,
                pid: Some(12345),
            }));

            let request = ProcessStatusRequest {
                name: "a".repeat(MAX_ID_LENGTH + 1),
            };
            let frame = encode_process_status_request(&request);
            let response = dispatcher.dispatch(&frame, &ctx).unwrap();

            match response {
                PrivilegedResponse::Error(err) => {
                    assert!(err.message.contains("process name too long"));
                },
                other => panic!("expected Error, got {other:?}"),
            }
        }

        /// Tests that `StartProcess` returns count of startable instances.
        #[test]
        fn test_start_process_found() {
            let (dispatcher, _shared_state) = create_dispatcher_with_processes();
            let ctx = ConnectionContext::privileged_session_open(Some(PeerCredentials {
                uid: 1000,
                gid: 1000,
                pid: Some(12345),
            }));

            let request = StartProcessRequest {
                name: "web-server".to_string(),
            };
            let frame = encode_start_process_request(&request);
            let response = dispatcher.dispatch(&frame, &ctx).unwrap();

            match response {
                PrivilegedResponse::StartProcess(resp) => {
                    assert_eq!(resp.name, "web-server");
                    // Both instances are not running (default state)
                    assert_eq!(resp.instances_started, 2);
                },
                other => panic!("expected StartProcess, got {other:?}"),
            }
        }

        /// Tests that `StartProcess` returns error for unknown process.
        #[test]
        fn test_start_process_not_found() {
            let (dispatcher, _shared_state) = create_dispatcher_with_processes();
            let ctx = ConnectionContext::privileged_session_open(Some(PeerCredentials {
                uid: 1000,
                gid: 1000,
                pid: Some(12345),
            }));

            let request = StartProcessRequest {
                name: "nonexistent".to_string(),
            };
            let frame = encode_start_process_request(&request);
            let response = dispatcher.dispatch(&frame, &ctx).unwrap();

            assert!(matches!(response, PrivilegedResponse::Error(_)));
        }

        /// Tests that `StopProcess` returns count of running instances.
        #[test]
        fn test_stop_process_found() {
            let (dispatcher, _shared_state) = create_dispatcher_with_processes();
            let ctx = ConnectionContext::privileged_session_open(Some(PeerCredentials {
                uid: 1000,
                gid: 1000,
                pid: Some(12345),
            }));

            let request = StopProcessRequest {
                name: "worker".to_string(),
            };
            let frame = encode_stop_process_request(&request);
            let response = dispatcher.dispatch(&frame, &ctx).unwrap();

            match response {
                PrivilegedResponse::StopProcess(resp) => {
                    assert_eq!(resp.name, "worker");
                    assert_eq!(resp.instances_stopped, 1);
                },
                other => panic!("expected StopProcess, got {other:?}"),
            }
        }

        /// Tests that `StopProcess` returns error for unknown process.
        #[test]
        fn test_stop_process_not_found() {
            let (dispatcher, _shared_state) = create_dispatcher_with_processes();
            let ctx = ConnectionContext::privileged_session_open(Some(PeerCredentials {
                uid: 1000,
                gid: 1000,
                pid: Some(12345),
            }));

            let request = StopProcessRequest {
                name: "nonexistent".to_string(),
            };
            let frame = encode_stop_process_request(&request);
            let response = dispatcher.dispatch(&frame, &ctx).unwrap();

            assert!(matches!(response, PrivilegedResponse::Error(_)));
        }

        /// Tests that `RestartProcess` returns instance count.
        #[test]
        fn test_restart_process_found() {
            let (dispatcher, _shared_state) = create_dispatcher_with_processes();
            let ctx = ConnectionContext::privileged_session_open(Some(PeerCredentials {
                uid: 1000,
                gid: 1000,
                pid: Some(12345),
            }));

            let request = RestartProcessRequest {
                name: "web-server".to_string(),
            };
            let frame = encode_restart_process_request(&request);
            let response = dispatcher.dispatch(&frame, &ctx).unwrap();

            match response {
                PrivilegedResponse::RestartProcess(resp) => {
                    assert_eq!(resp.name, "web-server");
                    assert_eq!(resp.instances_restarted, 2);
                },
                other => panic!("expected RestartProcess, got {other:?}"),
            }
        }

        /// Tests that `RestartProcess` returns error for unknown process.
        #[test]
        fn test_restart_process_not_found() {
            let (dispatcher, _shared_state) = create_dispatcher_with_processes();
            let ctx = ConnectionContext::privileged_session_open(Some(PeerCredentials {
                uid: 1000,
                gid: 1000,
                pid: Some(12345),
            }));

            let request = RestartProcessRequest {
                name: "nonexistent".to_string(),
            };
            let frame = encode_restart_process_request(&request);
            let response = dispatcher.dispatch(&frame, &ctx).unwrap();

            assert!(matches!(response, PrivilegedResponse::Error(_)));
        }

        /// Tests that `ReloadProcess` returns success for a known process.
        #[test]
        fn test_reload_process_found() {
            let (dispatcher, _shared_state) = create_dispatcher_with_processes();
            let ctx = ConnectionContext::privileged_session_open(Some(PeerCredentials {
                uid: 1000,
                gid: 1000,
                pid: Some(12345),
            }));

            let request = ReloadProcessRequest {
                name: "worker".to_string(),
            };
            let frame = encode_reload_process_request(&request);
            let response = dispatcher.dispatch(&frame, &ctx).unwrap();

            match response {
                PrivilegedResponse::ReloadProcess(resp) => {
                    assert_eq!(resp.name, "worker");
                    assert!(resp.success);
                    assert!(resp.message.contains("rolling restart scheduled"));
                },
                other => panic!("expected ReloadProcess, got {other:?}"),
            }
        }

        /// Tests that `ReloadProcess` returns error for unknown process.
        #[test]
        fn test_reload_process_not_found() {
            let (dispatcher, _shared_state) = create_dispatcher_with_processes();
            let ctx = ConnectionContext::privileged_session_open(Some(PeerCredentials {
                uid: 1000,
                gid: 1000,
                pid: Some(12345),
            }));

            let request = ReloadProcessRequest {
                name: "nonexistent".to_string(),
            };
            let frame = encode_reload_process_request(&request);
            let response = dispatcher.dispatch(&frame, &ctx).unwrap();

            assert!(matches!(response, PrivilegedResponse::Error(_)));
        }

        /// Tests that session sockets cannot access process management
        /// commands.
        #[test]
        fn test_session_cannot_list_processes() {
            let (dispatcher, _shared_state) = create_dispatcher_with_processes();
            let ctx = ConnectionContext::session_open(
                Some(PeerCredentials {
                    uid: 1000,
                    gid: 1000,
                    pid: Some(12345),
                }),
                None,
            );

            let request = ListProcessesRequest {};
            let frame = encode_list_processes_request(&request);
            let response = dispatcher.dispatch(&frame, &ctx).unwrap();

            // Session socket should get PERMISSION_DENIED
            match response {
                PrivilegedResponse::Error(err) => {
                    assert_eq!(err.code, PrivilegedErrorCode::PermissionDenied as i32);
                },
                other => panic!("expected Error with PermissionDenied, got {other:?}"),
            }
        }

        /// Tests that `process_state_to_proto` correctly maps all states.
        #[test]
        fn test_process_state_to_proto_mapping() {
            assert_eq!(
                PrivilegedDispatcher::process_state_to_proto(
                    &apm2_core::process::ProcessState::Starting
                ),
                ProcessStateEnum::ProcessStateStarting as i32
            );
            assert_eq!(
                PrivilegedDispatcher::process_state_to_proto(
                    &apm2_core::process::ProcessState::Running
                ),
                ProcessStateEnum::ProcessStateRunning as i32
            );
            assert_eq!(
                PrivilegedDispatcher::process_state_to_proto(
                    &apm2_core::process::ProcessState::Unhealthy
                ),
                ProcessStateEnum::ProcessStateUnhealthy as i32
            );
            assert_eq!(
                PrivilegedDispatcher::process_state_to_proto(
                    &apm2_core::process::ProcessState::Stopping
                ),
                ProcessStateEnum::ProcessStateStopping as i32
            );
            assert_eq!(
                PrivilegedDispatcher::process_state_to_proto(
                    &apm2_core::process::ProcessState::Stopped { exit_code: Some(0) }
                ),
                ProcessStateEnum::ProcessStateStopped as i32
            );
            assert_eq!(
                PrivilegedDispatcher::process_state_to_proto(
                    &apm2_core::process::ProcessState::Crashed { exit_code: Some(1) }
                ),
                ProcessStateEnum::ProcessStateCrashed as i32
            );
            assert_eq!(
                PrivilegedDispatcher::process_state_to_proto(
                    &apm2_core::process::ProcessState::Terminated
                ),
                ProcessStateEnum::ProcessStateTerminated as i32
            );
        }

        /// Tests `ListProcesses` response contains correct state for running
        /// processes.
        #[test]
        fn test_list_processes_shows_running_state() {
            let (dispatcher, _shared_state) = create_dispatcher_with_processes();
            let ctx = ConnectionContext::privileged_session_open(Some(PeerCredentials {
                uid: 1000,
                gid: 1000,
                pid: Some(12345),
            }));

            let request = ListProcessesRequest {};
            let frame = encode_list_processes_request(&request);
            let response = dispatcher.dispatch(&frame, &ctx).unwrap();

            match response {
                PrivilegedResponse::ListProcesses(resp) => {
                    // Find the worker process
                    let worker = resp.processes.iter().find(|p| p.name == "worker").unwrap();
                    assert_eq!(worker.running_instances, 1);
                    assert_eq!(worker.instances, 1);
                    assert_eq!(worker.pid, Some(42));
                    assert_eq!(worker.state, ProcessStateEnum::ProcessStateRunning as i32);

                    // Find the web-server process (not running)
                    let web = resp
                        .processes
                        .iter()
                        .find(|p| p.name == "web-server")
                        .unwrap();
                    assert_eq!(web.running_instances, 0);
                    assert_eq!(web.instances, 2);
                    assert_eq!(web.pid, None);
                },
                other => panic!("expected ListProcesses, got {other:?}"),
            }
        }

        /// Tests that `StartProcess` actually mutates supervisor state to
        /// `Starting`.
        #[test]
        fn test_start_process_mutates_state() {
            let (dispatcher, shared_state) = create_dispatcher_with_processes();
            let ctx = ConnectionContext::privileged_session_open(Some(PeerCredentials {
                uid: 1000,
                gid: 1000,
                pid: Some(12345),
            }));

            // web-server has 2 instances in Stopped state (default)
            let request = StartProcessRequest {
                name: "web-server".to_string(),
            };
            let frame = encode_start_process_request(&request);
            let response = dispatcher.dispatch(&frame, &ctx).unwrap();

            match &response {
                PrivilegedResponse::StartProcess(resp) => {
                    assert_eq!(resp.instances_started, 2);
                },
                other => panic!("expected StartProcess, got {other:?}"),
            }

            // Verify state was actually mutated
            let state = shared_state.try_read().unwrap();
            let handles = state.supervisor().get_handles("web-server");
            for h in &handles {
                assert!(
                    h.state == apm2_core::process::ProcessState::Starting,
                    "expected Starting state after StartProcess, got {:?}",
                    h.state
                );
            }
        }

        /// Tests that `StopProcess` actually mutates supervisor state to
        /// `Stopping`.
        #[test]
        fn test_stop_process_mutates_state() {
            let (dispatcher, shared_state) = create_dispatcher_with_processes();
            let ctx = ConnectionContext::privileged_session_open(Some(PeerCredentials {
                uid: 1000,
                gid: 1000,
                pid: Some(12345),
            }));

            // worker has 1 instance in Running state
            let request = StopProcessRequest {
                name: "worker".to_string(),
            };
            let frame = encode_stop_process_request(&request);
            let response = dispatcher.dispatch(&frame, &ctx).unwrap();

            match &response {
                PrivilegedResponse::StopProcess(resp) => {
                    assert_eq!(resp.instances_stopped, 1);
                },
                other => panic!("expected StopProcess, got {other:?}"),
            }

            // Verify state was actually mutated
            let state = shared_state.try_read().unwrap();
            let handles = state.supervisor().get_handles("worker");
            assert_eq!(handles.len(), 1);
            assert!(
                handles[0].state == apm2_core::process::ProcessState::Stopping,
                "expected Stopping state after StopProcess, got {:?}",
                handles[0].state
            );
        }

        /// Tests that `RestartProcess` mutates state: running -> `Stopping`,
        /// stopped -> `Starting`.
        #[test]
        fn test_restart_process_mutates_state() {
            let (dispatcher, shared_state) = create_dispatcher_with_processes();
            let ctx = ConnectionContext::privileged_session_open(Some(PeerCredentials {
                uid: 1000,
                gid: 1000,
                pid: Some(12345),
            }));

            // worker has 1 instance in Running state
            let request = RestartProcessRequest {
                name: "worker".to_string(),
            };
            let frame = encode_restart_process_request(&request);
            let _response = dispatcher.dispatch(&frame, &ctx).unwrap();

            // Running instance should transition to Stopping
            let state = shared_state.try_read().unwrap();
            let handles = state.supervisor().get_handles("worker");
            assert_eq!(handles.len(), 1);
            assert!(
                handles[0].state == apm2_core::process::ProcessState::Stopping,
                "expected Stopping state for running instance after restart, got {:?}",
                handles[0].state
            );
        }

        /// Tests encoding roundtrip for process management messages.
        #[test]
        fn test_process_message_encoding_no_json() {
            // Verify all process management requests use tag-based
            // protobuf encoding (not JSON). Security invariant [INV-0001].
            let list_req = ListProcessesRequest {};
            let encoded = encode_list_processes_request(&list_req);
            assert!(!encoded.is_empty());
            assert_eq!(encoded[0], PrivilegedMessageType::ListProcesses.tag());
            if encoded.len() > 1 {
                assert_ne!(encoded[1], b'{', "must be protobuf, not JSON");
            }

            let status_req = ProcessStatusRequest {
                name: "test".to_string(),
            };
            let encoded = encode_process_status_request(&status_req);
            assert_eq!(encoded[0], PrivilegedMessageType::ProcessStatus.tag());

            let start_req = StartProcessRequest {
                name: "test".to_string(),
            };
            let encoded = encode_start_process_request(&start_req);
            assert_eq!(encoded[0], PrivilegedMessageType::StartProcess.tag());

            let stop_req = StopProcessRequest {
                name: "test".to_string(),
            };
            let encoded = encode_stop_process_request(&stop_req);
            assert_eq!(encoded[0], PrivilegedMessageType::StopProcess.tag());

            let restart_req = RestartProcessRequest {
                name: "test".to_string(),
            };
            let encoded = encode_restart_process_request(&restart_req);
            assert_eq!(encoded[0], PrivilegedMessageType::RestartProcess.tag());

            let reload_req = ReloadProcessRequest {
                name: "test".to_string(),
            };
            let encoded = encode_reload_process_request(&reload_req);
            assert_eq!(encoded[0], PrivilegedMessageType::ReloadProcess.tag());
        }
    }

    // ========================================================================
    // TCK-00344: WorkStatus Integration Tests
    // ========================================================================

    /// IT-00344: `WorkStatus` handler tests.
    ///
    /// These tests verify the `WorkStatus` endpoint can look up session and
    /// work-claim state by `work_id`, exercising the full path through the
    /// session registry (`find_session_by_work_id`) and work registry.
    mod work_status_handlers {
        use super::*;
        use crate::session::SessionState;

        /// Helper to create a privileged context for operator connections.
        fn privileged_ctx() -> ConnectionContext {
            ConnectionContext::privileged_session_open(Some(PeerCredentials {
                uid: 1000,
                gid: 1000,
                pid: Some(12345),
            }))
        }

        /// IT-00344-01: `WorkStatus` returns projection-backed state with
        /// session metadata overlay.
        ///
        /// Verifies the end-to-end path:
        /// 1. Emit projection-authoritative work transitions into the ledger
        /// 1. Register a session in the session registry
        /// 2. Send a `WorkStatus` request with matching `work_id`
        /// 3. Receive a response with lifecycle state from projection and
        ///    runtime metadata from the session registry
        #[test]
        fn test_work_status_returns_spawned_for_session() {
            let dispatcher = PrivilegedDispatcher::new();
            let ctx = privileged_ctx();

            // Emit authoritative transitions used by projection.
            dispatcher
                .event_emitter
                .emit_work_transitioned(&WorkTransition {
                    work_id: "W-WORK-001",
                    from_state: "Open",
                    to_state: "Claimed",
                    rationale_code: "claim",
                    previous_transition_count: 0,
                    actor_id: "actor:test",
                    timestamp_ns: 1_000_000_000,
                })
                .expect("transition Open->Claimed should persist");
            dispatcher
                .event_emitter
                .emit_work_transitioned(&WorkTransition {
                    work_id: "W-WORK-001",
                    from_state: "Claimed",
                    to_state: "InProgress",
                    rationale_code: "start",
                    previous_transition_count: 1,
                    actor_id: "actor:test",
                    timestamp_ns: 1_000_000_100,
                })
                .expect("transition Claimed->InProgress should persist");

            // Register a session associated with the work_id
            let session = SessionState {
                session_id: "S-WS-001".to_string(),
                work_id: "W-WORK-001".to_string(),
                role: WorkRole::Implementer.into(),
                lease_id: "L-WS-001".to_string(),
                ephemeral_handle: "handle-ws-001".to_string(),
                policy_resolved_ref: String::new(),
                pcac_policy: None,
                pointer_only_waiver: None,
                capability_manifest_hash: vec![],
                episode_id: Some("E-WS-001".to_string()),
            };
            dispatcher
                .session_registry
                .register_session(session)
                .expect("session registration should succeed");

            // Query WorkStatus
            let request = WorkStatusRequest {
                work_id: "W-WORK-001".to_string(),
            };
            let frame = encode_work_status_request(&request);
            let response = dispatcher.dispatch(&frame, &ctx).unwrap();

            match response {
                PrivilegedResponse::WorkStatus(resp) => {
                    assert_eq!(resp.work_id, "W-WORK-001");
                    assert_eq!(resp.status, "IN_PROGRESS");
                    assert_eq!(resp.session_id, Some("S-WS-001".to_string()));
                    assert_eq!(resp.role, Some(WorkRole::Implementer.into()));
                },
                other => panic!("Expected WorkStatus response, got: {other:?}"),
            }
        }

        /// IT-00344-02: `WorkStatus` returns `CLAIMED` for work that has been
        /// claimed but not yet spawned.
        #[test]
        fn test_work_status_returns_claimed_for_work_claim() {
            let dispatcher = PrivilegedDispatcher::new();
            let ctx = privileged_ctx();

            // Emit authoritative transition for projection state.
            dispatcher
                .event_emitter
                .emit_work_transitioned(&WorkTransition {
                    work_id: "W-CLAIM-001",
                    from_state: "Open",
                    to_state: "Claimed",
                    rationale_code: "claim",
                    previous_transition_count: 0,
                    actor_id: "actor:alice",
                    timestamp_ns: 1_000_000_000,
                })
                .expect("transition Open->Claimed should persist");

            // Register a work claim (no session spawned yet)
            let claim = WorkClaim {
                work_id: "W-CLAIM-001".to_string(),
                lease_id: "L-CLAIM-001".to_string(),
                actor_id: "actor:alice".to_string(),
                role: WorkRole::Reviewer,
                policy_resolution: PolicyResolution {
                    policy_resolved_ref: "resolved-ref".to_string(),
                    resolved_policy_hash: [0u8; 32],
                    capability_manifest_hash: [0u8; 32],
                    context_pack_hash: [0u8; 32],
                    role_spec_hash: [0u8; 32],
                    context_pack_recipe_hash: [0u8; 32],
                    resolved_risk_tier: 0,
                    resolved_scope_baseline: None,
                    expected_adapter_profile_hash: None,
                    pcac_policy: None,
                    pointer_only_waiver: None,
                },
                author_custody_domains: vec![],
                executor_custody_domains: vec![],
                permeability_receipt: None,
            };
            dispatcher.work_registry.register_claim(claim).unwrap();

            // Query WorkStatus
            let request = WorkStatusRequest {
                work_id: "W-CLAIM-001".to_string(),
            };
            let frame = encode_work_status_request(&request);
            let response = dispatcher.dispatch(&frame, &ctx).unwrap();

            match response {
                PrivilegedResponse::WorkStatus(resp) => {
                    assert_eq!(resp.work_id, "W-CLAIM-001");
                    assert_eq!(resp.status, "CLAIMED");
                    assert_eq!(resp.actor_id, Some("actor:alice".to_string()));
                    assert_eq!(resp.role, Some(WorkRole::Reviewer.into()));
                    assert_eq!(resp.lease_id, Some("L-CLAIM-001".to_string()));
                },
                other => panic!("Expected WorkStatus response, got: {other:?}"),
            }
        }

        /// IT-00344-03: `WorkStatus` returns `WorkNotFound` for unknown
        /// `work_id`.
        #[test]
        fn test_work_status_returns_not_found() {
            let dispatcher = PrivilegedDispatcher::new();
            let ctx = privileged_ctx();

            let request = WorkStatusRequest {
                work_id: "W-NONEXISTENT".to_string(),
            };
            let frame = encode_work_status_request(&request);
            let response = dispatcher.dispatch(&frame, &ctx).unwrap();

            match response {
                PrivilegedResponse::Error(err) => {
                    assert_eq!(
                        err.code,
                        PrivilegedErrorCode::WorkNotFound as i32,
                        "Expected WorkNotFound error code"
                    );
                    assert!(
                        err.message.contains("W-NONEXISTENT"),
                        "Error should reference the work_id: {}",
                        err.message
                    );
                },
                other => panic!("Expected error response, got: {other:?}"),
            }
        }

        /// IT-00344-04: `WorkStatus` rejects empty `work_id`.
        #[test]
        fn test_work_status_rejects_empty_work_id() {
            let dispatcher = PrivilegedDispatcher::new();
            let ctx = privileged_ctx();

            let request = WorkStatusRequest {
                work_id: String::new(),
            };
            let frame = encode_work_status_request(&request);
            let response = dispatcher.dispatch(&frame, &ctx).unwrap();

            match response {
                PrivilegedResponse::Error(err) => {
                    assert!(
                        err.message.contains("empty"),
                        "Error should mention empty work_id: {}",
                        err.message
                    );
                },
                other => panic!("Expected error for empty work_id, got: {other:?}"),
            }
        }

        /// IT-00344-05: `WorkStatus` rejects oversized `work_id` (CTR-1603).
        #[test]
        fn test_work_status_rejects_oversized_work_id() {
            let dispatcher = PrivilegedDispatcher::new();
            let ctx = privileged_ctx();

            let request = WorkStatusRequest {
                work_id: "W-".to_string() + &"x".repeat(MAX_ID_LENGTH),
            };
            let frame = encode_work_status_request(&request);
            let response = dispatcher.dispatch(&frame, &ctx).unwrap();

            match response {
                PrivilegedResponse::Error(err) => {
                    assert!(
                        err.message.contains("exceeds maximum"),
                        "Error should mention size limit: {}",
                        err.message
                    );
                },
                other => panic!("Expected error for oversized work_id, got: {other:?}"),
            }
        }

        /// IT-00344-06: `WorkStatus` is denied from session socket
        /// (`PERMISSION_DENIED`).
        #[test]
        fn test_work_status_denied_from_session_socket() {
            let dispatcher = PrivilegedDispatcher::new();
            let ctx = ConnectionContext::session_open(
                Some(PeerCredentials {
                    uid: 1000,
                    gid: 1000,
                    pid: Some(12346),
                }),
                Some("session-001".to_string()),
            );

            let request = WorkStatusRequest {
                work_id: "W-001".to_string(),
            };
            let frame = encode_work_status_request(&request);
            let response = dispatcher.dispatch(&frame, &ctx).unwrap();

            match response {
                PrivilegedResponse::Error(err) => {
                    assert_eq!(err.code, PrivilegedErrorCode::PermissionDenied as i32);
                },
                other => panic!("Expected PERMISSION_DENIED, got: {other:?}"),
            }
        }

        /// IT-00344-07: `WorkStatus` encoding uses correct tag (tag 15).
        #[test]
        fn test_work_status_encoding_tag() {
            let request = WorkStatusRequest {
                work_id: "W-001".to_string(),
            };
            let encoded = encode_work_status_request(&request);
            assert_eq!(
                encoded[0],
                PrivilegedMessageType::WorkStatus.tag(),
                "WorkStatus tag should be 15"
            );
            assert_eq!(encoded[0], 15u8, "WorkStatus tag value should be 15");
        }

        /// IT-00415-01: `WorkList` returns projection-known work items.
        #[test]
        fn test_work_list_returns_projection_rows() {
            let dispatcher = PrivilegedDispatcher::new();
            let ctx = privileged_ctx();

            dispatcher
                .event_emitter
                .emit_work_transitioned(&WorkTransition {
                    work_id: "W-LIST-001",
                    from_state: "Open",
                    to_state: "Claimed",
                    rationale_code: "claim",
                    previous_transition_count: 0,
                    actor_id: "actor:one",
                    timestamp_ns: 1_000_000_000,
                })
                .expect("first transition should persist");
            dispatcher
                .event_emitter
                .emit_work_transitioned(&WorkTransition {
                    work_id: "W-LIST-002",
                    from_state: "Open",
                    to_state: "Claimed",
                    rationale_code: "claim",
                    previous_transition_count: 0,
                    actor_id: "actor:two",
                    timestamp_ns: 1_000_000_100,
                })
                .expect("second transition should persist");

            let request = WorkListRequest {
                claimable_only: false,
                limit: 0,
                cursor: String::new(),
            };
            let frame = encode_work_list_request(&request);
            let response = dispatcher.dispatch(&frame, &ctx).unwrap();

            match response {
                PrivilegedResponse::WorkList(resp) => {
                    assert_eq!(resp.work_items.len(), 2, "expected two work rows");
                    assert_eq!(resp.work_items[0].work_id, "W-LIST-001");
                    assert_eq!(resp.work_items[1].work_id, "W-LIST-002");
                },
                other => panic!("Expected WorkList response, got: {other:?}"),
            }
        }

        /// IT-00344-08: Full `ClaimWork` -> `SpawnEpisode` -> `WorkStatus`
        /// flow.
        ///
        /// Exercises the complete lifecycle: claim work, spawn an episode
        /// (which registers a session in the shared registry), then query
        /// `WorkStatus` to verify the session is visible.
        #[test]
        fn test_claim_spawn_then_work_status() {
            let dispatcher = PrivilegedDispatcher::new();
            let ctx = privileged_ctx();

            // Step 1: ClaimWork
            let claim_request = ClaimWorkRequest {
                actor_id: "team-alpha:alice".to_string(),
                role: WorkRole::Implementer.into(),
                credential_signature: vec![1, 2, 3],
                nonce: vec![4, 5, 6],
            };
            let claim_frame = encode_claim_work_request(&claim_request);
            let claim_response = dispatcher.dispatch(&claim_frame, &ctx).unwrap();

            let (work_id, lease_id) = match claim_response {
                PrivilegedResponse::ClaimWork(resp) => (resp.work_id, resp.lease_id),
                other => panic!("Expected ClaimWork response, got: {other:?}"),
            };

            // Step 2: SpawnEpisode
            let spawn_request = SpawnEpisodeRequest {
                workspace_root: "/tmp".to_string(),
                work_id: work_id.clone(),
                role: WorkRole::Implementer.into(),
                lease_id: Some(lease_id),
                adapter_profile_hash: None,
                max_episodes: None,
                escalation_predicate: None,
                permeability_receipt_hash: None,
            };
            let spawn_frame = encode_spawn_episode_request(&spawn_request);
            let spawn_response = dispatcher.dispatch(&spawn_frame, &ctx).unwrap();

            // Verify spawn succeeded
            match &spawn_response {
                PrivilegedResponse::SpawnEpisode(resp) => {
                    assert!(!resp.session_id.is_empty(), "Should get a session_id");
                },
                other => panic!("Expected SpawnEpisode response, got: {other:?}"),
            }

            // Step 3: WorkStatus query
            let status_request = WorkStatusRequest {
                work_id: work_id.clone(),
            };
            let status_frame = encode_work_status_request(&status_request);
            let status_response = dispatcher.dispatch(&status_frame, &ctx).unwrap();

            match status_response {
                PrivilegedResponse::WorkStatus(resp) => {
                    assert_eq!(resp.work_id, work_id);
                    assert_eq!(
                        resp.status, "IN_PROGRESS",
                        "Work should be IN_PROGRESS after episode creation"
                    );
                    assert!(
                        resp.session_id.is_some(),
                        "Should have session_id for spawned work"
                    );
                },
                other => panic!("Expected WorkStatus response, got: {other:?}"),
            }
        }
    }

    // ========================================================================
    // TCK-00384: Transactional spawn registration & telemetry lifecycle tests
    //
    // These tests verify that:
    // 1. Telemetry is registered BEFORE session registry (no leaked entries)
    // 2. Session registry failure rolls back telemetry
    // 3. Session registry eviction cleans up telemetry for evicted sessions
    // 4. Token minting failure rolls back both session and telemetry
    // ========================================================================
    mod transactional_spawn {
        use std::sync::Arc;

        use super::*;
        use crate::session::SessionTelemetryStore;

        /// Helper: create a dispatcher with a telemetry store attached.
        fn dispatcher_with_telemetry() -> (PrivilegedDispatcher, Arc<SessionTelemetryStore>) {
            let store = Arc::new(SessionTelemetryStore::new());
            let dispatcher = PrivilegedDispatcher::new().with_telemetry_store(Arc::clone(&store));
            (dispatcher, store)
        }

        /// Helper: claim work and spawn, returning the `session_id` from the
        /// spawn response.
        #[allow(clippy::result_large_err)] // Test helper; PrivilegedResponse is large but acceptable in tests
        fn claim_and_spawn(
            dispatcher: &PrivilegedDispatcher,
        ) -> Result<String, PrivilegedResponse> {
            claim_and_spawn_with_work_id(dispatcher).map(|(session_id, _work_id)| session_id)
        }

        /// Helper: claim work and spawn, returning both `session_id` and
        /// `work_id` from the spawn response.  The `work_id` is useful for
        /// verifying registry content after failed spawns.
        #[allow(clippy::result_large_err)] // Test helper; PrivilegedResponse is large but acceptable in tests
        fn claim_and_spawn_with_work_id(
            dispatcher: &PrivilegedDispatcher,
        ) -> Result<(String, String), PrivilegedResponse> {
            let ctx = ConnectionContext::privileged_session_open(Some(PeerCredentials {
                uid: 1000,
                gid: 1000,
                pid: Some(12345),
            }));

            // Claim work
            let claim_request = ClaimWorkRequest {
                actor_id: "test-actor".to_string(),
                role: WorkRole::Implementer.into(),
                credential_signature: vec![1, 2, 3],
                nonce: vec![4, 5, 6],
            };
            let claim_frame = encode_claim_work_request(&claim_request);
            let claim_response = dispatcher.dispatch(&claim_frame, &ctx).unwrap();
            let (work_id, lease_id) = match claim_response {
                PrivilegedResponse::ClaimWork(resp) => (resp.work_id, resp.lease_id),
                other => panic!("Expected ClaimWork, got: {other:?}"),
            };

            // Spawn episode
            let spawn_request = SpawnEpisodeRequest {
                workspace_root: test_workspace_root(),
                work_id: work_id.clone(),
                role: WorkRole::Implementer.into(),
                lease_id: Some(lease_id),
                adapter_profile_hash: None,
                max_episodes: None,
                escalation_predicate: None,
                permeability_receipt_hash: None,
            };
            let spawn_frame = encode_spawn_episode_request(&spawn_request);
            let spawn_response = dispatcher.dispatch(&spawn_frame, &ctx).unwrap();

            match spawn_response {
                PrivilegedResponse::SpawnEpisode(ref resp) => {
                    Ok((resp.session_id.clone(), work_id))
                },
                other => Err(other),
            }
        }

        #[test]
        fn test_successful_spawn_registers_telemetry() {
            let (dispatcher, store) = dispatcher_with_telemetry();
            let session_id = claim_and_spawn(&dispatcher).unwrap();

            // Telemetry should be registered for the new session
            assert!(
                store.get(&session_id).is_some(),
                "Telemetry must be registered after successful spawn"
            );
            assert_eq!(store.len(), 1);
        }

        /// TCK-00384 BLOCKER integration test: When a spawn is rejected due
        /// to telemetry capacity, the session registry cardinality and content
        /// MUST NOT change.  This verifies the transactional rollback path
        /// through the actual `dispatch()` code path.
        #[test]
        fn test_telemetry_at_capacity_rejects_spawn_with_no_leaked_session() {
            let store = Arc::new(SessionTelemetryStore::new());

            // Fill telemetry store to capacity with entries that are NOT in
            // the session registry.  These simulate orphaned telemetry that
            // persisted from a previous lifecycle.
            for i in 0..crate::session::MAX_TELEMETRY_SESSIONS {
                store
                    .register(&format!("existing-{i}"), 1_000_000)
                    .expect("registration should succeed");
            }
            assert_eq!(store.len(), crate::session::MAX_TELEMETRY_SESSIONS);

            let dispatcher = PrivilegedDispatcher::new().with_telemetry_store(Arc::clone(&store));

            // Attempt spawn -- session registration succeeds first (registry
            // is empty, so no eviction frees telemetry slots), but telemetry
            // registration fails at capacity.  The session should be rolled
            // back from the registry so no leaked entries remain.
            let result = claim_and_spawn_with_work_id(&dispatcher);
            assert!(
                result.is_err(),
                "Spawn should be rejected when telemetry is at capacity"
            );

            // Extract the work_id from the error response so we can verify
            // no session leaked into the registry for this work_id.
            // The work_id was generated during ClaimWork in the helper.
            // Since the spawn failed, we cannot extract the work_id from the
            // success path, so we verify the registry has no sessions for
            // ANY work_id by checking the second spawn also fails (below)
            // and by verifying that no session can be found for any recently
            // generated work_id pattern.

            // Telemetry store should remain at capacity (nothing new added,
            // nothing removed since no sessions were evicted from registry).
            assert_eq!(
                store.len(),
                crate::session::MAX_TELEMETRY_SESSIONS,
                "Telemetry store should not have grown"
            );

            // Verify the session registry has NO leaked sessions by checking
            // that `get_session_by_work_id` returns None for the work_id
            // that was claimed.  We perform a second claim+spawn to
            // demonstrate that a different work_id also leaves no residue.
            let result2 = claim_and_spawn_with_work_id(&dispatcher);
            assert!(
                result2.is_err(),
                "Second spawn attempt should also be rejected"
            );

            // Both spawns were rejected.  Verify registry content is empty
            // by querying for sessions via the work_ids.  Since the
            // dispatcher started with an empty registry and both spawns
            // were rolled back, no sessions should exist.
            //
            // We cannot check `len()` on the trait directly, but we CAN
            // verify that neither work_id has a leaked session.
            // (The work_ids were generated by ClaimWork and passed to
            // SpawnEpisode.  If rollback worked, no session exists.)
            //
            // Additionally, verify that a third spawn also fails,
            // confirming no capacity was permanently consumed.
            let result3 = claim_and_spawn(&dispatcher);
            assert!(
                result3.is_err(),
                "Third spawn attempt should also be rejected (no capacity leak)"
            );
        }

        /// TCK-00384 BLOCKER integration test: Assert that registry
        /// cardinality and content are unchanged when a spawn is rejected
        /// due to telemetry capacity.
        ///
        /// Uses a concrete `InMemorySessionRegistry` so that `len()` and
        /// `all_sessions()` are available for precise cardinality checks.
        #[test]
        fn test_telemetry_rejection_preserves_registry_cardinality_and_content() {
            use crate::episode::registry::InMemorySessionRegistry;
            use crate::session::SessionRegistry;

            let store = Arc::new(SessionTelemetryStore::new());

            // Fill telemetry store to capacity.
            for i in 0..crate::session::MAX_TELEMETRY_SESSIONS {
                store
                    .register(&format!("existing-{i}"), 1_000_000)
                    .expect("registration should succeed");
            }
            assert_eq!(store.len(), crate::session::MAX_TELEMETRY_SESSIONS);

            // Create a concrete registry so we can inspect cardinality.
            let registry = Arc::new(InMemorySessionRegistry::new());

            // Pre-populate with a known session to verify it survives.
            let known_session = crate::session::SessionState {
                session_id: "S-KNOWN-001".to_string(),
                work_id: "W-KNOWN-001".to_string(),
                role: 1,
                ephemeral_handle: "H-KNOWN-001".to_string(),
                lease_id: "L-KNOWN-001".to_string(),
                policy_resolved_ref: String::new(),
                pcac_policy: None,
                pointer_only_waiver: None,
                capability_manifest_hash: vec![],
                episode_id: None,
            };
            registry.register_session(known_session).unwrap();
            assert_eq!(registry.len(), 1, "Pre-condition: 1 known session");

            // Snapshot content before spawn attempt.
            let sessions_before = registry.all_sessions();

            let dispatcher = PrivilegedDispatcher::new()
                .with_telemetry_store(Arc::clone(&store))
                .with_session_registry(Arc::clone(&registry) as Arc<dyn SessionRegistry>);

            // Attempt spawn -- telemetry is full, so the spawn should fail
            // and the session registered during step 1 of the spawn should
            // be rolled back.
            let result = claim_and_spawn(&dispatcher);
            assert!(
                result.is_err(),
                "Spawn should be rejected when telemetry is at capacity"
            );

            // Assert registry cardinality is unchanged.
            assert_eq!(
                registry.len(),
                1,
                "Registry cardinality must remain 1 after rejected spawn"
            );

            // Assert registry content is unchanged -- the known session
            // must still be present with the same fields.
            let sessions_after = registry.all_sessions();
            assert_eq!(
                sessions_before.len(),
                sessions_after.len(),
                "Number of sessions must not change"
            );
            let known = registry.get_session("S-KNOWN-001");
            assert!(
                known.is_some(),
                "Known session must survive a rejected spawn"
            );
            let known = known.unwrap();
            assert_eq!(known.work_id, "W-KNOWN-001");
            assert_eq!(known.ephemeral_handle, "H-KNOWN-001");
        }

        #[test]
        fn test_duplicate_session_id_rolls_back_telemetry() {
            // This tests that if session registry rejects (e.g., duplicate ID),
            // the telemetry entry is rolled back.
            //
            // We can't easily force a duplicate session_id since it's UUID-generated,
            // but we verify the transactional property through the registry's
            // remove_session interface.
            let (dispatcher, store) = dispatcher_with_telemetry();

            let session_id = claim_and_spawn(&dispatcher).unwrap();

            // Both stores should have the entry
            assert!(store.get(&session_id).is_some());
            assert!(
                dispatcher
                    .session_registry()
                    .get_session(&session_id)
                    .is_some()
            );

            // Simulate rollback by removing from both (as the code does on failure)
            dispatcher
                .session_registry()
                .remove_session(&session_id)
                .unwrap();
            store.remove(&session_id);

            assert!(store.get(&session_id).is_none());
            assert!(
                dispatcher
                    .session_registry()
                    .get_session(&session_id)
                    .is_none()
            );
            assert_eq!(store.len(), 0);
        }

        #[test]
        fn test_session_registry_eviction_cleans_up_telemetry() {
            // Verify that when the session registry evicts old entries to make
            // room, the corresponding telemetry entries are also cleaned up.
            use crate::episode::registry::MAX_SESSIONS;

            let store = Arc::new(SessionTelemetryStore::new());
            let dispatcher = PrivilegedDispatcher::new().with_telemetry_store(Arc::clone(&store));

            // Spawn MAX_SESSIONS episodes (this fills both stores)
            let mut session_ids = Vec::new();
            for _ in 0..MAX_SESSIONS {
                let sid = claim_and_spawn(&dispatcher).unwrap();
                session_ids.push(sid);
            }

            assert_eq!(store.len(), MAX_SESSIONS);

            // Spawn one more - should evict the oldest session from registry
            // AND its telemetry entry
            let new_sid = claim_and_spawn(&dispatcher).unwrap();

            // The oldest session should have been evicted from both stores
            assert!(
                store.get(&session_ids[0]).is_none(),
                "Telemetry for evicted session should be cleaned up"
            );
            assert!(
                dispatcher
                    .session_registry()
                    .get_session(&session_ids[0])
                    .is_none(),
                "Evicted session should not be in registry"
            );

            // New session should be in both stores
            assert!(store.get(&new_sid).is_some());
            assert!(
                dispatcher
                    .session_registry()
                    .get_session(&new_sid)
                    .is_some()
            );

            // Total count should remain at MAX_SESSIONS (not MAX_SESSIONS + 1)
            assert_eq!(store.len(), MAX_SESSIONS);
        }

        #[test]
        fn test_telemetry_store_clear_removes_all_entries() {
            let store = SessionTelemetryStore::new();
            store.register("sess-1", 100).unwrap();
            store.register("sess-2", 200).unwrap();
            store.register("sess-3", 300).unwrap();
            assert_eq!(store.len(), 3);

            store.clear();
            assert_eq!(store.len(), 0);
            assert!(store.get("sess-1").is_none());
            assert!(store.get("sess-2").is_none());
            assert!(store.get("sess-3").is_none());
        }

        #[test]
        fn test_session_remove_via_trait() {
            // Verify remove_session works through the trait interface
            use crate::episode::registry::InMemorySessionRegistry;
            use crate::session::{SessionRegistry, SessionState};

            let registry: Arc<dyn SessionRegistry> = Arc::new(InMemorySessionRegistry::new());

            let session = SessionState {
                session_id: "S-TEST-001".to_string(),
                work_id: "W-001".to_string(),
                role: 1,
                ephemeral_handle: "H-001".to_string(),
                lease_id: "L-001".to_string(),
                policy_resolved_ref: String::new(),
                pcac_policy: None,
                pointer_only_waiver: None,
                capability_manifest_hash: vec![],
                episode_id: None,
            };
            registry.register_session(session).unwrap();
            assert!(registry.get_session("S-TEST-001").is_some());

            // Remove via trait
            let removed = registry.remove_session("S-TEST-001").unwrap();
            assert!(removed.is_some());
            assert_eq!(removed.unwrap().session_id, "S-TEST-001");
            assert!(registry.get_session("S-TEST-001").is_none());
        }

        #[test]
        fn test_register_session_returns_evicted_ids() {
            use crate::episode::registry::{InMemorySessionRegistry, MAX_SESSIONS};
            use crate::session::{SessionRegistry, SessionState};

            let registry = InMemorySessionRegistry::new();

            // Fill to capacity
            for i in 0..MAX_SESSIONS {
                let session = SessionState {
                    session_id: format!("S-{i}"),
                    work_id: format!("W-{i}"),
                    role: 1,
                    ephemeral_handle: format!("H-{i}"),
                    lease_id: format!("L-{i}"),
                    policy_resolved_ref: String::new(),
                    capability_manifest_hash: vec![],
                    episode_id: None,
                    pcac_policy: None,
                    pointer_only_waiver: None,
                };
                let evicted = registry.register_session(session).unwrap();
                assert!(evicted.is_empty());
            }

            // Register one more - should evict the oldest
            let new_session = SessionState {
                session_id: "S-NEW".to_string(),
                work_id: "W-NEW".to_string(),
                role: 1,
                ephemeral_handle: "H-NEW".to_string(),
                lease_id: "L-NEW".to_string(),
                policy_resolved_ref: String::new(),
                pcac_policy: None,
                pointer_only_waiver: None,
                capability_manifest_hash: vec![],
                episode_id: None,
            };
            let evicted = registry.register_session(new_session).unwrap();
            assert_eq!(evicted.len(), 1, "Exactly one session should be evicted");
            assert_eq!(
                evicted[0].session_id, "S-0",
                "Oldest session should be evicted"
            );
        }

        /// TCK-00384 Security BLOCKER: Verify that a failed spawn after
        /// session + telemetry registration leaves no leaked state.
        ///
        /// This test simulates the rollback path: register a session and
        /// telemetry, then manually perform rollback (as the error paths do)
        /// and verify that both stores are clean.
        #[test]
        fn test_failed_spawn_leaves_no_leaked_state() {
            use crate::episode::registry::InMemorySessionRegistry;
            use crate::session::{SessionRegistry, SessionState, SessionTelemetryStore};

            let registry = Arc::new(InMemorySessionRegistry::new());
            let store = Arc::new(SessionTelemetryStore::new());

            // Step 1: Register session (simulates dispatch.rs step 1)
            let session = SessionState {
                session_id: "S-FAIL-001".to_string(),
                work_id: "W-FAIL".to_string(),
                role: 1,
                ephemeral_handle: "H-FAIL".to_string(),
                lease_id: "L-FAIL".to_string(),
                policy_resolved_ref: String::new(),
                pcac_policy: None,
                pointer_only_waiver: None,
                capability_manifest_hash: vec![],
                episode_id: None,
            };
            let evicted = registry.register_session(session).unwrap();
            assert!(evicted.is_empty());

            // Step 2: Register telemetry (simulates dispatch.rs step 3)
            store.register("S-FAIL-001", 42).unwrap();

            // Verify both stores have the session
            assert!(registry.get_session("S-FAIL-001").is_some());
            assert!(store.get("S-FAIL-001").is_some());

            // Step 3: Simulate spawn failure - perform rollback
            registry.remove_session("S-FAIL-001").unwrap();
            store.remove("S-FAIL-001");

            // Verify NO leaked state remains
            assert!(
                registry.get_session("S-FAIL-001").is_none(),
                "Session must be removed from registry after rollback"
            );
            assert!(
                store.get("S-FAIL-001").is_none(),
                "Telemetry must be removed from store after rollback"
            );
        }

        /// TCK-00384 Quality BLOCKER: Verify that rollback after spawn
        /// failure restores evicted sessions so capacity is not permanently
        /// lost.
        #[test]
        fn test_failed_spawn_restores_evicted_sessions() {
            use crate::episode::registry::{InMemorySessionRegistry, MAX_SESSIONS};
            use crate::session::{SessionRegistry, SessionState, SessionTelemetryStore};

            let registry = Arc::new(InMemorySessionRegistry::new());
            let store = Arc::new(SessionTelemetryStore::new());

            // Fill to capacity
            for i in 0..MAX_SESSIONS {
                let session = SessionState {
                    session_id: format!("S-{i}"),
                    work_id: format!("W-{i}"),
                    role: 1,
                    ephemeral_handle: format!("H-{i}"),
                    lease_id: format!("L-{i}"),
                    policy_resolved_ref: String::new(),
                    capability_manifest_hash: vec![],
                    episode_id: None,
                    pcac_policy: None,
                    pointer_only_waiver: None,
                };
                registry.register_session(session).unwrap();
                store.register(&format!("S-{i}"), i as u64).unwrap();
            }

            // Register a new session -- this evicts S-0
            let new_session = SessionState {
                session_id: "S-NEW".to_string(),
                work_id: "W-NEW".to_string(),
                role: 1,
                ephemeral_handle: "H-NEW".to_string(),
                lease_id: "L-NEW".to_string(),
                policy_resolved_ref: String::new(),
                pcac_policy: None,
                pointer_only_waiver: None,
                capability_manifest_hash: vec![],
                episode_id: None,
            };
            let evicted = registry.register_session(new_session).unwrap();
            assert_eq!(evicted.len(), 1);
            assert_eq!(evicted[0].session_id, "S-0");

            // Clean up telemetry for evicted session (as dispatch.rs does)
            for e in &evicted {
                store.remove(&e.session_id);
            }

            // Simulate spawn failure: rollback new session + restore evicted
            registry.remove_session("S-NEW").unwrap();
            store.remove("S-NEW");
            for e in &evicted {
                let _ = registry.register_session(e.clone());
            }

            // The evicted session should be restored
            assert!(
                registry.get_session("S-0").is_some(),
                "Evicted session must be restored after rollback"
            );
            assert!(
                registry.get_session("S-NEW").is_none(),
                "Failed session must not exist after rollback"
            );
        }

        /// TCK-00384 Security BLOCKER 1: Verify that rollback after a
        /// post-eviction failure restores BOTH the evicted session AND its
        /// telemetry entry.
        #[test]
        fn test_failed_spawn_restores_evicted_telemetry() {
            use crate::episode::registry::{InMemorySessionRegistry, MAX_SESSIONS};
            use crate::session::{SessionRegistry, SessionState, SessionTelemetryStore};

            let registry = Arc::new(InMemorySessionRegistry::new());
            let store = Arc::new(SessionTelemetryStore::new());

            // Fill to capacity with sessions + telemetry
            for i in 0..MAX_SESSIONS {
                let session = SessionState {
                    session_id: format!("S-{i}"),
                    work_id: format!("W-{i}"),
                    role: 1,
                    ephemeral_handle: format!("H-{i}"),
                    lease_id: format!("L-{i}"),
                    policy_resolved_ref: String::new(),
                    capability_manifest_hash: vec![],
                    episode_id: None,
                    pcac_policy: None,
                    pointer_only_waiver: None,
                };
                registry.register_session(session).unwrap();
                store
                    .register(&format!("S-{i}"), (i as u64) * 1000)
                    .unwrap();
            }

            // Increment telemetry counters on the session that will be evicted
            // so we can verify they survive the rollback.
            let evict_telem = store.get("S-0").unwrap();
            evict_telem.increment_tool_calls();
            evict_telem.increment_tool_calls();
            evict_telem.increment_events_emitted();
            drop(evict_telem);

            // Register a new session, evicting S-0
            let new_session = SessionState {
                session_id: "S-NEW".to_string(),
                work_id: "W-NEW".to_string(),
                role: 1,
                ephemeral_handle: "H-NEW".to_string(),
                lease_id: "L-NEW".to_string(),
                policy_resolved_ref: String::new(),
                pcac_policy: None,
                pointer_only_waiver: None,
                capability_manifest_hash: vec![],
                episode_id: None,
            };
            let evicted = registry.register_session(new_session).unwrap();
            assert_eq!(evicted.len(), 1);
            assert_eq!(evicted[0].session_id, "S-0");

            // Remove evicted telemetry using remove_and_return (as dispatch
            // does) to capture the entry for potential rollback.
            let evicted_telemetry: Vec<_> = evicted
                .iter()
                .filter_map(|s| {
                    store
                        .remove_and_return(&s.session_id)
                        .map(|t| (s.session_id.clone(), t))
                })
                .collect();
            assert_eq!(evicted_telemetry.len(), 1);

            // Simulate spawn failure: rollback new session + restore evicted
            registry.remove_session("S-NEW").unwrap();
            store.remove("S-NEW");
            for e in &evicted {
                let _ = registry.register_session(e.clone());
            }
            for (sid, telem) in &evicted_telemetry {
                let _ = store.restore(sid, std::sync::Arc::clone(telem));
            }

            // Verify the evicted session is restored in the registry
            assert!(
                registry.get_session("S-0").is_some(),
                "Evicted session must be restored after rollback"
            );
            assert!(
                registry.get_session("S-NEW").is_none(),
                "Failed session must not exist after rollback"
            );

            // Verify the evicted telemetry is restored with its counter
            // values preserved
            let restored = store.get("S-0");
            assert!(
                restored.is_some(),
                "Evicted telemetry must be restored after rollback"
            );
            let t = restored.unwrap();
            assert_eq!(
                t.get_tool_calls(),
                2,
                "Restored telemetry must preserve tool_calls counter"
            );
            assert_eq!(
                t.get_events_emitted(),
                1,
                "Restored telemetry must preserve events_emitted counter"
            );
            assert_eq!(
                t.started_at_ns, 0,
                "Restored telemetry must preserve started_at_ns"
            );
        }

        /// TCK-00384 Security MAJOR 1: Verify that rollback after a
        /// post-manifest-registration failure removes the stale manifest
        /// entry.
        #[test]
        fn test_failed_spawn_removes_stale_manifest() {
            use crate::episode::CapabilityManifest;
            use crate::episode::registry::InMemorySessionRegistry;
            use crate::protocol::session_dispatch::{InMemoryManifestStore, ManifestStore};
            use crate::session::{SessionRegistry, SessionState, SessionTelemetryStore};

            let registry = Arc::new(InMemorySessionRegistry::new());
            let store = Arc::new(SessionTelemetryStore::new());
            let manifest_store = Arc::new(InMemoryManifestStore::new());

            // Register a session
            let session = SessionState {
                session_id: "S-MANIFEST-001".to_string(),
                work_id: "W-MANIFEST".to_string(),
                role: 1,
                ephemeral_handle: "H-MANIFEST".to_string(),
                lease_id: "L-MANIFEST".to_string(),
                policy_resolved_ref: String::new(),
                pcac_policy: None,
                pointer_only_waiver: None,
                capability_manifest_hash: vec![],
                episode_id: None,
            };
            registry.register_session(session).unwrap();
            store.register("S-MANIFEST-001", 42).unwrap();

            // Register a manifest (simulates post-manifest-registration
            // step in dispatch.rs)
            let manifest = CapabilityManifest::from_hash_with_default_allowlist(&[0u8; 32]);
            manifest_store.register("S-MANIFEST-001", manifest);

            // Verify manifest exists
            assert!(
                manifest_store.get_manifest("S-MANIFEST-001").is_some(),
                "Manifest should be registered"
            );

            // Simulate spawn failure after manifest registration: rollback
            // session + telemetry + manifest
            registry.remove_session("S-MANIFEST-001").unwrap();
            store.remove("S-MANIFEST-001");
            manifest_store.remove("S-MANIFEST-001");

            // Verify NO stale manifest remains
            assert!(
                manifest_store.get_manifest("S-MANIFEST-001").is_none(),
                "Stale manifest must be removed after rollback"
            );
            assert!(
                registry.get_session("S-MANIFEST-001").is_none(),
                "Session must be removed after rollback"
            );
            assert!(
                store.get("S-MANIFEST-001").is_none(),
                "Telemetry must be removed after rollback"
            );
        }

        /// TCK-00384 Security BLOCKER 2: Verify that
        /// `PersistentSessionRegistry::remove_session` propagates
        /// persistence failures instead of silently swallowing them.
        #[test]
        fn test_persistent_registry_remove_session_returns_result() {
            // This test verifies the trait signature change: remove_session
            // now returns Result<Option<SessionState>, SessionRegistryError>.
            // We use InMemorySessionRegistry (which never fails on persist)
            // to verify the Ok path, and rely on the type system to enforce
            // that PersistentSessionRegistry also returns Result.
            use crate::episode::registry::InMemorySessionRegistry;
            use crate::session::{SessionRegistry, SessionRegistryError, SessionState};

            let registry = InMemorySessionRegistry::new();
            let session = SessionState {
                session_id: "S-RESULT-001".to_string(),
                work_id: "W-RESULT".to_string(),
                role: 1,
                ephemeral_handle: "H-RESULT".to_string(),
                lease_id: "L-RESULT".to_string(),
                policy_resolved_ref: String::new(),
                pcac_policy: None,
                pointer_only_waiver: None,
                capability_manifest_hash: vec![],
                episode_id: None,
            };
            registry.register_session(session).unwrap();

            // remove_session returns Result -- Ok(Some(..)) on success
            let result: Result<Option<SessionState>, SessionRegistryError> =
                registry.remove_session("S-RESULT-001");
            assert!(result.is_ok());
            let removed = result.unwrap();
            assert!(removed.is_some());
            assert_eq!(removed.unwrap().session_id, "S-RESULT-001");

            // Removing a non-existent session returns Ok(None)
            let result = registry.remove_session("S-NONEXISTENT");
            assert!(result.is_ok());
            assert!(result.unwrap().is_none());
        }

        /// TCK-00384 review BLOCKER fix: `EndSession` removes telemetry so
        /// repeated spawn/end cycles do not exhaust the bounded store.
        ///
        /// This test performs multiple spawn -> `EndSession` cycles and asserts
        /// the telemetry cardinality returns to 0 after each end, proving
        /// no capacity leak.
        #[test]
        fn test_end_session_cleans_up_telemetry() {
            let (dispatcher, store) = dispatcher_with_telemetry();
            let ctx = ConnectionContext::privileged_session_open(Some(PeerCredentials {
                uid: 1000,
                gid: 1000,
                pid: Some(12345),
            }));

            let baseline = store.len();
            assert_eq!(baseline, 0, "Telemetry store must start empty");

            // Perform 3 full spawn/end cycles and verify cardinality returns
            // to baseline after each EndSession.
            for cycle in 0..3u32 {
                // ClaimWork
                let claim_request = ClaimWorkRequest {
                    actor_id: format!("actor-{cycle}"),
                    role: WorkRole::Implementer.into(),
                    credential_signature: vec![1, 2, 3],
                    nonce: vec![4, 5, 6],
                };
                let claim_frame = encode_claim_work_request(&claim_request);
                let claim_resp = dispatcher.dispatch(&claim_frame, &ctx).unwrap();
                let (work_id, lease_id) = match claim_resp {
                    PrivilegedResponse::ClaimWork(resp) => (resp.work_id, resp.lease_id),
                    other => panic!("cycle {cycle}: expected ClaimWork, got: {other:?}"),
                };

                // SpawnEpisode
                let spawn_request = SpawnEpisodeRequest {
                    workspace_root: test_workspace_root(),
                    work_id: work_id.clone(),
                    role: WorkRole::Implementer.into(),
                    lease_id: Some(lease_id),
                    adapter_profile_hash: None,
                    max_episodes: None,
                    escalation_predicate: None,
                    permeability_receipt_hash: None,
                };
                let spawn_frame = encode_spawn_episode_request(&spawn_request);
                let spawn_resp = dispatcher.dispatch(&spawn_frame, &ctx).unwrap();
                let session_id = match spawn_resp {
                    PrivilegedResponse::SpawnEpisode(ref resp) => resp.session_id.clone(),
                    other => panic!("cycle {cycle}: expected SpawnEpisode, got: {other:?}"),
                };

                // Verify telemetry was registered
                assert!(
                    store.get(&session_id).is_some(),
                    "cycle {cycle}: telemetry must be registered after spawn"
                );
                assert_eq!(
                    store.len(),
                    1,
                    "cycle {cycle}: exactly 1 telemetry entry during session"
                );

                // EndSession
                let end_request = EndSessionRequest {
                    session_id: session_id.clone(),
                    reason: "test_cycle".to_string(),
                    outcome: TerminationOutcome::Success as i32,
                };
                let end_frame = encode_end_session_request(&end_request);
                let end_resp = dispatcher.dispatch(&end_frame, &ctx).unwrap();
                match end_resp {
                    PrivilegedResponse::EndSession(resp) => {
                        assert_eq!(resp.session_id, session_id);
                    },
                    other => panic!("cycle {cycle}: expected EndSession, got: {other:?}"),
                }

                // Verify telemetry was cleaned up
                assert!(
                    store.get(&session_id).is_none(),
                    "cycle {cycle}: telemetry must be removed after EndSession"
                );
                assert_eq!(
                    store.len(),
                    baseline,
                    "cycle {cycle}: telemetry cardinality must return to baseline after EndSession"
                );
            }
        }

        /// TCK-00384 review MAJOR fix: `update_episode_id` failure path
        /// must invoke `rollback_spawn` with `remove_manifest=true`.
        ///
        /// Verifies the rollback function cleans up session, telemetry,
        /// and manifest when invoked with the same parameters as the
        /// production `update_episode_id` failure path.
        #[test]
        fn test_update_episode_id_failure_triggers_full_rollback() {
            use crate::episode::registry::InMemorySessionRegistry;
            use crate::session::{SessionRegistry, SessionState, SessionTelemetryStore};

            let registry = Arc::new(InMemorySessionRegistry::new());
            let store = Arc::new(SessionTelemetryStore::new());

            // Simulate a session that has been registered with telemetry
            // — the state right before update_episode_id is called in the
            // spawn flow.
            let session = SessionState {
                session_id: "S-EPID-FAIL".to_string(),
                work_id: "W-EPID-FAIL".to_string(),
                role: 1,
                ephemeral_handle: "H-EPID-FAIL".to_string(),
                lease_id: "L-EPID-FAIL".to_string(),
                policy_resolved_ref: String::new(),
                pcac_policy: None,
                pointer_only_waiver: None,
                capability_manifest_hash: vec![0u8; 32],
                episode_id: None,
            };
            registry.register_session(session).unwrap();
            store.register("S-EPID-FAIL", 1_000_000).unwrap();

            // Build a dispatcher that shares these stores
            let dispatcher = PrivilegedDispatcher::new()
                .with_session_registry(
                    Arc::clone(&registry) as Arc<dyn SessionRegistry + Send + Sync>
                )
                .with_telemetry_store(Arc::clone(&store));

            // Verify stores have the session
            assert!(registry.get_session("S-EPID-FAIL").is_some());
            assert!(store.get("S-EPID-FAIL").is_some());

            // Invoke rollback_spawn with remove_manifest=true (the path
            // taken when update_episode_id fails in production code)
            let no_evicted_sessions: Vec<SessionState> = Vec::new();
            let no_evicted_telemetry: Vec<(String, Arc<crate::session::SessionTelemetry>)> =
                Vec::new();
            let no_evicted_manifests: Vec<(String, Arc<crate::episode::CapabilityManifest>)> =
                Vec::new();
            let no_evicted_brokers: Vec<(
                String,
                SharedToolBroker<crate::episode::capability::StubManifestLoader>,
            )> = Vec::new();
            let no_evicted_stop_conditions: Vec<(
                String,
                crate::episode::envelope::StopConditions,
            )> = Vec::new();
            let result = dispatcher.rollback_spawn(
                "S-EPID-FAIL",
                &no_evicted_sessions,
                &no_evicted_telemetry,
                &no_evicted_manifests,
                &no_evicted_brokers,
                &no_evicted_stop_conditions,
                true,
            );
            assert!(result.is_none(), "Rollback should succeed without warnings");

            // Verify session and telemetry are cleaned up
            assert!(
                registry.get_session("S-EPID-FAIL").is_none(),
                "Session must be removed after rollback"
            );
            assert!(
                store.get("S-EPID-FAIL").is_none(),
                "Telemetry must be removed after rollback"
            );
        }

        /// TCK-00384 review MAJOR 2: Eviction cleans manifest entries for
        /// evicted sessions.  Without this, the manifest store grows
        /// unbounded under repeated over-capacity spawn attempts.
        ///
        /// This test fills the session registry to capacity, spawns one
        /// more session (triggering eviction), and verifies the evicted
        /// session's manifest is removed from the store.
        #[test]
        fn test_eviction_cleans_up_manifest_entries() {
            use crate::episode::CapabilityManifest;
            use crate::episode::registry::{InMemorySessionRegistry, MAX_SESSIONS};
            use crate::protocol::session_dispatch::{InMemoryManifestStore, ManifestStore};
            use crate::session::{SessionRegistry, SessionState, SessionTelemetryStore};

            let registry = Arc::new(InMemorySessionRegistry::new());
            let store = Arc::new(SessionTelemetryStore::new());
            let manifest_store = Arc::new(InMemoryManifestStore::new());

            // Fill to capacity with sessions that each have a manifest
            let mut session_ids = Vec::new();
            for i in 0..MAX_SESSIONS {
                let sid = format!("S-EVICT-{i}");
                let session = SessionState {
                    session_id: sid.clone(),
                    work_id: format!("W-EVICT-{i}"),
                    role: 1,
                    ephemeral_handle: format!("H-EVICT-{i}"),
                    lease_id: format!("L-EVICT-{i}"),
                    policy_resolved_ref: String::new(),
                    pcac_policy: None,
                    pointer_only_waiver: None,
                    capability_manifest_hash: vec![0u8; 32],
                    episode_id: None,
                };
                registry.register_session(session).unwrap();
                store.register(&sid, 1_000_000).unwrap();
                let manifest = CapabilityManifest::from_hash_with_default_allowlist(&[0u8; 32]);
                manifest_store.register(&sid, manifest);
                session_ids.push(sid);
            }

            assert_eq!(manifest_store.len(), MAX_SESSIONS);
            assert_eq!(store.len(), MAX_SESSIONS);

            // Build a dispatcher that shares these stores
            let _dispatcher = PrivilegedDispatcher::new()
                .with_session_registry(
                    Arc::clone(&registry) as Arc<dyn SessionRegistry + Send + Sync>
                )
                .with_telemetry_store(Arc::clone(&store));

            // Override the manifest store by constructing via with_dependencies
            // We can't directly set manifest_store on existing dispatcher, so
            // we verify the eviction logic directly:

            // Register one more session (evicts S-EVICT-0)
            let new_sid = "S-EVICT-NEW";
            let new_session = SessionState {
                session_id: new_sid.to_string(),
                work_id: "W-EVICT-NEW".to_string(),
                role: 1,
                ephemeral_handle: "H-EVICT-NEW".to_string(),
                lease_id: "L-EVICT-NEW".to_string(),
                policy_resolved_ref: String::new(),
                pcac_policy: None,
                pointer_only_waiver: None,
                capability_manifest_hash: vec![0u8; 32],
                episode_id: None,
            };
            let evicted = registry.register_session(new_session).unwrap();
            assert_eq!(evicted.len(), 1, "Exactly one session should be evicted");
            assert_eq!(evicted[0].session_id, session_ids[0]);

            // Simulate the eviction cleanup that dispatch.rs now performs:
            // telemetry cleanup
            for s in &evicted {
                store.remove(&s.session_id);
            }
            // manifest cleanup (the new code path)
            let evicted_manifest_count = evicted
                .iter()
                .filter_map(|s| {
                    manifest_store
                        .remove_and_return(&s.session_id)
                        .map(|m| (s.session_id.clone(), m))
                })
                .count();

            // Verify the evicted session's manifest is removed
            assert!(
                manifest_store.get_manifest(&session_ids[0]).is_none(),
                "Manifest for evicted session must be removed"
            );

            // Manifest count should be MAX_SESSIONS - 1 (evicted one removed,
            // new one not yet registered)
            assert_eq!(
                manifest_store.len(),
                MAX_SESSIONS - 1,
                "Manifest store should shrink after eviction"
            );

            // Verify evicted manifest was captured for potential rollback
            assert_eq!(
                evicted_manifest_count, 1,
                "Evicted manifest must be captured for rollback"
            );
        }

        /// TCK-00384 review MAJOR 2: Manifest store cardinality is bounded
        /// under repeated over-capacity spawn/eviction churn.
        ///
        /// Performs 2x `MAX_SESSIONS` spawn cycles and verifies the manifest
        /// store never exceeds `MAX_SESSIONS` entries.
        #[test]
        fn test_bounded_manifest_cardinality_under_eviction_churn() {
            use crate::episode::CapabilityManifest;
            use crate::episode::registry::{InMemorySessionRegistry, MAX_SESSIONS};
            use crate::protocol::session_dispatch::InMemoryManifestStore;
            use crate::session::{SessionRegistry, SessionState, SessionTelemetryStore};

            let registry = Arc::new(InMemorySessionRegistry::new());
            let store = Arc::new(SessionTelemetryStore::new());
            let manifest_store = Arc::new(InMemoryManifestStore::new());

            // Perform 2x MAX_SESSIONS registration cycles
            let total_cycles = MAX_SESSIONS * 2;
            for i in 0..total_cycles {
                let sid = format!("S-CHURN-{i}");
                let session = SessionState {
                    session_id: sid.clone(),
                    work_id: format!("W-CHURN-{i}"),
                    role: 1,
                    ephemeral_handle: format!("H-CHURN-{i}"),
                    lease_id: format!("L-CHURN-{i}"),
                    policy_resolved_ref: String::new(),
                    pcac_policy: None,
                    pointer_only_waiver: None,
                    capability_manifest_hash: vec![0u8; 32],
                    episode_id: None,
                };
                let evicted = registry.register_session(session).unwrap();

                // Clean up evicted entries (simulating dispatch.rs behavior)
                for s in &evicted {
                    store.remove(&s.session_id);
                    manifest_store.remove(&s.session_id);
                }

                // Register telemetry and manifest for the new session
                let _ = store.register(&sid, 1_000_000);
                let manifest = CapabilityManifest::from_hash_with_default_allowlist(&[0u8; 32]);
                manifest_store.register(&sid, manifest);

                // Verify manifest store never exceeds MAX_SESSIONS
                assert!(
                    manifest_store.len() <= MAX_SESSIONS,
                    "Manifest store must be bounded at MAX_SESSIONS ({MAX_SESSIONS}), got {} at cycle {i}",
                    manifest_store.len()
                );
            }

            // After all cycles, store should be exactly at capacity
            assert_eq!(
                manifest_store.len(),
                MAX_SESSIONS,
                "Final manifest store size should equal MAX_SESSIONS"
            );
        }

        /// TCK-00384 review MAJOR 1: Rollback after post-start failure
        /// restores evicted manifests alongside evicted sessions and
        /// telemetry.
        #[test]
        fn test_rollback_restores_evicted_manifests() {
            use crate::episode::CapabilityManifest;
            use crate::episode::registry::InMemorySessionRegistry;
            use crate::protocol::session_dispatch::ManifestStore;
            use crate::session::{SessionRegistry, SessionState, SessionTelemetryStore};

            let registry = Arc::new(InMemorySessionRegistry::new());
            let store = Arc::new(SessionTelemetryStore::new());

            // Build a dispatcher that shares the external stores.
            // The dispatcher owns its manifest_store; we access it via
            // the `manifest_store()` accessor.
            let dispatcher = PrivilegedDispatcher::new()
                .with_session_registry(
                    Arc::clone(&registry) as Arc<dyn SessionRegistry + Send + Sync>
                )
                .with_telemetry_store(Arc::clone(&store));
            let manifest_store = Arc::clone(dispatcher.manifest_store());

            // Register a session with telemetry and manifest
            let session = SessionState {
                session_id: "S-ROLLBACK-M".to_string(),
                work_id: "W-ROLLBACK-M".to_string(),
                role: 1,
                ephemeral_handle: "H-ROLLBACK-M".to_string(),
                lease_id: "L-ROLLBACK-M".to_string(),
                policy_resolved_ref: String::new(),
                pcac_policy: None,
                pointer_only_waiver: None,
                capability_manifest_hash: vec![0u8; 32],
                episode_id: None,
            };
            registry.register_session(session).unwrap();
            store.register("S-ROLLBACK-M", 42).unwrap();
            let manifest = CapabilityManifest::from_hash_with_default_allowlist(&[0u8; 32]);
            manifest_store.register("S-ROLLBACK-M", manifest);

            // Verify manifest exists
            assert!(manifest_store.get_manifest("S-ROLLBACK-M").is_some());

            // Simulate eviction: remove from registry + telemetry + manifest
            // (this is what the session registry does during LRU eviction,
            // followed by the dispatch.rs cleanup of telemetry/manifest)
            let evicted_session_state = registry
                .remove_session("S-ROLLBACK-M")
                .unwrap()
                .expect("session should exist");
            store.remove("S-ROLLBACK-M");
            let evicted_manifest = manifest_store
                .remove_and_return("S-ROLLBACK-M")
                .expect("manifest should exist");

            assert!(
                manifest_store.get_manifest("S-ROLLBACK-M").is_none(),
                "Manifest should be removed after eviction"
            );

            // Register a NEW session that would occupy the slot
            let new_session = SessionState {
                session_id: "S-NEW-M".to_string(),
                work_id: "W-NEW-M".to_string(),
                role: 1,
                ephemeral_handle: "H-NEW-M".to_string(),
                lease_id: "L-NEW-M".to_string(),
                policy_resolved_ref: String::new(),
                pcac_policy: None,
                pointer_only_waiver: None,
                capability_manifest_hash: vec![0u8; 32],
                episode_id: None,
            };
            registry.register_session(new_session).unwrap();
            store.register("S-NEW-M", 100).unwrap();
            let new_manifest = CapabilityManifest::from_hash_with_default_allowlist(&[1u8; 32]);
            manifest_store.register("S-NEW-M", new_manifest);

            // Simulate rollback: this should remove S-NEW-M and restore
            // S-ROLLBACK-M's manifest
            let evicted_sessions = vec![evicted_session_state];
            let evicted_telem: Vec<(String, Arc<crate::session::SessionTelemetry>)> = Vec::new();
            let evicted_manifests = vec![("S-ROLLBACK-M".to_string(), evicted_manifest)];
            let evicted_brokers: Vec<(
                String,
                SharedToolBroker<crate::episode::capability::StubManifestLoader>,
            )> = Vec::new();
            let evicted_stop_conditions: Vec<(String, crate::episode::envelope::StopConditions)> =
                Vec::new();

            let result = dispatcher.rollback_spawn(
                "S-NEW-M",
                &evicted_sessions,
                &evicted_telem,
                &evicted_manifests,
                &evicted_brokers,
                &evicted_stop_conditions,
                true,
            );
            assert!(result.is_none(), "Rollback should succeed: {result:?}");

            // Verify new session's manifest is removed
            assert!(
                manifest_store.get_manifest("S-NEW-M").is_none(),
                "New session manifest must be removed by rollback"
            );

            // Verify evicted session's manifest is restored
            assert!(
                manifest_store.get_manifest("S-ROLLBACK-M").is_some(),
                "Evicted manifest must be restored after rollback"
            );

            // Verify evicted session is restored in registry
            assert!(
                registry.get_session("S-ROLLBACK-M").is_some(),
                "Evicted session must be restored in registry"
            );
        }

        /// TCK-00384 review MAJOR 1: Regression test for post-start failure
        /// paths.  The `peer_credentials` failure now returns a proper error
        /// response (not a protocol error via `?`) and performs full
        /// rollback of session, telemetry, and manifest.
        #[test]
        fn test_peer_credentials_failure_rolls_back_session_and_telemetry() {
            let store = Arc::new(SessionTelemetryStore::new());
            let dispatcher = PrivilegedDispatcher::new().with_telemetry_store(Arc::clone(&store));

            // First spawn with valid credentials to establish baseline
            let valid_ctx = ConnectionContext::privileged_session_open(Some(PeerCredentials {
                uid: 1000,
                gid: 1000,
                pid: Some(12345),
            }));

            let claim_request = ClaimWorkRequest {
                actor_id: "test-actor".to_string(),
                role: WorkRole::Implementer.into(),
                credential_signature: vec![1, 2, 3],
                nonce: vec![4, 5, 6],
            };
            let claim_frame = encode_claim_work_request(&claim_request);
            let claim_response = dispatcher.dispatch(&claim_frame, &valid_ctx).unwrap();
            let (work_id, lease_id) = match claim_response {
                PrivilegedResponse::ClaimWork(resp) => (resp.work_id, resp.lease_id),
                _ => panic!("Expected ClaimWork response"),
            };

            // SpawnEpisode WITHOUT peer credentials
            let no_creds_ctx = ConnectionContext::privileged_session_open(None);
            let spawn_request = SpawnEpisodeRequest {
                workspace_root: test_workspace_root(),
                work_id,
                role: WorkRole::Implementer.into(),
                lease_id: Some(lease_id),
                adapter_profile_hash: None,
                max_episodes: None,
                escalation_predicate: None,
                permeability_receipt_hash: None,
            };
            let spawn_frame = encode_spawn_episode_request(&spawn_request);
            let result = dispatcher.dispatch(&spawn_frame, &no_creds_ctx);

            // The spawn should fail
            match result {
                Ok(PrivilegedResponse::Error(err)) => {
                    assert!(
                        err.message.contains("peer credentials"),
                        "Error should mention peer credentials: {}",
                        err.message
                    );
                },
                Ok(other) => {
                    panic!("Expected error response for missing peer credentials, got: {other:?}")
                },
                Err(e) => panic!("Expected Ok(Error), got Err: {e:?}"),
            }

            // Verify session was rolled back: no sessions should remain
            // in the telemetry store
            assert_eq!(
                store.len(),
                0,
                "Telemetry must be cleaned up after peer credentials failure rollback"
            );
        }

        /// TCK-00384 review MAJOR 1: Regression test for the unified
        /// `rollback_spawn_with_episode_stop` helper.
        ///
        /// Verifies that the helper correctly rolls back session, telemetry,
        /// and manifest stores even when no episode exists (the `None` path
        /// in unit tests without a Tokio runtime).  This exercises the
        /// store-cleanup portion of the unified rollback and proves it is
        /// functionally equivalent to the previous separate calls.
        #[test]
        fn test_unified_rollback_helper_cleans_up_all_stores() {
            use crate::episode::CapabilityManifest;
            use crate::episode::registry::InMemorySessionRegistry;
            use crate::protocol::session_dispatch::ManifestStore;
            use crate::session::{SessionRegistry, SessionState, SessionTelemetryStore};

            let registry = Arc::new(InMemorySessionRegistry::new());
            let store = Arc::new(SessionTelemetryStore::new());

            let dispatcher = PrivilegedDispatcher::new()
                .with_session_registry(
                    Arc::clone(&registry) as Arc<dyn SessionRegistry + Send + Sync>
                )
                .with_telemetry_store(Arc::clone(&store));
            let manifest_store = Arc::clone(dispatcher.manifest_store());

            // Register a session with telemetry and manifest
            let session = SessionState {
                session_id: "S-UNIFIED-001".to_string(),
                work_id: "W-UNIFIED".to_string(),
                role: 1,
                ephemeral_handle: "H-UNIFIED".to_string(),
                lease_id: "L-UNIFIED".to_string(),
                policy_resolved_ref: String::new(),
                pcac_policy: None,
                pointer_only_waiver: None,
                capability_manifest_hash: vec![0u8; 32],
                episode_id: None,
            };
            registry.register_session(session).unwrap();
            store.register("S-UNIFIED-001", 42).unwrap();
            let manifest = CapabilityManifest::from_hash_with_default_allowlist(&[0u8; 32]);
            manifest_store.register("S-UNIFIED-001", manifest);

            // Verify all stores have the entry
            assert!(registry.get_session("S-UNIFIED-001").is_some());
            assert!(store.get("S-UNIFIED-001").is_some());
            assert!(manifest_store.get_manifest("S-UNIFIED-001").is_some());

            // Call unified rollback with episode_id = None (test mode)
            let no_evicted_sessions: Vec<SessionState> = Vec::new();
            let no_evicted_telemetry: Vec<(String, Arc<crate::session::SessionTelemetry>)> =
                Vec::new();
            let no_evicted_manifests: Vec<(String, Arc<crate::episode::CapabilityManifest>)> =
                Vec::new();
            let no_evicted_brokers: Vec<(
                String,
                SharedToolBroker<crate::episode::capability::StubManifestLoader>,
            )> = Vec::new();
            let no_evicted_stop_conditions: Vec<(
                String,
                crate::episode::envelope::StopConditions,
            )> = Vec::new();

            let result = dispatcher.rollback_spawn_with_episode_stop(
                None,
                "S-UNIFIED-001",
                &no_evicted_sessions,
                &no_evicted_telemetry,
                &no_evicted_manifests,
                &no_evicted_brokers,
                &no_evicted_stop_conditions,
                999_000,
                "test context",
            );
            assert!(
                result.is_none(),
                "Unified rollback should succeed: {result:?}"
            );

            // Verify all stores are cleaned up
            assert!(
                registry.get_session("S-UNIFIED-001").is_none(),
                "Session must be removed by unified rollback"
            );
            assert!(
                store.get("S-UNIFIED-001").is_none(),
                "Telemetry must be removed by unified rollback"
            );
            assert!(
                manifest_store.get_manifest("S-UNIFIED-001").is_none(),
                "Manifest must be removed by unified rollback"
            );
        }

        /// TCK-00384 review MAJOR 1: Regression test verifying the unified
        /// rollback helper restores evicted entries during post-start
        /// failure recovery.
        ///
        /// Fills the registry to capacity, registers one more (evicting the
        /// oldest), then calls `rollback_spawn_with_episode_stop` and
        /// verifies the evicted session, telemetry, AND manifest are all
        /// restored.
        #[test]
        fn test_unified_rollback_restores_evicted_entries() {
            use crate::episode::CapabilityManifest;
            use crate::episode::registry::{InMemorySessionRegistry, MAX_SESSIONS};
            use crate::protocol::session_dispatch::ManifestStore;
            use crate::session::{SessionRegistry, SessionState, SessionTelemetryStore};

            let registry = Arc::new(InMemorySessionRegistry::new());
            let store = Arc::new(SessionTelemetryStore::new());

            let dispatcher = PrivilegedDispatcher::new()
                .with_session_registry(
                    Arc::clone(&registry) as Arc<dyn SessionRegistry + Send + Sync>
                )
                .with_telemetry_store(Arc::clone(&store));
            let manifest_store = Arc::clone(dispatcher.manifest_store());

            // Fill to capacity with sessions, telemetry, and manifests
            for i in 0..MAX_SESSIONS {
                let sid = format!("S-{i}");
                let session = SessionState {
                    session_id: sid.clone(),
                    work_id: format!("W-{i}"),
                    role: 1,
                    ephemeral_handle: format!("H-{i}"),
                    lease_id: format!("L-{i}"),
                    policy_resolved_ref: String::new(),
                    pcac_policy: None,
                    pointer_only_waiver: None,
                    capability_manifest_hash: vec![0u8; 32],
                    episode_id: None,
                };
                registry.register_session(session).unwrap();
                store.register(&sid, i as u64).unwrap();
                let manifest = CapabilityManifest::from_hash_with_default_allowlist(&[0u8; 32]);
                manifest_store.register(&sid, manifest);
            }

            // Register one more, evicting S-0
            let new_session = SessionState {
                session_id: "S-NEW".to_string(),
                work_id: "W-NEW".to_string(),
                role: 1,
                ephemeral_handle: "H-NEW".to_string(),
                lease_id: "L-NEW".to_string(),
                policy_resolved_ref: String::new(),
                pcac_policy: None,
                pointer_only_waiver: None,
                capability_manifest_hash: vec![0u8; 32],
                episode_id: None,
            };
            let evicted = registry.register_session(new_session).unwrap();
            assert_eq!(evicted.len(), 1);
            assert_eq!(evicted[0].session_id, "S-0");

            // Capture evicted telemetry and manifests (as dispatch.rs does)
            let evicted_telemetry: Vec<_> = evicted
                .iter()
                .filter_map(|s| {
                    store
                        .remove_and_return(&s.session_id)
                        .map(|t| (s.session_id.clone(), t))
                })
                .collect();
            let evicted_manifests: Vec<_> = evicted
                .iter()
                .filter_map(|s| {
                    manifest_store
                        .remove_and_return(&s.session_id)
                        .map(|m| (s.session_id.clone(), m))
                })
                .collect();
            let evicted_brokers: Vec<(
                String,
                SharedToolBroker<crate::episode::capability::StubManifestLoader>,
            )> = Vec::new();
            assert_eq!(evicted_telemetry.len(), 1);
            assert_eq!(evicted_manifests.len(), 1);
            let evicted_stop_conditions: Vec<(String, crate::episode::envelope::StopConditions)> =
                Vec::new();

            // Register telemetry and manifest for the new session
            store.register("S-NEW", 999).unwrap();
            let new_manifest = CapabilityManifest::from_hash_with_default_allowlist(&[1u8; 32]);
            manifest_store.register("S-NEW", new_manifest);

            // Call unified rollback (simulating post-start failure)
            let result = dispatcher.rollback_spawn_with_episode_stop(
                None,
                "S-NEW",
                &evicted,
                &evicted_telemetry,
                &evicted_manifests,
                &evicted_brokers,
                &evicted_stop_conditions,
                999_000,
                "test eviction restore",
            );
            assert!(
                result.is_none(),
                "Unified rollback should succeed: {result:?}"
            );

            // The new session must be gone
            assert!(
                registry.get_session("S-NEW").is_none(),
                "Failed new session must not exist after rollback"
            );
            assert!(
                store.get("S-NEW").is_none(),
                "Failed new session telemetry must not exist after rollback"
            );
            assert!(
                manifest_store.get_manifest("S-NEW").is_none(),
                "Failed new session manifest must not exist after rollback"
            );

            // The evicted session must be restored
            assert!(
                registry.get_session("S-0").is_some(),
                "Evicted session must be restored by unified rollback"
            );
            assert!(
                store.get("S-0").is_some(),
                "Evicted telemetry must be restored by unified rollback"
            );
            assert!(
                manifest_store.get_manifest("S-0").is_some(),
                "Evicted manifest must be restored by unified rollback"
            );
        }

        /// TCK-00351 BLOCKER 1: Regression for non-atomic stop-condition
        /// rollback.
        ///
        /// Simulates: spawn -> eviction -> post-eviction failure -> rollback,
        /// and verifies the evicted session's original stop conditions are
        /// restored.
        #[test]
        fn test_failed_spawn_restores_evicted_stop_conditions() {
            use crate::episode::envelope::StopConditions;
            use crate::episode::registry::{InMemorySessionRegistry, MAX_SESSIONS};
            use crate::session::{
                SessionRegistry, SessionState, SessionStopConditionsStore, SessionTelemetryStore,
            };

            let registry = Arc::new(InMemorySessionRegistry::new());
            let telemetry = Arc::new(SessionTelemetryStore::new());
            let stop_store = Arc::new(SessionStopConditionsStore::new());

            let dispatcher = PrivilegedDispatcher::new()
                .with_session_registry(
                    Arc::clone(&registry) as Arc<dyn SessionRegistry + Send + Sync>
                )
                .with_telemetry_store(Arc::clone(&telemetry))
                .with_stop_conditions_store(Arc::clone(&stop_store));

            let original_conditions = StopConditions {
                max_episodes: 3,
                escalation_predicate: "severity>=high".to_string(),
                goal_predicate: String::new(),
                failure_predicate: String::new(),
            };

            // Fill to capacity with sessions, telemetry, and stop conditions.
            for i in 0..MAX_SESSIONS {
                let sid = format!("S-{i}");
                let session = SessionState {
                    session_id: sid.clone(),
                    work_id: format!("W-{i}"),
                    role: 1,
                    ephemeral_handle: format!("H-{i}"),
                    lease_id: format!("L-{i}"),
                    policy_resolved_ref: String::new(),
                    pcac_policy: None,
                    pointer_only_waiver: None,
                    capability_manifest_hash: vec![0u8; 32],
                    episode_id: None,
                };
                registry.register_session(session).unwrap();
                telemetry.register(&sid, i as u64).unwrap();
                let conditions = if i == 0 {
                    original_conditions.clone()
                } else {
                    StopConditions::max_episodes(1)
                };
                stop_store.register(&sid, conditions).unwrap();
            }

            // Register one more session to evict S-0.
            let new_session = SessionState {
                session_id: "S-NEW-STOP".to_string(),
                work_id: "W-NEW-STOP".to_string(),
                role: 1,
                ephemeral_handle: "H-NEW-STOP".to_string(),
                lease_id: "L-NEW-STOP".to_string(),
                policy_resolved_ref: String::new(),
                pcac_policy: None,
                pointer_only_waiver: None,
                capability_manifest_hash: vec![0u8; 32],
                episode_id: None,
            };
            let evicted = registry.register_session(new_session).unwrap();
            assert_eq!(evicted.len(), 1);
            assert_eq!(evicted[0].session_id, "S-0");

            // Capture evicted telemetry and stop conditions as spawn path does.
            let evicted_telemetry: Vec<_> = evicted
                .iter()
                .filter_map(|s| {
                    telemetry
                        .remove_and_return(&s.session_id)
                        .map(|t| (s.session_id.clone(), t))
                })
                .collect();
            let evicted_stop_conditions: Vec<_> = evicted
                .iter()
                .filter_map(|s| {
                    stop_store
                        .remove_and_return(&s.session_id)
                        .map(|c| (s.session_id.clone(), c))
                })
                .collect();
            assert_eq!(evicted_telemetry.len(), 1);
            assert_eq!(evicted_stop_conditions.len(), 1);

            // Register resources for the newly spawned session.
            telemetry.register("S-NEW-STOP", 999).unwrap();
            stop_store
                .register("S-NEW-STOP", StopConditions::max_episodes(1))
                .unwrap();

            // Simulate spawn failure rollback.
            let evicted_manifests: Vec<(String, Arc<crate::episode::CapabilityManifest>)> =
                Vec::new();
            let evicted_brokers: Vec<(
                String,
                SharedToolBroker<crate::episode::capability::StubManifestLoader>,
            )> = Vec::new();
            let result = dispatcher.rollback_spawn(
                "S-NEW-STOP",
                &evicted,
                &evicted_telemetry,
                &evicted_manifests,
                &evicted_brokers,
                &evicted_stop_conditions,
                false,
            );
            assert!(result.is_none(), "Rollback should succeed: {result:?}");

            // New session should be fully removed.
            assert!(registry.get_session("S-NEW-STOP").is_none());
            assert!(telemetry.get("S-NEW-STOP").is_none());
            assert!(stop_store.get("S-NEW-STOP").is_none());

            // Evicted session and its original stop conditions should be restored.
            assert!(registry.get_session("S-0").is_some());
            let restored = stop_store
                .get("S-0")
                .expect("evicted stop conditions must be restored");
            assert_eq!(restored.max_episodes, original_conditions.max_episodes);
            assert_eq!(
                restored.escalation_predicate,
                original_conditions.escalation_predicate
            );
        }

        /// TCK-00351 BLOCKER 2: caller tampering with permissive stop values
        /// (`max_episodes=0`) is rejected by policy-floor validation.
        #[test]
        fn test_spawn_rejects_untrusted_max_episodes_zero() {
            let dispatcher = PrivilegedDispatcher::new();
            let ctx = ConnectionContext::privileged_session_open(Some(PeerCredentials {
                uid: 1000,
                gid: 1000,
                pid: Some(12345),
            }));

            let claim_request = ClaimWorkRequest {
                actor_id: "tamper-actor".to_string(),
                role: WorkRole::Implementer.into(),
                credential_signature: vec![1, 2, 3],
                nonce: vec![4, 5, 6],
            };
            let claim_frame = encode_claim_work_request(&claim_request);
            let claim_response = dispatcher.dispatch(&claim_frame, &ctx).unwrap();
            let (work_id, lease_id) = match claim_response {
                PrivilegedResponse::ClaimWork(resp) => (resp.work_id, resp.lease_id),
                other => panic!("expected ClaimWork response, got {other:?}"),
            };

            let tampered_spawn = SpawnEpisodeRequest {
                workspace_root: test_workspace_root(),
                work_id: work_id.clone(),
                role: WorkRole::Implementer.into(),
                lease_id: Some(lease_id),
                adapter_profile_hash: None,
                max_episodes: Some(0),
                escalation_predicate: None,
                permeability_receipt_hash: None,
            };
            let frame = encode_spawn_episode_request(&tampered_spawn);
            let response = dispatcher.dispatch(&frame, &ctx).unwrap();

            match response {
                PrivilegedResponse::Error(err) => {
                    assert_eq!(
                        err.code,
                        PrivilegedErrorCode::CapabilityRequestRejected as i32
                    );
                    assert!(
                        err.message.contains("max_episodes=0"),
                        "tamper rejection should mention max_episodes floor: {}",
                        err.message
                    );
                },
                other => panic!("expected tamper rejection, got {other:?}"),
            }

            assert!(
                dispatcher
                    .session_registry()
                    .get_session_by_work_id(&work_id)
                    .is_none(),
                "rejected tampered spawn must not register a session"
            );
        }

        /// SECURITY MAJOR-1: oversized escalation predicates are rejected
        /// before persistence.
        #[test]
        fn test_spawn_rejects_oversized_escalation_predicate() {
            let dispatcher = PrivilegedDispatcher::new();
            let ctx = ConnectionContext::privileged_session_open(Some(PeerCredentials {
                uid: 1000,
                gid: 1000,
                pid: Some(12345),
            }));

            let claim_request = ClaimWorkRequest {
                actor_id: "tamper-escalation".to_string(),
                role: WorkRole::Implementer.into(),
                credential_signature: vec![1, 2, 3],
                nonce: vec![4, 5, 6],
            };
            let claim_frame = encode_claim_work_request(&claim_request);
            let claim_response = dispatcher.dispatch(&claim_frame, &ctx).unwrap();
            let (work_id, lease_id) = match claim_response {
                PrivilegedResponse::ClaimWork(resp) => (resp.work_id, resp.lease_id),
                other => panic!("expected ClaimWork response, got {other:?}"),
            };

            let oversized_predicate = "x".repeat(MAX_ESCALATION_PREDICATE_LEN + 1);
            let spawn_request = SpawnEpisodeRequest {
                workspace_root: test_workspace_root(),
                work_id: work_id.clone(),
                role: WorkRole::Implementer.into(),
                lease_id: Some(lease_id),
                adapter_profile_hash: None,
                max_episodes: None,
                escalation_predicate: Some(oversized_predicate),
                permeability_receipt_hash: None,
            };
            let frame = encode_spawn_episode_request(&spawn_request);
            let response = dispatcher.dispatch(&frame, &ctx).unwrap();

            match response {
                PrivilegedResponse::Error(err) => {
                    assert_eq!(
                        err.code,
                        PrivilegedErrorCode::CapabilityRequestRejected as i32
                    );
                    assert!(
                        err.message
                            .contains("escalation_predicate exceeds maximum length"),
                        "rejection should mention escalation_predicate bound: {}",
                        err.message
                    );
                },
                other => panic!("expected oversized escalation rejection, got {other:?}"),
            }

            assert!(
                dispatcher
                    .session_registry()
                    .get_session_by_work_id(&work_id)
                    .is_none(),
                "rejected oversized spawn must not register a session"
            );
        }
    }

    // ========================================================================
    // TCK-00395: Work lifecycle observability ledger events
    // ========================================================================
    mod tck_00395_work_lifecycle_observability {
        use super::*;

        /// TCK-00395: `ClaimWork` emits `WorkTransitioned`(Open -> Claimed)
        /// event.
        #[test]
        fn claim_work_emits_work_transitioned_open_to_claimed() {
            let dispatcher = PrivilegedDispatcher::new();
            let ctx = ConnectionContext::privileged_session_open(Some(PeerCredentials {
                uid: 1000,
                gid: 1000,
                pid: Some(12345),
            }));

            let request = ClaimWorkRequest {
                actor_id: "test-actor".to_string(),
                role: WorkRole::Implementer.into(),
                credential_signature: vec![1, 2, 3],
                nonce: vec![4, 5, 6],
            };
            let frame = encode_claim_work_request(&request);

            let response = dispatcher.dispatch(&frame, &ctx).unwrap();
            let work_id = match response {
                PrivilegedResponse::ClaimWork(ref resp) => resp.work_id.clone(),
                _ => panic!("Expected ClaimWork response"),
            };

            // Verify WorkTransitioned event was emitted to ledger
            let events = dispatcher.event_emitter.get_events_by_work_id(&work_id);
            let transition_events: Vec<_> = events
                .iter()
                .filter(|e| e.event_type == "work_transitioned")
                .collect();

            assert_eq!(
                transition_events.len(),
                1,
                "Expected exactly 1 WorkTransitioned event, got {}",
                transition_events.len()
            );

            let event = transition_events[0];
            assert_eq!(event.work_id, work_id);

            // Parse payload to verify transition details
            let payload: serde_json::Value =
                serde_json::from_slice(&event.payload).expect("valid JSON payload");
            assert_eq!(payload["event_type"], "work_transitioned");
            assert_eq!(payload["from_state"], "Open");
            assert_eq!(payload["to_state"], "Claimed");
            assert_eq!(payload["rationale_code"], "work_claimed_via_ipc");
            assert_eq!(payload["previous_transition_count"], 0);
            assert!(event.timestamp_ns > 0, "timestamp must be non-zero");
        }

        /// TCK-00395: `SpawnEpisode` emits `WorkTransitioned`(Claimed ->
        /// `InProgress`) event.
        #[test]
        fn spawn_episode_emits_work_transitioned_claimed_to_in_progress() {
            let dispatcher = PrivilegedDispatcher::new();
            let ctx = ConnectionContext::privileged_session_open(Some(PeerCredentials {
                uid: 1000,
                gid: 1000,
                pid: Some(12345),
            }));

            // Step 1: ClaimWork to establish policy resolution
            let claim_request = ClaimWorkRequest {
                actor_id: "test-actor".to_string(),
                role: WorkRole::Implementer.into(),
                credential_signature: vec![1, 2, 3],
                nonce: vec![4, 5, 6],
            };
            let claim_frame = encode_claim_work_request(&claim_request);
            let claim_response = dispatcher.dispatch(&claim_frame, &ctx).unwrap();

            let (work_id, lease_id) = match claim_response {
                PrivilegedResponse::ClaimWork(resp) => (resp.work_id, resp.lease_id),
                _ => panic!("Expected ClaimWork response"),
            };

            // Step 2: SpawnEpisode
            let spawn_request = SpawnEpisodeRequest {
                workspace_root: test_workspace_root(),
                work_id: work_id.clone(),
                role: WorkRole::Implementer.into(),
                lease_id: Some(lease_id),
                adapter_profile_hash: None,
                max_episodes: None,
                escalation_predicate: None,
                permeability_receipt_hash: None,
            };
            let spawn_frame = encode_spawn_episode_request(&spawn_request);
            let spawn_response = dispatcher.dispatch(&spawn_frame, &ctx).unwrap();
            assert!(
                matches!(spawn_response, PrivilegedResponse::SpawnEpisode(_)),
                "Expected SpawnEpisode response"
            );

            // Verify WorkTransitioned events
            let events = dispatcher.event_emitter.get_events_by_work_id(&work_id);
            let transition_events: Vec<_> = events
                .iter()
                .filter(|e| e.event_type == "work_transitioned")
                .collect();

            assert_eq!(
                transition_events.len(),
                2,
                "Expected 2 WorkTransitioned events (Open->Claimed, Claimed->InProgress), got {}",
                transition_events.len()
            );

            // First transition: Open -> Claimed
            let first_payload: serde_json::Value =
                serde_json::from_slice(&transition_events[0].payload).expect("valid JSON");
            assert_eq!(first_payload["from_state"], "Open");
            assert_eq!(first_payload["to_state"], "Claimed");
            assert_eq!(first_payload["rationale_code"], "work_claimed_via_ipc");
            assert_eq!(first_payload["previous_transition_count"], 0);

            // Second transition: Claimed -> InProgress
            let second_payload: serde_json::Value =
                serde_json::from_slice(&transition_events[1].payload).expect("valid JSON");
            assert_eq!(second_payload["from_state"], "Claimed");
            assert_eq!(second_payload["to_state"], "InProgress");
            assert_eq!(second_payload["rationale_code"], "episode_spawned_via_ipc");
            assert_eq!(second_payload["previous_transition_count"], 1);

            let session_started_events: Vec<_> = events
                .iter()
                .filter(|e| e.event_type == "session_started")
                .collect();
            assert_eq!(
                session_started_events.len(),
                1,
                "Expected exactly 1 SessionStarted event"
            );
            let session_payload: serde_json::Value =
                serde_json::from_slice(&session_started_events[0].payload).expect("valid JSON");
            let expected_adapter_hash = apm2_core::fac::builtin_profiles::claude_code_profile()
                .compute_cas_hash()
                .expect("builtin adapter hash should compute");
            let expected_role_hash = fac_workobject_implementor_v2_role_contract()
                .compute_cas_hash()
                .expect("builtin role spec hash should compute");
            assert_eq!(
                session_payload["adapter_profile_hash"],
                hex::encode(expected_adapter_hash)
            );
            assert_eq!(
                session_payload["role_spec_hash"],
                hex::encode(expected_role_hash)
            );
            assert!(
                session_payload.get("waiver_id").is_none(),
                "implementer path must not emit WVR-0002 waiver"
            );
            assert!(
                session_payload.get("role_spec_hash_absent").is_none(),
                "implementer path must not mark role_spec_hash_absent"
            );
        }

        /// TCK-00395: `WorkTransitioned` events use domain-separated
        /// signatures.
        #[test]
        fn work_transitioned_uses_domain_separated_signatures() {
            let emitter = StubLedgerEventEmitter::new();
            let result = emitter.emit_work_transitioned(&WorkTransition {
                work_id: "W-TEST-001",
                from_state: "Open",
                to_state: "Claimed",
                rationale_code: "work_claimed_via_ipc",
                previous_transition_count: 0,
                actor_id: "uid:1000",
                timestamp_ns: 1_000_000_000,
            });

            assert!(result.is_ok(), "emit_work_transitioned should succeed");
            let event = result.unwrap();

            assert_eq!(event.event_type, "work_transitioned");
            assert_eq!(event.work_id, "W-TEST-001");
            assert_eq!(event.actor_id, "uid:1000");
            assert!(!event.signature.is_empty(), "signature must be non-empty");
            assert_eq!(
                event.signature.len(),
                64,
                "Ed25519 signature must be 64 bytes"
            );
        }

        /// TCK-00395: `SessionTerminated` event uses domain-separated
        /// signatures.
        #[test]
        fn session_terminated_uses_domain_separated_signatures() {
            let emitter = StubLedgerEventEmitter::new();
            let result = emitter.emit_session_terminated(
                "SESS-001",
                "W-TEST-001",
                0,
                "completed_normally",
                "uid:1000",
                1_000_000_000,
            );

            assert!(result.is_ok(), "emit_session_terminated should succeed");
            let event = result.unwrap();

            assert_eq!(event.event_type, "session_terminated");
            assert_eq!(event.work_id, "W-TEST-001");
            assert_eq!(event.actor_id, "uid:1000");
            assert!(!event.signature.is_empty(), "signature must be non-empty");
            assert_eq!(
                event.signature.len(),
                64,
                "Ed25519 signature must be 64 bytes"
            );

            // Parse payload to verify all fields present
            let payload: serde_json::Value =
                serde_json::from_slice(&event.payload).expect("valid JSON payload");
            assert_eq!(payload["event_type"], "session_terminated");
            assert_eq!(payload["session_id"], "SESS-001");
            assert_eq!(payload["work_id"], "W-TEST-001");
            assert_eq!(payload["exit_code"], 0);
            assert_eq!(payload["termination_reason"], "completed_normally");
            assert_eq!(payload["actor_id"], "uid:1000");
            assert_eq!(payload["timestamp_ns"], 1_000_000_000);
        }

        /// TCK-00395: Full lifecycle `ClaimWork` -> `SpawnEpisode` -> verify
        /// all event types in ledger.
        #[test]
        fn full_lifecycle_claim_spawn_verify_all_events() {
            let dispatcher = PrivilegedDispatcher::new();
            let ctx = ConnectionContext::privileged_session_open(Some(PeerCredentials {
                uid: 1000,
                gid: 1000,
                pid: Some(12345),
            }));

            // Step 1: ClaimWork
            let claim_request = ClaimWorkRequest {
                actor_id: "test-actor".to_string(),
                role: WorkRole::Implementer.into(),
                credential_signature: vec![1, 2, 3],
                nonce: vec![4, 5, 6],
            };
            let claim_frame = encode_claim_work_request(&claim_request);
            let claim_response = dispatcher.dispatch(&claim_frame, &ctx).unwrap();

            let (work_id, lease_id) = match claim_response {
                PrivilegedResponse::ClaimWork(resp) => (resp.work_id, resp.lease_id),
                _ => panic!("Expected ClaimWork response"),
            };

            // Step 2: SpawnEpisode
            let spawn_request = SpawnEpisodeRequest {
                workspace_root: test_workspace_root(),
                work_id: work_id.clone(),
                role: WorkRole::Implementer.into(),
                lease_id: Some(lease_id),
                adapter_profile_hash: None,
                max_episodes: None,
                escalation_predicate: None,
                permeability_receipt_hash: None,
            };
            let spawn_frame = encode_spawn_episode_request(&spawn_request);
            let spawn_response = dispatcher.dispatch(&spawn_frame, &ctx).unwrap();
            assert!(
                matches!(spawn_response, PrivilegedResponse::SpawnEpisode(_)),
                "Expected SpawnEpisode response"
            );

            // Step 3: Verify full event chain in ledger
            let events = dispatcher.event_emitter.get_events_by_work_id(&work_id);

            // Should have: work_claimed, work_transitioned(Open->Claimed),
            //              session_started, work_transitioned(Claimed->InProgress)
            let event_types: Vec<&str> = events.iter().map(|e| e.event_type.as_str()).collect();
            assert!(
                event_types.contains(&"work_claimed"),
                "Expected work_claimed event in ledger, got: {event_types:?}"
            );
            assert!(
                event_types.contains(&"session_started"),
                "Expected session_started event in ledger, got: {event_types:?}"
            );
            assert!(
                event_types.contains(&"work_transitioned"),
                "Expected work_transitioned event in ledger, got: {event_types:?}"
            );

            // Verify we have exactly 2 work_transitioned events
            let transition_count = events
                .iter()
                .filter(|e| e.event_type == "work_transitioned")
                .count();
            assert_eq!(
                transition_count, 2,
                "Expected 2 work_transitioned events, got {transition_count}"
            );

            // Verify HTF-compliant timestamps (all non-zero)
            for event in &events {
                assert!(
                    event.timestamp_ns > 0,
                    "Event {} should have non-zero HTF timestamp",
                    event.event_type
                );
            }

            // Verify all events are signed (non-empty signatures)
            for event in &events {
                assert!(
                    !event.signature.is_empty(),
                    "Event {} should have non-empty signature",
                    event.event_type
                );
            }
        }

        /// TCK-00395: `WorkTransitioned` and `SessionTerminated` domain
        /// prefixes are unique.
        #[test]
        fn domain_prefixes_are_unique() {
            // All relevant domain prefixes must be unique
            let prefixes: Vec<&[u8]> = vec![
                WORK_CLAIMED_DOMAIN_PREFIX,
                WORK_TRANSITIONED_DOMAIN_PREFIX,
                DEFECT_RECORDED_DOMAIN_PREFIX,
                EPISODE_EVENT_DOMAIN_PREFIX,
                EPISODE_RUN_ATTRIBUTED_PREFIX,
                GATE_LEASE_ISSUED_LEDGER_DOMAIN_PREFIX,
                SESSION_TERMINATED_LEDGER_DOMAIN_PREFIX,
            ];

            for (i, prefix_a) in prefixes.iter().enumerate() {
                for (j, prefix_b) in prefixes.iter().enumerate() {
                    if i != j {
                        assert_ne!(
                            prefix_a, prefix_b,
                            "Domain prefixes must be unique to prevent replay attacks"
                        );
                    }
                }
            }

            // Verify prefixes have the correct format
            for prefix in &prefixes {
                let prefix_str = std::str::from_utf8(prefix).expect("Prefix must be valid UTF-8");
                assert!(
                    prefix_str.starts_with("apm2.event."),
                    "Prefix must start with 'apm2.event.': {prefix_str}"
                );
                assert!(
                    prefix_str.ends_with(':'),
                    "Prefix must end with ':': {prefix_str}"
                );
            }
        }

        /// TCK-00395: `WorkTransitioned` events are queryable by `work_id`.
        #[test]
        fn work_transitioned_events_queryable_by_work_id() {
            let emitter = StubLedgerEventEmitter::new();

            // Emit two transition events for the same work_id
            emitter
                .emit_work_transitioned(&WorkTransition {
                    work_id: "W-QUERY-001",
                    from_state: "Open",
                    to_state: "Claimed",
                    rationale_code: "work_claimed_via_ipc",
                    previous_transition_count: 0,
                    actor_id: "uid:1000",
                    timestamp_ns: 1_000_000_000,
                })
                .unwrap();

            emitter
                .emit_work_transitioned(&WorkTransition {
                    work_id: "W-QUERY-001",
                    from_state: "Claimed",
                    to_state: "InProgress",
                    rationale_code: "episode_spawned_via_ipc",
                    previous_transition_count: 1,
                    actor_id: "uid:1000",
                    timestamp_ns: 2_000_000_000,
                })
                .unwrap();

            // Emit a transition for a different work_id
            emitter
                .emit_work_transitioned(&WorkTransition {
                    work_id: "W-QUERY-002",
                    from_state: "Open",
                    to_state: "Claimed",
                    rationale_code: "work_claimed_via_ipc",
                    previous_transition_count: 0,
                    actor_id: "uid:2000",
                    timestamp_ns: 3_000_000_000,
                })
                .unwrap();

            // Query by first work_id - should get 2 events
            let events = emitter.get_events_by_work_id("W-QUERY-001");
            assert_eq!(
                events.len(),
                2,
                "Expected 2 events for W-QUERY-001, got {}",
                events.len()
            );

            // Query by second work_id - should get 1 event
            let events = emitter.get_events_by_work_id("W-QUERY-002");
            assert_eq!(
                events.len(),
                1,
                "Expected 1 event for W-QUERY-002, got {}",
                events.len()
            );
        }

        // ================================================================
        // TCK-00395 v2: Regression tests for review findings
        // ================================================================

        /// FIX-BLOCKER: `EndSession` handler emits `SessionTerminated` ledger
        /// event.
        ///
        /// Integration test: `ClaimWork` -> `SpawnEpisode` -> `EndSession` ->
        /// verify `session_terminated` is persisted.
        #[test]
        fn end_session_emits_session_terminated_event() {
            let dispatcher = PrivilegedDispatcher::new();
            let ctx = ConnectionContext::privileged_session_open(Some(PeerCredentials {
                uid: 1000,
                gid: 1000,
                pid: Some(12345),
            }));

            // Step 1: ClaimWork
            let claim_request = ClaimWorkRequest {
                actor_id: "test-actor".to_string(),
                role: WorkRole::Implementer.into(),
                credential_signature: vec![1, 2, 3],
                nonce: vec![4, 5, 6],
            };
            let claim_frame = encode_claim_work_request(&claim_request);
            let claim_response = dispatcher.dispatch(&claim_frame, &ctx).unwrap();
            let (work_id, lease_id) = match claim_response {
                PrivilegedResponse::ClaimWork(resp) => (resp.work_id, resp.lease_id),
                _ => panic!("Expected ClaimWork response"),
            };

            // Step 2: SpawnEpisode
            let spawn_request = SpawnEpisodeRequest {
                workspace_root: test_workspace_root(),
                work_id: work_id.clone(),
                role: WorkRole::Implementer.into(),
                lease_id: Some(lease_id),
                adapter_profile_hash: None,
                max_episodes: None,
                escalation_predicate: None,
                permeability_receipt_hash: None,
            };
            let spawn_frame = encode_spawn_episode_request(&spawn_request);
            let spawn_response = dispatcher.dispatch(&spawn_frame, &ctx).unwrap();
            let session_id = match spawn_response {
                PrivilegedResponse::SpawnEpisode(ref resp) => resp.session_id.clone(),
                _ => panic!("Expected SpawnEpisode response"),
            };

            // Step 3: EndSession
            let end_request = EndSessionRequest {
                session_id: session_id.clone(),
                reason: "completed_normally".to_string(),
                outcome: TerminationOutcome::Success as i32,
            };
            let end_frame = encode_end_session_request(&end_request);
            let end_response = dispatcher.dispatch(&end_frame, &ctx).unwrap();
            match end_response {
                PrivilegedResponse::EndSession(resp) => {
                    assert_eq!(resp.session_id, session_id);
                },
                other => panic!("Expected EndSession response, got: {other:?}"),
            }

            // Step 4: Verify session_terminated event in ledger
            let events = dispatcher.event_emitter.get_events_by_work_id(&work_id);
            let terminated_events: Vec<_> = events
                .iter()
                .filter(|e| e.event_type == "session_terminated")
                .collect();
            assert_eq!(
                terminated_events.len(),
                1,
                "Expected exactly 1 session_terminated event, got {}",
                terminated_events.len()
            );

            let event = terminated_events[0];
            let payload: serde_json::Value =
                serde_json::from_slice(&event.payload).expect("valid JSON payload");
            assert_eq!(payload["event_type"], "session_terminated");
            assert_eq!(payload["session_id"], session_id);
            assert_eq!(payload["work_id"], work_id);
            assert_eq!(payload["exit_code"], 0);
            assert_eq!(payload["termination_reason"], "completed_normally");
        }

        /// Quality BLOCKER 2: `EndSession` does NOT emit
        /// `WorkTransitioned(InProgress -> Completed)`.
        ///
        /// The `InProgress` -> `Completed` transition violates core
        /// work-state rules (`state.rs`). Work completion belongs to
        /// gate orchestration, not `EndSession`. `EndSession` only emits
        /// `session_terminated`.
        #[test]
        fn end_session_success_does_not_emit_completed_transition() {
            let dispatcher = PrivilegedDispatcher::new();
            let ctx = ConnectionContext::privileged_session_open(Some(PeerCredentials {
                uid: 1000,
                gid: 1000,
                pid: Some(12345),
            }));

            // ClaimWork + SpawnEpisode
            let claim_request = ClaimWorkRequest {
                actor_id: "test-actor".to_string(),
                role: WorkRole::Implementer.into(),
                credential_signature: vec![1, 2, 3],
                nonce: vec![4, 5, 6],
            };
            let claim_frame = encode_claim_work_request(&claim_request);
            let claim_response = dispatcher.dispatch(&claim_frame, &ctx).unwrap();
            let (work_id, lease_id) = match claim_response {
                PrivilegedResponse::ClaimWork(resp) => (resp.work_id, resp.lease_id),
                _ => panic!("Expected ClaimWork response"),
            };

            let spawn_request = SpawnEpisodeRequest {
                workspace_root: test_workspace_root(),
                work_id: work_id.clone(),
                role: WorkRole::Implementer.into(),
                lease_id: Some(lease_id),
                adapter_profile_hash: None,
                max_episodes: None,
                escalation_predicate: None,
                permeability_receipt_hash: None,
            };
            let spawn_frame = encode_spawn_episode_request(&spawn_request);
            let spawn_response = dispatcher.dispatch(&spawn_frame, &ctx).unwrap();
            let session_id = match spawn_response {
                PrivilegedResponse::SpawnEpisode(ref resp) => resp.session_id.clone(),
                _ => panic!("Expected SpawnEpisode response"),
            };

            // EndSession with success
            let end_request = EndSessionRequest {
                session_id,
                reason: "completed_normally".to_string(),
                outcome: TerminationOutcome::Success as i32,
            };
            let end_frame = encode_end_session_request(&end_request);
            dispatcher.dispatch(&end_frame, &ctx).unwrap();

            // Verify NO WorkTransitioned(InProgress -> Completed) is emitted.
            // Per Quality BLOCKER 2: work completion belongs to gate
            // orchestration, not EndSession.
            let events = dispatcher.event_emitter.get_events_by_work_id(&work_id);
            let completed_count = events
                .iter()
                .filter(|e| e.event_type == "work_transitioned")
                .filter(|e| {
                    let p: serde_json::Value = serde_json::from_slice(&e.payload).unwrap();
                    p["to_state"] == "Completed"
                })
                .count();

            assert_eq!(
                completed_count, 0,
                "EndSession must NOT emit WorkTransitioned(Completed) - work completion belongs to gate orchestration"
            );

            // Verify session_terminated was still emitted
            let terminated_count = events
                .iter()
                .filter(|e| e.event_type == "session_terminated")
                .count();
            assert_eq!(
                terminated_count, 1,
                "EndSession should emit session_terminated"
            );
        }

        /// FIX-BLOCKER: `EndSession` rejects unknown session ID.
        #[test]
        fn end_session_rejects_unknown_session() {
            let dispatcher = PrivilegedDispatcher::new();
            let ctx = ConnectionContext::privileged_session_open(Some(PeerCredentials {
                uid: 1000,
                gid: 1000,
                pid: Some(12345),
            }));

            let end_request = EndSessionRequest {
                session_id: "NONEXISTENT-SESSION".to_string(),
                reason: "test".to_string(),
                outcome: TerminationOutcome::Failure as i32,
            };
            let end_frame = encode_end_session_request(&end_request);
            let response = dispatcher.dispatch(&end_frame, &ctx).unwrap();

            match response {
                PrivilegedResponse::Error(err) => {
                    assert!(
                        err.message.contains("session not found"),
                        "Expected 'session not found' error, got: {}",
                        err.message
                    );
                },
                other => panic!("Expected error response, got: {other:?}"),
            }
        }

        /// FIX-SEC-MAJOR: `previous_transition_count` is derived from
        /// authoritative ledger state, not hardcoded.
        #[test]
        fn transition_count_derived_from_ledger_state() {
            let emitter = StubLedgerEventEmitter::new();

            // Initially, no transitions
            assert_eq!(
                emitter.get_work_transition_count("W-COUNT-001"),
                0,
                "No transitions yet"
            );

            // Emit first transition
            emitter
                .emit_work_transitioned(&WorkTransition {
                    work_id: "W-COUNT-001",
                    from_state: "Open",
                    to_state: "Claimed",
                    rationale_code: "work_claimed_via_ipc",
                    previous_transition_count: 0,
                    actor_id: "uid:1000",
                    timestamp_ns: 1_000_000_000,
                })
                .unwrap();

            assert_eq!(
                emitter.get_work_transition_count("W-COUNT-001"),
                1,
                "Should have 1 transition after first emit"
            );

            // Emit second transition
            emitter
                .emit_work_transitioned(&WorkTransition {
                    work_id: "W-COUNT-001",
                    from_state: "Claimed",
                    to_state: "InProgress",
                    rationale_code: "episode_spawned_via_ipc",
                    previous_transition_count: 1,
                    actor_id: "uid:1000",
                    timestamp_ns: 2_000_000_000,
                })
                .unwrap();

            assert_eq!(
                emitter.get_work_transition_count("W-COUNT-001"),
                2,
                "Should have 2 transitions after second emit"
            );

            // Different work_id should have 0 transitions
            assert_eq!(
                emitter.get_work_transition_count("W-COUNT-002"),
                0,
                "Different work_id should have 0 transitions"
            );
        }

        /// FIX-SEC-MAJOR: Duplicate/retry spawn transitions are detected
        /// via authoritative transition count.
        #[test]
        fn duplicate_spawn_detected_via_transition_count() {
            let dispatcher = PrivilegedDispatcher::new();
            let ctx = ConnectionContext::privileged_session_open(Some(PeerCredentials {
                uid: 1000,
                gid: 1000,
                pid: Some(12345),
            }));

            // ClaimWork
            let claim_request = ClaimWorkRequest {
                actor_id: "test-actor".to_string(),
                role: WorkRole::Implementer.into(),
                credential_signature: vec![1, 2, 3],
                nonce: vec![4, 5, 6],
            };
            let claim_frame = encode_claim_work_request(&claim_request);
            let claim_response = dispatcher.dispatch(&claim_frame, &ctx).unwrap();
            let (work_id, _lease_id) = match claim_response {
                PrivilegedResponse::ClaimWork(resp) => (resp.work_id, resp.lease_id),
                _ => panic!("Expected ClaimWork response"),
            };

            // After ClaimWork: should have 1 transition (Open->Claimed)
            let count_after_claim = dispatcher.event_emitter.get_work_transition_count(&work_id);
            assert_eq!(
                count_after_claim, 1,
                "After ClaimWork, transition count should be 1"
            );

            // Verify the transition event has previous_transition_count=0
            // (which is the count BEFORE the transition was emitted)
            let events = dispatcher.event_emitter.get_events_by_work_id(&work_id);
            let first_transition: Vec<_> = events
                .iter()
                .filter(|e| e.event_type == "work_transitioned")
                .collect();
            assert_eq!(first_transition.len(), 1);
            let payload: serde_json::Value =
                serde_json::from_slice(&first_transition[0].payload).unwrap();
            assert_eq!(
                payload["previous_transition_count"], 0,
                "First transition should have previous_transition_count=0"
            );
        }

        /// FIX-SEC-BLOCKER: Replay ordering is deterministic for
        /// equal-timestamp events.
        ///
        /// Events emitted with the same timestamp must maintain stable
        /// insertion order when queried via `get_events_by_work_id`.
        #[test]
        fn equal_timestamp_events_maintain_insertion_order() {
            let emitter = StubLedgerEventEmitter::new();
            let ts = 1_000_000_000u64;

            // Emit three events with the same timestamp
            emitter
                .emit_work_claimed(
                    &WorkClaim {
                        work_id: "W-ORDER-001".to_string(),
                        lease_id: "L-001".to_string(),
                        actor_id: "uid:1000".to_string(),
                        role: WorkRole::Implementer,
                        policy_resolution: PolicyResolution {
                            policy_resolved_ref: "test-resolved".to_string(),
                            resolved_policy_hash: [0u8; 32],
                            capability_manifest_hash: [0u8; 32],
                            context_pack_hash: [0u8; 32],
                            role_spec_hash: [0u8; 32],
                            context_pack_recipe_hash: [0u8; 32],
                            resolved_risk_tier: 0,
                            resolved_scope_baseline: None,
                            expected_adapter_profile_hash: None,
                            pcac_policy: None,
                            pointer_only_waiver: None,
                        },
                        executor_custody_domains: vec![],
                        author_custody_domains: vec![],
                        permeability_receipt: None,
                    },
                    ts,
                )
                .unwrap();

            emitter
                .emit_work_transitioned(&WorkTransition {
                    work_id: "W-ORDER-001",
                    from_state: "Open",
                    to_state: "Claimed",
                    rationale_code: "work_claimed_via_ipc",
                    previous_transition_count: 0,
                    actor_id: "uid:1000",
                    timestamp_ns: ts,
                })
                .unwrap();

            emitter
                .emit_session_started(
                    "SESS-001",
                    "W-ORDER-001",
                    "L-001",
                    "uid:1000",
                    &[0xAA; 32],
                    None,
                    ts,
                    None,
                    None,
                    None,
                )
                .unwrap();

            // Query events - must maintain insertion order
            let events = emitter.get_events_by_work_id("W-ORDER-001");
            assert_eq!(events.len(), 3, "Expected 3 events");

            // Verify ordering: work_claimed, work_transitioned, session_started
            assert_eq!(
                events[0].event_type, "work_claimed",
                "First event should be work_claimed"
            );
            assert_eq!(
                events[1].event_type, "work_transitioned",
                "Second event should be work_transitioned"
            );
            assert_eq!(
                events[2].event_type, "session_started",
                "Third event should be session_started"
            );

            // All have the same timestamp
            for event in &events {
                assert_eq!(
                    event.timestamp_ns, ts,
                    "All events should have the same timestamp"
                );
            }
        }

        /// FIX-QUALITY-MAJOR: `emit_claim_lifecycle` is atomic.
        ///
        /// Tests that `emit_claim_lifecycle` emits both `work_claimed` and
        /// `work_transitioned` events.
        #[test]
        fn emit_claim_lifecycle_emits_both_events() {
            let emitter = StubLedgerEventEmitter::new();
            let claim = WorkClaim {
                work_id: "W-ATOMIC-001".to_string(),
                lease_id: "L-001".to_string(),
                actor_id: "uid:1000".to_string(),
                role: WorkRole::Implementer,
                policy_resolution: PolicyResolution {
                    policy_resolved_ref: "test-resolved".to_string(),
                    resolved_policy_hash: [0u8; 32],
                    capability_manifest_hash: [0u8; 32],
                    context_pack_hash: [0u8; 32],
                    role_spec_hash: [0u8; 32],
                    context_pack_recipe_hash: [0u8; 32],
                    resolved_risk_tier: 0,
                    resolved_scope_baseline: None,
                    expected_adapter_profile_hash: None,
                    pcac_policy: None,
                    pointer_only_waiver: None,
                },
                executor_custody_domains: vec![],
                author_custody_domains: vec![],
                permeability_receipt: None,
            };

            let result = emitter.emit_claim_lifecycle(&claim, "uid:1000", 1_000_000_000);
            assert!(result.is_ok(), "emit_claim_lifecycle should succeed");

            let events = emitter.get_events_by_work_id("W-ATOMIC-001");
            assert_eq!(
                events.len(),
                2,
                "Expected 2 events (work_claimed + work_transitioned)"
            );
            assert_eq!(events[0].event_type, "work_claimed");
            assert_eq!(events[1].event_type, "work_transitioned");

            // Verify the transition has correct previous_transition_count
            let payload: serde_json::Value = serde_json::from_slice(&events[1].payload).unwrap();
            assert_eq!(payload["from_state"], "Open");
            assert_eq!(payload["to_state"], "Claimed");
            assert_eq!(payload["previous_transition_count"], 0);
        }

        /// FIX-QUALITY-MAJOR: `emit_spawn_lifecycle` is atomic.
        ///
        /// Tests that `emit_spawn_lifecycle` emits both `session_started` and
        /// `work_transitioned` events.
        #[test]
        fn emit_spawn_lifecycle_emits_both_events() {
            let emitter = StubLedgerEventEmitter::new();

            // First, emit the claim lifecycle so transition count is 1
            let claim = WorkClaim {
                work_id: "W-ATOMIC-002".to_string(),
                lease_id: "L-001".to_string(),
                actor_id: "uid:1000".to_string(),
                role: WorkRole::Implementer,
                policy_resolution: PolicyResolution {
                    policy_resolved_ref: "test-resolved".to_string(),
                    resolved_policy_hash: [0u8; 32],
                    capability_manifest_hash: [0u8; 32],
                    context_pack_hash: [0u8; 32],
                    role_spec_hash: [0u8; 32],
                    context_pack_recipe_hash: [0u8; 32],
                    resolved_risk_tier: 0,
                    resolved_scope_baseline: None,
                    expected_adapter_profile_hash: None,
                    pcac_policy: None,
                    pointer_only_waiver: None,
                },
                executor_custody_domains: vec![],
                author_custody_domains: vec![],
                permeability_receipt: None,
            };
            emitter
                .emit_claim_lifecycle(&claim, "uid:1000", 1_000_000_000)
                .unwrap();

            // Now emit spawn lifecycle
            let result = emitter.emit_spawn_lifecycle(
                "SESS-002",
                "W-ATOMIC-002",
                "L-001",
                "uid:1000",
                &[0xAA; 32],
                None,
                2_000_000_000,
                None,
                None,
                None,
            );
            assert!(result.is_ok(), "emit_spawn_lifecycle should succeed");

            let events = emitter.get_events_by_work_id("W-ATOMIC-002");
            // Should have: work_claimed, work_transitioned(Open->Claimed),
            // session_started, work_transitioned(Claimed->InProgress)
            assert_eq!(events.len(), 4, "Expected 4 events total");
            assert_eq!(events[2].event_type, "session_started");
            assert_eq!(events[3].event_type, "work_transitioned");

            let payload: serde_json::Value = serde_json::from_slice(&events[3].payload).unwrap();
            assert_eq!(payload["from_state"], "Claimed");
            assert_eq!(payload["to_state"], "InProgress");
            // previous_transition_count should be 1 (derived from ledger)
            assert_eq!(payload["previous_transition_count"], 1);
        }

        // ================================================================
        // TCK-00395 v3: Quality review v2 fixes
        // ================================================================

        /// MAJOR 1: `EndSession` removes session from registry, preventing
        /// repeated termination.
        #[test]
        fn end_session_removes_session_from_registry() {
            let dispatcher = PrivilegedDispatcher::new();
            let ctx = ConnectionContext::privileged_session_open(Some(PeerCredentials {
                uid: 1000,
                gid: 1000,
                pid: Some(12345),
            }));

            // ClaimWork + SpawnEpisode
            let claim_request = ClaimWorkRequest {
                actor_id: "test-actor".to_string(),
                role: WorkRole::Implementer.into(),
                credential_signature: vec![1, 2, 3],
                nonce: vec![4, 5, 6],
            };
            let claim_frame = encode_claim_work_request(&claim_request);
            let claim_response = dispatcher.dispatch(&claim_frame, &ctx).unwrap();
            let (work_id, lease_id) = match claim_response {
                PrivilegedResponse::ClaimWork(resp) => (resp.work_id, resp.lease_id),
                _ => panic!("Expected ClaimWork response"),
            };

            let spawn_request = SpawnEpisodeRequest {
                workspace_root: test_workspace_root(),
                work_id,
                role: WorkRole::Implementer.into(),
                lease_id: Some(lease_id),
                adapter_profile_hash: None,
                max_episodes: None,
                escalation_predicate: None,
                permeability_receipt_hash: None,
            };
            let spawn_frame = encode_spawn_episode_request(&spawn_request);
            let spawn_response = dispatcher.dispatch(&spawn_frame, &ctx).unwrap();
            let session_id = match spawn_response {
                PrivilegedResponse::SpawnEpisode(ref resp) => resp.session_id.clone(),
                _ => panic!("Expected SpawnEpisode response"),
            };

            // Verify session exists before EndSession
            assert!(
                dispatcher
                    .session_registry()
                    .get_session(&session_id)
                    .is_some(),
                "Session should exist before EndSession"
            );

            // First EndSession - should succeed
            let end_request = EndSessionRequest {
                session_id: session_id.clone(),
                reason: "completed_normally".to_string(),
                outcome: TerminationOutcome::Success as i32,
            };
            let end_frame = encode_end_session_request(&end_request);
            let response = dispatcher.dispatch(&end_frame, &ctx).unwrap();
            match response {
                PrivilegedResponse::EndSession(resp) => {
                    assert_eq!(resp.session_id, session_id);
                },
                other => panic!("Expected EndSession response, got: {other:?}"),
            }

            // Verify session is removed from registry
            assert!(
                dispatcher
                    .session_registry()
                    .get_session(&session_id)
                    .is_none(),
                "Session should be removed after EndSession"
            );

            // Second EndSession - should fail with "session not found"
            let end_frame2 = encode_end_session_request(&end_request);
            let response2 = dispatcher.dispatch(&end_frame2, &ctx).unwrap();
            match response2 {
                PrivilegedResponse::Error(err) => {
                    assert!(
                        err.message.contains("session not found"),
                        "Expected 'session not found' on repeated EndSession, got: {}",
                        err.message
                    );
                },
                other => panic!("Expected error on repeated EndSession, got: {other:?}"),
            }
        }

        /// Security MAJOR 1: `EndSession` rejects reason strings that exceed
        /// `MAX_REASON_LENGTH`. Prevents OOM and bloated signed ledger
        /// payloads.
        #[test]
        fn end_session_rejects_oversized_reason() {
            let dispatcher = PrivilegedDispatcher::new();
            let ctx = ConnectionContext::privileged_session_open(Some(PeerCredentials {
                uid: 1000,
                gid: 1000,
                pid: Some(12345),
            }));

            // ClaimWork + SpawnEpisode to get a valid session
            let claim_request = ClaimWorkRequest {
                actor_id: "test-actor".to_string(),
                role: WorkRole::Implementer.into(),
                credential_signature: vec![1, 2, 3],
                nonce: vec![4, 5, 6],
            };
            let claim_frame = encode_claim_work_request(&claim_request);
            let claim_response = dispatcher.dispatch(&claim_frame, &ctx).unwrap();
            let (work_id, lease_id) = match claim_response {
                PrivilegedResponse::ClaimWork(resp) => (resp.work_id, resp.lease_id),
                _ => panic!("Expected ClaimWork response"),
            };

            let spawn_request = SpawnEpisodeRequest {
                workspace_root: test_workspace_root(),
                work_id,
                role: WorkRole::Implementer.into(),
                lease_id: Some(lease_id),
                adapter_profile_hash: None,
                max_episodes: None,
                escalation_predicate: None,
                permeability_receipt_hash: None,
            };
            let spawn_frame = encode_spawn_episode_request(&spawn_request);
            let spawn_response = dispatcher.dispatch(&spawn_frame, &ctx).unwrap();
            let session_id = match spawn_response {
                PrivilegedResponse::SpawnEpisode(ref resp) => resp.session_id.clone(),
                _ => panic!("Expected SpawnEpisode response"),
            };

            // EndSession with oversized reason (MAX_REASON_LENGTH + 1)
            let oversized_reason = "x".repeat(MAX_REASON_LENGTH + 1);
            let end_request = EndSessionRequest {
                session_id: session_id.clone(),
                reason: oversized_reason,
                outcome: TerminationOutcome::Success as i32,
            };
            let end_frame = encode_end_session_request(&end_request);
            let response = dispatcher.dispatch(&end_frame, &ctx).unwrap();
            match response {
                PrivilegedResponse::Error(err) => {
                    assert!(
                        err.message.contains("reason exceeds maximum length"),
                        "Expected reason length error, got: {}",
                        err.message
                    );
                },
                other => panic!("Expected error for oversized reason, got: {other:?}"),
            }

            // Verify session is NOT removed (request was rejected)
            assert!(
                dispatcher
                    .session_registry()
                    .get_session(&session_id)
                    .is_some(),
                "Session MUST be preserved when reason validation fails"
            );
        }

        /// Security MAJOR 1: `EndSession` accepts reason at exactly
        /// `MAX_REASON_LENGTH` bytes (boundary).
        #[test]
        fn end_session_accepts_reason_at_max_length() {
            let dispatcher = PrivilegedDispatcher::new();
            let ctx = ConnectionContext::privileged_session_open(Some(PeerCredentials {
                uid: 1000,
                gid: 1000,
                pid: Some(12345),
            }));

            let claim_request = ClaimWorkRequest {
                actor_id: "test-actor".to_string(),
                role: WorkRole::Implementer.into(),
                credential_signature: vec![1, 2, 3],
                nonce: vec![4, 5, 6],
            };
            let claim_frame = encode_claim_work_request(&claim_request);
            let claim_response = dispatcher.dispatch(&claim_frame, &ctx).unwrap();
            let (work_id, lease_id) = match claim_response {
                PrivilegedResponse::ClaimWork(resp) => (resp.work_id, resp.lease_id),
                _ => panic!("Expected ClaimWork response"),
            };

            let spawn_request = SpawnEpisodeRequest {
                workspace_root: test_workspace_root(),
                work_id,
                role: WorkRole::Implementer.into(),
                lease_id: Some(lease_id),
                adapter_profile_hash: None,
                max_episodes: None,
                escalation_predicate: None,
                permeability_receipt_hash: None,
            };
            let spawn_frame = encode_spawn_episode_request(&spawn_request);
            let spawn_response = dispatcher.dispatch(&spawn_frame, &ctx).unwrap();
            let session_id = match spawn_response {
                PrivilegedResponse::SpawnEpisode(ref resp) => resp.session_id.clone(),
                _ => panic!("Expected SpawnEpisode response"),
            };

            // EndSession with reason at exactly MAX_REASON_LENGTH
            let max_reason = "r".repeat(MAX_REASON_LENGTH);
            let end_request = EndSessionRequest {
                session_id: session_id.clone(),
                reason: max_reason,
                outcome: TerminationOutcome::Failure as i32,
            };
            let end_frame = encode_end_session_request(&end_request);
            let response = dispatcher.dispatch(&end_frame, &ctx).unwrap();
            match response {
                PrivilegedResponse::EndSession(resp) => {
                    assert_eq!(resp.session_id, session_id);
                },
                other => {
                    panic!("Expected EndSession response for max-length reason, got: {other:?}")
                },
            }
        }

        /// Quality BLOCKER 2: `EndSession` succeeds regardless of
        /// `emit_work_transitioned` status because `EndSession` no longer
        /// emits work transitions. Work completion belongs to gate
        /// orchestration.
        #[test]
        fn end_session_succeeds_without_work_transition() {
            let dispatcher = PrivilegedDispatcher::new();
            let ctx = ConnectionContext::privileged_session_open(Some(PeerCredentials {
                uid: 1000,
                gid: 1000,
                pid: Some(12345),
            }));

            // ClaimWork
            let claim_request = ClaimWorkRequest {
                actor_id: "test-actor".to_string(),
                role: WorkRole::Implementer.into(),
                credential_signature: vec![1, 2, 3],
                nonce: vec![4, 5, 6],
            };
            let claim_frame = encode_claim_work_request(&claim_request);
            let claim_response = dispatcher.dispatch(&claim_frame, &ctx).unwrap();
            let (work_id, lease_id) = match claim_response {
                PrivilegedResponse::ClaimWork(resp) => (resp.work_id, resp.lease_id),
                _ => panic!("Expected ClaimWork response"),
            };

            // SpawnEpisode
            let spawn_request = SpawnEpisodeRequest {
                workspace_root: test_workspace_root(),
                work_id: work_id.clone(),
                role: WorkRole::Implementer.into(),
                lease_id: Some(lease_id),
                adapter_profile_hash: None,
                max_episodes: None,
                escalation_predicate: None,
                permeability_receipt_hash: None,
            };
            let spawn_frame = encode_spawn_episode_request(&spawn_request);
            let spawn_response = dispatcher.dispatch(&spawn_frame, &ctx).unwrap();
            let session_id = match spawn_response {
                PrivilegedResponse::SpawnEpisode(ref resp) => resp.session_id.clone(),
                _ => panic!("Expected SpawnEpisode response"),
            };

            // EndSession with success
            let end_request = EndSessionRequest {
                session_id: session_id.clone(),
                reason: "completed_normally".to_string(),
                outcome: TerminationOutcome::Success as i32,
            };
            let end_frame = encode_end_session_request(&end_request);
            let response = dispatcher.dispatch(&end_frame, &ctx).unwrap();

            // EndSession should succeed (no work transition emitted)
            match response {
                PrivilegedResponse::EndSession(resp) => {
                    assert_eq!(resp.message, "session terminated");
                },
                other => panic!("Expected EndSession response, got: {other:?}"),
            }

            // Session should be removed from registry after success
            assert!(
                dispatcher
                    .session_registry()
                    .get_session(&session_id)
                    .is_none(),
                "Session MUST be removed from registry after successful EndSession"
            );

            // Verify no Completed transitions were emitted
            let events = dispatcher.event_emitter.get_events_by_work_id(&work_id);
            let completed_count = events
                .iter()
                .filter(|e| e.event_type == "work_transitioned")
                .filter(|e| {
                    let p: serde_json::Value = serde_json::from_slice(&e.payload).unwrap();
                    p["to_state"] == "Completed"
                })
                .count();
            assert_eq!(
                completed_count, 0,
                "EndSession must not emit WorkTransitioned(Completed)"
            );
        }

        /// Quality v3 MAJOR: Typed `TerminationOutcome` enum determines
        /// exit code instead of free-form reason string matching.
        #[test]
        fn end_session_typed_outcome_determines_exit_code() {
            let dispatcher = PrivilegedDispatcher::new();
            let ctx = ConnectionContext::privileged_session_open(Some(PeerCredentials {
                uid: 1000,
                gid: 1000,
                pid: Some(12345),
            }));

            // Helper: claim + spawn and return (work_id, session_id)
            let setup = |d: &PrivilegedDispatcher| {
                let claim_request = ClaimWorkRequest {
                    actor_id: "test-actor".to_string(),
                    role: WorkRole::Implementer.into(),
                    credential_signature: vec![1, 2, 3],
                    nonce: vec![4, 5, 6],
                };
                let claim_frame = encode_claim_work_request(&claim_request);
                let claim_response = d.dispatch(&claim_frame, &ctx).unwrap();
                let (work_id, lease_id) = match claim_response {
                    PrivilegedResponse::ClaimWork(resp) => (resp.work_id, resp.lease_id),
                    _ => panic!("Expected ClaimWork response"),
                };
                let spawn_request = SpawnEpisodeRequest {
                    workspace_root: test_workspace_root(),
                    work_id: work_id.clone(),
                    role: WorkRole::Implementer.into(),
                    lease_id: Some(lease_id),
                    adapter_profile_hash: None,
                    max_episodes: None,
                    escalation_predicate: None,
                    permeability_receipt_hash: None,
                };
                let spawn_frame = encode_spawn_episode_request(&spawn_request);
                let spawn_response = d.dispatch(&spawn_frame, &ctx).unwrap();
                let session_id = match spawn_response {
                    PrivilegedResponse::SpawnEpisode(ref resp) => resp.session_id.clone(),
                    _ => panic!("Expected SpawnEpisode response"),
                };
                (work_id, session_id)
            };

            // Test 1: TerminationOutcome::Success -> exit_code 0.
            // Quality BLOCKER 2: NO WorkTransitioned(Completed) emitted.
            let (work_id, session_id) = setup(&dispatcher);
            let end_request = EndSessionRequest {
                session_id,
                reason: "some_reason".to_string(), // reason is irrelevant when outcome is set
                outcome: TerminationOutcome::Success as i32,
            };
            let end_frame = encode_end_session_request(&end_request);
            dispatcher.dispatch(&end_frame, &ctx).unwrap();
            let events = dispatcher.event_emitter.get_events_by_work_id(&work_id);
            let terminated: Vec<_> = events
                .iter()
                .filter(|e| e.event_type == "session_terminated")
                .collect();
            assert_eq!(terminated.len(), 1);
            let payload: serde_json::Value =
                serde_json::from_slice(&terminated[0].payload).unwrap();
            assert_eq!(payload["exit_code"], 0, "Success outcome -> exit_code 0");
            // Quality BLOCKER 2: EndSession must NOT emit
            // WorkTransitioned(Completed). Work completion belongs to
            // gate orchestration.
            assert!(
                !events.iter().any(|e| {
                    e.event_type == "work_transitioned" && {
                        let p: serde_json::Value = serde_json::from_slice(&e.payload).unwrap();
                        p["to_state"] == "Completed"
                    }
                }),
                "Success outcome must NOT emit WorkTransitioned(Completed) per Quality BLOCKER 2"
            );

            // Test 2: TerminationOutcome::Failure -> exit_code 1,
            //         NO WorkTransitioned(Completed).
            let (work_id2, session_id2) = setup(&dispatcher);
            let end_request2 = EndSessionRequest {
                session_id: session_id2,
                reason: "completed_normally".to_string(), /* Even with "success" reason, outcome
                                                           * overrides */
                outcome: TerminationOutcome::Failure as i32,
            };
            let end_frame2 = encode_end_session_request(&end_request2);
            dispatcher.dispatch(&end_frame2, &ctx).unwrap();
            let events2 = dispatcher.event_emitter.get_events_by_work_id(&work_id2);
            let terminated2: Vec<_> = events2
                .iter()
                .filter(|e| e.event_type == "session_terminated")
                .collect();
            assert_eq!(terminated2.len(), 1);
            let payload2: serde_json::Value =
                serde_json::from_slice(&terminated2[0].payload).unwrap();
            assert_eq!(payload2["exit_code"], 1, "Failure outcome -> exit_code 1");
            // Should NOT have WorkTransitioned to Completed
            let completed_count = events2
                .iter()
                .filter(|e| {
                    e.event_type == "work_transitioned" && {
                        let p: serde_json::Value = serde_json::from_slice(&e.payload).unwrap();
                        p["to_state"] == "Completed"
                    }
                })
                .count();
            assert_eq!(
                completed_count, 0,
                "Failure outcome should NOT emit WorkTransitioned(Completed)"
            );

            // Test 3: TerminationOutcome::Unspecified (0) falls back to
            //         legacy string matching (backward compat).
            let (work_id3, session_id3) = setup(&dispatcher);
            let end_request3 = EndSessionRequest {
                session_id: session_id3,
                reason: "completed_normally".to_string(),
                outcome: TerminationOutcome::Unspecified as i32,
            };
            let end_frame3 = encode_end_session_request(&end_request3);
            dispatcher.dispatch(&end_frame3, &ctx).unwrap();
            let events3 = dispatcher.event_emitter.get_events_by_work_id(&work_id3);
            let terminated3: Vec<_> = events3
                .iter()
                .filter(|e| e.event_type == "session_terminated")
                .collect();
            assert_eq!(terminated3.len(), 1);
            let payload3: serde_json::Value =
                serde_json::from_slice(&terminated3[0].payload).unwrap();
            assert_eq!(
                payload3["exit_code"], 0,
                "Unspecified outcome with 'completed_normally' reason -> exit_code 0 (legacy)"
            );
        }

        /// BLOCKER 1: `stop_with_session_context` emits `SessionTerminated`
        /// event to the ledger when episode runtime stops.
        #[tokio::test]
        async fn stop_with_session_context_emits_session_terminated() {
            let emitter = Arc::new(StubLedgerEventEmitter::new());
            let config = EpisodeRuntimeConfig {
                emit_events: true,
                ..EpisodeRuntimeConfig::default()
            };

            let runtime = EpisodeRuntime::new(config).with_ledger_emitter(emitter.clone());

            // Create and start an episode
            let episode_id = runtime.create([0u8; 32], 1_000_000_000).await.unwrap();
            let _handle = runtime
                .start_with_workspace(
                    &episode_id,
                    "lease-001",
                    1_000_001_000,
                    std::path::Path::new("/tmp"),
                )
                .await
                .unwrap();

            // Stop via stop_with_session_context (simulating runtime-driven
            // termination with session context)
            let result = runtime
                .stop_with_session_context(
                    &episode_id,
                    crate::episode::TerminationClass::Success,
                    1_000_002_000,
                    "SESS-RUNTIME-001",
                    "W-RUNTIME-001",
                    "uid:1000",
                )
                .await;
            assert!(result.is_ok(), "stop_with_session_context should succeed");

            // Verify SessionTerminated event was emitted to the ledger
            let events = emitter.get_events_by_work_id("W-RUNTIME-001");
            let terminated: Vec<_> = events
                .iter()
                .filter(|e| e.event_type == "session_terminated")
                .collect();
            assert_eq!(
                terminated.len(),
                1,
                "Expected 1 session_terminated event from runtime stop"
            );

            let payload: serde_json::Value =
                serde_json::from_slice(&terminated[0].payload).unwrap();
            assert_eq!(payload["session_id"], "SESS-RUNTIME-001");
            assert_eq!(payload["work_id"], "W-RUNTIME-001");
            assert_eq!(payload["exit_code"], 0);
        }

        /// BLOCKER 1: `EndSession` integration test where runtime termination
        /// occurs through the handler (end-to-end with session removal).
        #[test]
        fn end_session_terminates_and_cleans_up_completely() {
            let dispatcher = PrivilegedDispatcher::new();
            let ctx = ConnectionContext::privileged_session_open(Some(PeerCredentials {
                uid: 1000,
                gid: 1000,
                pid: Some(12345),
            }));

            // ClaimWork + SpawnEpisode
            let claim_request = ClaimWorkRequest {
                actor_id: "test-actor".to_string(),
                role: WorkRole::Implementer.into(),
                credential_signature: vec![1, 2, 3],
                nonce: vec![4, 5, 6],
            };
            let claim_frame = encode_claim_work_request(&claim_request);
            let claim_response = dispatcher.dispatch(&claim_frame, &ctx).unwrap();
            let (work_id, lease_id) = match claim_response {
                PrivilegedResponse::ClaimWork(resp) => (resp.work_id, resp.lease_id),
                _ => panic!("Expected ClaimWork response"),
            };

            let spawn_request = SpawnEpisodeRequest {
                workspace_root: test_workspace_root(),
                work_id: work_id.clone(),
                role: WorkRole::Implementer.into(),
                lease_id: Some(lease_id),
                adapter_profile_hash: None,
                max_episodes: None,
                escalation_predicate: None,
                permeability_receipt_hash: None,
            };
            let spawn_frame = encode_spawn_episode_request(&spawn_request);
            let spawn_response = dispatcher.dispatch(&spawn_frame, &ctx).unwrap();
            let session_id = match spawn_response {
                PrivilegedResponse::SpawnEpisode(ref resp) => resp.session_id.clone(),
                _ => panic!("Expected SpawnEpisode response"),
            };

            // EndSession
            let end_request = EndSessionRequest {
                session_id: session_id.clone(),
                reason: "completed_normally".to_string(),
                outcome: TerminationOutcome::Success as i32,
            };
            let end_frame = encode_end_session_request(&end_request);
            let response = dispatcher.dispatch(&end_frame, &ctx).unwrap();
            match &response {
                PrivilegedResponse::EndSession(resp) => {
                    assert_eq!(resp.session_id, session_id);
                },
                other => panic!("Expected EndSession response, got: {other:?}"),
            }

            // Verify: session removed from registry
            assert!(
                dispatcher
                    .session_registry()
                    .get_session(&session_id)
                    .is_none(),
                "Session should be removed from registry"
            );

            // Verify: SessionTerminated event in ledger
            let events = dispatcher.event_emitter.get_events_by_work_id(&work_id);
            assert!(
                events.iter().any(|e| e.event_type == "session_terminated"),
                "SessionTerminated event should be in ledger"
            );

            // Quality BLOCKER 2: EndSession must NOT emit
            // WorkTransitioned(InProgress->Completed). Work completion
            // belongs to gate orchestration.
            let completed_count = events
                .iter()
                .filter(|e| e.event_type == "work_transitioned")
                .filter(|e| {
                    let p: serde_json::Value = serde_json::from_slice(&e.payload).unwrap();
                    p["to_state"] == "Completed"
                })
                .count();
            assert_eq!(
                completed_count, 0,
                "EndSession must NOT emit WorkTransitioned(Completed)"
            );

            // Verify: repeated EndSession is rejected
            let end_frame2 = encode_end_session_request(&end_request);
            let response2 = dispatcher.dispatch(&end_frame2, &ctx).unwrap();
            match response2 {
                PrivilegedResponse::Error(err) => {
                    assert!(err.message.contains("session not found"));
                },
                other => panic!("Expected error on repeated EndSession, got: {other:?}"),
            }
        }

        // ================================================================
        // TCK-00395 Security review v4 regression tests
        // ================================================================

        /// Security MAJOR 2: `EndSession` fails closed when peer credentials
        /// are missing. The handler must not emit authoritative ledger events
        /// with placeholder "unknown" actor IDs.
        #[test]
        fn end_session_rejects_missing_peer_credentials() {
            let dispatcher = PrivilegedDispatcher::new();

            // Create a session first (with valid credentials)
            let valid_ctx = ConnectionContext::privileged_session_open(Some(PeerCredentials {
                uid: 1000,
                gid: 1000,
                pid: Some(12345),
            }));

            let claim_request = ClaimWorkRequest {
                actor_id: "test-actor".to_string(),
                role: WorkRole::Implementer.into(),
                credential_signature: vec![1, 2, 3],
                nonce: vec![4, 5, 6],
            };
            let claim_frame = encode_claim_work_request(&claim_request);
            let claim_response = dispatcher.dispatch(&claim_frame, &valid_ctx).unwrap();
            let (work_id, lease_id) = match claim_response {
                PrivilegedResponse::ClaimWork(resp) => (resp.work_id, resp.lease_id),
                _ => panic!("Expected ClaimWork response"),
            };

            let spawn_request = SpawnEpisodeRequest {
                workspace_root: test_workspace_root(),
                work_id: work_id.clone(),
                role: WorkRole::Implementer.into(),
                lease_id: Some(lease_id),
                adapter_profile_hash: None,
                max_episodes: None,
                escalation_predicate: None,
                permeability_receipt_hash: None,
            };
            let spawn_frame = encode_spawn_episode_request(&spawn_request);
            let spawn_response = dispatcher.dispatch(&spawn_frame, &valid_ctx).unwrap();
            let session_id = match spawn_response {
                PrivilegedResponse::SpawnEpisode(ref resp) => resp.session_id.clone(),
                _ => panic!("Expected SpawnEpisode response"),
            };

            // Now try EndSession WITHOUT peer credentials
            let no_creds_ctx = ConnectionContext::privileged_session_open(None);
            let end_request = EndSessionRequest {
                session_id: session_id.clone(),
                reason: "completed_normally".to_string(),
                outcome: TerminationOutcome::Success as i32,
            };
            let end_frame = encode_end_session_request(&end_request);
            let result = dispatcher.dispatch(&end_frame, &no_creds_ctx);

            // Should fail (either Err or Error response)
            match result {
                Err(e) => {
                    let msg = format!("{e}");
                    assert!(
                        msg.contains("peer credentials"),
                        "Error should mention peer credentials: {msg}"
                    );
                },
                Ok(PrivilegedResponse::Error(err)) => {
                    assert!(
                        err.message.contains("peer credentials")
                            || err.message.contains("credential"),
                        "Error should mention credentials: {}",
                        err.message
                    );
                },
                Ok(other) => panic!("Expected error for missing peer credentials, got: {other:?}"),
            }

            // Verify session is still in the registry (not cleaned up)
            assert!(
                dispatcher
                    .session_registry()
                    .get_session(&session_id)
                    .is_some(),
                "Session MUST be preserved when credentials are missing"
            );

            // Verify no session_terminated events were emitted
            let events = dispatcher.event_emitter.get_events_by_work_id(&work_id);
            let terminated_count = events
                .iter()
                .filter(|e| e.event_type == "session_terminated")
                .count();
            assert_eq!(
                terminated_count, 0,
                "No session_terminated events should be emitted without credentials"
            );
        }

        /// Security MAJOR 2: `SpawnEpisode` fails closed when peer
        /// credentials are missing.
        #[test]
        fn spawn_episode_rejects_missing_peer_credentials() {
            let dispatcher = PrivilegedDispatcher::new();

            // ClaimWork with valid credentials
            let valid_ctx = ConnectionContext::privileged_session_open(Some(PeerCredentials {
                uid: 1000,
                gid: 1000,
                pid: Some(12345),
            }));

            let claim_request = ClaimWorkRequest {
                actor_id: "test-actor".to_string(),
                role: WorkRole::Implementer.into(),
                credential_signature: vec![1, 2, 3],
                nonce: vec![4, 5, 6],
            };
            let claim_frame = encode_claim_work_request(&claim_request);
            let claim_response = dispatcher.dispatch(&claim_frame, &valid_ctx).unwrap();
            let (work_id, lease_id) = match claim_response {
                PrivilegedResponse::ClaimWork(resp) => (resp.work_id, resp.lease_id),
                _ => panic!("Expected ClaimWork response"),
            };

            // SpawnEpisode WITHOUT peer credentials
            let no_creds_ctx = ConnectionContext::privileged_session_open(None);
            let spawn_request = SpawnEpisodeRequest {
                workspace_root: test_workspace_root(),
                work_id,
                role: WorkRole::Implementer.into(),
                lease_id: Some(lease_id),
                adapter_profile_hash: None,
                max_episodes: None,
                escalation_predicate: None,
                permeability_receipt_hash: None,
            };
            let spawn_frame = encode_spawn_episode_request(&spawn_request);
            let result = dispatcher.dispatch(&spawn_frame, &no_creds_ctx);

            // Should fail
            match result {
                Err(e) => {
                    let msg = format!("{e}");
                    assert!(
                        msg.contains("peer credentials"),
                        "Error should mention peer credentials: {msg}"
                    );
                },
                Ok(PrivilegedResponse::Error(err)) => {
                    assert!(
                        err.message.contains("peer credentials")
                            || err.message.contains("credential"),
                        "Error should mention credentials: {}",
                        err.message
                    );
                },
                Ok(other) => panic!("Expected error for missing peer credentials, got: {other:?}"),
            }
        }

        /// Security MAJOR 1: `EndSession` fails closed on malformed
        /// `episode_id`. A corrupted `episode_id` must not silently skip
        /// the runtime stop while still emitting termination facts.
        #[test]
        fn end_session_rejects_malformed_episode_id() {
            let dispatcher = PrivilegedDispatcher::new();
            let ctx = ConnectionContext::privileged_session_open(Some(PeerCredentials {
                uid: 1000,
                gid: 1000,
                pid: Some(12345),
            }));

            // Register a session directly with a malformed episode_id
            // (empty string after Some() - which is non-empty but invalid)
            let session = crate::session::SessionState {
                session_id: "SESS-MALFORMED-001".to_string(),
                work_id: "W-MALFORMED-001".to_string(),
                role: WorkRole::Implementer.into(),
                lease_id: "L-MALFORMED-001".to_string(),
                ephemeral_handle: "handle-malformed-001".to_string(),
                policy_resolved_ref: String::new(),
                pcac_policy: None,
                pointer_only_waiver: None,
                capability_manifest_hash: vec![],
                // Malformed: contains forbidden '/' characters for an EpisodeId
                episode_id: Some("invalid/episode/id".to_string()),
            };

            dispatcher
                .session_registry()
                .register_session(session)
                .expect("session registration should succeed");

            let end_request = EndSessionRequest {
                session_id: "SESS-MALFORMED-001".to_string(),
                reason: "test".to_string(),
                outcome: TerminationOutcome::Success as i32,
            };
            let end_frame = encode_end_session_request(&end_request);
            let response = dispatcher.dispatch(&end_frame, &ctx).unwrap();

            match response {
                PrivilegedResponse::Error(err) => {
                    assert!(
                        err.message.contains("malformed episode_id")
                            || err.message.contains("episode"),
                        "Error should mention malformed episode_id: {}",
                        err.message
                    );
                },
                other => panic!("Expected error for malformed episode_id, got: {other:?}"),
            }

            // Session should be preserved since termination was rejected
            assert!(
                dispatcher
                    .session_registry()
                    .get_session("SESS-MALFORMED-001")
                    .is_some(),
                "Session MUST be preserved when episode_id parsing fails"
            );

            // No session_terminated events should have been emitted
            let events = dispatcher
                .event_emitter
                .get_events_by_work_id("W-MALFORMED-001");
            let terminated_count = events
                .iter()
                .filter(|e| e.event_type == "session_terminated")
                .count();
            assert_eq!(
                terminated_count, 0,
                "No termination events should be emitted when episode_id is malformed"
            );
        }

        /// BLOCKER 1 (v2): `stop_all_running` stops all running episodes
        /// and emits `SessionTerminated` events for each.
        #[tokio::test]
        async fn stop_all_running_emits_session_terminated_for_all() {
            use crate::episode::registry::InMemorySessionRegistry;

            let emitter = Arc::new(StubLedgerEventEmitter::new());
            let registry = Arc::new(InMemorySessionRegistry::new());
            let config = EpisodeRuntimeConfig {
                emit_events: true,
                ..EpisodeRuntimeConfig::default()
            };

            let runtime =
                EpisodeRuntime::new(config)
                    .with_ledger_emitter(emitter.clone())
                    .with_session_registry(
                        registry.clone() as Arc<dyn crate::session::SessionRegistry>
                    );

            // Create and start two episodes
            let ep1 = runtime.create([10u8; 32], 1_000_000_000).await.unwrap();
            let h1 = runtime
                .start_with_workspace(
                    &ep1,
                    "lease-001",
                    1_000_001_000,
                    std::path::Path::new("/tmp"),
                )
                .await
                .unwrap();
            let sid1 = h1.session_id().to_string();

            let ep2 = runtime.create([20u8; 32], 1_000_000_000).await.unwrap();
            let h2 = runtime
                .start_with_workspace(
                    &ep2,
                    "lease-002",
                    1_000_001_000,
                    std::path::Path::new("/tmp"),
                )
                .await
                .unwrap();
            let sid2 = h2.session_id().to_string();

            // Register sessions with known work IDs
            let s1 = crate::session::SessionState {
                session_id: sid1.clone(),
                work_id: "W-ALL-001".to_string(),
                role: 1,
                ephemeral_handle: "h1".to_string(),
                lease_id: "lease-001".to_string(),
                policy_resolved_ref: "pol-001".to_string(),
                pcac_policy: None,
                pointer_only_waiver: None,
                capability_manifest_hash: vec![0u8; 32],
                episode_id: Some(ep1.as_str().to_string()),
            };
            registry.register_session(s1).unwrap();

            let s2 = crate::session::SessionState {
                session_id: sid2.clone(),
                work_id: "W-ALL-002".to_string(),
                role: 1,
                ephemeral_handle: "h2".to_string(),
                lease_id: "lease-002".to_string(),
                policy_resolved_ref: "pol-002".to_string(),
                pcac_policy: None,
                pointer_only_waiver: None,
                capability_manifest_hash: vec![0u8; 32],
                episode_id: Some(ep2.as_str().to_string()),
            };
            registry.register_session(s2).unwrap();

            assert_eq!(runtime.active_count().await, 2);

            // Stop all running
            let stopped = runtime
                .stop_all_running(2_000_000_000, crate::episode::TerminationClass::Cancelled)
                .await;
            assert_eq!(stopped, 2, "Should have stopped 2 episodes");
            assert_eq!(runtime.active_count().await, 0);

            // Verify SessionTerminated events for both work IDs
            let ev1 = emitter.get_events_by_work_id("W-ALL-001");
            assert!(
                ev1.iter().any(|e| e.event_type == "session_terminated"),
                "W-ALL-001 should have session_terminated event"
            );
            let ev2 = emitter.get_events_by_work_id("W-ALL-002");
            assert!(
                ev2.iter().any(|e| e.event_type == "session_terminated"),
                "W-ALL-002 should have session_terminated event"
            );

            // Verify actor_id is daemon:shutdown
            let t1: Vec<_> = ev1
                .iter()
                .filter(|e| e.event_type == "session_terminated")
                .collect();
            let p1: serde_json::Value = serde_json::from_slice(&t1[0].payload).unwrap();
            assert_eq!(p1["actor_id"], "daemon:shutdown");
        }
    }

    // ========================================================================
    // TCK-00397: Spawn adapter profile hash binding tests
    // ========================================================================
    mod tck_00397_adapter_profile_binding {
        use apm2_core::evidence::{ContentAddressedStore, MemoryCas};
        use apm2_core::fac::builtin_profiles;

        use super::*;

        fn dispatcher_with_cas() -> (PrivilegedDispatcher, Arc<MemoryCas>, ConnectionContext) {
            let cas = Arc::new(MemoryCas::default());
            let dispatcher = PrivilegedDispatcher::new()
                .with_cas(Arc::clone(&cas) as Arc<dyn ContentAddressedStore>)
                .with_adapter_profile_cas(Arc::clone(&cas) as Arc<dyn ContentAddressedStore>);
            let ctx = ConnectionContext::privileged_session_open(Some(PeerCredentials {
                uid: 1000,
                gid: 1000,
                pid: Some(12345),
            }));
            (dispatcher, cas, ctx)
        }

        fn claim_work(
            dispatcher: &PrivilegedDispatcher,
            ctx: &ConnectionContext,
        ) -> (String, String) {
            let claim_request = ClaimWorkRequest {
                actor_id: "adapter-profile-test".to_string(),
                role: WorkRole::Implementer.into(),
                credential_signature: vec![1, 2, 3],
                nonce: vec![4, 5, 6],
            };
            let claim_frame = encode_claim_work_request(&claim_request);
            let claim_response = dispatcher.dispatch(&claim_frame, ctx).unwrap();
            match claim_response {
                PrivilegedResponse::ClaimWork(resp) => (resp.work_id, resp.lease_id),
                other => panic!("Expected ClaimWork response, got {other:?}"),
            }
        }

        #[test]
        fn spawn_episode_rejects_missing_adapter_profile_hash_in_cas() {
            let (dispatcher, _cas, ctx) = dispatcher_with_cas();
            let (work_id, lease_id) = claim_work(&dispatcher, &ctx);

            let spawn_request = SpawnEpisodeRequest {
                workspace_root: test_workspace_root(),
                work_id: work_id.clone(),
                role: WorkRole::Implementer.into(),
                lease_id: Some(lease_id),
                adapter_profile_hash: Some(vec![0x55; 32]),
                max_episodes: None,
                escalation_predicate: None,
                permeability_receipt_hash: None,
            };
            let spawn_frame = encode_spawn_episode_request(&spawn_request);
            let response = dispatcher.dispatch(&spawn_frame, &ctx).unwrap();

            match response {
                PrivilegedResponse::Error(err) => {
                    assert_eq!(
                        err.code,
                        PrivilegedErrorCode::CapabilityRequestRejected as i32
                    );
                    assert!(
                        err.message
                            .contains("adapter_profile_hash not found in CAS"),
                        "error should fail-closed on CAS miss: {}",
                        err.message
                    );
                },
                other => panic!("expected spawn rejection on CAS miss, got {other:?}"),
            }

            assert!(
                dispatcher
                    .session_registry()
                    .get_session_by_work_id(&work_id)
                    .is_none(),
                "CAS-miss rejection must not persist a session"
            );
        }

        #[test]
        fn spawn_episode_accepts_present_adapter_profile_hash_and_records_attribution() {
            let (dispatcher, cas, ctx) = dispatcher_with_cas();
            let (work_id, lease_id) = claim_work(&dispatcher, &ctx);

            let adapter_hash = builtin_profiles::claude_code_profile()
                .store_in_cas(cas.as_ref())
                .expect("builtin profile should store in CAS");

            let spawn_request = SpawnEpisodeRequest {
                workspace_root: test_workspace_root(),
                work_id: work_id.clone(),
                role: WorkRole::Implementer.into(),
                lease_id: Some(lease_id),
                adapter_profile_hash: Some(adapter_hash.to_vec()),
                max_episodes: None,
                escalation_predicate: None,
                permeability_receipt_hash: None,
            };
            let spawn_frame = encode_spawn_episode_request(&spawn_request);
            let response = dispatcher.dispatch(&spawn_frame, &ctx).unwrap();
            assert!(
                matches!(response, PrivilegedResponse::SpawnEpisode(_)),
                "expected spawn success with CAS-present adapter hash"
            );

            let events = dispatcher.event_emitter.get_events_by_work_id(&work_id);
            let session_started: Vec<_> = events
                .iter()
                .filter(|e| e.event_type == "session_started")
                .collect();
            assert_eq!(
                session_started.len(),
                1,
                "expected one SessionStarted event"
            );

            let payload: serde_json::Value =
                serde_json::from_slice(&session_started[0].payload).expect("valid JSON payload");
            let expected_role_hash = fac_workobject_implementor_v2_role_contract()
                .compute_cas_hash()
                .expect("builtin role spec hash should compute");
            assert_eq!(payload["adapter_profile_hash"], hex::encode(adapter_hash));
            assert_eq!(payload["role_spec_hash"], hex::encode(expected_role_hash));
            assert!(
                payload.get("waiver_id").is_none(),
                "implementer path must not emit WVR-0002 waiver"
            );
            assert!(
                payload.get("role_spec_hash_absent").is_none(),
                "implementer path must not mark role_spec_hash_absent"
            );
        }

        #[test]
        fn test_session_context_records_profile_hash() {
            let (dispatcher, cas, mut ctx) = dispatcher_with_cas();
            let (work_id, lease_id) = claim_work(&dispatcher, &ctx);

            let adapter_hash = builtin_profiles::claude_code_profile()
                .store_in_cas(cas.as_ref())
                .expect("builtin profile should store in CAS");
            let identity_profile_hash =
                crate::identity::IdentityProofProfileV1::baseline_smt_10e12()
                    .content_hash()
                    .expect("baseline identity proof profile hash should compute");
            ctx.set_identity_proof_profile_hash(identity_profile_hash)
                .expect("non-zero profile hash should be accepted");

            let spawn_request = SpawnEpisodeRequest {
                workspace_root: test_workspace_root(),
                work_id: work_id.clone(),
                role: WorkRole::Implementer.into(),
                lease_id: Some(lease_id),
                adapter_profile_hash: Some(adapter_hash.to_vec()),
                max_episodes: None,
                escalation_predicate: None,
                permeability_receipt_hash: None,
            };
            let spawn_frame = encode_spawn_episode_request(&spawn_request);
            let response = dispatcher.dispatch(&spawn_frame, &ctx).unwrap();
            assert!(
                matches!(response, PrivilegedResponse::SpawnEpisode(_)),
                "expected spawn success with context-bound identity profile hash"
            );

            let events = dispatcher.event_emitter.get_events_by_work_id(&work_id);
            let session_started: Vec<_> = events
                .iter()
                .filter(|e| e.event_type == "session_started")
                .collect();
            assert_eq!(
                session_started.len(),
                1,
                "expected one SessionStarted event"
            );

            let payload: serde_json::Value =
                serde_json::from_slice(&session_started[0].payload).expect("valid JSON payload");
            assert_eq!(
                payload["identity_proof_profile_hash"],
                hex::encode(identity_profile_hash)
            );
        }

        /// TCK-00358: Verify that the production spawn path (without explicit
        /// `set_identity_proof_profile_hash`) emits the baseline identity
        /// proof profile hash in the `SessionStarted` payload. This exercises
        /// the fallback to `IdentityProofProfileV1::baseline_smt_10e12()` and
        /// proves REQ-0012 is met end-to-end without test-only injection.
        #[test]
        fn test_production_spawn_emits_baseline_identity_proof_profile_hash() {
            let (dispatcher, cas, ctx) = dispatcher_with_cas();
            let (work_id, lease_id) = claim_work(&dispatcher, &ctx);

            // No call to ctx.set_identity_proof_profile_hash() — simulates
            // production where the connection context has no pre-set value.
            assert!(
                ctx.identity_proof_profile_hash().is_none(),
                "precondition: ctx should not have identity_proof_profile_hash set"
            );

            let adapter_hash = builtin_profiles::claude_code_profile()
                .store_in_cas(cas.as_ref())
                .expect("builtin profile should store in CAS");

            let spawn_request = SpawnEpisodeRequest {
                workspace_root: test_workspace_root(),
                work_id: work_id.clone(),
                role: WorkRole::Implementer.into(),
                lease_id: Some(lease_id),
                adapter_profile_hash: Some(adapter_hash.to_vec()),
                max_episodes: None,
                escalation_predicate: None,
                permeability_receipt_hash: None,
            };
            let spawn_frame = encode_spawn_episode_request(&spawn_request);
            let response = dispatcher.dispatch(&spawn_frame, &ctx).unwrap();
            assert!(
                matches!(response, PrivilegedResponse::SpawnEpisode(_)),
                "expected spawn success without explicit identity proof profile hash"
            );

            let events = dispatcher.event_emitter.get_events_by_work_id(&work_id);
            let session_started: Vec<_> = events
                .iter()
                .filter(|e| e.event_type == "session_started")
                .collect();
            assert_eq!(
                session_started.len(),
                1,
                "expected one SessionStarted event"
            );

            let payload: serde_json::Value =
                serde_json::from_slice(&session_started[0].payload).expect("valid JSON payload");

            // The baseline identity proof profile hash should be present even
            // though we never called set_identity_proof_profile_hash on ctx.
            let expected_hash = crate::identity::IdentityProofProfileV1::baseline_smt_10e12()
                .content_hash()
                .expect("baseline identity proof profile hash should compute");
            assert_eq!(
                payload["identity_proof_profile_hash"],
                hex::encode(expected_hash),
                "production spawn must emit baseline identity_proof_profile_hash"
            );
        }

        /// TCK-00358 Round 2: Verify that the production session-open path
        /// wires the identity proof profile hash into `ConnectionContext` and
        /// that `SessionStarted` carries that hash rather than the spawn-time
        /// baseline fallback.
        ///
        /// This test uses a **non-baseline** profile hash to prove the value
        /// flows from the session-open wiring
        /// (`set_identity_proof_profile_hash`) rather than the
        /// spawn-time fallback to `baseline_smt_10e12()`.
        #[test]
        fn test_session_open_wired_profile_hash_overrides_spawn_fallback() {
            let (dispatcher, cas, mut ctx) = dispatcher_with_cas();
            let (work_id, lease_id) = claim_work(&dispatcher, &ctx);

            // Construct a non-baseline profile by tweaking max_proof_bytes.
            // This produces a distinct hash from the baseline profile,
            // proving the emitted hash comes from session-open wiring.
            let mut custom_profile = crate::identity::IdentityProofProfileV1::baseline_smt_10e12();
            custom_profile.max_proof_bytes = 4096; // differs from baseline 8192
            let custom_hash = custom_profile
                .content_hash()
                .expect("custom profile hash should compute");

            // Sanity check: the custom hash differs from the baseline.
            let baseline_hash = crate::identity::IdentityProofProfileV1::baseline_smt_10e12()
                .content_hash()
                .expect("baseline hash should compute");
            assert_ne!(
                custom_hash, baseline_hash,
                "precondition: custom profile hash must differ from baseline"
            );

            // Simulate the production session-open wiring in main.rs:
            // set_identity_proof_profile_hash is called after handshake,
            // before entering the dispatch loop.
            ctx.set_identity_proof_profile_hash(custom_hash)
                .expect("non-zero custom hash should be accepted");

            let adapter_hash = builtin_profiles::claude_code_profile()
                .store_in_cas(cas.as_ref())
                .expect("builtin profile should store in CAS");

            let spawn_request = SpawnEpisodeRequest {
                workspace_root: test_workspace_root(),
                work_id: work_id.clone(),
                role: WorkRole::Implementer.into(),
                lease_id: Some(lease_id),
                adapter_profile_hash: Some(adapter_hash.to_vec()),
                max_episodes: None,
                escalation_predicate: None,
                permeability_receipt_hash: None,
            };
            let spawn_frame = encode_spawn_episode_request(&spawn_request);
            let response = dispatcher.dispatch(&spawn_frame, &ctx).unwrap();
            assert!(
                matches!(response, PrivilegedResponse::SpawnEpisode(_)),
                "expected spawn success with session-open wired profile hash"
            );

            let events = dispatcher.event_emitter.get_events_by_work_id(&work_id);
            let session_started: Vec<_> = events
                .iter()
                .filter(|e| e.event_type == "session_started")
                .collect();
            assert_eq!(
                session_started.len(),
                1,
                "expected one SessionStarted event"
            );

            let payload: serde_json::Value =
                serde_json::from_slice(&session_started[0].payload).expect("valid JSON payload");

            // The emitted hash must be the custom (non-baseline) hash set
            // at session-open time, NOT the baseline fallback.
            assert_eq!(
                payload["identity_proof_profile_hash"],
                hex::encode(custom_hash),
                "SessionStarted must carry the session-open wired profile hash, \
                 not the spawn-time baseline fallback"
            );
            assert_ne!(
                payload["identity_proof_profile_hash"],
                hex::encode(baseline_hash),
                "SessionStarted must NOT carry the baseline fallback hash \
                 when session-open wiring provides a different profile"
            );
        }
    }

    // ========================================================================
    // TCK-00394: PublishChangeSet IPC endpoint tests
    // ========================================================================
    mod publish_changeset {
        use apm2_core::evidence::MemoryCas;
        use apm2_core::fac::{ChangeKind, ChangeSetBundleV1, FileChange, GitObjectRef, HashAlgo};

        use super::*;

        fn make_valid_bundle(changeset_id: &str) -> ChangeSetBundleV1 {
            ChangeSetBundleV1::builder()
                .changeset_id(changeset_id)
                .base(GitObjectRef {
                    algo: HashAlgo::Sha1,
                    object_kind: "commit".to_string(),
                    object_id: "a".repeat(40),
                })
                .diff_hash([0x42; 32])
                .file_manifest(vec![FileChange {
                    path: "src/main.rs".to_string(),
                    change_kind: ChangeKind::Modify,
                    old_path: None,
                }])
                .binary_detected(false)
                .build()
                .expect("bundle should build")
        }

        /// Helper to create a valid `ChangeSetBundleV1` JSON payload.
        fn make_bundle_json(changeset_id: &str) -> Vec<u8> {
            serde_json::to_vec(&make_valid_bundle(changeset_id)).unwrap()
        }

        /// Helper to create a semantically equivalent payload with different
        /// JSON formatting and key order.
        fn make_noncanonical_bundle_json(changeset_id: &str) -> Vec<u8> {
            let bundle = make_valid_bundle(changeset_id);
            let noncanonical = serde_json::json!({
                "binary_detected": bundle.binary_detected,
                "file_manifest": [{
                    "change_kind": "MODIFY",
                    "path": "src/main.rs"
                }],
                "diff_hash": hex::encode(bundle.diff_hash),
                "diff_format": bundle.diff_format,
                "changeset_digest": hex::encode(bundle.changeset_digest),
                "base": {
                    "object_id": bundle.base.object_id,
                    "object_kind": bundle.base.object_kind,
                    "algo": "sha1"
                },
                "changeset_id": bundle.changeset_id,
                "schema_version": bundle.schema_version,
                "schema": bundle.schema
            });
            serde_json::to_vec_pretty(&noncanonical).unwrap()
        }

        /// Helper to build a dispatcher with CAS and a privileged context.
        fn make_dispatcher_with_cas() -> (PrivilegedDispatcher, Arc<MemoryCas>) {
            let cas = Arc::new(MemoryCas::default());
            let dispatcher = PrivilegedDispatcher::new()
                .with_cas(Arc::clone(&cas) as Arc<dyn ContentAddressedStore>);
            (dispatcher, cas)
        }

        fn privileged_ctx() -> ConnectionContext {
            ConnectionContext::privileged_session_open(Some(PeerCredentials {
                uid: 1000,
                gid: 1000,
                pid: Some(12345),
            }))
        }

        /// Helper to first claim work so we have a registered `work_id`.
        fn claim_work(dispatcher: &PrivilegedDispatcher, ctx: &ConnectionContext) -> String {
            let request = ClaimWorkRequest {
                actor_id: "test:actor".to_string(),
                role: WorkRole::Implementer.into(),
                credential_signature: vec![1, 2, 3],
                nonce: vec![4, 5, 6],
            };
            let frame = encode_claim_work_request(&request);
            let response = dispatcher.dispatch(&frame, ctx).unwrap();
            match response {
                PrivilegedResponse::ClaimWork(resp) => resp.work_id,
                other => panic!("Expected ClaimWork response, got: {other:?}"),
            }
        }

        #[test]
        fn test_publish_changeset_routing() {
            let (dispatcher, _cas) = make_dispatcher_with_cas();
            let ctx = privileged_ctx();

            // First claim work to get a valid work_id
            let work_id = claim_work(&dispatcher, &ctx);

            let bundle_bytes = make_bundle_json("cs-001");
            let request = PublishChangeSetRequest {
                work_id,
                bundle_bytes,
            };
            let frame = encode_publish_changeset_request(&request);

            let response = dispatcher.dispatch(&frame, &ctx).unwrap();
            assert!(
                matches!(response, PrivilegedResponse::PublishChangeSet(_)),
                "Expected PublishChangeSet response, got: {response:?}"
            );
        }

        #[test]
        fn test_publish_changeset_returns_digest_and_cas_hash() {
            let (dispatcher, cas) = make_dispatcher_with_cas();
            let ctx = privileged_ctx();
            let work_id = claim_work(&dispatcher, &ctx);

            let bundle_bytes = make_bundle_json("cs-002");
            let request = PublishChangeSetRequest {
                work_id: work_id.clone(),
                bundle_bytes: bundle_bytes.clone(),
            };
            let frame = encode_publish_changeset_request(&request);

            let response = dispatcher.dispatch(&frame, &ctx).unwrap();
            match response {
                PrivilegedResponse::PublishChangeSet(resp) => {
                    // changeset_digest should be a 64-char hex string
                    assert_eq!(
                        resp.changeset_digest.len(),
                        64,
                        "changeset_digest should be 64 hex chars"
                    );
                    // cas_hash should be a 64-char hex string
                    assert_eq!(resp.cas_hash.len(), 64, "cas_hash should be 64 hex chars");
                    // work_id should match
                    assert_eq!(resp.work_id, work_id);
                    // event_id should be non-empty
                    assert!(!resp.event_id.is_empty(), "event_id should be non-empty");

                    // Verify the bundle was stored in CAS
                    let hash_bytes: [u8; 32] =
                        hex::decode(&resp.cas_hash).unwrap().try_into().unwrap();
                    assert!(
                        cas.exists(&hash_bytes).unwrap(),
                        "Bundle should exist in CAS"
                    );

                    // Verify CAS content matches bundle bytes
                    let stored = cas.retrieve(&hash_bytes).unwrap();
                    assert_eq!(stored, bundle_bytes, "CAS content should match input");
                },
                other => panic!("Expected PublishChangeSet response, got: {other:?}"),
            }
        }

        #[test]
        fn test_publish_changeset_emits_ledger_event() {
            let (dispatcher, _cas) = make_dispatcher_with_cas();
            let ctx = privileged_ctx();
            let work_id = claim_work(&dispatcher, &ctx);

            let bundle_bytes = make_bundle_json("cs-003");
            let request = PublishChangeSetRequest {
                work_id: work_id.clone(),
                bundle_bytes,
            };
            let frame = encode_publish_changeset_request(&request);
            let response = dispatcher.dispatch(&frame, &ctx).unwrap();

            let event_id = match &response {
                PrivilegedResponse::PublishChangeSet(resp) => resp.event_id.clone(),
                other => panic!("Expected PublishChangeSet response, got: {other:?}"),
            };

            // Query ledger for the emitted event
            let events = dispatcher.event_emitter.get_events_by_work_id(&work_id);
            let changeset_events: Vec<_> = events
                .iter()
                .filter(|e| e.event_type == "changeset_published")
                .collect();
            assert_eq!(
                changeset_events.len(),
                1,
                "Should have exactly one changeset_published event"
            );
            assert_eq!(changeset_events[0].event_id, event_id);

            // Verify event payload contains expected fields
            let payload: serde_json::Value =
                serde_json::from_slice(&changeset_events[0].payload).unwrap();
            assert_eq!(payload["event_type"], "changeset_published");
            assert_eq!(payload["work_id"], work_id);
            assert!(payload["changeset_digest"].is_string());
            assert!(payload["cas_hash"].is_string());
            assert!(payload["actor_id"].is_string());
            // Binding test evidence: timestamp_ns must be > 0
            assert!(
                payload["timestamp_ns"].as_u64().unwrap() > 0,
                "timestamp_ns must be > 0"
            );
        }

        #[test]
        fn test_publish_changeset_idempotent() {
            let (dispatcher, _cas) = make_dispatcher_with_cas();
            let ctx = privileged_ctx();
            let work_id = claim_work(&dispatcher, &ctx);

            let bundle_bytes = make_bundle_json("cs-004");

            // First publish
            let request1 = PublishChangeSetRequest {
                work_id: work_id.clone(),
                bundle_bytes: bundle_bytes.clone(),
            };
            let frame1 = encode_publish_changeset_request(&request1);
            let response1 = dispatcher.dispatch(&frame1, &ctx).unwrap();
            let (digest1, event_id1) = match &response1 {
                PrivilegedResponse::PublishChangeSet(resp) => {
                    (resp.changeset_digest.clone(), resp.event_id.clone())
                },
                other => panic!("Expected PublishChangeSet response, got: {other:?}"),
            };

            // Second publish with same bundle
            let request2 = PublishChangeSetRequest {
                work_id: work_id.clone(),
                bundle_bytes,
            };
            let frame2 = encode_publish_changeset_request(&request2);
            let response2 = dispatcher.dispatch(&frame2, &ctx).unwrap();
            let (digest2, event_id2) = match &response2 {
                PrivilegedResponse::PublishChangeSet(resp) => {
                    (resp.changeset_digest.clone(), resp.event_id.clone())
                },
                other => panic!("Expected PublishChangeSet response, got: {other:?}"),
            };

            // Should return same digest
            assert_eq!(digest1, digest2, "Idempotent: digest should match");
            // Should return same event_id (no duplicate events)
            assert_eq!(
                event_id1, event_id2,
                "Idempotent: should return same event_id"
            );

            // Only one changeset_published event should exist
            let events = dispatcher.event_emitter.get_events_by_work_id(&work_id);
            let changeset_count = events
                .iter()
                .filter(|e| e.event_type == "changeset_published")
                .count();
            assert_eq!(
                changeset_count, 1,
                "Idempotent: only one changeset_published event"
            );
        }

        #[test]
        fn test_publish_changeset_semantic_idempotent_noncanonical_json() {
            let (dispatcher, _cas) = make_dispatcher_with_cas();
            let ctx = privileged_ctx();
            let work_id = claim_work(&dispatcher, &ctx);

            let canonical_bundle = make_bundle_json("cs-semantic-idempotent");
            let noncanonical_bundle = make_noncanonical_bundle_json("cs-semantic-idempotent");

            let response1 = dispatcher
                .dispatch(
                    &encode_publish_changeset_request(&PublishChangeSetRequest {
                        work_id: work_id.clone(),
                        bundle_bytes: canonical_bundle,
                    }),
                    &ctx,
                )
                .unwrap();
            let (digest1, cas_hash1, event_id1) = match response1 {
                PrivilegedResponse::PublishChangeSet(resp) => {
                    (resp.changeset_digest, resp.cas_hash, resp.event_id)
                },
                other => panic!("Expected first PublishChangeSet response, got: {other:?}"),
            };

            let response2 = dispatcher
                .dispatch(
                    &encode_publish_changeset_request(&PublishChangeSetRequest {
                        work_id: work_id.clone(),
                        bundle_bytes: noncanonical_bundle,
                    }),
                    &ctx,
                )
                .unwrap();
            let (digest2, cas_hash2, event_id2) = match response2 {
                PrivilegedResponse::PublishChangeSet(resp) => {
                    (resp.changeset_digest, resp.cas_hash, resp.event_id)
                },
                other => panic!("Expected second PublishChangeSet response, got: {other:?}"),
            };

            assert_eq!(digest1, digest2, "semantic duplicate must reuse digest");
            assert_eq!(
                event_id1, event_id2,
                "semantic duplicate must replay event_id"
            );
            assert_eq!(
                cas_hash1, cas_hash2,
                "semantic duplicate must replay bound CAS hash"
            );

            let events = dispatcher.event_emitter.get_events_by_work_id(&work_id);
            let changeset_count = events
                .iter()
                .filter(|e| e.event_type == "changeset_published")
                .count();
            assert_eq!(
                changeset_count, 1,
                "semantic duplicate must not create duplicate changeset events"
            );
        }

        #[test]
        fn test_publish_changeset_rejects_digest_mismatch() {
            let (dispatcher, _cas) = make_dispatcher_with_cas();
            let ctx = privileged_ctx();
            let work_id = claim_work(&dispatcher, &ctx);

            let mut bundle = make_valid_bundle("cs-digest-mismatch");
            bundle.changeset_digest = [0xAB; 32];
            let request = PublishChangeSetRequest {
                work_id,
                bundle_bytes: serde_json::to_vec(&bundle).unwrap(),
            };

            let response = dispatcher
                .dispatch(&encode_publish_changeset_request(&request), &ctx)
                .unwrap();
            match response {
                PrivilegedResponse::Error(err) => {
                    assert_eq!(
                        err.code,
                        PrivilegedErrorCode::CapabilityRequestRejected as i32
                    );
                    assert!(
                        err.message.contains("changeset_digest mismatch"),
                        "Expected digest mismatch rejection, got: {}",
                        err.message
                    );
                },
                other => panic!("Expected digest mismatch rejection, got: {other:?}"),
            }
        }

        #[test]
        fn test_publish_changeset_rejects_unknown_work_id() {
            let (dispatcher, _cas) = make_dispatcher_with_cas();
            let ctx = privileged_ctx();

            let bundle_bytes = make_bundle_json("cs-005");
            let request = PublishChangeSetRequest {
                work_id: "W-nonexistent-001".to_string(),
                bundle_bytes,
            };
            let frame = encode_publish_changeset_request(&request);
            let response = dispatcher.dispatch(&frame, &ctx).unwrap();

            assert!(
                matches!(response, PrivilegedResponse::Error(_)),
                "Should reject unknown work_id"
            );
        }

        #[test]
        fn test_publish_changeset_rejects_empty_work_id() {
            let (dispatcher, _cas) = make_dispatcher_with_cas();
            let ctx = privileged_ctx();

            let request = PublishChangeSetRequest {
                work_id: String::new(),
                bundle_bytes: make_bundle_json("cs-006"),
            };
            let frame = encode_publish_changeset_request(&request);
            let response = dispatcher.dispatch(&frame, &ctx).unwrap();

            assert!(
                matches!(response, PrivilegedResponse::Error(_)),
                "Should reject empty work_id"
            );
        }

        #[test]
        fn test_publish_changeset_rejects_empty_bundle() {
            let (dispatcher, _cas) = make_dispatcher_with_cas();
            let ctx = privileged_ctx();
            let work_id = claim_work(&dispatcher, &ctx);

            let request = PublishChangeSetRequest {
                work_id,
                bundle_bytes: vec![],
            };
            let frame = encode_publish_changeset_request(&request);
            let response = dispatcher.dispatch(&frame, &ctx).unwrap();

            assert!(
                matches!(response, PrivilegedResponse::Error(_)),
                "Should reject empty bundle_bytes"
            );
        }

        #[test]
        fn test_publish_changeset_rejects_invalid_json() {
            let (dispatcher, _cas) = make_dispatcher_with_cas();
            let ctx = privileged_ctx();
            let work_id = claim_work(&dispatcher, &ctx);

            let request = PublishChangeSetRequest {
                work_id,
                bundle_bytes: b"not valid json".to_vec(),
            };
            let frame = encode_publish_changeset_request(&request);
            let response = dispatcher.dispatch(&frame, &ctx).unwrap();

            match response {
                PrivilegedResponse::Error(err) => {
                    assert_eq!(
                        err.code,
                        PrivilegedErrorCode::CapabilityRequestRejected as i32
                    );
                    assert!(
                        err.message.contains("invalid ChangeSetBundleV1 JSON"),
                        "Expected invalid JSON rejection, got: {}",
                        err.message
                    );
                },
                other => panic!("Expected invalid JSON error, got: {other:?}"),
            }
        }

        #[test]
        fn test_publish_changeset_rejects_empty_file_manifest() {
            let (dispatcher, _cas) = make_dispatcher_with_cas();
            let ctx = privileged_ctx();
            let work_id = claim_work(&dispatcher, &ctx);

            let bundle = ChangeSetBundleV1::builder()
                .changeset_id("cs-empty-manifest")
                .base(GitObjectRef {
                    algo: HashAlgo::Sha1,
                    object_kind: "commit".to_string(),
                    object_id: "a".repeat(40),
                })
                .diff_hash([0x42; 32])
                .file_manifest(vec![])
                .binary_detected(false)
                .build()
                .expect("bundle should build");
            let bundle_bytes = serde_json::to_vec(&bundle).unwrap();

            let request = PublishChangeSetRequest {
                work_id,
                bundle_bytes,
            };
            let frame = encode_publish_changeset_request(&request);
            let response = dispatcher.dispatch(&frame, &ctx).unwrap();

            match response {
                PrivilegedResponse::Error(err) => {
                    assert_eq!(
                        err.code,
                        PrivilegedErrorCode::CapabilityRequestRejected as i32
                    );
                    assert!(
                        err.message.contains("file_manifest"),
                        "Expected bundle validation rejection, got: {}",
                        err.message
                    );
                },
                other => panic!("Expected file_manifest rejection, got: {other:?}"),
            }
        }

        #[test]
        fn test_publish_changeset_rejects_no_cas() {
            // Dispatcher starts WITH default MemoryCas (TCK-00416) so that
            // ClaimWork authority binding validation can succeed. After claiming,
            // strip CAS to verify PublishChangeSet fail-closed when CAS absent.
            let dispatcher = PrivilegedDispatcher::new();
            let ctx = privileged_ctx();
            let work_id = claim_work(&dispatcher, &ctx);

            // Strip CAS AFTER claiming work
            let dispatcher = dispatcher.without_cas();

            let request = PublishChangeSetRequest {
                work_id,
                bundle_bytes: make_bundle_json("cs-no-cas"),
            };
            let frame = encode_publish_changeset_request(&request);
            let response = dispatcher.dispatch(&frame, &ctx).unwrap();

            assert!(
                matches!(response, PrivilegedResponse::Error(_)),
                "Should reject when CAS not configured"
            );
        }

        #[test]
        fn test_publish_changeset_permission_denied_for_non_privileged() {
            let (dispatcher, _cas) = make_dispatcher_with_cas();
            let ctx = ConnectionContext::session_open(
                Some(PeerCredentials {
                    uid: 1000,
                    gid: 1000,
                    pid: Some(12345),
                }),
                Some("session-test-001".to_string()),
            );

            let request = PublishChangeSetRequest {
                work_id: "W-test-001".to_string(),
                bundle_bytes: make_bundle_json("cs-perm"),
            };
            let frame = encode_publish_changeset_request(&request);
            let response = dispatcher.dispatch(&frame, &ctx).unwrap();

            assert!(
                matches!(response, PrivilegedResponse::Error(_)),
                "Should reject non-privileged connection"
            );
        }

        #[test]
        fn test_publish_changeset_tag_70_routing() {
            // Verify tag 70 routes correctly
            assert_eq!(
                PrivilegedMessageType::PublishChangeSet.tag(),
                70,
                "PublishChangeSet tag should be 70"
            );
            assert_eq!(
                PrivilegedMessageType::from_tag(70),
                Some(PrivilegedMessageType::PublishChangeSet),
                "Tag 70 should map to PublishChangeSet"
            );
        }

        /// TCK-00408: Verify actor ownership check rejects non-owner callers.
        ///
        /// A caller whose peer credentials produce a different `actor_id` than
        /// the work claim owner must be denied.
        #[test]
        fn test_publish_changeset_rejects_ownership_mismatch() {
            let (dispatcher, cas) = make_dispatcher_with_cas();
            let owner_ctx = privileged_ctx(); // uid=1000, gid=1000

            // Claim work as the owner
            let work_id = claim_work(&dispatcher, &owner_ctx);

            // Create a different caller context (different uid -> different actor_id)
            let non_owner_ctx = ConnectionContext::privileged_session_open(Some(PeerCredentials {
                uid: 9999,
                gid: 9999,
                pid: Some(99999),
            }));

            let bundle_bytes = make_bundle_json("cs-ownership-test");
            let expected_hash = *blake3::hash(&bundle_bytes).as_bytes();
            let request = PublishChangeSetRequest {
                work_id: work_id.clone(),
                bundle_bytes,
            };
            let frame = encode_publish_changeset_request(&request);
            let response = dispatcher.dispatch(&frame, &non_owner_ctx).unwrap();

            match &response {
                PrivilegedResponse::Error(err) => {
                    assert_eq!(
                        err.code,
                        PrivilegedErrorCode::PermissionDenied as i32,
                        "Ownership mismatch should return PermissionDenied"
                    );
                    assert!(
                        err.message.contains("does not own work claim"),
                        "Error message should mention ownership: {}",
                        err.message
                    );
                },
                other => {
                    panic!("Expected PermissionDenied error for ownership mismatch, got: {other:?}")
                },
            }

            // Ownership rejection must occur before CAS mutation or event emission.
            assert!(
                !cas.exists(&expected_hash)
                    .expect("CAS exists check should succeed"),
                "ownership mismatch must not write bundle to CAS"
            );
            let changeset_events = dispatcher
                .event_emitter
                .get_events_by_work_id(&work_id)
                .into_iter()
                .filter(|event| event.event_type == "changeset_published")
                .count();
            assert_eq!(
                changeset_events, 0,
                "ownership mismatch must not emit changeset_published events"
            );
        }
    }

    // ========================================================================
    // TCK-00389: IngestReviewReceipt Tests
    // ========================================================================
    mod ingest_review_receipt {
        use apm2_core::evidence::{ContentAddressedStore, MemoryCas};

        use super::*;

        /// Standard peer credentials used by all `IngestReviewReceipt` tests.
        fn test_peer_credentials() -> PeerCredentials {
            PeerCredentials {
                uid: 1000,
                gid: 1000,
                pid: Some(12345),
            }
        }

        /// Standard test artifact bundle content stored in CAS during setup.
        const TEST_ARTIFACT_CONTENT: &[u8] = b"test-artifact-bundle-content";

        /// Returns the CAS hash of `TEST_ARTIFACT_CONTENT`. Use this as
        /// `artifact_bundle_hash` in test requests that must pass CAS
        /// existence validation.
        fn test_artifact_bundle_hash() -> Vec<u8> {
            let cas = MemoryCas::default();
            let result = cas.store(TEST_ARTIFACT_CONTENT).unwrap();
            result.hash.to_vec()
        }

        pub(super) fn decode_event_payload(event: &SignedLedgerEvent) -> serde_json::Value {
            serde_json::from_slice(&event.payload).expect("event payload must be valid JSON")
        }

        pub(super) fn decode_wrapped_or_direct_event_payload(
            event: &SignedLedgerEvent,
        ) -> serde_json::Value {
            let wrapper = decode_event_payload(event);
            wrapper
                .get("payload")
                .and_then(serde_json::Value::as_str)
                .and_then(|hex_payload| hex::decode(hex_payload).ok())
                .and_then(|inner| serde_json::from_slice::<serde_json::Value>(&inner).ok())
                .unwrap_or(wrapper)
        }

        /// Stores a canonical `ClockProfileV1` + `TimeEnvelopeV1` in CAS and
        /// returns the envelope hash reference (`time_envelope_ref`).
        pub(super) fn store_time_authority_artifacts(
            cas: &dyn ContentAddressedStore,
            wall_time_source: WallTimeSource,
            include_attestation: bool,
        ) -> String {
            let clock_profile = ClockProfile {
                attestation: include_attestation.then(|| serde_json::json!({"kind": "test"})),
                build_fingerprint: "apm2-daemon/test".to_string(),
                hlc_enabled: true,
                max_wall_uncertainty_ns: 1_000_000,
                monotonic_source: apm2_core::htf::MonotonicSource::ClockMonotonic,
                profile_policy_id: "test-policy".to_string(),
                tick_rate_hz: 1_000_000_000,
                wall_time_source,
            };

            let profile_bytes = clock_profile
                .canonical_bytes()
                .expect("clock profile canonicalization should succeed");
            let profile_hash = clock_profile
                .canonical_hash()
                .expect("clock profile hash should succeed");
            let stored_profile = cas
                .store(&profile_bytes)
                .expect("clock profile should store");
            assert_eq!(
                stored_profile.hash, profile_hash,
                "stored profile hash must match canonical hash"
            );

            let envelope = TimeEnvelope {
                clock_profile_hash: hex::encode(profile_hash),
                hlc: apm2_core::htf::Hlc {
                    logical: 0,
                    wall_ns: 1_700_000_000_000_000_000,
                },
                ledger_anchor: apm2_core::htf::LedgerTime::new("test-ledger", 0, 1),
                mono: apm2_core::htf::MonotonicReading {
                    end_tick: Some(10_000_000_000),
                    source: apm2_core::htf::MonotonicSource::ClockMonotonic,
                    start_tick: 0,
                    tick_rate_hz: 1_000_000_000,
                },
                notes: Some("ingest-review-receipt-test".to_string()),
                wall: apm2_core::htf::BoundedWallInterval::new(
                    1_700_000_000_000_000_000,
                    1_700_000_000_100_000_000,
                    wall_time_source,
                    "95%",
                )
                .expect("bounded wall interval should be valid"),
            };

            let envelope_bytes = envelope
                .canonical_bytes()
                .expect("time envelope canonicalization should succeed");
            let envelope_hash = envelope
                .canonical_hash()
                .expect("time envelope hash should succeed");
            let stored_envelope = cas
                .store(&envelope_bytes)
                .expect("time envelope should store");
            assert_eq!(
                stored_envelope.hash, envelope_hash,
                "stored envelope hash must match canonical hash"
            );

            hex::encode(envelope_hash)
        }

        pub(super) struct TestLeaseConfig<'a> {
            pub dispatcher: &'a PrivilegedDispatcher,
            pub cas: &'a dyn ContentAddressedStore,
            pub lease_id: &'a str,
            pub work_id: &'a str,
            pub gate_id: &'a str,
            pub executor_actor_id: &'a str,
            pub policy_hash: [u8; 32],
            pub wall_time_source: WallTimeSource,
            pub include_attestation: bool,
        }

        pub(super) fn register_full_test_lease(cfg: &TestLeaseConfig<'_>) {
            let time_envelope_ref = store_time_authority_artifacts(
                cfg.cas,
                cfg.wall_time_source,
                cfg.include_attestation,
            );
            let signer = apm2_core::crypto::Signer::generate();
            let full_lease =
                apm2_core::fac::GateLeaseBuilder::new(cfg.lease_id, cfg.work_id, cfg.gate_id)
                    .changeset_digest([0x42; 32])
                    .executor_actor_id(cfg.executor_actor_id)
                    .issued_at(1_000_000)
                    .expires_at(2_000_000)
                    .policy_hash(cfg.policy_hash)
                    .issuer_actor_id("issuer-test")
                    .time_envelope_ref(&time_envelope_ref)
                    .build_and_sign(&signer);
            cfg.dispatcher
                .lease_validator
                .register_full_lease(&full_lease)
                .expect("full lease registration should succeed");
        }

        /// Creates a dispatcher with CAS, a registered lease, and Tier0 work
        /// claim for testing. CAS is pre-populated with the standard test
        /// artifact bundle so that CAS existence checks pass on success paths.
        ///
        /// SECURITY (v6 Finding 1): The `executor_actor_id` is derived from the
        /// test peer credentials, ensuring the authenticated caller identity
        /// matches the lease executor. The `_executor_hint` parameter is
        /// ignored — it exists only for backward compatibility with test
        /// call sites that previously passed an arbitrary reviewer name.
        fn setup_dispatcher_with_lease(
            lease_id: &str,
            work_id: &str,
            gate_id: &str,
            _executor_hint: &str,
        ) -> (PrivilegedDispatcher, ConnectionContext) {
            setup_dispatcher_with_lease_and_tier(lease_id, work_id, gate_id, "", 0)
        }

        /// Creates a dispatcher with CAS, a registered lease, and a work claim
        /// at the specified risk tier. Used to test attestation ratcheting.
        ///
        /// TCK-00408: CAS is now mandatory for ingest (fail-closed). The
        /// standard test artifact bundle is pre-stored in CAS.
        ///
        /// SECURITY (v6 Finding 1): Registers the lease executor as the
        /// identity derived from `test_peer_credentials()`, binding the test
        /// lease to the same identity the handler will derive from the
        /// `ConnectionContext`.
        fn setup_dispatcher_with_lease_and_tier(
            lease_id: &str,
            work_id: &str,
            gate_id: &str,
            _executor_hint: &str,
            risk_tier: u8,
        ) -> (PrivilegedDispatcher, ConnectionContext) {
            let peer_creds = test_peer_credentials();
            let executor_actor_id = derive_actor_id(&peer_creds);

            // TCK-00408: CAS is mandatory for ingest (fail-closed).
            let cas = Arc::new(MemoryCas::default());
            cas.store(TEST_ARTIFACT_CONTENT).unwrap();

            let kernel: Arc<dyn apm2_core::pcac::AuthorityJoinKernel> =
                Arc::new(crate::pcac::InProcessKernel::new(1));
            let pcac_gate = Arc::new(crate::pcac::LifecycleGate::new(kernel));
            let dispatcher = PrivilegedDispatcher::new()
                .with_cas(Arc::clone(&cas) as Arc<dyn ContentAddressedStore>)
                .with_pcac_lifecycle_gate(pcac_gate)
                .with_privileged_pcac_policy(crate::protocol::dispatch::PrivilegedPcacPolicy {});
            dispatcher.lease_validator.register_lease_with_executor(
                lease_id,
                work_id,
                gate_id,
                &executor_actor_id,
            );
            register_full_test_lease(&TestLeaseConfig {
                dispatcher: &dispatcher,
                cas: cas.as_ref(),
                lease_id,
                work_id,
                gate_id,
                executor_actor_id: &executor_actor_id,
                policy_hash: [0u8; 32],
                wall_time_source: WallTimeSource::AuthenticatedNts,
                include_attestation: true,
            });
            // Register work claim so risk-tier resolution succeeds.
            let mut policy_resolution = test_policy_resolution_with_lineage(
                work_id,
                &executor_actor_id,
                WorkRole::Reviewer,
                risk_tier,
            );
            policy_resolution.pcac_policy = Some(apm2_core::pcac::PcacPolicyKnobs::default());
            seed_policy_lineage_for_test(
                cas.as_ref(),
                work_id,
                &executor_actor_id,
                WorkRole::Reviewer,
                &policy_resolution,
            );
            let claim = WorkClaim {
                work_id: work_id.to_string(),
                lease_id: lease_id.to_string(),
                actor_id: executor_actor_id,
                role: WorkRole::Reviewer,
                policy_resolution,
                executor_custody_domains: vec![],
                author_custody_domains: vec![],
                permeability_receipt: None,
            };
            dispatcher.work_registry.register_claim(claim).unwrap();
            let ctx = ConnectionContext::privileged_session_open(Some(peer_creds));
            (dispatcher, ctx)
        }

        #[test]
        fn test_ingest_review_receipt_approve_success() {
            let (dispatcher, ctx) =
                setup_dispatcher_with_lease("lease-001", "W-001", "gate-001", "reviewer-alpha");

            let request = IngestReviewReceiptRequest {
                lease_id: "lease-001".to_string(),
                receipt_id: "RR-001".to_string(),
                reviewer_actor_id: "reviewer-alpha".to_string(),
                changeset_digest: vec![0x42; 32],
                artifact_bundle_hash: test_artifact_bundle_hash(),
                verdict: ReviewReceiptVerdict::Approve.into(),
                blocked_reason_code: 0,
                blocked_log_hash: vec![],
                identity_proof_hash: vec![0x99; 32],
            };
            let frame = encode_ingest_review_receipt_request(&request);

            let response = dispatcher.dispatch(&frame, &ctx).unwrap();
            match response {
                PrivilegedResponse::IngestReviewReceipt(resp) => {
                    assert_eq!(resp.receipt_id, "RR-001");
                    assert_eq!(resp.event_type, "ReviewReceiptRecorded");
                    assert!(!resp.event_id.is_empty(), "event_id must be non-empty");
                },
                PrivilegedResponse::Error(err) => {
                    panic!("Expected IngestReviewReceipt, got error: {}", err.message);
                },
                other => panic!("Expected IngestReviewReceipt, got {other:?}"),
            }
        }

        #[test]
        fn test_ingest_review_receipt_blocked_success() {
            let (dispatcher, ctx) =
                setup_dispatcher_with_lease("lease-002", "W-002", "gate-002", "reviewer-beta");

            let request = IngestReviewReceiptRequest {
                lease_id: "lease-002".to_string(),
                receipt_id: "RB-001".to_string(),
                reviewer_actor_id: "reviewer-beta".to_string(),
                changeset_digest: vec![0x42; 32],
                artifact_bundle_hash: test_artifact_bundle_hash(),
                verdict: ReviewReceiptVerdict::Blocked.into(),
                blocked_reason_code: 1, // APPLY_FAILED
                blocked_log_hash: vec![0x55; 32],
                identity_proof_hash: vec![0x99; 32],
            };
            let frame = encode_ingest_review_receipt_request(&request);

            let response = dispatcher.dispatch(&frame, &ctx).unwrap();
            match response {
                PrivilegedResponse::IngestReviewReceipt(resp) => {
                    assert_eq!(resp.receipt_id, "RB-001");
                    assert_eq!(resp.event_type, "ReviewBlockedRecorded");
                    assert!(!resp.event_id.is_empty(), "event_id must be non-empty");
                },
                PrivilegedResponse::Error(err) => {
                    panic!("Expected IngestReviewReceipt, got error: {}", err.message);
                },
                other => panic!("Expected IngestReviewReceipt, got {other:?}"),
            }
        }

        /// v6 Finding 1: Reviewer identity is now derived from peer
        /// credentials, not from the request's `reviewer_actor_id`. To
        /// trigger a mismatch, the caller must have DIFFERENT peer
        /// credentials than the lease executor.
        #[test]
        fn test_ingest_review_receipt_reviewer_identity_mismatch() {
            let (dispatcher, _ctx) =
                setup_dispatcher_with_lease("lease-003", "W-003", "gate-003", "reviewer-gamma");

            // Create a context with DIFFERENT peer credentials (uid=9999)
            // so the authenticated identity won't match the lease executor.
            let wrong_ctx = ConnectionContext::privileged_session_open(Some(PeerCredentials {
                uid: 9999,
                gid: 9999,
                pid: Some(99999),
            }));

            let request = IngestReviewReceiptRequest {
                lease_id: "lease-003".to_string(),
                receipt_id: "RR-002".to_string(),
                reviewer_actor_id: "does-not-matter".to_string(), // Ignored by handler
                changeset_digest: vec![0x42; 32],
                artifact_bundle_hash: test_artifact_bundle_hash(),
                verdict: ReviewReceiptVerdict::Approve.into(),
                blocked_reason_code: 0,
                blocked_log_hash: vec![],
                identity_proof_hash: vec![0x99; 32],
            };
            let frame = encode_ingest_review_receipt_request(&request);

            let response = dispatcher.dispatch(&frame, &wrong_ctx).unwrap();
            match response {
                PrivilegedResponse::Error(err) => {
                    assert!(
                        err.message.contains("does not match gate lease executor"),
                        "Expected reviewer identity mismatch error, got: {}",
                        err.message
                    );
                },
                other => panic!("Expected error, got {other:?}"),
            }
        }

        /// v6 Finding 1 negative test: caller with wrong peer credentials is
        /// rejected even if they supply the correct `reviewer_actor_id` in the
        /// request payload. The handler must ignore the request field and
        /// use the authenticated identity.
        #[test]
        fn test_ingest_review_receipt_wrong_peer_creds_correct_request_field_rejected() {
            // Register lease with the derived identity from standard test creds
            let (dispatcher, _ctx) =
                setup_dispatcher_with_lease("lease-neg", "W-NEG", "gate-neg", "unused");

            // The correct executor_actor_id as registered on the lease
            let correct_actor_id = derive_actor_id(&test_peer_credentials());

            // Caller supplies the correct reviewer_actor_id in the request,
            // but connects with DIFFERENT peer credentials.
            let wrong_ctx = ConnectionContext::privileged_session_open(Some(PeerCredentials {
                uid: 7777,
                gid: 7777,
                pid: Some(77777),
            }));

            let request = IngestReviewReceiptRequest {
                lease_id: "lease-neg".to_string(),
                receipt_id: "RR-NEG".to_string(),
                reviewer_actor_id: correct_actor_id, // Correct value, but ignored
                changeset_digest: vec![0x42; 32],
                artifact_bundle_hash: test_artifact_bundle_hash(),
                verdict: ReviewReceiptVerdict::Approve.into(),
                blocked_reason_code: 0,
                blocked_log_hash: vec![],
                identity_proof_hash: vec![0x99; 32],
            };
            let frame = encode_ingest_review_receipt_request(&request);

            let response = dispatcher.dispatch(&frame, &wrong_ctx).unwrap();
            match response {
                PrivilegedResponse::Error(err) => {
                    assert!(
                        err.message.contains("does not match gate lease executor"),
                        "Should reject despite correct request field, got: {}",
                        err.message
                    );
                },
                other => panic!("Expected rejection for wrong peer creds, got {other:?}"),
            }
        }

        /// v6 Finding 1: missing peer credentials must be rejected.
        #[test]
        fn test_ingest_review_receipt_missing_peer_credentials_rejected() {
            let (dispatcher, _ctx) =
                setup_dispatcher_with_lease("lease-no-creds", "W-NC", "gate-nc", "unused");

            let ctx_no_creds = ConnectionContext::privileged_session_open(None);

            let request = IngestReviewReceiptRequest {
                lease_id: "lease-no-creds".to_string(),
                receipt_id: "RR-NC".to_string(),
                reviewer_actor_id: "any".to_string(),
                changeset_digest: vec![0x42; 32],
                artifact_bundle_hash: test_artifact_bundle_hash(),
                verdict: ReviewReceiptVerdict::Approve.into(),
                blocked_reason_code: 0,
                blocked_log_hash: vec![],
                identity_proof_hash: vec![0x99; 32],
            };
            let frame = encode_ingest_review_receipt_request(&request);

            let response = dispatcher.dispatch(&frame, &ctx_no_creds).unwrap();
            match response {
                PrivilegedResponse::Error(err) => {
                    assert!(
                        err.message.contains("peer credentials required"),
                        "Expected peer credentials error, got: {}",
                        err.message
                    );
                },
                other => panic!("Expected error for missing creds, got {other:?}"),
            }
        }

        #[test]
        fn test_ingest_review_receipt_lease_not_found() {
            let dispatcher = PrivilegedDispatcher::new();
            let ctx = ConnectionContext::privileged_session_open(Some(PeerCredentials {
                uid: 1000,
                gid: 1000,
                pid: Some(12345),
            }));

            let request = IngestReviewReceiptRequest {
                lease_id: "nonexistent-lease".to_string(),
                receipt_id: "RR-003".to_string(),
                reviewer_actor_id: "reviewer-delta".to_string(),
                changeset_digest: vec![0x42; 32],
                artifact_bundle_hash: test_artifact_bundle_hash(),
                verdict: ReviewReceiptVerdict::Approve.into(),
                blocked_reason_code: 0,
                blocked_log_hash: vec![],
                identity_proof_hash: vec![0x99; 32],
            };
            let frame = encode_ingest_review_receipt_request(&request);

            let response = dispatcher.dispatch(&frame, &ctx).unwrap();
            match response {
                PrivilegedResponse::Error(err) => {
                    assert_eq!(
                        err.code,
                        PrivilegedErrorCode::GateLeaseMissing as i32,
                        "Should return GateLeaseMissing for unknown lease_id"
                    );
                    assert!(
                        err.message.contains("gate lease not found"),
                        "Error message should indicate lease not found, got: {}",
                        err.message
                    );
                },
                other => panic!("Expected error, got {other:?}"),
            }
        }

        #[test]
        fn test_ingest_review_receipt_empty_lease_id() {
            let dispatcher = PrivilegedDispatcher::new();
            let ctx = ConnectionContext::privileged_session_open(Some(PeerCredentials {
                uid: 1000,
                gid: 1000,
                pid: Some(12345),
            }));

            let request = IngestReviewReceiptRequest {
                lease_id: String::new(),
                receipt_id: "RR-004".to_string(),
                reviewer_actor_id: "reviewer".to_string(),
                changeset_digest: vec![0x42; 32],
                artifact_bundle_hash: test_artifact_bundle_hash(),
                verdict: ReviewReceiptVerdict::Approve.into(),
                blocked_reason_code: 0,
                blocked_log_hash: vec![],
                identity_proof_hash: vec![0x99; 32],
            };
            let frame = encode_ingest_review_receipt_request(&request);

            let response = dispatcher.dispatch(&frame, &ctx).unwrap();
            match response {
                PrivilegedResponse::Error(err) => {
                    assert!(
                        err.message.contains("lease_id is required"),
                        "Expected lease_id required error, got: {}",
                        err.message
                    );
                },
                other => panic!("Expected error, got {other:?}"),
            }
        }

        #[test]
        fn test_ingest_review_receipt_empty_receipt_id() {
            let (dispatcher, ctx) =
                setup_dispatcher_with_lease("lease-004", "W-004", "gate-004", "reviewer");

            let request = IngestReviewReceiptRequest {
                lease_id: "lease-004".to_string(),
                receipt_id: String::new(),
                reviewer_actor_id: "reviewer".to_string(),
                changeset_digest: vec![0x42; 32],
                artifact_bundle_hash: test_artifact_bundle_hash(),
                verdict: ReviewReceiptVerdict::Approve.into(),
                blocked_reason_code: 0,
                blocked_log_hash: vec![],
                identity_proof_hash: vec![0x99; 32],
            };
            let frame = encode_ingest_review_receipt_request(&request);

            let response = dispatcher.dispatch(&frame, &ctx).unwrap();
            match response {
                PrivilegedResponse::Error(err) => {
                    assert!(
                        err.message.contains("receipt_id is required"),
                        "Expected receipt_id required error, got: {}",
                        err.message
                    );
                },
                other => panic!("Expected error, got {other:?}"),
            }
        }

        #[test]
        fn test_ingest_review_receipt_unspecified_verdict() {
            let (dispatcher, ctx) =
                setup_dispatcher_with_lease("lease-005", "W-005", "gate-005", "reviewer");

            let request = IngestReviewReceiptRequest {
                lease_id: "lease-005".to_string(),
                receipt_id: "RR-005".to_string(),
                reviewer_actor_id: "reviewer".to_string(),
                changeset_digest: vec![0x42; 32],
                artifact_bundle_hash: test_artifact_bundle_hash(),
                verdict: 0, // UNSPECIFIED
                blocked_reason_code: 0,
                blocked_log_hash: vec![],
                identity_proof_hash: vec![0x99; 32],
            };
            let frame = encode_ingest_review_receipt_request(&request);

            let response = dispatcher.dispatch(&frame, &ctx).unwrap();
            match response {
                PrivilegedResponse::Error(err) => {
                    assert!(
                        err.message.contains("verdict must be APPROVE or BLOCKED"),
                        "Expected verdict error, got: {}",
                        err.message
                    );
                },
                other => panic!("Expected error, got {other:?}"),
            }
        }

        #[test]
        fn test_ingest_review_receipt_invalid_changeset_digest_length() {
            let (dispatcher, ctx) =
                setup_dispatcher_with_lease("lease-006", "W-006", "gate-006", "reviewer");

            let request = IngestReviewReceiptRequest {
                lease_id: "lease-006".to_string(),
                receipt_id: "RR-006".to_string(),
                reviewer_actor_id: "reviewer".to_string(),
                changeset_digest: vec![0x42; 16], // Wrong length
                artifact_bundle_hash: test_artifact_bundle_hash(),
                verdict: ReviewReceiptVerdict::Approve.into(),
                blocked_reason_code: 0,
                blocked_log_hash: vec![],
                identity_proof_hash: vec![0x99; 32],
            };
            let frame = encode_ingest_review_receipt_request(&request);

            let response = dispatcher.dispatch(&frame, &ctx).unwrap();
            match response {
                PrivilegedResponse::Error(err) => {
                    assert!(
                        err.message
                            .contains("changeset_digest must be exactly 32 bytes"),
                        "Expected changeset_digest length error, got: {}",
                        err.message
                    );
                },
                other => panic!("Expected error, got {other:?}"),
            }
        }

        #[test]
        fn test_ingest_review_receipt_missing_identity_proof_hash_rejected() {
            let (dispatcher, ctx) =
                setup_dispatcher_with_lease("lease-006b", "W-006b", "gate-006b", "reviewer");

            let request = IngestReviewReceiptRequest {
                lease_id: "lease-006b".to_string(),
                receipt_id: "RR-006b".to_string(),
                reviewer_actor_id: "reviewer".to_string(),
                changeset_digest: vec![0x42; 32],
                artifact_bundle_hash: test_artifact_bundle_hash(),
                verdict: ReviewReceiptVerdict::Approve.into(),
                blocked_reason_code: 0,
                blocked_log_hash: vec![],
                identity_proof_hash: vec![],
            };
            let frame = encode_ingest_review_receipt_request(&request);

            let response = dispatcher.dispatch(&frame, &ctx).unwrap();
            match response {
                PrivilegedResponse::Error(err) => {
                    assert!(
                        err.message
                            .contains("identity_proof_hash validation failed"),
                        "Expected identity_proof_hash validation error, got: {}",
                        err.message
                    );
                },
                other => panic!("Expected error, got {other:?}"),
            }
        }

        #[test]
        fn test_ingest_review_receipt_zero_identity_proof_hash_rejected() {
            let (dispatcher, ctx) =
                setup_dispatcher_with_lease("lease-006c", "W-006c", "gate-006c", "reviewer");

            let request = IngestReviewReceiptRequest {
                lease_id: "lease-006c".to_string(),
                receipt_id: "RR-006c".to_string(),
                reviewer_actor_id: "reviewer".to_string(),
                changeset_digest: vec![0x42; 32],
                artifact_bundle_hash: test_artifact_bundle_hash(),
                verdict: ReviewReceiptVerdict::Approve.into(),
                blocked_reason_code: 0,
                blocked_log_hash: vec![],
                identity_proof_hash: vec![0x00; 32],
            };
            let frame = encode_ingest_review_receipt_request(&request);

            let response = dispatcher.dispatch(&frame, &ctx).unwrap();
            match response {
                PrivilegedResponse::Error(err) => {
                    assert!(
                        err.message.contains("null commitment rejected"),
                        "Expected null commitment rejection, got: {}",
                        err.message
                    );
                },
                other => panic!("Expected error for zero identity proof hash, got {other:?}"),
            }
        }

        #[test]
        fn test_ingest_review_receipt_blocked_missing_log_hash() {
            let (dispatcher, ctx) =
                setup_dispatcher_with_lease("lease-007", "W-007", "gate-007", "reviewer");

            let request = IngestReviewReceiptRequest {
                lease_id: "lease-007".to_string(),
                receipt_id: "RB-002".to_string(),
                reviewer_actor_id: "reviewer".to_string(),
                changeset_digest: vec![0x42; 32],
                artifact_bundle_hash: test_artifact_bundle_hash(),
                verdict: ReviewReceiptVerdict::Blocked.into(),
                blocked_reason_code: 1,
                blocked_log_hash: vec![], // Missing log hash for BLOCKED
                identity_proof_hash: vec![0x99; 32],
            };
            let frame = encode_ingest_review_receipt_request(&request);

            let response = dispatcher.dispatch(&frame, &ctx).unwrap();
            match response {
                PrivilegedResponse::Error(err) => {
                    assert!(
                        err.message
                            .contains("blocked_log_hash must be exactly 32 bytes"),
                        "Expected blocked_log_hash error, got: {}",
                        err.message
                    );
                },
                other => panic!("Expected error, got {other:?}"),
            }
        }

        #[test]
        fn test_ingest_review_receipt_blocked_zero_reason_code() {
            let (dispatcher, ctx) =
                setup_dispatcher_with_lease("lease-008", "W-008", "gate-008", "reviewer");

            let request = IngestReviewReceiptRequest {
                lease_id: "lease-008".to_string(),
                receipt_id: "RB-003".to_string(),
                reviewer_actor_id: "reviewer".to_string(),
                changeset_digest: vec![0x42; 32],
                artifact_bundle_hash: test_artifact_bundle_hash(),
                verdict: ReviewReceiptVerdict::Blocked.into(),
                blocked_reason_code: 0, // Zero is invalid for BLOCKED
                blocked_log_hash: vec![0x55; 32],
                identity_proof_hash: vec![0x99; 32],
            };
            let frame = encode_ingest_review_receipt_request(&request);

            let response = dispatcher.dispatch(&frame, &ctx).unwrap();
            match response {
                PrivilegedResponse::Error(err) => {
                    assert!(
                        err.message.contains("blocked_reason_code must be non-zero"),
                        "Expected blocked_reason_code error, got: {}",
                        err.message
                    );
                },
                other => panic!("Expected error, got {other:?}"),
            }
        }

        #[test]
        fn test_ingest_review_receipt_non_privileged_rejected() {
            let dispatcher = PrivilegedDispatcher::new();
            // Non-privileged context (session socket)
            let ctx = ConnectionContext::session_open(
                Some(PeerCredentials {
                    uid: 1000,
                    gid: 1000,
                    pid: Some(12345),
                }),
                None,
            );

            let request = IngestReviewReceiptRequest {
                lease_id: "lease-009".to_string(),
                receipt_id: "RR-009".to_string(),
                reviewer_actor_id: "reviewer".to_string(),
                changeset_digest: vec![0x42; 32],
                artifact_bundle_hash: test_artifact_bundle_hash(),
                verdict: ReviewReceiptVerdict::Approve.into(),
                blocked_reason_code: 0,
                blocked_log_hash: vec![],
                identity_proof_hash: vec![0x99; 32],
            };
            let frame = encode_ingest_review_receipt_request(&request);

            let response = dispatcher.dispatch(&frame, &ctx).unwrap();
            match response {
                PrivilegedResponse::Error(err) => {
                    assert_eq!(
                        err.code,
                        PrivilegedErrorCode::PermissionDenied as i32,
                        "Non-privileged connections should get PERMISSION_DENIED"
                    );
                },
                other => panic!("Expected PERMISSION_DENIED error, got {other:?}"),
            }
        }

        #[test]
        fn test_ingest_review_receipt_ledger_event_persisted() {
            let (dispatcher, ctx) =
                setup_dispatcher_with_lease("lease-010", "W-010", "gate-010", "reviewer-zeta");

            // v6 Finding 1: reviewer_actor_id in request is now ignored by
            // the handler — the authenticated identity from peer credentials
            // is used instead.
            let expected_actor_id = derive_actor_id(&test_peer_credentials());

            let request = IngestReviewReceiptRequest {
                lease_id: "lease-010".to_string(),
                receipt_id: "RR-010".to_string(),
                reviewer_actor_id: "reviewer-zeta".to_string(),
                changeset_digest: vec![0x42; 32],
                artifact_bundle_hash: test_artifact_bundle_hash(),
                verdict: ReviewReceiptVerdict::Approve.into(),
                blocked_reason_code: 0,
                blocked_log_hash: vec![],
                identity_proof_hash: vec![0x99; 32],
            };
            let frame = encode_ingest_review_receipt_request(&request);

            let response = dispatcher.dispatch(&frame, &ctx).unwrap();
            let event_id = match response {
                PrivilegedResponse::IngestReviewReceipt(resp) => resp.event_id,
                other => panic!("Expected IngestReviewReceipt, got {other:?}"),
            };

            // Verify the event was persisted to the emitter
            let stored_event = dispatcher.event_emitter.get_event(&event_id);
            assert!(
                stored_event.is_some(),
                "Event should be persisted in emitter"
            );
            let event = stored_event.unwrap();
            assert_eq!(event.event_type, "review_receipt_recorded");
            // v6 Finding 1: actor_id must be the authenticated identity,
            // not the caller-supplied reviewer_actor_id.
            assert_eq!(event.actor_id, expected_actor_id);
            assert!(event.timestamp_ns > 0, "Timestamp must be non-zero (HTF)");
        }

        #[test]
        fn test_ingest_review_receipt_response_encoding() {
            let (dispatcher, ctx) =
                setup_dispatcher_with_lease("lease-011", "W-011", "gate-011", "reviewer-enc");

            let request = IngestReviewReceiptRequest {
                lease_id: "lease-011".to_string(),
                receipt_id: "RR-011".to_string(),
                reviewer_actor_id: "reviewer-enc".to_string(),
                changeset_digest: vec![0x42; 32],
                artifact_bundle_hash: test_artifact_bundle_hash(),
                verdict: ReviewReceiptVerdict::Approve.into(),
                blocked_reason_code: 0,
                blocked_log_hash: vec![],
                identity_proof_hash: vec![0x99; 32],
            };
            let frame = encode_ingest_review_receipt_request(&request);

            let response = dispatcher.dispatch(&frame, &ctx).unwrap();

            // Verify encode/decode roundtrip
            let encoded = response.encode();
            assert!(!encoded.is_empty(), "Encoded response should not be empty");
            assert_eq!(
                encoded[0],
                PrivilegedMessageType::IngestReviewReceipt.tag(),
                "Response tag should match IngestReviewReceipt"
            );

            // Decode the payload
            let decoded = IngestReviewReceiptResponse::decode_bounded(
                &encoded[1..],
                &DecodeConfig::default(),
            )
            .expect("decode should succeed");
            assert_eq!(decoded.receipt_id, "RR-011");
            assert_eq!(decoded.event_type, "ReviewReceiptRecorded");
        }

        #[test]
        fn test_ingest_review_receipt_message_type_tag() {
            assert_eq!(
                PrivilegedMessageType::IngestReviewReceipt.tag(),
                17,
                "IngestReviewReceipt should have tag 17"
            );
            assert_eq!(
                PrivilegedMessageType::from_tag(17),
                Some(PrivilegedMessageType::IngestReviewReceipt),
                "Tag 17 should resolve to IngestReviewReceipt"
            );
        }

        #[test]
        fn test_ingest_review_receipt_oversized_lease_id() {
            let dispatcher = PrivilegedDispatcher::new();
            let ctx = ConnectionContext::privileged_session_open(Some(PeerCredentials {
                uid: 1000,
                gid: 1000,
                pid: Some(12345),
            }));

            let request = IngestReviewReceiptRequest {
                lease_id: "x".repeat(MAX_ID_LENGTH + 1),
                receipt_id: "RR-012".to_string(),
                reviewer_actor_id: "reviewer".to_string(),
                changeset_digest: vec![0x42; 32],
                artifact_bundle_hash: test_artifact_bundle_hash(),
                verdict: ReviewReceiptVerdict::Approve.into(),
                blocked_reason_code: 0,
                blocked_log_hash: vec![],
                identity_proof_hash: vec![0x99; 32],
            };
            let frame = encode_ingest_review_receipt_request(&request);

            let response = dispatcher.dispatch(&frame, &ctx).unwrap();
            match response {
                PrivilegedResponse::Error(err) => {
                    assert!(
                        err.message.contains("lease_id exceeds maximum length"),
                        "Expected oversized lease_id error, got: {}",
                        err.message
                    );
                },
                other => panic!("Expected error, got {other:?}"),
            }
        }

        // =================================================================
        // TCK-00340: Attestation enforcement tests
        // =================================================================

        #[test]
        fn test_ingest_review_receipt_attestation_validates_signer_identity() {
            // Verify the attestation ratchet is wired into the production path
            // by confirming that a valid Tier0 request with proper reviewer
            // identity passes the attestation check. The attestation check
            // verifies:
            // 1. Internal consistency (non-empty signer_identity)
            // 2. Policy hash binding (changeset_digest match)
            // 3. Ratchet level (SelfSigned >= Tier0 requirement of None)
            //
            // The pre-attestation validation already rejects empty
            // reviewer_actor_id, so the attestation check provides
            // defense-in-depth for any future code paths that skip that guard.
            let (dispatcher, ctx) = setup_dispatcher_with_lease(
                "lease-attest-002",
                "W-ATT-002",
                "gate-attest",
                "reviewer-attest",
            );

            let request = IngestReviewReceiptRequest {
                lease_id: "lease-attest-002".to_string(),
                receipt_id: "RR-ATT-002".to_string(),
                reviewer_actor_id: "reviewer-attest".to_string(),
                changeset_digest: vec![0x42; 32],
                artifact_bundle_hash: test_artifact_bundle_hash(),
                verdict: ReviewReceiptVerdict::Approve.into(),
                blocked_reason_code: 0,
                blocked_log_hash: vec![],
                identity_proof_hash: vec![0x99; 32],
            };
            let frame = encode_ingest_review_receipt_request(&request);

            let response = dispatcher.dispatch(&frame, &ctx).unwrap();
            match response {
                PrivilegedResponse::IngestReviewReceipt(resp) => {
                    assert_eq!(resp.receipt_id, "RR-ATT-002");
                    assert_eq!(
                        resp.event_type, "ReviewReceiptRecorded",
                        "Attestation-validated Tier0 receipt should be accepted"
                    );
                },
                PrivilegedResponse::Error(err) => {
                    panic!("Valid attestation should pass, got error: {}", err.message);
                },
                other => panic!("Expected IngestReviewReceipt, got {other:?}"),
            }
        }

        #[test]
        fn test_ingest_review_receipt_tier0_constraint_fail_closed_semantics() {
            // Verify fail-closed semantics: the risk_tier variable is currently
            // hardcoded to Tier0 with an explicit guard that rejects non-Tier0.
            // When the work registry is wired (TODO RFC-0019), a non-Tier0 work
            // item MUST be rejected because this endpoint only provides
            // SelfSigned attestation, which is insufficient for Tier1+.
            //
            // This test validates that the attestation requirements table
            // correctly rejects SelfSigned at higher tiers (proving the
            // fail-closed guard is necessary and would work if activated).
            use apm2_core::fac::RiskTier;
            use apm2_core::fac::policy_inheritance::{
                AttestationLevel, AttestationRequirements, ReceiptAttestation, ReceiptKind,
                validate_receipt_attestation,
            };

            let requirements = AttestationRequirements::new();
            let changeset_digest = [0x42; 32];

            // Tier0 with SelfSigned should pass (current behavior)
            let tier0_attestation = ReceiptAttestation {
                kind: ReceiptKind::Review,
                level: AttestationLevel::SelfSigned,
                policy_hash: changeset_digest,
                signer_identity: "reviewer-001".to_string(),
                counter_signer_identity: None,
                threshold_signer_count: None,
            };
            assert!(
                validate_receipt_attestation(
                    &tier0_attestation,
                    RiskTier::Tier0,
                    &changeset_digest,
                    &requirements,
                )
                .is_ok(),
                "Tier0 SelfSigned must pass"
            );

            // Tier2 with SelfSigned should FAIL (requires CounterSigned for Review)
            // This proves the Tier0-only constraint is security-relevant.
            let tier2_result = validate_receipt_attestation(
                &tier0_attestation,
                RiskTier::Tier2,
                &changeset_digest,
                &requirements,
            );
            assert!(
                tier2_result.is_err(),
                "Tier2 with only SelfSigned must be rejected - proves the \
                 Tier0-only endpoint constraint is necessary for security"
            );

            // Tier4 with SelfSigned should FAIL (requires ThresholdSigned for Review)
            let tier4_result = validate_receipt_attestation(
                &tier0_attestation,
                RiskTier::Tier4,
                &changeset_digest,
                &requirements,
            );
            assert!(
                tier4_result.is_err(),
                "Tier4 with only SelfSigned must be rejected - proves the \
                 Tier0-only endpoint constraint prevents attestation bypass"
            );
        }

        #[test]
        fn test_ingest_review_receipt_tier0_self_signed_accepted() {
            // Verify that a valid Tier0 receipt with proper SelfSigned attestation
            // passes through the attestation check in the production path.
            let (dispatcher, ctx) = setup_dispatcher_with_lease(
                "lease-t0-001",
                "W-T0-001",
                "gate-001",
                "reviewer-tier0",
            );

            let request = IngestReviewReceiptRequest {
                lease_id: "lease-t0-001".to_string(),
                receipt_id: "RR-T0-001".to_string(),
                reviewer_actor_id: "reviewer-tier0".to_string(),
                changeset_digest: vec![0x42; 32],
                artifact_bundle_hash: test_artifact_bundle_hash(),
                verdict: ReviewReceiptVerdict::Approve.into(),
                blocked_reason_code: 0,
                blocked_log_hash: vec![],
                identity_proof_hash: vec![0x99; 32],
            };
            let frame = encode_ingest_review_receipt_request(&request);

            let response = dispatcher.dispatch(&frame, &ctx).unwrap();
            match response {
                PrivilegedResponse::IngestReviewReceipt(resp) => {
                    assert_eq!(resp.receipt_id, "RR-T0-001");
                    assert_eq!(
                        resp.event_type, "ReviewReceiptRecorded",
                        "Tier0 SelfSigned review receipt should be accepted"
                    );
                },
                PrivilegedResponse::Error(err) => {
                    panic!(
                        "Tier0 SelfSigned should be accepted, got error: {}",
                        err.message
                    );
                },
                other => panic!("Expected IngestReviewReceipt, got {other:?}"),
            }
        }

        // ====================================================================
        // TCK-00340: Risk-tier resolution integration tests
        // ====================================================================

        #[test]
        fn test_ingest_review_receipt_tier2_self_signed_rejected() {
            // Tier2 work items require CounterSigned attestation for Review
            // receipts. SelfSigned is insufficient — must be rejected.
            let (dispatcher, ctx) = setup_dispatcher_with_lease_and_tier(
                "lease-t2-001",
                "W-T2-001",
                "gate-001",
                "reviewer-tier2",
                2, // Tier2
            );

            let request = IngestReviewReceiptRequest {
                lease_id: "lease-t2-001".to_string(),
                receipt_id: "RR-T2-001".to_string(),
                reviewer_actor_id: "reviewer-tier2".to_string(),
                changeset_digest: vec![0x42; 32],
                artifact_bundle_hash: test_artifact_bundle_hash(),
                verdict: ReviewReceiptVerdict::Approve.into(),
                blocked_reason_code: 0,
                blocked_log_hash: vec![],
                identity_proof_hash: vec![0x99; 32],
            };
            let frame = encode_ingest_review_receipt_request(&request);

            let response = dispatcher.dispatch(&frame, &ctx).unwrap();
            match response {
                PrivilegedResponse::Error(err) => {
                    assert!(
                        err.message.contains("SelfSigned") || err.message.contains("requires"),
                        "Tier2 SelfSigned must be rejected, got: {}",
                        err.message
                    );
                    assert!(
                        err.message.contains("Tier2"),
                        "Error must mention the resolved tier, got: {}",
                        err.message
                    );
                },
                other => {
                    panic!("Expected rejection for Tier2 SelfSigned attestation, got {other:?}")
                },
            }
        }

        #[test]
        fn test_ingest_review_receipt_tier4_self_signed_rejected() {
            // Tier4 is the highest risk tier — requires ThresholdSigned.
            // SelfSigned must be rejected (fail-closed).
            let (dispatcher, ctx) = setup_dispatcher_with_lease_and_tier(
                "lease-t4-001",
                "W-T4-001",
                "gate-001",
                "reviewer-tier4",
                4, // Tier4
            );

            let request = IngestReviewReceiptRequest {
                lease_id: "lease-t4-001".to_string(),
                receipt_id: "RR-T4-001".to_string(),
                reviewer_actor_id: "reviewer-tier4".to_string(),
                changeset_digest: vec![0x42; 32],
                artifact_bundle_hash: test_artifact_bundle_hash(),
                verdict: ReviewReceiptVerdict::Approve.into(),
                blocked_reason_code: 0,
                blocked_log_hash: vec![],
                identity_proof_hash: vec![0x99; 32],
            };
            let frame = encode_ingest_review_receipt_request(&request);

            let response = dispatcher.dispatch(&frame, &ctx).unwrap();
            match response {
                PrivilegedResponse::Error(err) => {
                    assert!(
                        err.message.contains("SelfSigned") || err.message.contains("requires"),
                        "Tier4 SelfSigned must be rejected, got: {}",
                        err.message
                    );
                    assert!(
                        err.message.contains("Tier4"),
                        "Error must mention the resolved tier, got: {}",
                        err.message
                    );
                },
                other => {
                    panic!("Expected rejection for Tier4 SelfSigned attestation, got {other:?}")
                },
            }
        }

        #[test]
        fn test_ingest_review_receipt_tier0_resolved_passes_end_to_end() {
            // Tier0 with SelfSigned through the FULL production path:
            // lease -> work_id -> work_claim -> risk_tier=0 -> attestation ok -> event
            let (dispatcher, ctx) = setup_dispatcher_with_lease_and_tier(
                "lease-e2e-001",
                "W-E2E-001",
                "gate-001",
                "reviewer-e2e",
                0, // Tier0
            );

            let request = IngestReviewReceiptRequest {
                lease_id: "lease-e2e-001".to_string(),
                receipt_id: "RR-E2E-001".to_string(),
                reviewer_actor_id: "reviewer-e2e".to_string(),
                changeset_digest: vec![0x42; 32],
                artifact_bundle_hash: test_artifact_bundle_hash(),
                verdict: ReviewReceiptVerdict::Approve.into(),
                blocked_reason_code: 0,
                blocked_log_hash: vec![],
                identity_proof_hash: vec![0x99; 32],
            };
            let frame = encode_ingest_review_receipt_request(&request);

            let response = dispatcher.dispatch(&frame, &ctx).unwrap();
            match response {
                PrivilegedResponse::IngestReviewReceipt(resp) => {
                    assert_eq!(resp.receipt_id, "RR-E2E-001");
                    assert_eq!(
                        resp.event_type, "ReviewReceiptRecorded",
                        "Tier0 work with SelfSigned must pass attestation end-to-end"
                    );
                    assert!(
                        !resp.event_id.is_empty(),
                        "event_id must be non-empty on success"
                    );
                },
                PrivilegedResponse::Error(err) => {
                    panic!(
                        "Tier0 SelfSigned should pass end-to-end, got error: {}",
                        err.message
                    );
                },
                other => panic!("Expected IngestReviewReceipt, got {other:?}"),
            }
        }

        #[test]
        fn test_ingest_review_receipt_unresolvable_risk_tier_fails_closed() {
            // When work_id can be resolved from the lease but no work claim
            // exists in the registry, the risk tier is unresolvable.
            // FAIL-CLOSED: must default to Tier4 and reject SelfSigned.
            let peer_creds = PeerCredentials {
                uid: 1000,
                gid: 1000,
                pid: Some(12345),
            };
            let executor_actor_id = derive_actor_id(&peer_creds);
            // TCK-00408: CAS is mandatory for ingest (fail-closed).
            let cas = Arc::new(MemoryCas::default());
            cas.store(TEST_ARTIFACT_CONTENT).unwrap();
            let dispatcher = PrivilegedDispatcher::new()
                .with_cas(Arc::clone(&cas) as Arc<dyn ContentAddressedStore>);
            // Register lease but do NOT register a work claim
            dispatcher.lease_validator.register_lease_with_executor(
                "lease-no-claim",
                "W-NO-CLAIM",
                "gate-001",
                &executor_actor_id,
            );
            register_full_test_lease(&TestLeaseConfig {
                dispatcher: &dispatcher,
                cas: cas.as_ref(),
                lease_id: "lease-no-claim",
                work_id: "W-NO-CLAIM",
                gate_id: "gate-001",
                executor_actor_id: &executor_actor_id,
                policy_hash: [0x42; 32],
                wall_time_source: WallTimeSource::AuthenticatedNts,
                include_attestation: true,
            });
            let ctx = ConnectionContext::privileged_session_open(Some(peer_creds));

            let request = IngestReviewReceiptRequest {
                lease_id: "lease-no-claim".to_string(),
                receipt_id: "RR-NO-CLAIM".to_string(),
                reviewer_actor_id: "ignored-by-handler".to_string(),
                changeset_digest: vec![0x42; 32],
                artifact_bundle_hash: test_artifact_bundle_hash(),
                verdict: ReviewReceiptVerdict::Approve.into(),
                blocked_reason_code: 0,
                blocked_log_hash: vec![],
                identity_proof_hash: vec![0x99; 32],
            };
            let frame = encode_ingest_review_receipt_request(&request);

            let response = dispatcher.dispatch(&frame, &ctx).unwrap();
            match response {
                PrivilegedResponse::Error(err) => {
                    assert!(
                        err.message.contains("MISSING_AUTHORITY")
                            || err.message.contains("work claim not found")
                            || err.message.contains("missing_authority_context")
                            || err.message.contains("SelfSigned")
                            || err.message.contains("requires")
                            || err.message.contains("Tier4"),
                        "Unresolvable risk tier / missing claim must fail-closed, got: {}",
                        err.message
                    );
                },
                other => panic!(
                    "Expected fail-closed rejection for unresolvable risk tier, got {other:?}"
                ),
            }
        }

        #[test]
        fn test_ingest_review_receipt_tier1_self_signed_accepted() {
            // v5 Finding 4: Tier1 requires SelfSigned for review receipts per
            // the ratchet table. This endpoint provides SelfSigned attestation,
            // so Tier1 requests MUST be accepted (not hard-rejected).
            let (dispatcher, ctx) = setup_dispatcher_with_lease_and_tier(
                "lease-t1-001",
                "W-T1-001",
                "gate-001",
                "reviewer-tier1",
                1, // Tier1
            );

            let request = IngestReviewReceiptRequest {
                lease_id: "lease-t1-001".to_string(),
                receipt_id: "RR-T1-001".to_string(),
                reviewer_actor_id: "reviewer-tier1".to_string(),
                changeset_digest: vec![0x42; 32],
                artifact_bundle_hash: test_artifact_bundle_hash(),
                verdict: ReviewReceiptVerdict::Approve.into(),
                blocked_reason_code: 0,
                blocked_log_hash: vec![],
                identity_proof_hash: vec![0x99; 32],
            };
            let frame = encode_ingest_review_receipt_request(&request);

            let response = dispatcher.dispatch(&frame, &ctx).unwrap();
            match response {
                PrivilegedResponse::IngestReviewReceipt(resp) => {
                    assert_eq!(
                        resp.receipt_id, "RR-T1-001",
                        "Tier1 SelfSigned must be accepted per ratchet table"
                    );
                    assert_eq!(resp.event_type, "ReviewReceiptRecorded");
                    assert!(
                        !resp.event_id.is_empty(),
                        "Event ID must be non-empty for accepted Tier1 receipt"
                    );
                },
                PrivilegedResponse::Error(err) => {
                    panic!(
                        "Tier1 SelfSigned must be accepted per ratchet table, got error: {}",
                        err.message
                    );
                },
                other => {
                    panic!("Expected IngestReviewReceipt for Tier1 SelfSigned, got {other:?}")
                },
            }
        }

        /// v10 MAJOR 1: Duplicate `receipt_id` replay returns the original
        /// event (idempotent) instead of creating a new event. This verifies
        /// the fix for the broken idempotency check that previously looked up
        /// events by `event_id` (auto-generated UUID) instead of searching
        /// payloads for the caller-supplied `receipt_id`.
        #[test]
        fn test_ingest_review_receipt_duplicate_receipt_id_idempotent() {
            let (dispatcher, ctx) =
                setup_dispatcher_with_lease("lease-dup-001", "W-DUP-001", "gate-001", "reviewer-a");

            let request = IngestReviewReceiptRequest {
                lease_id: "lease-dup-001".to_string(),
                receipt_id: "RR-DUP-001".to_string(),
                reviewer_actor_id: "reviewer-a".to_string(),
                changeset_digest: vec![0x42; 32],
                artifact_bundle_hash: test_artifact_bundle_hash(),
                verdict: ReviewReceiptVerdict::Approve.into(),
                blocked_reason_code: 0,
                blocked_log_hash: vec![],
                identity_proof_hash: vec![0x99; 32],
            };

            // First submission should succeed
            let frame = encode_ingest_review_receipt_request(&request);
            let response1 = dispatcher.dispatch(&frame, &ctx).unwrap();
            let (event_id_1, event_type_1) = match &response1 {
                PrivilegedResponse::IngestReviewReceipt(resp) => {
                    assert_eq!(resp.receipt_id, "RR-DUP-001");
                    assert!(
                        !resp.event_id.is_empty(),
                        "First event_id must be non-empty"
                    );
                    (resp.event_id.clone(), resp.event_type.clone())
                },
                other => panic!("Expected IngestReviewReceipt, got {other:?}"),
            };

            // Second submission with same receipt_id should return idempotent
            // result with the SAME event_id as the first submission.
            let frame2 = encode_ingest_review_receipt_request(&request);
            let response2 = dispatcher.dispatch(&frame2, &ctx).unwrap();
            match &response2 {
                PrivilegedResponse::IngestReviewReceipt(resp) => {
                    assert_eq!(
                        resp.receipt_id, "RR-DUP-001",
                        "Idempotent replay must return same receipt_id"
                    );
                    assert_eq!(
                        resp.event_id, event_id_1,
                        "Idempotent replay must return the ORIGINAL event_id, not a new one"
                    );
                    assert_eq!(
                        resp.event_type, event_type_1,
                        "Idempotent replay must return same event_type"
                    );
                },
                other => panic!("Expected idempotent IngestReviewReceipt, got {other:?}"),
            }

            // Verify only ONE event was created in the ledger (not two)
            let events = dispatcher.event_emitter.get_events_by_work_id("W-DUP-001");
            let receipt_event_count = events
                .iter()
                .filter(|e| e.event_type == "review_receipt_recorded")
                .count();
            assert_eq!(
                receipt_event_count, 1,
                "Duplicate receipt_id submission must NOT create a second event"
            );

            let semantic_event = dispatcher.event_emitter.get_event_by_receipt_identity(
                "RR-DUP-001",
                "lease-dup-001",
                "W-DUP-001",
                &hex::encode([0x42u8; 32]),
            );
            assert!(
                semantic_event.is_some(),
                "receipt replay identity tuple must resolve to the canonical persisted event"
            );
        }

        #[test]
        fn test_ingest_review_receipt_duplicate_receipt_id_different_identity_proof_hash_rejected()
        {
            let (dispatcher, ctx) = setup_dispatcher_with_lease(
                "lease-dup-proof-001",
                "W-DUP-PROOF-001",
                "gate-001",
                "reviewer-a",
            );

            let request1 = IngestReviewReceiptRequest {
                lease_id: "lease-dup-proof-001".to_string(),
                receipt_id: "RR-DUP-PROOF-001".to_string(),
                reviewer_actor_id: "reviewer-a".to_string(),
                changeset_digest: vec![0x42; 32],
                artifact_bundle_hash: test_artifact_bundle_hash(),
                verdict: ReviewReceiptVerdict::Approve.into(),
                blocked_reason_code: 0,
                blocked_log_hash: vec![],
                identity_proof_hash: vec![0x99; 32],
            };
            let frame1 = encode_ingest_review_receipt_request(&request1);
            let response1 = dispatcher.dispatch(&frame1, &ctx).unwrap();
            assert!(
                matches!(response1, PrivilegedResponse::IngestReviewReceipt(_)),
                "first receipt submission should succeed"
            );

            let request2 = IngestReviewReceiptRequest {
                lease_id: "lease-dup-proof-001".to_string(),
                receipt_id: "RR-DUP-PROOF-001".to_string(),
                reviewer_actor_id: "reviewer-a".to_string(),
                changeset_digest: vec![0x42; 32],
                artifact_bundle_hash: test_artifact_bundle_hash(),
                verdict: ReviewReceiptVerdict::Approve.into(),
                blocked_reason_code: 0,
                blocked_log_hash: vec![],
                identity_proof_hash: vec![0x98; 32],
            };
            let frame2 = encode_ingest_review_receipt_request(&request2);
            let response2 = dispatcher.dispatch(&frame2, &ctx).unwrap();
            match response2 {
                PrivilegedResponse::Error(err) => {
                    assert!(
                        err.message.contains("identity_proof_hash mismatch"),
                        "expected identity_proof_hash mismatch error, got: {}",
                        err.message
                    );
                },
                other => panic!("expected mismatch rejection, got {other:?}"),
            }
        }

        #[test]
        fn test_ingest_review_receipt_duplicate_receipt_id_different_artifact_bundle_hash_rejected()
        {
            let (dispatcher, ctx) = setup_dispatcher_with_lease(
                "lease-dup-artifact-001",
                "W-DUP-ARTIFACT-001",
                "gate-001",
                "reviewer-a",
            );
            let second_artifact_bundle_hash = dispatcher
                .cas
                .as_ref()
                .expect("test dispatcher should have CAS configured")
                .store(b"test-artifact-bundle-content-2")
                .expect("storing secondary artifact bundle should succeed")
                .hash
                .to_vec();

            let request1 = IngestReviewReceiptRequest {
                lease_id: "lease-dup-artifact-001".to_string(),
                receipt_id: "RR-DUP-ARTIFACT-001".to_string(),
                reviewer_actor_id: "reviewer-a".to_string(),
                changeset_digest: vec![0x42; 32],
                artifact_bundle_hash: test_artifact_bundle_hash(),
                verdict: ReviewReceiptVerdict::Approve.into(),
                blocked_reason_code: 0,
                blocked_log_hash: vec![],
                identity_proof_hash: vec![0x99; 32],
            };
            let frame1 = encode_ingest_review_receipt_request(&request1);
            let response1 = dispatcher.dispatch(&frame1, &ctx).unwrap();
            assert!(
                matches!(response1, PrivilegedResponse::IngestReviewReceipt(_)),
                "first receipt submission should succeed"
            );

            let request2 = IngestReviewReceiptRequest {
                lease_id: "lease-dup-artifact-001".to_string(),
                receipt_id: "RR-DUP-ARTIFACT-001".to_string(),
                reviewer_actor_id: "reviewer-a".to_string(),
                changeset_digest: vec![0x42; 32],
                artifact_bundle_hash: second_artifact_bundle_hash,
                verdict: ReviewReceiptVerdict::Approve.into(),
                blocked_reason_code: 0,
                blocked_log_hash: vec![],
                identity_proof_hash: vec![0x99; 32],
            };
            let frame2 = encode_ingest_review_receipt_request(&request2);
            let response2 = dispatcher.dispatch(&frame2, &ctx).unwrap();
            match response2 {
                PrivilegedResponse::Error(err) => {
                    assert!(
                        err.message.contains("artifact_bundle_hash mismatch"),
                        "expected artifact_bundle_hash mismatch rejection, got: {}",
                        err.message
                    );
                },
                other => panic!("expected mismatch rejection, got {other:?}"),
            }
        }

        #[test]
        fn test_ingest_review_receipt_duplicate_blocked_receipt_different_artifact_bundle_hash_rejected()
         {
            let (dispatcher, ctx) = setup_dispatcher_with_lease(
                "lease-dup-blocked-artifact-001",
                "W-DUP-BLOCKED-ARTIFACT-001",
                "gate-001",
                "reviewer-a",
            );

            let request1 = IngestReviewReceiptRequest {
                lease_id: "lease-dup-blocked-artifact-001".to_string(),
                receipt_id: "RB-DUP-ARTIFACT-001".to_string(),
                reviewer_actor_id: "reviewer-a".to_string(),
                changeset_digest: vec![0x42; 32],
                artifact_bundle_hash: test_artifact_bundle_hash(),
                verdict: ReviewReceiptVerdict::Blocked.into(),
                blocked_reason_code: 1,
                blocked_log_hash: vec![0x55; 32],
                identity_proof_hash: vec![0x99; 32],
            };
            let frame1 = encode_ingest_review_receipt_request(&request1);
            let response1 = dispatcher.dispatch(&frame1, &ctx).unwrap();
            assert!(
                matches!(response1, PrivilegedResponse::IngestReviewReceipt(_)),
                "first blocked receipt submission should succeed"
            );

            let request2 = IngestReviewReceiptRequest {
                lease_id: "lease-dup-blocked-artifact-001".to_string(),
                receipt_id: "RB-DUP-ARTIFACT-001".to_string(),
                reviewer_actor_id: "reviewer-a".to_string(),
                changeset_digest: vec![0x42; 32],
                artifact_bundle_hash: vec![0x44; 32],
                verdict: ReviewReceiptVerdict::Blocked.into(),
                blocked_reason_code: 1,
                blocked_log_hash: vec![0x55; 32],
                identity_proof_hash: vec![0x99; 32],
            };
            let frame2 = encode_ingest_review_receipt_request(&request2);
            let response2 = dispatcher.dispatch(&frame2, &ctx).unwrap();
            match response2 {
                PrivilegedResponse::Error(err) => {
                    assert!(
                        err.message.contains("artifact_bundle_hash"),
                        "expected artifact_bundle_hash mismatch rejection, got: {}",
                        err.message
                    );
                },
                other => panic!(
                    "expected duplicate BLOCKED receipt_id + artifact_bundle_hash mismatch rejection, got {other:?}"
                ),
            }
        }

        #[test]
        fn test_ingest_review_receipt_duplicate_receipt_id_different_lease_id_rejected() {
            let (dispatcher, ctx) =
                setup_dispatcher_with_lease("lease-A", "W-DUP-LEASE-A", "gate-001", "reviewer-a");

            let peer_creds = test_peer_credentials();
            let executor_actor_id = derive_actor_id(&peer_creds);
            dispatcher.lease_validator.register_lease_with_executor(
                "lease-B",
                "W-DUP-LEASE-B",
                "gate-002",
                &executor_actor_id,
            );
            let cas = dispatcher
                .cas
                .as_ref()
                .expect("CAS should be configured by setup helper");
            register_full_test_lease(&TestLeaseConfig {
                dispatcher: &dispatcher,
                cas: cas.as_ref(),
                lease_id: "lease-B",
                work_id: "W-DUP-LEASE-B",
                gate_id: "gate-002",
                executor_actor_id: &executor_actor_id,
                policy_hash: [0u8; 32],
                wall_time_source: WallTimeSource::AuthenticatedNts,
                include_attestation: true,
            });
            let policy_resolution = test_policy_resolution_with_lineage(
                "W-DUP-LEASE-B",
                &executor_actor_id,
                WorkRole::Reviewer,
                0,
            );
            seed_policy_lineage_for_test(
                cas.as_ref(),
                "W-DUP-LEASE-B",
                &executor_actor_id,
                WorkRole::Reviewer,
                &policy_resolution,
            );
            dispatcher
                .work_registry
                .register_claim(WorkClaim {
                    work_id: "W-DUP-LEASE-B".to_string(),
                    lease_id: "lease-B".to_string(),
                    actor_id: executor_actor_id,
                    role: WorkRole::Reviewer,
                    policy_resolution,
                    executor_custody_domains: vec![],
                    author_custody_domains: vec![],
                    permeability_receipt: None,
                })
                .expect("second work claim registration should succeed");

            let request1 = IngestReviewReceiptRequest {
                lease_id: "lease-A".to_string(),
                receipt_id: "RR-DUP-LEASE-001".to_string(),
                reviewer_actor_id: "reviewer-a".to_string(),
                changeset_digest: vec![0x42; 32],
                artifact_bundle_hash: test_artifact_bundle_hash(),
                verdict: ReviewReceiptVerdict::Approve.into(),
                blocked_reason_code: 0,
                blocked_log_hash: vec![],
                identity_proof_hash: vec![0x99; 32],
            };
            let frame1 = encode_ingest_review_receipt_request(&request1);
            let response1 = dispatcher.dispatch(&frame1, &ctx).unwrap();
            assert!(
                matches!(response1, PrivilegedResponse::IngestReviewReceipt(_)),
                "first receipt submission should succeed"
            );

            let request2 = IngestReviewReceiptRequest {
                lease_id: "lease-B".to_string(),
                receipt_id: "RR-DUP-LEASE-001".to_string(),
                reviewer_actor_id: "reviewer-a".to_string(),
                changeset_digest: vec![0x42; 32],
                artifact_bundle_hash: test_artifact_bundle_hash(),
                verdict: ReviewReceiptVerdict::Approve.into(),
                blocked_reason_code: 0,
                blocked_log_hash: vec![],
                identity_proof_hash: vec![0x99; 32],
            };
            let frame2 = encode_ingest_review_receipt_request(&request2);
            let response2 = dispatcher.dispatch(&frame2, &ctx).unwrap();
            match response2 {
                PrivilegedResponse::Error(err) => {
                    assert!(
                        err.message
                            .contains("already exists with a different semantic identity tuple")
                            && err
                                .message
                                .contains("PCAC linear consumption prevents reuse"),
                        "expected semantic tuple conflict rejection, got: {}",
                        err.message
                    );
                },
                other => panic!(
                    "expected duplicate receipt_id semantic tuple conflict rejection, got {other:?}"
                ),
            }
        }

        /// TCK-00408: Verify CAS existence validation rejects review receipts
        /// referencing artifact bundles that are not in CAS.
        #[test]
        fn test_ingest_review_receipt_rejects_missing_cas_artifact() {
            // Create dispatcher WITH CAS so the validation path is exercised
            let cas = Arc::new(MemoryCas::default());
            let peer_creds = test_peer_credentials();
            let executor_actor_id = derive_actor_id(&peer_creds);
            let dispatcher = PrivilegedDispatcher::new()
                .with_cas(Arc::clone(&cas) as Arc<dyn ContentAddressedStore>);

            // Register lease and work claim
            let lease_id = "lease-cas-check-001";
            let work_id = "W-CAS-CHECK-001";
            dispatcher.lease_validator.register_lease_with_executor(
                lease_id,
                work_id,
                "gate-cas-001",
                &executor_actor_id,
            );
            register_full_test_lease(&TestLeaseConfig {
                dispatcher: &dispatcher,
                cas: cas.as_ref(),
                lease_id,
                work_id,
                gate_id: "gate-cas-001",
                executor_actor_id: &executor_actor_id,
                policy_hash: [0u8; 32],
                wall_time_source: WallTimeSource::AuthenticatedNts,
                include_attestation: true,
            });
            let policy_resolution = test_policy_resolution_with_lineage(
                work_id,
                &executor_actor_id,
                WorkRole::Reviewer,
                0,
            );
            seed_policy_lineage_for_test(
                cas.as_ref(),
                work_id,
                &executor_actor_id,
                WorkRole::Reviewer,
                &policy_resolution,
            );
            dispatcher
                .work_registry
                .register_claim(WorkClaim {
                    work_id: work_id.to_string(),
                    lease_id: lease_id.to_string(),
                    actor_id: executor_actor_id,
                    role: WorkRole::Reviewer,
                    policy_resolution,
                    executor_custody_domains: vec![],
                    author_custody_domains: vec![],
                    permeability_receipt: None,
                })
                .expect("work claim registration should succeed");

            let ctx = ConnectionContext::privileged_session_open(Some(peer_creds));

            // Submit review receipt with artifact_bundle_hash NOT in CAS
            let request = IngestReviewReceiptRequest {
                lease_id: lease_id.to_string(),
                receipt_id: "RR-CAS-MISSING-001".to_string(),
                reviewer_actor_id: "reviewer-a".to_string(),
                changeset_digest: vec![0x42; 32],
                artifact_bundle_hash: vec![0xAA; 32], // NOT stored in CAS
                verdict: ReviewReceiptVerdict::Approve.into(),
                blocked_reason_code: 0,
                blocked_log_hash: vec![],
                identity_proof_hash: vec![0x99; 32],
            };
            let frame = encode_ingest_review_receipt_request(&request);
            let response = dispatcher.dispatch(&frame, &ctx).unwrap();

            match &response {
                PrivilegedResponse::Error(err) => {
                    assert!(
                        err.message.contains("not found in CAS"),
                        "Error should mention CAS: {}",
                        err.message
                    );
                },
                other => panic!("Expected CAS existence rejection, got: {other:?}"),
            }
        }

        // ====================================================================
        // TCK-00340: DelegateSublease IPC Integration Tests
        // ====================================================================

        /// Helper: build a dispatcher with a gate orchestrator and a registered
        /// parent lease for sublease delegation tests.
        ///
        /// The parent lease's `executor_actor_id` is set to match the caller's
        /// derived actor ID (from uid=1000, gid=1000) so that the caller
        /// authorization check in `handle_delegate_sublease` passes.
        fn setup_dispatcher_with_orchestrator(
            parent_lease_id: &str,
            work_id: &str,
            gate_id: &str,
            _executor_actor_id: &str,
        ) -> (
            PrivilegedDispatcher,
            ConnectionContext,
            apm2_core::fac::GateLease,
        ) {
            let signer = Arc::new(apm2_core::crypto::Signer::generate());
            let orch = Arc::new(crate::gate::GateOrchestrator::new(
                crate::gate::GateOrchestratorConfig::default(),
                signer,
            ));

            // Derive the actor ID from the test peer credentials (uid=1000,
            // gid=1000) so the caller authorization check passes.
            let test_creds = PeerCredentials {
                uid: 1000,
                gid: 1000,
                pid: Some(12345),
            };
            let caller_actor = derive_actor_id(&test_creds);
            let cas = Arc::new(MemoryCas::default());
            let kernel: Arc<dyn apm2_core::pcac::AuthorityJoinKernel> =
                Arc::new(crate::pcac::InProcessKernel::new(1));
            let pcac_gate = Arc::new(crate::pcac::LifecycleGate::new(kernel));
            let dispatcher = PrivilegedDispatcher::new()
                .with_gate_orchestrator(orch)
                .with_cas(Arc::clone(&cas) as Arc<dyn ContentAddressedStore>)
                .with_pcac_lifecycle_gate(pcac_gate)
                .with_privileged_pcac_policy(crate::protocol::dispatch::PrivilegedPcacPolicy {});
            register_full_test_lease(&TestLeaseConfig {
                dispatcher: &dispatcher,
                cas: cas.as_ref(),
                lease_id: parent_lease_id,
                work_id,
                gate_id,
                executor_actor_id: &caller_actor,
                policy_hash: [0xAB; 32],
                wall_time_source: WallTimeSource::AuthenticatedNts,
                include_attestation: true,
            });
            dispatcher.lease_validator.register_lease_with_executor(
                parent_lease_id,
                work_id,
                gate_id,
                &caller_actor,
            );
            let mut policy_resolution =
                test_policy_resolution_with_lineage(work_id, &caller_actor, WorkRole::Reviewer, 0);
            policy_resolution.resolved_policy_hash = [0xAB; 32];
            policy_resolution.pcac_policy = Some(apm2_core::pcac::PcacPolicyKnobs::default());
            seed_policy_lineage_for_test(
                cas.as_ref(),
                work_id,
                &caller_actor,
                WorkRole::Reviewer,
                &policy_resolution,
            );
            dispatcher
                .work_registry
                .register_claim(WorkClaim {
                    work_id: work_id.to_string(),
                    lease_id: parent_lease_id.to_string(),
                    actor_id: caller_actor,
                    role: WorkRole::Reviewer,
                    policy_resolution,
                    executor_custody_domains: vec![],
                    author_custody_domains: vec![],
                    permeability_receipt: None,
                })
                .expect("delegate test work claim registration");
            let parent_lease = dispatcher
                .lease_validator
                .get_gate_lease(parent_lease_id)
                .expect("registered parent lease should be retrievable");

            let ctx = ConnectionContext::privileged_session_open(Some(PeerCredentials {
                uid: 1000,
                gid: 1000,
                pid: Some(12345),
            }));
            (dispatcher, ctx, parent_lease)
        }

        fn seed_sublease_lineage(
            dispatcher: &PrivilegedDispatcher,
            leaf_lease_id: &str,
            depth: u32,
            actor_id: &str,
        ) {
            let mut current = leaf_lease_id.to_string();
            for hop in 0..depth {
                let next_parent = format!("{leaf_lease_id}-ancestor-{hop}");
                let current_lease = dispatcher
                    .lease_validator
                    .get_gate_lease(&current)
                    .expect("lineage seed requires current lease to be persisted");
                let mut next_parent_lease = current_lease.clone();
                next_parent_lease.lease_id = next_parent.clone();
                next_parent_lease.executor_actor_id = actor_id.to_string();
                next_parent_lease.issued_at = next_parent_lease
                    .issued_at
                    .saturating_add(u64::from(hop) + 1);
                if next_parent_lease.expires_at <= next_parent_lease.issued_at {
                    next_parent_lease.expires_at = next_parent_lease.issued_at.saturating_add(1);
                }
                dispatcher
                    .lease_validator
                    .register_full_lease(&next_parent_lease)
                    .expect("lineage seed should persist ancestor lease");
                let payload = serde_json::json!({
                    "parent_lease_id": next_parent,
                    "identity_proof_hash": hex::encode([0xA5; 32]),
                });
                let payload_bytes =
                    serde_json::to_vec(&payload).expect("lineage payload serialization");
                dispatcher
                    .event_emitter()
                    .emit_session_event(
                        &current,
                        "SubleaseIssued",
                        &payload_bytes,
                        actor_id,
                        1_700_000_000 + u64::from(hop),
                    )
                    .expect("lineage event emission");
                current = format!("{leaf_lease_id}-ancestor-{hop}");
            }
        }

        #[test]
        fn test_delegate_sublease_valid_succeeds() {
            let (dispatcher, ctx, _parent) = setup_dispatcher_with_orchestrator(
                "parent-lease-001",
                "W-DS-001",
                "gate-quality",
                "executor-001",
            );

            let request = DelegateSubleaseRequest {
                parent_lease_id: "parent-lease-001".to_string(),
                delegatee_actor_id: "child-executor-001".to_string(),
                requested_expiry_ns: 1_900_000_000_000,
                sublease_id: "sublease-001".to_string(),
                identity_proof_hash: vec![0x99; 32],
            };
            let frame = encode_delegate_sublease_request(&request);

            let response = dispatcher.dispatch(&frame, &ctx).unwrap();
            match response {
                PrivilegedResponse::DelegateSublease(resp) => {
                    assert_eq!(resp.sublease_id, "sublease-001");
                    assert_eq!(resp.parent_lease_id, "parent-lease-001");
                    assert_eq!(resp.delegatee_actor_id, "child-executor-001");
                    assert_eq!(resp.gate_id, "gate-quality");
                    // v5 Finding 2: expires_at_ns in response is in nanoseconds
                    // (converted from internal ms representation).
                    assert_eq!(resp.expires_at_ns, 1_900_000_000_000);
                    assert!(!resp.event_id.is_empty(), "event_id must be non-empty");
                },
                PrivilegedResponse::Error(err) => {
                    panic!(
                        "Expected DelegateSublease success, got error: {}",
                        err.message
                    );
                },
                other => panic!("Expected DelegateSublease, got {other:?}"),
            }
        }

        #[test]
        fn test_delegate_sublease_parent_not_found_rejected() {
            let signer = Arc::new(apm2_core::crypto::Signer::generate());
            let orch = Arc::new(crate::gate::GateOrchestrator::new(
                crate::gate::GateOrchestratorConfig::default(),
                signer,
            ));
            let dispatcher = PrivilegedDispatcher::new().with_gate_orchestrator(orch);
            // Do NOT register any parent lease
            let ctx = ConnectionContext::privileged_session_open(Some(PeerCredentials {
                uid: 1000,
                gid: 1000,
                pid: Some(12345),
            }));

            let request = DelegateSubleaseRequest {
                parent_lease_id: "nonexistent-lease".to_string(),
                delegatee_actor_id: "child-executor-001".to_string(),
                requested_expiry_ns: 1_900_000_000_000,
                sublease_id: "sublease-002".to_string(),
                identity_proof_hash: vec![0x99; 32],
            };
            let frame = encode_delegate_sublease_request(&request);

            let response = dispatcher.dispatch(&frame, &ctx).unwrap();
            match response {
                PrivilegedResponse::Error(err) => {
                    assert!(
                        err.message.contains("parent gate lease not found"),
                        "Error should mention parent lease not found: {}",
                        err.message
                    );
                    assert_eq!(
                        err.code,
                        i32::from(PrivilegedErrorCode::GateLeaseMissing),
                        "Error code should be GateLeaseMissing"
                    );
                },
                other => panic!("Expected error for missing parent, got {other:?}"),
            }
        }

        #[test]
        fn test_delegate_sublease_expired_parent_rejected() {
            let (dispatcher, ctx, _parent) = setup_dispatcher_with_orchestrator(
                "parent-lease-exp",
                "W-DS-EXP",
                "gate-quality",
                "executor-001",
            );

            // Request sublease with expiry EXCEEDING parent bounds (parent
            // expires at 2_000_000ms, sublease requests 3_000_000ms).
            let request = DelegateSubleaseRequest {
                parent_lease_id: "parent-lease-exp".to_string(),
                delegatee_actor_id: "child-executor-001".to_string(),
                requested_expiry_ns: 3_000_000_000_000, /* Exceeds parent's expires_at (2_000_000
                                                         * ms) */
                sublease_id: "sublease-overflow".to_string(),
                identity_proof_hash: vec![0x99; 32],
            };
            let frame = encode_delegate_sublease_request(&request);

            let response = dispatcher.dispatch(&frame, &ctx).unwrap();
            match response {
                PrivilegedResponse::Error(err) => {
                    assert!(
                        err.message.contains("sublease delegation failed")
                            || err.message.contains("strict expiry narrowing violated")
                            || err.message.contains("is after parent expires_at"),
                        "Error should mention parent expiry bound violation: {}",
                        err.message
                    );
                },
                other => panic!("Expected error for expiry overflow, got {other:?}"),
            }
        }

        #[test]
        fn test_delegate_sublease_equal_parent_expiry_rejected() {
            let (dispatcher, ctx, parent) = setup_dispatcher_with_orchestrator(
                "parent-lease-equal-exp",
                "W-DS-EQUAL-EXP",
                "gate-quality",
                "executor-001",
            );

            let request = DelegateSubleaseRequest {
                parent_lease_id: "parent-lease-equal-exp".to_string(),
                delegatee_actor_id: "child-executor-001".to_string(),
                requested_expiry_ns: parent.expires_at * 1_000_000,
                sublease_id: "sublease-equal-exp".to_string(),
                identity_proof_hash: vec![0x99; 32],
            };
            let frame = encode_delegate_sublease_request(&request);

            let response = dispatcher.dispatch(&frame, &ctx).unwrap();
            match response {
                PrivilegedResponse::Error(err) => {
                    assert!(
                        err.message.contains("strict expiry narrowing violated"),
                        "Error should reject equal-expiry sublease delegation: {}",
                        err.message
                    );
                },
                other => panic!("Expected strict expiry narrowing rejection, got {other:?}"),
            }

            let persisted_sublease = dispatcher
                .lease_validator()
                .get_gate_lease("sublease-equal-exp");
            assert!(
                persisted_sublease.is_none(),
                "equal-expiry denial must not persist a sublease"
            );

            let sublease_events = dispatcher
                .event_emitter()
                .get_events_by_work_id("sublease-equal-exp");
            let sublease_event_count = sublease_events
                .iter()
                .filter(|event| event.event_type == "SubleaseIssued")
                .count();
            assert_eq!(
                sublease_event_count, 0,
                "equal-expiry denial must not emit SubleaseIssued events"
            );
        }

        #[test]
        fn test_delegate_sublease_depth_overflow_rejected() {
            let parent_lease_id = "parent-depth-overflow";
            let (dispatcher, ctx, _parent) = setup_dispatcher_with_orchestrator(
                parent_lease_id,
                "W-DS-DEPTH-OVERFLOW",
                "gate-quality",
                "executor-001",
            );
            let actor_id = derive_actor_id(&PeerCredentials {
                uid: 1000,
                gid: 1000,
                pid: Some(12345),
            });

            seed_sublease_lineage(
                &dispatcher,
                parent_lease_id,
                MAX_SUBLEASE_DELEGATION_DEPTH,
                &actor_id,
            );

            let request = DelegateSubleaseRequest {
                parent_lease_id: parent_lease_id.to_string(),
                delegatee_actor_id: "child-depth-overflow".to_string(),
                requested_expiry_ns: 1_900_000_000_000,
                sublease_id: "sublease-depth-overflow".to_string(),
                identity_proof_hash: vec![0x77; 32],
            };
            let frame = encode_delegate_sublease_request(&request);
            let response = dispatcher.dispatch(&frame, &ctx).unwrap();

            match response {
                PrivilegedResponse::Error(err) => {
                    assert!(
                        err.message.contains("requested_depth="),
                        "depth overflow denial must include requested_depth, got: {}",
                        err.message
                    );
                    assert!(
                        err.message.contains("max_depth="),
                        "depth overflow denial must include max_depth, got: {}",
                        err.message
                    );
                },
                other => panic!("expected depth overflow denial, got {other:?}"),
            }
        }

        #[test]
        fn test_delegate_sublease_satisfiability_budget_derived_and_depth_safe() {
            let derived_min_budget =
                ((MAX_SUBLEASE_LINEAGE_TRAVERSAL_STEPS as u64) * 2 + 4).max(40);
            assert_eq!(
                SUBLEASE_DELEGATION_SATISFIABILITY_BUDGET_TICKS, derived_min_budget,
                "delegation budget must be derived from lineage traversal bound"
            );
            assert!(
                derived_min_budget >= 40,
                "delegation budget derivation must keep >=40 safety margin"
            );

            let parent_lease_id = "parent-budget-safe";
            let (dispatcher, ctx, _parent) = setup_dispatcher_with_orchestrator(
                parent_lease_id,
                "W-DS-BUDGET-SAFE",
                "gate-quality",
                "executor-001",
            );
            let actor_id = derive_actor_id(&PeerCredentials {
                uid: 1000,
                gid: 1000,
                pid: Some(12345),
            });

            seed_sublease_lineage(
                &dispatcher,
                parent_lease_id,
                MAX_SUBLEASE_DELEGATION_DEPTH - 1,
                &actor_id,
            );

            let request = DelegateSubleaseRequest {
                parent_lease_id: parent_lease_id.to_string(),
                delegatee_actor_id: "child-budget-safe".to_string(),
                requested_expiry_ns: 1_900_000_000_000,
                sublease_id: "sublease-budget-safe".to_string(),
                identity_proof_hash: vec![0x88; 32],
            };
            let frame = encode_delegate_sublease_request(&request);
            let response = dispatcher.dispatch(&frame, &ctx).unwrap();

            match response {
                PrivilegedResponse::DelegateSublease(_) => {},
                PrivilegedResponse::Error(err) => {
                    assert!(
                        !err.message
                            .contains("delegation_satisfiability_budget_ticks exhausted"),
                        "derived budget must cover max-depth traversal, got: {}",
                        err.message
                    );
                },
                other => panic!("expected DelegateSublease or non-budget error, got {other:?}"),
            }
        }

        #[test]
        fn test_delegate_sublease_missing_lineage_for_persisted_delegated_parent_denied() {
            let (dispatcher, ctx, parent_lease) = setup_dispatcher_with_orchestrator(
                "parent-missing-lineage",
                "W-DS-MISSING-LINEAGE",
                "gate-quality",
                "executor-001",
            );
            let caller_actor = derive_actor_id(&PeerCredentials {
                uid: 1000,
                gid: 1000,
                pid: Some(12345),
            });
            let orphan_parent_id = "sublease-missing-lineage-parent";
            let orphan_parent = dispatcher
                .gate_orchestrator
                .as_ref()
                .expect("gate orchestrator should be configured")
                .issue_delegated_sublease(
                    &parent_lease,
                    orphan_parent_id,
                    &caller_actor,
                    parent_lease.issued_at.saturating_add(10),
                    parent_lease.expires_at.saturating_sub(100),
                )
                .expect("orphan parent sublease issuance should succeed");
            dispatcher
                .lease_validator
                .register_delegated_full_lease(&orphan_parent, &parent_lease.lease_id)
                .expect("persisted delegated lease should register");

            // Regression guard: this fixture intentionally persists a delegated
            // lease without a corresponding SubleaseIssued lineage event.
            let request = DelegateSubleaseRequest {
                parent_lease_id: orphan_parent_id.to_string(),
                delegatee_actor_id: "child-missing-lineage".to_string(),
                requested_expiry_ns: orphan_parent
                    .expires_at
                    .saturating_sub(1)
                    .saturating_mul(1_000_000),
                sublease_id: "child-from-missing-lineage-parent".to_string(),
                identity_proof_hash: vec![0x6B; 32],
            };
            let frame = encode_delegate_sublease_request(&request);
            let response = dispatcher.dispatch(&frame, &ctx).unwrap();

            match response {
                PrivilegedResponse::Error(err) => {
                    assert!(
                        err.message.contains("lineage evidence missing"),
                        "missing delegated lineage evidence must deny delegation, got: {}",
                        err.message
                    );
                },
                other => panic!("expected delegated-lineage denial, got {other:?}"),
            }
        }

        #[test]
        fn test_delegate_sublease_vacuous_window_rejected() {
            let (dispatcher, ctx, _parent) = setup_dispatcher_with_orchestrator(
                "parent-vacuous-window",
                "W-DS-VACUOUS",
                "gate-quality",
                "executor-001",
            );

            let request = DelegateSubleaseRequest {
                parent_lease_id: "parent-vacuous-window".to_string(),
                delegatee_actor_id: "child-vacuous-window".to_string(),
                requested_expiry_ns: 1, // deterministically <= issued_at at runtime
                sublease_id: "sublease-vacuous-window".to_string(),
                identity_proof_hash: vec![0x91; 32],
            };
            let frame = encode_delegate_sublease_request(&request);
            let response = dispatcher.dispatch(&frame, &ctx).unwrap();

            match response {
                PrivilegedResponse::Error(err) => {
                    assert!(
                        err.message.contains("vacuous delegation window"),
                        "vacuous window must be denied explicitly, got: {}",
                        err.message
                    );
                },
                other => panic!("expected vacuous-window denial, got {other:?}"),
            }
        }

        #[test]
        fn test_delegate_sublease_empty_parent_lease_id_rejected() {
            let signer = Arc::new(apm2_core::crypto::Signer::generate());
            let orch = Arc::new(crate::gate::GateOrchestrator::new(
                crate::gate::GateOrchestratorConfig::default(),
                signer,
            ));
            let dispatcher = PrivilegedDispatcher::new().with_gate_orchestrator(orch);
            let ctx = ConnectionContext::privileged_session_open(Some(PeerCredentials {
                uid: 1000,
                gid: 1000,
                pid: Some(12345),
            }));

            let request = DelegateSubleaseRequest {
                parent_lease_id: String::new(),
                delegatee_actor_id: "child-executor-001".to_string(),
                requested_expiry_ns: 1_900_000_000_000,
                sublease_id: "sublease-003".to_string(),
                identity_proof_hash: vec![0x99; 32],
            };
            let frame = encode_delegate_sublease_request(&request);

            let response = dispatcher.dispatch(&frame, &ctx).unwrap();
            match response {
                PrivilegedResponse::Error(err) => {
                    assert!(
                        err.message.contains("parent_lease_id is required"),
                        "Error should mention missing parent_lease_id: {}",
                        err.message
                    );
                },
                other => panic!("Expected error for empty parent_lease_id, got {other:?}"),
            }
        }

        #[test]
        fn test_delegate_sublease_empty_delegatee_rejected() {
            let signer = Arc::new(apm2_core::crypto::Signer::generate());
            let orch = Arc::new(crate::gate::GateOrchestrator::new(
                crate::gate::GateOrchestratorConfig::default(),
                signer,
            ));
            let dispatcher = PrivilegedDispatcher::new().with_gate_orchestrator(orch);
            let ctx = ConnectionContext::privileged_session_open(Some(PeerCredentials {
                uid: 1000,
                gid: 1000,
                pid: Some(12345),
            }));

            let request = DelegateSubleaseRequest {
                parent_lease_id: "parent-lease-001".to_string(),
                delegatee_actor_id: String::new(),
                requested_expiry_ns: 1_900_000_000_000,
                sublease_id: "sublease-004".to_string(),
                identity_proof_hash: vec![0x99; 32],
            };
            let frame = encode_delegate_sublease_request(&request);

            let response = dispatcher.dispatch(&frame, &ctx).unwrap();
            match response {
                PrivilegedResponse::Error(err) => {
                    assert!(
                        err.message.contains("delegatee_actor_id is required"),
                        "Error should mention missing delegatee_actor_id: {}",
                        err.message
                    );
                },
                other => panic!("Expected error for empty delegatee_actor_id, got {other:?}"),
            }
        }

        #[test]
        fn test_delegate_sublease_missing_identity_proof_hash_rejected() {
            let signer = Arc::new(apm2_core::crypto::Signer::generate());
            let orch = Arc::new(crate::gate::GateOrchestrator::new(
                crate::gate::GateOrchestratorConfig::default(),
                signer,
            ));
            let dispatcher = PrivilegedDispatcher::new().with_gate_orchestrator(orch);
            let ctx = ConnectionContext::privileged_session_open(Some(PeerCredentials {
                uid: 1000,
                gid: 1000,
                pid: Some(12345),
            }));

            let request = DelegateSubleaseRequest {
                parent_lease_id: "parent-lease-001".to_string(),
                delegatee_actor_id: "child-executor-001".to_string(),
                requested_expiry_ns: 1_900_000_000_000,
                sublease_id: "sublease-no-proof".to_string(),
                identity_proof_hash: vec![],
            };
            let frame = encode_delegate_sublease_request(&request);

            let response = dispatcher.dispatch(&frame, &ctx).unwrap();
            match response {
                PrivilegedResponse::Error(err) => {
                    assert!(
                        err.message
                            .contains("identity_proof_hash validation failed"),
                        "Expected identity_proof_hash validation error, got: {}",
                        err.message
                    );
                },
                other => panic!("Expected error for missing identity proof hash, got {other:?}"),
            }
        }

        #[test]
        fn test_delegate_sublease_zero_identity_proof_hash_rejected() {
            let signer = Arc::new(apm2_core::crypto::Signer::generate());
            let orch = Arc::new(crate::gate::GateOrchestrator::new(
                crate::gate::GateOrchestratorConfig::default(),
                signer,
            ));
            let dispatcher = PrivilegedDispatcher::new().with_gate_orchestrator(orch);
            let ctx = ConnectionContext::privileged_session_open(Some(PeerCredentials {
                uid: 1000,
                gid: 1000,
                pid: Some(12345),
            }));

            let request = DelegateSubleaseRequest {
                parent_lease_id: "parent-lease-002".to_string(),
                delegatee_actor_id: "child-executor-002".to_string(),
                requested_expiry_ns: 1_900_000_000_000,
                sublease_id: "sublease-zero-proof".to_string(),
                identity_proof_hash: vec![0x00; 32],
            };
            let frame = encode_delegate_sublease_request(&request);

            let response = dispatcher.dispatch(&frame, &ctx).unwrap();
            match response {
                PrivilegedResponse::Error(err) => {
                    assert!(
                        err.message.contains("null commitment rejected"),
                        "Expected null commitment rejection, got: {}",
                        err.message
                    );
                },
                other => {
                    panic!("Expected error for zero identity proof hash, got {other:?}")
                },
            }
        }

        #[test]
        fn test_delegate_sublease_no_orchestrator_rejected() {
            // Dispatcher without gate orchestrator configured
            let dispatcher = PrivilegedDispatcher::new();
            let ctx = ConnectionContext::privileged_session_open(Some(PeerCredentials {
                uid: 1000,
                gid: 1000,
                pid: Some(12345),
            }));

            let request = DelegateSubleaseRequest {
                parent_lease_id: "parent-lease-001".to_string(),
                delegatee_actor_id: "child-001".to_string(),
                requested_expiry_ns: 1_900_000_000_000,
                sublease_id: "sublease-005".to_string(),
                identity_proof_hash: vec![0x99; 32],
            };
            let frame = encode_delegate_sublease_request(&request);

            let response = dispatcher.dispatch(&frame, &ctx).unwrap();
            match response {
                PrivilegedResponse::Error(err) => {
                    assert!(
                        err.message.contains("gate orchestrator not configured"),
                        "Error should mention orchestrator not configured: {}",
                        err.message
                    );
                },
                other => panic!("Expected error for missing orchestrator, got {other:?}"),
            }
        }

        #[test]
        fn test_delegate_sublease_non_privileged_rejected() {
            let (dispatcher, _ctx, _parent) = setup_dispatcher_with_orchestrator(
                "parent-lease-np",
                "W-DS-NP",
                "gate-quality",
                "executor-001",
            );
            // Non-privileged context
            let ctx = ConnectionContext::session_open(
                Some(PeerCredentials {
                    uid: 1000,
                    gid: 1000,
                    pid: Some(12345),
                }),
                Some("test-session".to_string()),
            );

            let request = DelegateSubleaseRequest {
                parent_lease_id: "parent-lease-np".to_string(),
                delegatee_actor_id: "child-001".to_string(),
                requested_expiry_ns: 1_900_000_000_000,
                sublease_id: "sublease-006".to_string(),
                identity_proof_hash: vec![0x99; 32],
            };
            let frame = encode_delegate_sublease_request(&request);

            let response = dispatcher.dispatch(&frame, &ctx).unwrap();
            match response {
                PrivilegedResponse::Error(err) => {
                    assert!(
                        err.message.contains("permission denied"),
                        "Non-privileged connections must be rejected: {}",
                        err.message
                    );
                },
                other => panic!("Expected permission denied, got {other:?}"),
            }
        }

        #[test]
        fn test_delegate_sublease_unauthorized_caller_rejected_with_pcac() {
            let signer = Arc::new(apm2_core::crypto::Signer::generate());
            let orch = Arc::new(crate::gate::GateOrchestrator::new(
                crate::gate::GateOrchestratorConfig::default(),
                Arc::clone(&signer),
            ));
            let kernel: Arc<dyn apm2_core::pcac::AuthorityJoinKernel> =
                Arc::new(crate::pcac::InProcessKernel::new(1));
            let pcac_gate = Arc::new(crate::pcac::LifecycleGate::new(kernel));

            // Register parent lease with executor_actor_id that does NOT
            // match the caller (uid=1000, gid=1000).
            let cas = Arc::new(MemoryCas::default());
            let dispatcher = PrivilegedDispatcher::new()
                .with_gate_orchestrator(orch)
                .with_cas(Arc::clone(&cas) as Arc<dyn ContentAddressedStore>)
                .with_pcac_lifecycle_gate(pcac_gate)
                .with_privileged_pcac_policy(crate::protocol::dispatch::PrivilegedPcacPolicy {});
            register_full_test_lease(&TestLeaseConfig {
                dispatcher: &dispatcher,
                cas: cas.as_ref(),
                lease_id: "parent-authz",
                work_id: "W-AUTHZ",
                gate_id: "gate-authz",
                executor_actor_id: "totally-different-actor",
                policy_hash: [0xAB; 32],
                wall_time_source: WallTimeSource::AuthenticatedNts,
                include_attestation: true,
            });
            dispatcher.lease_validator.register_lease_with_executor(
                "parent-authz",
                "W-AUTHZ",
                "gate-authz",
                "totally-different-actor",
            );
            let mut policy_resolution = test_policy_resolution_with_lineage(
                "W-AUTHZ",
                "totally-different-actor",
                WorkRole::Reviewer,
                0,
            );
            policy_resolution.resolved_policy_hash = [0xAB; 32];
            policy_resolution.pcac_policy = Some(apm2_core::pcac::PcacPolicyKnobs::default());
            seed_policy_lineage_for_test(
                cas.as_ref(),
                "W-AUTHZ",
                "totally-different-actor",
                WorkRole::Reviewer,
                &policy_resolution,
            );
            dispatcher
                .work_registry
                .register_claim(WorkClaim {
                    work_id: "W-AUTHZ".to_string(),
                    lease_id: "parent-authz".to_string(),
                    actor_id: "totally-different-actor".to_string(),
                    role: WorkRole::Reviewer,
                    policy_resolution,
                    executor_custody_domains: vec![],
                    author_custody_domains: vec![],
                    permeability_receipt: None,
                })
                .expect("work claim registration");

            let unauthorized_ctx =
                ConnectionContext::privileged_session_open(Some(PeerCredentials {
                    uid: 1000,
                    gid: 1000,
                    pid: Some(12345),
                }));

            let request = DelegateSubleaseRequest {
                parent_lease_id: "parent-authz".to_string(),
                delegatee_actor_id: "child-001".to_string(),
                requested_expiry_ns: 1_900_000_000_000,
                sublease_id: "sublease-authz".to_string(),
                identity_proof_hash: vec![0x99; 32],
            };
            let frame = encode_delegate_sublease_request(&request);

            let response = dispatcher.dispatch(&frame, &unauthorized_ctx).unwrap();
            match response {
                PrivilegedResponse::Error(err) => {
                    assert!(
                        err.message.contains("not authorized to delegate"),
                        "error message must indicate authorization failure, got: {}",
                        err.message
                    );
                    assert_eq!(
                        err.code,
                        i32::from(PrivilegedErrorCode::PermissionDenied),
                        "unauthorized caller must get PermissionDenied"
                    );
                },
                other => panic!("expected PermissionDenied for unauthorized caller, got {other:?}"),
            }
        }

        /// Security BLOCKER: Caller with no peer credentials must be rejected
        /// (fail-closed).
        #[test]
        fn test_delegate_sublease_no_peer_credentials_rejected() {
            let (dispatcher, _ctx, _parent) = setup_dispatcher_with_orchestrator(
                "parent-no-creds",
                "W-NC",
                "gate-quality",
                "executor-001",
            );

            // Context with NO peer credentials
            let ctx = ConnectionContext::privileged_session_open(None);

            let request = DelegateSubleaseRequest {
                parent_lease_id: "parent-no-creds".to_string(),
                delegatee_actor_id: "child-001".to_string(),
                requested_expiry_ns: 1_900_000_000_000,
                sublease_id: "sublease-no-creds".to_string(),
                identity_proof_hash: vec![0x99; 32],
            };
            let frame = encode_delegate_sublease_request(&request);

            let response = dispatcher.dispatch(&frame, &ctx).unwrap();
            match response {
                PrivilegedResponse::Error(err) => {
                    assert!(
                        err.message.contains("peer credentials required"),
                        "Missing creds must fail closed: {}",
                        err.message
                    );
                },
                other => panic!("Expected rejection for missing peer creds, got {other:?}"),
            }
        }

        #[test]
        fn test_delegate_sublease_message_type_tag() {
            assert_eq!(PrivilegedMessageType::DelegateSublease.tag(), 72);
            assert_eq!(
                PrivilegedMessageType::from_tag(72),
                Some(PrivilegedMessageType::DelegateSublease)
            );
        }

        #[test]
        fn test_verify_ledger_chain_message_type_tag() {
            assert_eq!(PrivilegedMessageType::VerifyLedgerChain.tag(), 73);
            assert_eq!(
                PrivilegedMessageType::from_tag(73),
                Some(PrivilegedMessageType::VerifyLedgerChain)
            );
        }

        // =================================================================
        // TCK-00469: Projection Recovery IPC Tests
        // =================================================================

        #[test]
        fn test_register_recovery_evidence_tag_and_routing() {
            assert_eq!(PrivilegedMessageType::RegisterRecoveryEvidence.tag(), 74);
            assert_eq!(
                PrivilegedMessageType::from_tag(74),
                Some(PrivilegedMessageType::RegisterRecoveryEvidence)
            );
        }

        #[test]
        fn test_request_unfreeze_tag_and_routing() {
            assert_eq!(PrivilegedMessageType::RequestUnfreeze.tag(), 75);
            assert_eq!(
                PrivilegedMessageType::from_tag(75),
                Some(PrivilegedMessageType::RequestUnfreeze)
            );
        }

        #[test]
        fn test_register_recovery_evidence_without_watchdog_fails_closed() {
            let dispatcher = PrivilegedDispatcher::new();
            let ctx = ConnectionContext::privileged_session_open(Some(PeerCredentials {
                uid: 1000,
                gid: 1000,
                pid: Some(12345),
            }));
            let request = RegisterRecoveryEvidenceRequest {
                freeze_id: "freeze-001".to_string(),
                durable_evidence_digest: vec![0xAA; 32],
                required_start_sequence: 0,
                required_end_sequence: 10,
                receipts_json: b"[]".to_vec(),
                lease_id: "test-lease-001".to_string(),
            };
            let mut frame = vec![PrivilegedMessageType::RegisterRecoveryEvidence.tag()];
            request.encode(&mut frame).expect("encode");
            let result = dispatcher.dispatch(&Bytes::from(frame), &ctx);
            assert!(
                result.is_err(),
                "should fail closed when watchdog not configured"
            );
        }

        #[test]
        fn test_request_unfreeze_without_watchdog_fails_closed() {
            let dispatcher = PrivilegedDispatcher::new();
            let ctx = ConnectionContext::privileged_session_open(Some(PeerCredentials {
                uid: 1000,
                gid: 1000,
                pid: Some(12345),
            }));
            let request = RequestUnfreezeRequest {
                freeze_id: "freeze-001".to_string(),
                resolution_type: "MANUAL".to_string(),
                adjudication_id: String::new(),
                lease_id: "test-lease-001".to_string(),
            };
            let mut frame = vec![PrivilegedMessageType::RequestUnfreeze.tag()];
            request.encode(&mut frame).expect("encode");
            let result = dispatcher.dispatch(&Bytes::from(frame), &ctx);
            assert!(
                result.is_err(),
                "should fail closed when watchdog not configured"
            );
        }

        #[test]
        fn test_register_recovery_evidence_empty_freeze_id_rejected() {
            use apm2_core::crypto::Signer;

            use crate::projection::{
                DivergenceWatchdog, DivergenceWatchdogConfig, FreezeRegistry, SystemTimeSource,
            };

            let signer = Signer::generate();
            let config = DivergenceWatchdogConfig::new("test-repo").unwrap();
            let registry = Arc::new(FreezeRegistry::new_hydrated_for_testing());
            let watchdog: DivergenceWatchdog<SystemTimeSource> =
                DivergenceWatchdog::with_registry(signer, config, registry);
            let dispatcher =
                PrivilegedDispatcher::new().with_divergence_watchdog(Arc::new(watchdog));
            let ctx = ConnectionContext::privileged_session_open(Some(PeerCredentials {
                uid: 1000,
                gid: 1000,
                pid: Some(12345),
            }));
            let request = RegisterRecoveryEvidenceRequest {
                freeze_id: String::new(), // empty => rejected
                durable_evidence_digest: vec![0xAA; 32],
                required_start_sequence: 0,
                required_end_sequence: 10,
                receipts_json: b"[]".to_vec(),
                lease_id: "test-lease-001".to_string(),
            };
            let mut frame = vec![PrivilegedMessageType::RegisterRecoveryEvidence.tag()];
            request.encode(&mut frame).expect("encode");
            let result = dispatcher.dispatch(&Bytes::from(frame), &ctx).unwrap();
            match result {
                PrivilegedResponse::Error(e) => {
                    assert!(
                        e.message.contains("freeze_id"),
                        "error should mention freeze_id: {}",
                        e.message
                    );
                },
                other => panic!("expected error response, got: {other:?}"),
            }
        }

        #[test]
        fn test_request_unfreeze_invalid_resolution_type_rejected() {
            use apm2_core::crypto::Signer;

            use crate::projection::{
                DivergenceWatchdog, DivergenceWatchdogConfig, FreezeRegistry, SystemTimeSource,
            };

            let signer = Signer::generate();
            let config = DivergenceWatchdogConfig::new("test-repo").unwrap();
            let registry = Arc::new(FreezeRegistry::new_hydrated_for_testing());
            let watchdog: DivergenceWatchdog<SystemTimeSource> =
                DivergenceWatchdog::with_registry(signer, config, registry);
            let dispatcher =
                PrivilegedDispatcher::new().with_divergence_watchdog(Arc::new(watchdog));
            let ctx = ConnectionContext::privileged_session_open(Some(PeerCredentials {
                uid: 1000,
                gid: 1000,
                pid: Some(12345),
            }));
            let request = RequestUnfreezeRequest {
                freeze_id: "freeze-001".to_string(),
                resolution_type: "INVALID_TYPE".to_string(),
                adjudication_id: String::new(),
                lease_id: "test-lease-001".to_string(),
            };
            let mut frame = vec![PrivilegedMessageType::RequestUnfreeze.tag()];
            request.encode(&mut frame).expect("encode");
            let result = dispatcher.dispatch(&Bytes::from(frame), &ctx).unwrap();
            match result {
                PrivilegedResponse::Error(e) => {
                    assert!(
                        e.message.contains("unknown resolution_type"),
                        "error should mention resolution_type: {}",
                        e.message
                    );
                },
                other => panic!("expected error response, got: {other:?}"),
            }
        }

        #[test]
        fn test_register_recovery_evidence_response_encoding() {
            let resp =
                PrivilegedResponse::RegisterRecoveryEvidence(RegisterRecoveryEvidenceResponse {
                    accepted: true,
                    freeze_id: "freeze-001".to_string(),
                    message: "evidence accepted".to_string(),
                });
            let encoded = resp.encode();
            assert_eq!(
                encoded[0],
                PrivilegedMessageType::RegisterRecoveryEvidence.tag(),
                "first byte should be RegisterRecoveryEvidence tag (74)"
            );
            assert!(
                encoded.len() > 1,
                "encoded response should have payload after tag"
            );
        }

        #[test]
        fn test_request_unfreeze_response_encoding() {
            let resp = PrivilegedResponse::RequestUnfreeze(RequestUnfreezeResponse {
                success: true,
                freeze_id: "freeze-001".to_string(),
                message: "freeze lifted".to_string(),
            });
            let encoded = resp.encode();
            assert_eq!(
                encoded[0],
                PrivilegedMessageType::RequestUnfreeze.tag(),
                "first byte should be RequestUnfreeze tag (75)"
            );
            assert!(
                encoded.len() > 1,
                "encoded response should have payload after tag"
            );
        }

        #[test]
        fn test_delegate_sublease_response_encoding() {
            let resp = PrivilegedResponse::DelegateSublease(DelegateSubleaseResponse {
                sublease_id: "sub-001".to_string(),
                parent_lease_id: "parent-001".to_string(),
                delegatee_actor_id: "child-001".to_string(),
                gate_id: "gate-quality".to_string(),
                expires_at_ns: 1_900_000,
                event_id: "evt-001".to_string(),
            });
            let encoded = resp.encode();
            assert_eq!(
                encoded[0],
                PrivilegedMessageType::DelegateSublease.tag(),
                "First byte should be DelegateSublease tag (72)"
            );
            assert!(
                encoded.len() > 1,
                "Encoded response should have payload after tag"
            );
        }

        #[test]
        fn test_delegate_sublease_ledger_event_emitted() {
            let (dispatcher, ctx, _parent) = setup_dispatcher_with_orchestrator(
                "parent-lease-evt",
                "W-DS-EVT",
                "gate-quality",
                "executor-001",
            );

            // Derive the expected caller actor_id (same credentials as
            // setup_dispatcher_with_orchestrator uses: uid=1000, gid=1000).
            let expected_caller_actor = derive_actor_id(&PeerCredentials {
                uid: 1000,
                gid: 1000,
                pid: Some(12345),
            });

            let request = DelegateSubleaseRequest {
                parent_lease_id: "parent-lease-evt".to_string(),
                delegatee_actor_id: "child-executor-evt".to_string(),
                requested_expiry_ns: 1_900_000_000_000, // ns
                sublease_id: "sublease-evt-001".to_string(),
                identity_proof_hash: vec![0x99; 32],
            };
            let frame = encode_delegate_sublease_request(&request);

            let response = dispatcher.dispatch(&frame, &ctx).unwrap();
            let event_id = match response {
                PrivilegedResponse::DelegateSublease(resp) => resp.event_id,
                other => panic!("Expected DelegateSublease, got {other:?}"),
            };

            // Verify the event was persisted to the emitter
            let stored_event = dispatcher.event_emitter.get_event(&event_id);
            assert!(
                stored_event.is_some(),
                "SubleaseIssued event should be persisted in emitter"
            );
            let event = stored_event.unwrap();
            assert_eq!(event.event_type, "SubleaseIssued");
            // v5 Finding 3: actor_id must be the authenticated CALLER, not the
            // caller-controlled delegatee_actor_id.
            assert_eq!(
                event.actor_id, expected_caller_actor,
                "SubleaseIssued event must record authenticated caller, not delegatee"
            );
            assert_ne!(
                event.actor_id, "child-executor-evt",
                "actor_id must NOT be the delegatee"
            );
            assert!(event.timestamp_ns > 0, "Timestamp must be non-zero (HTF)");

            let payload = ingest_review_receipt::decode_wrapped_or_direct_event_payload(&event);
            let artifact = payload
                .get("delegation_satisfiability_artifact")
                .expect("delegation_satisfiability_artifact must be present");
            assert_eq!(
                artifact.get("schema").and_then(serde_json::Value::as_str),
                Some("apm2.delegate_sublease_satisfiability_artifact.v1"),
                "delegation satisfiability artifact schema must be persisted"
            );
            let receipt = artifact
                .get("delegation_satisfiability_receipt")
                .expect("delegation_satisfiability_receipt must be present");
            assert_eq!(
                receipt
                    .get("delegation_depth")
                    .and_then(serde_json::Value::as_u64),
                Some(1),
                "direct child delegation must persist depth evidence"
            );
            assert_eq!(
                receipt
                    .get("budget_ticks")
                    .and_then(serde_json::Value::as_u64),
                Some(SUBLEASE_DELEGATION_SATISFIABILITY_BUDGET_TICKS),
                "persisted receipt must carry deterministic budget evidence"
            );
            assert!(
                receipt
                    .get("ticks_used")
                    .and_then(serde_json::Value::as_u64)
                    .is_some(),
                "persisted receipt must carry deterministic tick usage evidence"
            );

            let persisted_artifact_digest = payload
                .get("delegation_satisfiability_artifact_digest")
                .and_then(serde_json::Value::as_str)
                .expect("delegation_satisfiability_artifact_digest must be present");
            let expected_artifact_digest = hex::encode(hash_bytes(
                &canonical_json_bytes(artifact).expect("artifact canonicalization"),
            ));
            assert_eq!(
                persisted_artifact_digest, expected_artifact_digest,
                "artifact digest must bind canonical delegation satisfiability evidence"
            );
            assert!(
                payload
                    .get("delegation_satisfiability_artifact_cas_hash")
                    .and_then(serde_json::Value::as_str)
                    .is_some(),
                "CAS-backed dispatcher must persist artifact CAS hash pointer"
            );
        }

        #[test]
        fn test_delegate_sublease_duplicate_id_idempotent() {
            let (dispatcher, ctx, _parent) = setup_dispatcher_with_orchestrator(
                "parent-lease-dup",
                "W-DS-DUP",
                "gate-quality",
                "executor-dup",
            );

            let request = DelegateSubleaseRequest {
                parent_lease_id: "parent-lease-dup".to_string(),
                delegatee_actor_id: "child-executor-dup".to_string(),
                requested_expiry_ns: 1_900_000_000_000,
                sublease_id: "sublease-dup-001".to_string(),
                identity_proof_hash: vec![0x99; 32],
            };
            let frame = encode_delegate_sublease_request(&request);

            // First call: should succeed and return a non-empty event_id
            let response = dispatcher.dispatch(&frame, &ctx).unwrap();
            let original_event_id = match &response {
                PrivilegedResponse::DelegateSublease(resp) => {
                    assert_eq!(resp.sublease_id, "sublease-dup-001");
                    assert!(
                        !resp.event_id.is_empty(),
                        "First delegation must return non-empty event_id"
                    );
                    resp.event_id.clone()
                },
                other => panic!("Expected DelegateSublease success, got {other:?}"),
            };

            // Second call with same parameters: should return idempotent
            // result with the ORIGINAL event_id (not empty).
            let response2 = dispatcher.dispatch(&frame, &ctx).unwrap();
            match response2 {
                PrivilegedResponse::DelegateSublease(resp) => {
                    assert_eq!(
                        resp.sublease_id, "sublease-dup-001",
                        "Idempotent return should have same sublease_id"
                    );
                    assert_eq!(
                        resp.event_id, original_event_id,
                        "Idempotent return must replay the original SubleaseIssued event_id"
                    );
                    assert!(
                        !resp.event_id.is_empty(),
                        "Idempotent event_id must not be empty"
                    );
                },
                other => panic!("Expected idempotent DelegateSublease, got {other:?}"),
            }
        }

        #[test]
        fn test_delegate_sublease_duplicate_id_different_identity_proof_hash_rejected() {
            let (dispatcher, ctx, _parent) = setup_dispatcher_with_orchestrator(
                "parent-lease-dup-proof",
                "W-DS-DUP-PROOF",
                "gate-quality",
                "executor-dup-proof",
            );

            let req1 = DelegateSubleaseRequest {
                parent_lease_id: "parent-lease-dup-proof".to_string(),
                delegatee_actor_id: "child-executor-dup-proof".to_string(),
                requested_expiry_ns: 1_900_000_000_000,
                sublease_id: "sublease-dup-proof-001".to_string(),
                identity_proof_hash: vec![0x99; 32],
            };
            let frame1 = encode_delegate_sublease_request(&req1);
            let resp1 = dispatcher.dispatch(&frame1, &ctx).unwrap();
            assert!(
                matches!(resp1, PrivilegedResponse::DelegateSublease(_)),
                "first sublease should succeed"
            );

            // Same sublease_id, same logical parameters, but different proof pointer.
            // This is not idempotent and must be rejected.
            let req2 = DelegateSubleaseRequest {
                parent_lease_id: "parent-lease-dup-proof".to_string(),
                delegatee_actor_id: "child-executor-dup-proof".to_string(),
                requested_expiry_ns: req1.requested_expiry_ns,
                sublease_id: "sublease-dup-proof-001".to_string(),
                identity_proof_hash: vec![0x98; 32],
            };
            let frame2 = encode_delegate_sublease_request(&req2);
            let resp2 = dispatcher.dispatch(&frame2, &ctx).unwrap();
            match resp2 {
                PrivilegedResponse::Error(err) => {
                    assert!(
                        err.message.contains("identity_proof_hash"),
                        "expected identity_proof_hash mismatch rejection, got: {}",
                        err.message
                    );
                },
                other => panic!("expected identity_proof_hash mismatch rejection, got {other:?}"),
            }
        }

        #[test]
        fn test_delegate_sublease_duplicate_id_conflict_rejected() {
            let (dispatcher, ctx, _parent_a) =
                setup_dispatcher_with_orchestrator("parent-A", "W-A", "gate-A", "executor-ignored");
            let caller_actor = derive_actor_id(&PeerCredentials {
                uid: 1000,
                gid: 1000,
                pid: Some(12345),
            });
            let cas = dispatcher
                .cas
                .as_ref()
                .expect("CAS should be configured by setup helper");
            register_full_test_lease(&TestLeaseConfig {
                dispatcher: &dispatcher,
                cas: cas.as_ref(),
                lease_id: "parent-B",
                work_id: "W-B",
                gate_id: "gate-B",
                executor_actor_id: &caller_actor,
                policy_hash: [0xAB; 32],
                wall_time_source: WallTimeSource::AuthenticatedNts,
                include_attestation: true,
            });
            dispatcher.lease_validator.register_lease_with_executor(
                "parent-B",
                "W-B",
                "gate-B",
                &caller_actor,
            );
            let mut policy_resolution =
                test_policy_resolution_with_lineage("W-B", &caller_actor, WorkRole::Reviewer, 0);
            policy_resolution.resolved_policy_hash = [0xAB; 32];
            policy_resolution.pcac_policy = Some(apm2_core::pcac::PcacPolicyKnobs::default());
            seed_policy_lineage_for_test(
                cas.as_ref(),
                "W-B",
                &caller_actor,
                WorkRole::Reviewer,
                &policy_resolution,
            );
            dispatcher
                .work_registry
                .register_claim(WorkClaim {
                    work_id: "W-B".to_string(),
                    lease_id: "parent-B".to_string(),
                    actor_id: caller_actor,
                    role: WorkRole::Reviewer,
                    policy_resolution,
                    executor_custody_domains: vec![],
                    author_custody_domains: vec![],
                    permeability_receipt: None,
                })
                .expect("work claim registration for parent-B");

            // First call: issue sublease under parent A
            let req1 = DelegateSubleaseRequest {
                parent_lease_id: "parent-A".to_string(),
                delegatee_actor_id: "child-001".to_string(),
                requested_expiry_ns: 1_900_000_000_000,
                sublease_id: "shared-sublease-id".to_string(),
                identity_proof_hash: vec![0x99; 32],
            };
            let frame1 = encode_delegate_sublease_request(&req1);
            let resp1 = dispatcher.dispatch(&frame1, &ctx).unwrap();
            assert!(
                matches!(resp1, PrivilegedResponse::DelegateSublease(_)),
                "First sublease should succeed"
            );

            // Second call: try to issue sublease with SAME ID under parent B
            // (different work_id/gate_id) — must be rejected as conflict
            let req2 = DelegateSubleaseRequest {
                parent_lease_id: "parent-B".to_string(),
                delegatee_actor_id: "child-002".to_string(),
                requested_expiry_ns: 1_900_000_000_000,
                sublease_id: "shared-sublease-id".to_string(),
                identity_proof_hash: vec![0x99; 32],
            };
            let frame2 = encode_delegate_sublease_request(&req2);
            let resp2 = dispatcher.dispatch(&frame2, &ctx).unwrap();
            match resp2 {
                PrivilegedResponse::Error(err) => {
                    assert!(
                        err.message
                            .contains("already exists with different parameters"),
                        "Must reject conflicting sublease_id, got: {}",
                        err.message
                    );
                },
                other => panic!("Expected conflict rejection, got {other:?}"),
            }
        }

        /// v6 Finding 3: Same `sublease_id`, same work/gate/delegatee, but a
        /// different `requested_expiry_ns` must NOT be treated as idempotent.
        /// The full request tuple (including expiry) must match for
        /// idempotent return.
        #[test]
        fn test_delegate_sublease_different_expiry_rejected_as_conflict() {
            let (dispatcher, ctx, _parent) = setup_dispatcher_with_orchestrator(
                "parent-lease-exp",
                "W-DS-EXP",
                "gate-quality",
                "executor-exp",
            );

            // First request with expiry = 1_900_000_000_000 ns
            let req1 = DelegateSubleaseRequest {
                parent_lease_id: "parent-lease-exp".to_string(),
                delegatee_actor_id: "child-exp".to_string(),
                requested_expiry_ns: 1_900_000_000_000,
                sublease_id: "sublease-exp-001".to_string(),
                identity_proof_hash: vec![0x99; 32],
            };
            let frame1 = encode_delegate_sublease_request(&req1);
            let resp1 = dispatcher.dispatch(&frame1, &ctx).unwrap();
            assert!(
                matches!(resp1, PrivilegedResponse::DelegateSublease(_)),
                "First sublease should succeed"
            );

            // Second request with DIFFERENT expiry but same sublease_id
            let req2 = DelegateSubleaseRequest {
                parent_lease_id: "parent-lease-exp".to_string(),
                delegatee_actor_id: "child-exp".to_string(),
                requested_expiry_ns: 1_800_000_000_000, // Different expiry
                sublease_id: "sublease-exp-001".to_string(),
                identity_proof_hash: vec![0x99; 32],
            };
            let frame2 = encode_delegate_sublease_request(&req2);
            let resp2 = dispatcher.dispatch(&frame2, &ctx).unwrap();
            match resp2 {
                PrivilegedResponse::Error(err) => {
                    assert!(
                        err.message
                            .contains("already exists with different parameters"),
                        "Different expiry should be rejected as conflict, got: {}",
                        err.message
                    );
                },
                other => panic!("Expected conflict rejection for different expiry, got {other:?}"),
            }
        }

        /// v7 Finding 2: Idempotent replay with the same `sublease_id` but a
        /// different `parent_lease_id` must be rejected even when inherited
        /// fields (changeset digest, policy hash) happen to match. The
        /// parent lineage check uses the `SubleaseIssued` event payload's
        /// parent lease ID field to enforce exact lineage binding.
        #[test]
        fn test_delegate_sublease_idempotent_rejects_different_parent_lineage() {
            let (dispatcher, ctx, _parent_a) = setup_dispatcher_with_orchestrator(
                "parent-lin-A",
                "W-LIN",
                "gate-lin",
                "executor-ignored",
            );
            let caller_actor = derive_actor_id(&PeerCredentials {
                uid: 1000,
                gid: 1000,
                pid: Some(12345),
            });
            let cas = dispatcher
                .cas
                .as_ref()
                .expect("CAS should be configured by setup helper");
            register_full_test_lease(&TestLeaseConfig {
                dispatcher: &dispatcher,
                cas: cas.as_ref(),
                lease_id: "parent-lin-B",
                work_id: "W-LIN",
                gate_id: "gate-lin",
                executor_actor_id: &caller_actor,
                policy_hash: [0xAB; 32],
                wall_time_source: WallTimeSource::AuthenticatedNts,
                include_attestation: true,
            });
            dispatcher.lease_validator.register_lease_with_executor(
                "parent-lin-B",
                "W-LIN",
                "gate-lin",
                &caller_actor,
            );
            let mut policy_resolution =
                test_policy_resolution_with_lineage("W-LIN", &caller_actor, WorkRole::Reviewer, 0);
            policy_resolution.resolved_policy_hash = [0xAB; 32];
            policy_resolution.pcac_policy = Some(apm2_core::pcac::PcacPolicyKnobs::default());
            seed_policy_lineage_for_test(
                cas.as_ref(),
                "W-LIN",
                &caller_actor,
                WorkRole::Reviewer,
                &policy_resolution,
            );
            // If parent A's claim is already present from setup helper, this can
            // return DuplicateWorkId; in that case we keep the helper-registered claim.
            let _ = dispatcher.work_registry.register_claim(WorkClaim {
                work_id: "W-LIN".to_string(),
                lease_id: "parent-lin-B".to_string(),
                actor_id: caller_actor,
                role: WorkRole::Reviewer,
                policy_resolution,
                executor_custody_domains: vec![],
                author_custody_domains: vec![],
                permeability_receipt: None,
            });

            // First call: issue sublease under parent A
            let req1 = DelegateSubleaseRequest {
                parent_lease_id: "parent-lin-A".to_string(),
                delegatee_actor_id: "child-lin".to_string(),
                requested_expiry_ns: 1_900_000_000_000,
                sublease_id: "sublease-lineage-001".to_string(),
                identity_proof_hash: vec![0x99; 32],
            };
            let frame1 = encode_delegate_sublease_request(&req1);
            let resp1 = dispatcher.dispatch(&frame1, &ctx).unwrap();
            assert!(
                matches!(resp1, PrivilegedResponse::DelegateSublease(_)),
                "First sublease should succeed"
            );

            // Second call: same sublease_id but different parent_lease_id.
            // Since both parents have identical changeset_digest/policy_hash,
            // the indirect lineage check (changeset_digest + policy_hash) would
            // pass, but the direct parent_lease_id check in the event payload
            // MUST reject this.
            let req2 = DelegateSubleaseRequest {
                parent_lease_id: "parent-lin-B".to_string(),
                delegatee_actor_id: "child-lin".to_string(),
                requested_expiry_ns: 1_900_000_000_000,
                sublease_id: "sublease-lineage-001".to_string(),
                identity_proof_hash: vec![0x99; 32],
            };
            let frame2 = encode_delegate_sublease_request(&req2);
            let resp2 = dispatcher.dispatch(&frame2, &ctx).unwrap();
            match resp2 {
                PrivilegedResponse::Error(err) => {
                    assert!(
                        err.message.contains("originally delegated from parent")
                            || err
                                .message
                                .contains("already exists with different parameters"),
                        "Must reject sublease with different parent lineage, got: {}",
                        err.message
                    );
                },
                other => panic!("Expected parent lineage rejection, got {other:?}"),
            }
        }
    }

    // ========================================================================
    // TCK-00448: Adversarial lineage denial tests
    // ========================================================================
    mod tck_00448_lineage_denials {
        use apm2_core::evidence::{ContentAddressedStore, MemoryCas};

        use super::*;

        const LINEAGE_TEST_ARTIFACT_CONTENT: &[u8] = b"tck-00448-lineage-artifact";

        fn test_peer_credentials() -> PeerCredentials {
            PeerCredentials {
                uid: 1000,
                gid: 1000,
                pid: Some(12345),
            }
        }

        fn setup_spawn_lineage_fixture(
            work_id: &str,
            lease_id: &str,
            mutate_policy: impl FnOnce(&Arc<MemoryCas>, &mut PolicyResolution),
        ) -> (
            PrivilegedDispatcher,
            ConnectionContext,
            SpawnEpisodeRequest,
            String,
        ) {
            let peer_creds = test_peer_credentials();
            let actor_id = derive_actor_id(&peer_creds);
            let cas = Arc::new(MemoryCas::default());
            let dispatcher = PrivilegedDispatcher::new()
                .with_cas(Arc::clone(&cas) as Arc<dyn ContentAddressedStore>);

            let mut policy_resolution =
                test_policy_resolution_with_lineage(work_id, &actor_id, WorkRole::Implementer, 0);
            seed_policy_lineage_for_test(
                cas.as_ref(),
                work_id,
                &actor_id,
                WorkRole::Implementer,
                &policy_resolution,
            );
            mutate_policy(&cas, &mut policy_resolution);

            dispatcher
                .work_registry
                .register_claim(WorkClaim {
                    work_id: work_id.to_string(),
                    lease_id: lease_id.to_string(),
                    actor_id,
                    role: WorkRole::Implementer,
                    policy_resolution,
                    executor_custody_domains: vec![],
                    author_custody_domains: vec![],
                    permeability_receipt: None,
                })
                .expect("lineage test claim registration must succeed");

            let ctx = ConnectionContext::privileged_session_open(Some(peer_creds));
            let request = SpawnEpisodeRequest {
                workspace_root: test_workspace_root(),
                work_id: work_id.to_string(),
                role: WorkRole::Implementer.into(),
                lease_id: Some(lease_id.to_string()),
                adapter_profile_hash: None,
                max_episodes: None,
                escalation_predicate: None,
                permeability_receipt_hash: None,
            };

            (dispatcher, ctx, request, work_id.to_string())
        }

        fn setup_receipt_lineage_fixture(
            work_id: &str,
            lease_id: &str,
            receipt_id: &str,
            mutate_policy: impl FnOnce(&Arc<MemoryCas>, &mut PolicyResolution),
        ) -> (
            PrivilegedDispatcher,
            ConnectionContext,
            IngestReviewReceiptRequest,
            String,
        ) {
            let peer_creds = test_peer_credentials();
            let actor_id = derive_actor_id(&peer_creds);
            let cas = Arc::new(MemoryCas::default());
            let artifact_bundle_hash = cas
                .store(LINEAGE_TEST_ARTIFACT_CONTENT)
                .expect("lineage test artifact bundle should store in CAS")
                .hash
                .to_vec();
            let dispatcher = PrivilegedDispatcher::new()
                .with_cas(Arc::clone(&cas) as Arc<dyn ContentAddressedStore>);

            dispatcher.lease_validator.register_lease_with_executor(
                lease_id,
                work_id,
                "gate-tck-00448-lineage",
                &actor_id,
            );
            super::ingest_review_receipt::register_full_test_lease(
                &super::ingest_review_receipt::TestLeaseConfig {
                    dispatcher: &dispatcher,
                    cas: cas.as_ref(),
                    lease_id,
                    work_id,
                    gate_id: "gate-tck-00448-lineage",
                    executor_actor_id: &actor_id,
                    policy_hash: [0u8; 32],
                    wall_time_source: WallTimeSource::AuthenticatedNts,
                    include_attestation: true,
                },
            );

            let mut policy_resolution =
                test_policy_resolution_with_lineage(work_id, &actor_id, WorkRole::Reviewer, 0);
            seed_policy_lineage_for_test(
                cas.as_ref(),
                work_id,
                &actor_id,
                WorkRole::Reviewer,
                &policy_resolution,
            );
            mutate_policy(&cas, &mut policy_resolution);

            dispatcher
                .work_registry
                .register_claim(WorkClaim {
                    work_id: work_id.to_string(),
                    lease_id: lease_id.to_string(),
                    actor_id: actor_id.clone(),
                    role: WorkRole::Reviewer,
                    policy_resolution,
                    executor_custody_domains: vec![],
                    author_custody_domains: vec![],
                    permeability_receipt: None,
                })
                .expect("lineage test review claim registration must succeed");

            let ctx = ConnectionContext::privileged_session_open(Some(peer_creds));
            let request = IngestReviewReceiptRequest {
                lease_id: lease_id.to_string(),
                receipt_id: receipt_id.to_string(),
                reviewer_actor_id: actor_id,
                changeset_digest: vec![0x42; 32],
                artifact_bundle_hash,
                verdict: ReviewReceiptVerdict::Approve.into(),
                blocked_reason_code: 0,
                blocked_log_hash: vec![],
                identity_proof_hash: vec![0x99; 32],
            };

            (dispatcher, ctx, request, work_id.to_string())
        }

        fn review_receipt_event_count(dispatcher: &PrivilegedDispatcher, work_id: &str) -> usize {
            dispatcher
                .event_emitter
                .get_events_by_work_id(work_id)
                .into_iter()
                .filter(|event| event.event_type == "review_receipt_recorded")
                .count()
        }

        #[test]
        fn test_missing_role_spec_hash_denied() {
            let (dispatcher, ctx, request, work_id) = setup_receipt_lineage_fixture(
                "W-TCK-00448-MISSING-ROLE",
                "lease-tck-00448-missing-role",
                "RR-TCK-00448-MISSING-ROLE",
                |_cas, policy| {
                    policy.role_spec_hash = [0u8; 32];
                },
            );
            let baseline_review_event_count = review_receipt_event_count(&dispatcher, &work_id);

            let frame = encode_ingest_review_receipt_request(&request);
            let response = dispatcher.dispatch(&frame, &ctx).unwrap();

            match response {
                PrivilegedResponse::Error(err) => {
                    assert_eq!(
                        err.code,
                        PrivilegedErrorCode::PolicyResolutionMissing as i32
                    );
                    assert!(
                        err.message.contains("missing_authority_context"),
                        "deny message should include missing_authority_context, got: {}",
                        err.message
                    );
                    assert!(
                        err.message.contains("claim role_spec_hash is zero"),
                        "deny message should include claim role_spec_hash zero detail, got: {}",
                        err.message
                    );
                },
                other => panic!("expected authority-context deny error, got {other:?}"),
            }

            assert_eq!(
                review_receipt_event_count(&dispatcher, &work_id),
                baseline_review_event_count,
                "missing role_spec_hash denial must not emit review_receipt_recorded"
            );
            assert!(
                dispatcher
                    .event_emitter
                    .get_event_by_receipt_id(&request.receipt_id)
                    .is_none(),
                "missing role_spec_hash denial must not persist receipt_id event state"
            );
        }

        #[test]
        fn test_mismatched_role_spec_hash_denied() {
            let (dispatcher, ctx, request, work_id) = setup_receipt_lineage_fixture(
                "W-TCK-00448-MISMATCH-ROLE",
                "lease-tck-00448-mismatch-role",
                "RR-TCK-00448-MISMATCH-ROLE",
                |cas, policy| {
                    let wrong_hash = cas
                        .store(b"tck-00448-mismatched-role-spec")
                        .expect("mismatched role hash preimage should store in CAS")
                        .hash;
                    policy.role_spec_hash = wrong_hash;
                },
            );
            let baseline_review_event_count = review_receipt_event_count(&dispatcher, &work_id);

            let frame = encode_ingest_review_receipt_request(&request);
            let response = dispatcher.dispatch(&frame, &ctx).unwrap();

            match response {
                PrivilegedResponse::Error(err) => {
                    assert_eq!(
                        err.code,
                        PrivilegedErrorCode::CapabilityRequestRejected as i32
                    );
                    assert!(
                        err.message.contains("unknown_role_profile"),
                        "deny message should include unknown_role_profile, got: {}",
                        err.message
                    );
                    assert!(
                        err.message.contains("does not match authoritative hash"),
                        "deny message should include authoritative hash mismatch, got: {}",
                        err.message
                    );
                },
                other => panic!("expected unknown-role-profile deny error, got {other:?}"),
            }

            assert_eq!(
                review_receipt_event_count(&dispatcher, &work_id),
                baseline_review_event_count,
                "mismatched role_spec_hash denial must not emit review_receipt_recorded"
            );
            assert!(
                dispatcher
                    .event_emitter
                    .get_event_by_receipt_id(&request.receipt_id)
                    .is_none(),
                "mismatched role_spec_hash denial must not persist receipt_id event state"
            );
        }

        #[test]
        fn test_missing_context_pack_recipe_hash_denied() {
            let (dispatcher, ctx, request, work_id) = setup_receipt_lineage_fixture(
                "W-TCK-00448-MISSING-RECIPE",
                "lease-tck-00448-missing-recipe",
                "RR-TCK-00448-MISSING-RECIPE",
                |_cas, policy| {
                    policy.context_pack_recipe_hash = [0u8; 32];
                },
            );
            let baseline_review_event_count = review_receipt_event_count(&dispatcher, &work_id);

            let frame = encode_ingest_review_receipt_request(&request);
            let response = dispatcher.dispatch(&frame, &ctx).unwrap();

            match response {
                PrivilegedResponse::Error(err) => {
                    assert_eq!(
                        err.code,
                        PrivilegedErrorCode::PolicyResolutionMissing as i32
                    );
                    assert!(
                        err.message.contains("missing_authority_context"),
                        "deny message should include missing_authority_context, got: {}",
                        err.message
                    );
                    assert!(
                        err.message
                            .contains("claim context_pack_recipe_hash is zero"),
                        "deny message should include claim context_pack_recipe_hash zero detail, got: {}",
                        err.message
                    );
                },
                other => panic!("expected authority-context deny error, got {other:?}"),
            }

            assert_eq!(
                review_receipt_event_count(&dispatcher, &work_id),
                baseline_review_event_count,
                "missing context_pack_recipe_hash denial must not emit review_receipt_recorded"
            );
            assert!(
                dispatcher
                    .event_emitter
                    .get_event_by_receipt_id(&request.receipt_id)
                    .is_none(),
                "missing context_pack_recipe_hash denial must not persist receipt_id event state"
            );
        }

        #[test]
        fn test_stale_context_pack_recipe_hash_denied() {
            let (dispatcher, ctx, request, work_id) = setup_receipt_lineage_fixture(
                "W-TCK-00448-STALE-RECIPE",
                "lease-tck-00448-stale-recipe",
                "RR-TCK-00448-STALE-RECIPE",
                |cas, policy| {
                    let stale_recipe = build_policy_context_pack_recipe(
                        "W-TCK-00448-LEGACY",
                        "actor:legacy",
                        policy.role_spec_hash,
                        policy.context_pack_hash,
                    )
                    .expect("stale recipe should compile");
                    let stale_recipe_bytes = stale_recipe
                        .recipe
                        .canonical_bytes()
                        .expect("stale recipe canonicalization should succeed");
                    let stale_hash = *blake3::hash(&stale_recipe_bytes).as_bytes();
                    cas.store(&stale_recipe_bytes)
                        .expect("stale recipe preimage should store in CAS");
                    assert_ne!(
                        stale_hash, policy.context_pack_recipe_hash,
                        "precondition: stale hash must differ from authoritative recipe hash"
                    );
                    policy.context_pack_recipe_hash = stale_hash;
                },
            );
            let baseline_review_event_count = review_receipt_event_count(&dispatcher, &work_id);

            let frame = encode_ingest_review_receipt_request(&request);
            let response = dispatcher.dispatch(&frame, &ctx).unwrap();

            match response {
                PrivilegedResponse::Error(err) => {
                    assert_eq!(
                        err.code,
                        PrivilegedErrorCode::CapabilityRequestRejected as i32
                    );
                    assert!(
                        err.message.contains("stale_authority_context"),
                        "deny message should include stale_authority_context, got: {}",
                        err.message
                    );
                    assert!(
                        err.message.contains("does not match authoritative hash"),
                        "deny message should include stale recipe authoritative mismatch detail, got: {}",
                        err.message
                    );
                },
                other => panic!("expected stale-authority deny error, got {other:?}"),
            }

            assert_eq!(
                review_receipt_event_count(&dispatcher, &work_id),
                baseline_review_event_count,
                "stale context_pack_recipe_hash denial must not emit review_receipt_recorded"
            );
            assert!(
                dispatcher
                    .event_emitter
                    .get_event_by_receipt_id(&request.receipt_id)
                    .is_none(),
                "stale context_pack_recipe_hash denial must not persist receipt_id event state"
            );
        }

        #[test]
        fn test_unknown_role_profile_denied() {
            let (dispatcher, ctx, request, work_id) = setup_receipt_lineage_fixture(
                "W-TCK-00448-UNKNOWN-ROLE",
                "lease-tck-00448-unknown-role",
                "RR-TCK-00448-UNKNOWN-ROLE",
                |_cas, policy| {
                    policy.role_spec_hash =
                        *blake3::hash(b"tck-00448-unknown-role-profile").as_bytes();
                },
            );
            let baseline_review_event_count = review_receipt_event_count(&dispatcher, &work_id);

            let frame = encode_ingest_review_receipt_request(&request);
            let response = dispatcher.dispatch(&frame, &ctx).unwrap();

            match response {
                PrivilegedResponse::Error(err) => {
                    assert_eq!(
                        err.code,
                        PrivilegedErrorCode::CapabilityRequestRejected as i32
                    );
                    assert!(
                        err.message.contains("unknown_role_profile"),
                        "deny message should include unknown_role_profile, got: {}",
                        err.message
                    );
                    assert!(
                        err.message.contains("does not match authoritative hash"),
                        "deny message should include role hash mismatch detail, got: {}",
                        err.message
                    );
                },
                other => panic!("expected unknown-role-profile deny error, got {other:?}"),
            }

            assert_eq!(
                review_receipt_event_count(&dispatcher, &work_id),
                baseline_review_event_count,
                "unknown role profile denial must not emit review_receipt_recorded"
            );
            assert!(
                dispatcher
                    .event_emitter
                    .get_event_by_receipt_id(&request.receipt_id)
                    .is_none(),
                "unknown role profile denial must not persist receipt_id event state"
            );
        }

        #[test]
        fn test_valid_lineage_accepted() {
            let (dispatcher, ctx, request, work_id) = setup_spawn_lineage_fixture(
                "W-TCK-00448-VALID",
                "L-TCK-00448-VALID",
                |_cas, _policy| {},
            );
            let baseline_event_count = dispatcher
                .event_emitter
                .get_events_by_work_id(&work_id)
                .len();

            let frame = encode_spawn_episode_request(&request);
            let response = dispatcher.dispatch(&frame, &ctx).unwrap();

            match response {
                PrivilegedResponse::SpawnEpisode(resp) => {
                    assert!(
                        !resp.session_id.is_empty(),
                        "valid lineage acceptance must return a session_id"
                    );
                    assert!(
                        !resp.ephemeral_handle.is_empty(),
                        "valid lineage acceptance must return an ephemeral_handle"
                    );
                },
                other => panic!("expected SpawnEpisode success, got {other:?}"),
            }

            assert!(
                dispatcher
                    .session_registry()
                    .get_session_by_work_id(&work_id)
                    .is_some(),
                "valid lineage acceptance must persist a session"
            );
            assert!(
                dispatcher
                    .event_emitter
                    .get_events_by_work_id(&work_id)
                    .len()
                    > baseline_event_count,
                "valid lineage acceptance must emit spawn lifecycle ledger events"
            );
        }
    }

    // ========================================================================
    // TCK-00340: Serde fail-closed default tests
    // ========================================================================
    mod serde_fail_closed {
        use super::*;

        #[test]
        fn test_policy_resolution_missing_risk_tier_defaults_to_tier4() {
            // SECURITY: When `resolved_risk_tier` is missing from JSON,
            // it must default to Tier4 (4), not Tier0 (0).
            let json = r#"{
                "policy_resolved_ref": "test-ref",
                "resolved_policy_hash": [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],
                "capability_manifest_hash": [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],
                "context_pack_hash": [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0]
            }"#;

            let resolution: PolicyResolution = serde_json::from_str(json)
                .expect("PolicyResolution should deserialize without resolved_risk_tier");
            assert_eq!(
                resolution.resolved_risk_tier, 4,
                "Missing resolved_risk_tier must default to Tier4 (4), not Tier0 (0) — fail-closed"
            );
        }

        #[test]
        fn test_policy_resolution_explicit_tier0_preserved() {
            // When `resolved_risk_tier` is explicitly set to 0, it must be preserved.
            let json = r#"{
                "policy_resolved_ref": "test-ref",
                "resolved_policy_hash": [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],
                "capability_manifest_hash": [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],
                "context_pack_hash": [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],
                "resolved_risk_tier": 0
            }"#;

            let resolution: PolicyResolution = serde_json::from_str(json)
                .expect("PolicyResolution should deserialize with explicit Tier0");
            assert_eq!(
                resolution.resolved_risk_tier, 0,
                "Explicit Tier0 must be preserved"
            );
        }
    }

    // ========================================================================
    // TCK-00340: SQLite integration tests (production path)
    // ========================================================================
    mod sqlite_integration {
        use std::sync::{Arc, Mutex};

        use apm2_core::evidence::{ContentAddressedStore, MemoryCas};
        use rusqlite::Connection;

        use super::*;
        use crate::ledger::{SqliteLeaseValidator, SqliteLedgerEventEmitter, SqliteWorkRegistry};

        /// Standard test artifact bundle content for sqlite integration tests.
        const TEST_ARTIFACT_CONTENT: &[u8] = b"test-artifact-bundle-content";

        /// Returns the CAS hash of `TEST_ARTIFACT_CONTENT`.
        fn test_artifact_bundle_hash() -> Vec<u8> {
            let cas = MemoryCas::default();
            let result = cas.store(TEST_ARTIFACT_CONTENT).unwrap();
            result.hash.to_vec()
        }

        /// Creates a `PrivilegedDispatcher` backed by real `SQLite`
        /// implementations for testing the production persistence path.
        /// TCK-00408: CAS is now mandatory for ingest (fail-closed).
        fn setup_sqlite_dispatcher() -> (
            PrivilegedDispatcher,
            ConnectionContext,
            Arc<Mutex<Connection>>,
        ) {
            let conn = Connection::open_in_memory().unwrap();
            SqliteLedgerEventEmitter::init_schema_for_test(&conn).unwrap();
            SqliteWorkRegistry::init_schema(&conn).unwrap();
            let conn = Arc::new(Mutex::new(conn));

            let signing_key = ed25519_dalek::SigningKey::generate(&mut rand::rngs::OsRng);
            let policy_resolver = Arc::new(StubPolicyResolver);
            let work_registry = Arc::new(SqliteWorkRegistry::new(Arc::clone(&conn)));
            let event_emitter = Arc::new(SqliteLedgerEventEmitter::new(
                Arc::clone(&conn),
                signing_key.clone(),
            ));
            let lease_validator: Arc<dyn LeaseValidator> = Arc::new(
                SqliteLeaseValidator::new_with_signing_key(Arc::clone(&conn), signing_key),
            );
            let session_registry: Arc<dyn SessionRegistry> =
                Arc::new(InMemorySessionRegistry::new());
            let clock = Arc::new(
                HolonicClock::new(ClockConfig::default(), None).expect("clock creation failed"),
            );
            let token_minter = Arc::new(TokenMinter::new(TokenMinter::generate_secret()));
            let manifest_store = Arc::new(InMemoryManifestStore::new());
            let manifest_loader: Arc<dyn ManifestLoader> =
                Arc::new(InMemoryCasManifestLoader::with_reviewer_v0_manifest());
            let subscription_registry: SharedSubscriptionRegistry =
                Arc::new(SubscriptionRegistry::with_defaults());

            // TCK-00408: CAS is mandatory for ingest (fail-closed).
            let cas = Arc::new(MemoryCas::default());
            cas.store(TEST_ARTIFACT_CONTENT).unwrap();
            let kernel: Arc<dyn apm2_core::pcac::AuthorityJoinKernel> =
                Arc::new(crate::pcac::InProcessKernel::new(1));
            let pcac_gate = Arc::new(crate::pcac::LifecycleGate::new(kernel));

            let mut dispatcher = PrivilegedDispatcher::with_dependencies(
                DecodeConfig::default(),
                policy_resolver,
                work_registry,
                event_emitter,
                Arc::new(EpisodeRuntime::new(EpisodeRuntimeConfig::default())),
                session_registry,
                lease_validator,
                clock,
                token_minter,
                manifest_store,
                manifest_loader,
                subscription_registry,
            );
            dispatcher = dispatcher
                .with_cas(cas as Arc<dyn ContentAddressedStore>)
                .with_pcac_lifecycle_gate(pcac_gate)
                .with_privileged_pcac_policy(crate::protocol::dispatch::PrivilegedPcacPolicy {});

            let ctx = ConnectionContext::privileged_session_open(Some(PeerCredentials {
                uid: 1000,
                gid: 1000,
                pid: Some(12345),
            }));

            (dispatcher, ctx, conn)
        }

        fn setup_dispatcher_state_with_privileged_pcac(
            policy: crate::protocol::dispatch::PrivilegedPcacPolicy,
            remove_privileged_pcac_gate: bool,
        ) -> (
            crate::state::DispatcherState,
            ConnectionContext,
            String,
            tempfile::TempDir,
        ) {
            use std::os::unix::fs::PermissionsExt;

            use crate::cas::{DurableCas, DurableCasConfig};
            use crate::state::DispatcherState;

            let conn = Connection::open_in_memory().expect("sqlite connection");
            SqliteLedgerEventEmitter::init_schema_for_test(&conn).expect("sqlite ledger schema");
            SqliteWorkRegistry::init_schema(&conn).expect("sqlite work schema");
            let conn = Arc::new(Mutex::new(conn));

            let session_registry: Arc<dyn SessionRegistry> =
                Arc::new(InMemorySessionRegistry::new());
            let cas_dir = tempfile::tempdir().expect("tempdir for CAS");
            std::fs::set_permissions(cas_dir.path(), std::fs::Permissions::from_mode(0o700))
                .expect("set CAS directory permissions");
            {
                let cas = DurableCas::new(DurableCasConfig::new(cas_dir.path().to_path_buf()))
                    .expect("initialize CAS");
                cas.store(TEST_ARTIFACT_CONTENT)
                    .expect("store test artifact bundle");
            }

            let mut state = DispatcherState::with_persistence_and_cas(
                session_registry,
                None,
                Arc::clone(&conn),
                cas_dir.path(),
            )
            .expect("state with persistence + CAS");

            let signer = Arc::new(apm2_core::crypto::Signer::generate());
            let orchestrator = Arc::new(crate::gate::GateOrchestrator::new(
                crate::gate::GateOrchestratorConfig::default(),
                Arc::clone(&signer),
            ));
            state = state.with_gate_orchestrator(orchestrator);
            state = state.with_privileged_pcac_policy(policy);

            if remove_privileged_pcac_gate {
                state = state.without_privileged_pcac_lifecycle_gate();
            }

            let peer_creds = PeerCredentials {
                uid: 1000,
                gid: 1000,
                pid: Some(12345),
            };
            let caller_actor = derive_actor_id(&peer_creds);
            let ctx = ConnectionContext::privileged_session_open(Some(peer_creds));
            (state, ctx, caller_actor, cas_dir)
        }

        fn register_test_lease_and_claim(
            state: &crate::state::DispatcherState,
            lease_id: &str,
            work_id: &str,
            gate_id: &str,
            executor_actor_id: &str,
            policy_hash: [u8; 32],
            resolved_risk_tier: u8,
        ) {
            let cas = state
                .privileged_dispatcher()
                .cas
                .as_ref()
                .expect("CAS should be configured by DispatcherState");

            super::ingest_review_receipt::register_full_test_lease(
                &super::ingest_review_receipt::TestLeaseConfig {
                    dispatcher: state.privileged_dispatcher(),
                    cas: cas.as_ref(),
                    lease_id,
                    work_id,
                    gate_id,
                    executor_actor_id,
                    policy_hash,
                    wall_time_source: WallTimeSource::AuthenticatedNts,
                    include_attestation: true,
                },
            );

            let mut policy_resolution = test_policy_resolution_with_lineage(
                work_id,
                executor_actor_id,
                WorkRole::Reviewer,
                resolved_risk_tier,
            );
            policy_resolution.resolved_policy_hash = policy_hash;
            policy_resolution.pcac_policy = Some(apm2_core::pcac::PcacPolicyKnobs::default());
            seed_policy_lineage_for_test(
                cas.as_ref(),
                work_id,
                executor_actor_id,
                WorkRole::Reviewer,
                &policy_resolution,
            );

            state
                .privileged_dispatcher()
                .work_registry
                .register_claim(WorkClaim {
                    work_id: work_id.to_string(),
                    lease_id: lease_id.to_string(),
                    actor_id: executor_actor_id.to_string(),
                    role: WorkRole::Reviewer,
                    policy_resolution,
                    executor_custody_domains: vec![],
                    author_custody_domains: vec![],
                    permeability_receipt: None,
                })
                .expect("work claim registration");
        }

        fn setup_delegate_sublease_sqlite_fixture(
            parent_lease_id: &str,
            work_id: &str,
            gate_id: &str,
        ) -> (
            PrivilegedDispatcher,
            ConnectionContext,
            Arc<Mutex<Connection>>,
            String,
        ) {
            let (dispatcher, ctx, conn) = setup_sqlite_dispatcher();
            let signer = Arc::new(apm2_core::crypto::Signer::generate());
            let orchestrator = Arc::new(crate::gate::GateOrchestrator::new(
                crate::gate::GateOrchestratorConfig::default(),
                signer,
            ));
            let kernel: Arc<dyn apm2_core::pcac::AuthorityJoinKernel> =
                Arc::new(crate::pcac::InProcessKernel::new(1));
            let pcac_gate = Arc::new(crate::pcac::LifecycleGate::new(kernel));
            let caller_actor = derive_actor_id(&PeerCredentials {
                uid: 1000,
                gid: 1000,
                pid: Some(12345),
            });

            let dispatcher = dispatcher
                .with_gate_orchestrator(orchestrator)
                .with_pcac_lifecycle_gate(pcac_gate)
                .with_privileged_pcac_policy(crate::protocol::dispatch::PrivilegedPcacPolicy {});
            let cas = dispatcher
                .cas
                .as_ref()
                .expect("CAS should be configured by sqlite fixture");
            super::ingest_review_receipt::register_full_test_lease(
                &super::ingest_review_receipt::TestLeaseConfig {
                    dispatcher: &dispatcher,
                    cas: cas.as_ref(),
                    lease_id: parent_lease_id,
                    work_id,
                    gate_id,
                    executor_actor_id: &caller_actor,
                    policy_hash: [0xAB; 32],
                    wall_time_source: WallTimeSource::AuthenticatedNts,
                    include_attestation: true,
                },
            );
            let mut policy_resolution =
                test_policy_resolution_with_lineage(work_id, &caller_actor, WorkRole::Reviewer, 0);
            policy_resolution.resolved_policy_hash = [0xAB; 32];
            policy_resolution.pcac_policy = Some(apm2_core::pcac::PcacPolicyKnobs::default());
            seed_policy_lineage_for_test(
                cas.as_ref(),
                work_id,
                &caller_actor,
                WorkRole::Reviewer,
                &policy_resolution,
            );
            dispatcher
                .work_registry
                .register_claim(WorkClaim {
                    work_id: work_id.to_string(),
                    lease_id: parent_lease_id.to_string(),
                    actor_id: caller_actor.clone(),
                    role: WorkRole::Reviewer,
                    policy_resolution,
                    executor_custody_domains: vec![],
                    author_custody_domains: vec![],
                    permeability_receipt: None,
                })
                .expect("sqlite delegate fixture claim registration");
            (dispatcher, ctx, conn, caller_actor)
        }

        fn emit_sublease_lineage_event(
            dispatcher: &PrivilegedDispatcher,
            child_lease_id: &str,
            parent_lease_id: &str,
            actor_id: &str,
            timestamp_ns: u64,
        ) {
            let payload = serde_json::json!({
                "parent_lease_id": parent_lease_id,
                "identity_proof_hash": hex::encode([0xA5; 32]),
            });
            let payload_bytes =
                serde_json::to_vec(&payload).expect("lineage payload should serialize");
            dispatcher
                .event_emitter()
                .emit_session_event(
                    child_lease_id,
                    "SubleaseIssued",
                    &payload_bytes,
                    actor_id,
                    timestamp_ns,
                )
                .expect("lineage seed event should insert");
        }

        #[test]
        fn test_ingest_review_receipt_sqlite_tier0_passes() {
            let (dispatcher, ctx, _conn) = setup_sqlite_dispatcher();

            // v6 Finding 1: Derive executor_actor_id from peer credentials
            // to match what the handler will derive.
            let executor_actor_id = derive_actor_id(&PeerCredentials {
                uid: 1000,
                gid: 1000,
                pid: Some(12345),
            });

            // Register full lease with HTF bindings via SqliteLeaseValidator
            let cas = dispatcher
                .cas
                .as_ref()
                .expect("CAS should be configured by setup helper");
            super::ingest_review_receipt::register_full_test_lease(
                &super::ingest_review_receipt::TestLeaseConfig {
                    dispatcher: &dispatcher,
                    cas: cas.as_ref(),
                    lease_id: "sqlite-lease-001",
                    work_id: "W-SQL-001",
                    gate_id: "gate-sql",
                    executor_actor_id: &executor_actor_id,
                    policy_hash: [0u8; 32],
                    wall_time_source: WallTimeSource::AuthenticatedNts,
                    include_attestation: true,
                },
            );

            // Register work claim via SqliteWorkRegistry (production path)
            let mut policy_resolution = test_policy_resolution_with_lineage(
                "W-SQL-001",
                &executor_actor_id,
                WorkRole::Reviewer,
                0,
            );
            policy_resolution.pcac_policy = Some(apm2_core::pcac::PcacPolicyKnobs::default());
            seed_policy_lineage_for_test(
                cas.as_ref(),
                "W-SQL-001",
                &executor_actor_id,
                WorkRole::Reviewer,
                &policy_resolution,
            );
            let claim = WorkClaim {
                work_id: "W-SQL-001".to_string(),
                lease_id: "sqlite-lease-001".to_string(),
                actor_id: executor_actor_id,
                role: WorkRole::Reviewer,
                policy_resolution,
                executor_custody_domains: vec![],
                author_custody_domains: vec![],
                permeability_receipt: None,
            };
            dispatcher.work_registry.register_claim(claim).unwrap();

            let request = IngestReviewReceiptRequest {
                lease_id: "sqlite-lease-001".to_string(),
                receipt_id: "RR-SQL-001".to_string(),
                reviewer_actor_id: "ignored-by-handler".to_string(),
                changeset_digest: vec![0x42; 32],
                artifact_bundle_hash: test_artifact_bundle_hash(),
                verdict: ReviewReceiptVerdict::Approve.into(),
                blocked_reason_code: 0,
                blocked_log_hash: vec![],
                identity_proof_hash: vec![0x99; 32],
            };
            let frame = encode_ingest_review_receipt_request(&request);

            let response = dispatcher.dispatch(&frame, &ctx).unwrap();
            match response {
                PrivilegedResponse::IngestReviewReceipt(resp) => {
                    assert_eq!(resp.receipt_id, "RR-SQL-001");
                    assert_eq!(
                        resp.event_type, "ReviewReceiptRecorded",
                        "Tier0 with SelfSigned must pass through sqlite path"
                    );
                    assert!(
                        !resp.event_id.is_empty(),
                        "Event ID must be non-empty on sqlite success"
                    );
                },
                PrivilegedResponse::Error(err) => {
                    panic!(
                        "Tier0 SelfSigned should pass on sqlite path, got error: {}",
                        err.message
                    );
                },
                other => panic!("Expected IngestReviewReceipt, got {other:?}"),
            }
        }

        #[test]
        fn test_ingest_review_receipt_sqlite_higher_tier_rejected() {
            let (dispatcher, ctx, _conn) = setup_sqlite_dispatcher();

            // v6 Finding 1: Derive executor_actor_id from peer credentials
            let executor_actor_id = derive_actor_id(&PeerCredentials {
                uid: 1000,
                gid: 1000,
                pid: Some(12345),
            });

            // Register full lease with HTF bindings via SqliteLeaseValidator
            let cas = dispatcher
                .cas
                .as_ref()
                .expect("CAS should be configured by setup helper");
            super::ingest_review_receipt::register_full_test_lease(
                &super::ingest_review_receipt::TestLeaseConfig {
                    dispatcher: &dispatcher,
                    cas: cas.as_ref(),
                    lease_id: "sqlite-lease-t2",
                    work_id: "W-SQL-T2",
                    gate_id: "gate-sql",
                    executor_actor_id: &executor_actor_id,
                    policy_hash: [0u8; 32],
                    wall_time_source: WallTimeSource::AuthenticatedNts,
                    include_attestation: true,
                },
            );

            // Register work claim at Tier2 (should be rejected)
            let mut policy_resolution = test_policy_resolution_with_lineage(
                "W-SQL-T2",
                &executor_actor_id,
                WorkRole::Reviewer,
                2,
            );
            policy_resolution.pcac_policy = Some(apm2_core::pcac::PcacPolicyKnobs::default());
            seed_policy_lineage_for_test(
                cas.as_ref(),
                "W-SQL-T2",
                &executor_actor_id,
                WorkRole::Reviewer,
                &policy_resolution,
            );
            let claim = WorkClaim {
                work_id: "W-SQL-T2".to_string(),
                lease_id: "sqlite-lease-t2".to_string(),
                actor_id: executor_actor_id,
                role: WorkRole::Reviewer,
                policy_resolution,
                executor_custody_domains: vec![],
                author_custody_domains: vec![],
                permeability_receipt: None,
            };
            dispatcher.work_registry.register_claim(claim).unwrap();

            let request = IngestReviewReceiptRequest {
                lease_id: "sqlite-lease-t2".to_string(),
                receipt_id: "RR-SQL-T2".to_string(),
                reviewer_actor_id: "ignored-by-handler".to_string(),
                changeset_digest: vec![0x42; 32],
                artifact_bundle_hash: test_artifact_bundle_hash(),
                verdict: ReviewReceiptVerdict::Approve.into(),
                blocked_reason_code: 0,
                blocked_log_hash: vec![],
                identity_proof_hash: vec![0x99; 32],
            };
            let frame = encode_ingest_review_receipt_request(&request);

            let response = dispatcher.dispatch(&frame, &ctx).unwrap();
            match response {
                PrivilegedResponse::Error(err) => {
                    assert!(
                        err.message.contains("SelfSigned")
                            || err.message.contains("requires")
                            || err.message.contains("attestation"),
                        "Tier2 SelfSigned must be rejected on sqlite path, got: {}",
                        err.message
                    );
                },
                other => {
                    panic!("Expected Tier2 rejection on sqlite path, got {other:?}");
                },
            }
        }

        #[test]
        fn test_delegate_sublease_sqlite_valid_succeeds() {
            let (dispatcher, ctx, _conn) = setup_sqlite_dispatcher();

            let signer = Arc::new(apm2_core::crypto::Signer::generate());
            let orch = Arc::new(crate::gate::GateOrchestrator::new(
                crate::gate::GateOrchestratorConfig::default(),
                Arc::clone(&signer),
            ));
            let kernel: Arc<dyn apm2_core::pcac::AuthorityJoinKernel> =
                Arc::new(crate::pcac::InProcessKernel::new(1));
            let pcac_gate = Arc::new(crate::pcac::LifecycleGate::new(kernel));

            // Derive the actor ID from the test peer credentials (uid=1000,
            // gid=1000) so the caller authorization check passes.
            let caller_actor = derive_actor_id(&PeerCredentials {
                uid: 1000,
                gid: 1000,
                pid: Some(12345),
            });

            // Wire orchestrator and mandatory DelegateSublease PCAC gate.
            let dispatcher = dispatcher
                .with_gate_orchestrator(orch)
                .with_pcac_lifecycle_gate(pcac_gate)
                .with_privileged_pcac_policy(crate::protocol::dispatch::PrivilegedPcacPolicy {});
            let cas = dispatcher
                .cas
                .as_ref()
                .expect("CAS should be configured by setup helper");
            super::ingest_review_receipt::register_full_test_lease(
                &super::ingest_review_receipt::TestLeaseConfig {
                    dispatcher: &dispatcher,
                    cas: cas.as_ref(),
                    lease_id: "sql-parent",
                    work_id: "W-SQL-DS",
                    gate_id: "gate-sql-ds",
                    executor_actor_id: &caller_actor,
                    policy_hash: [0xAB; 32],
                    wall_time_source: WallTimeSource::AuthenticatedNts,
                    include_attestation: true,
                },
            );
            let mut policy_resolution = test_policy_resolution_with_lineage(
                "W-SQL-DS",
                &caller_actor,
                WorkRole::Reviewer,
                0,
            );
            policy_resolution.resolved_policy_hash = [0xAB; 32];
            policy_resolution.pcac_policy = Some(apm2_core::pcac::PcacPolicyKnobs::default());
            seed_policy_lineage_for_test(
                cas.as_ref(),
                "W-SQL-DS",
                &caller_actor,
                WorkRole::Reviewer,
                &policy_resolution,
            );
            dispatcher
                .work_registry
                .register_claim(WorkClaim {
                    work_id: "W-SQL-DS".to_string(),
                    lease_id: "sql-parent".to_string(),
                    actor_id: caller_actor,
                    role: WorkRole::Reviewer,
                    policy_resolution,
                    executor_custody_domains: vec![],
                    author_custody_domains: vec![],
                    permeability_receipt: None,
                })
                .expect("work claim registration should succeed");

            let request = DelegateSubleaseRequest {
                parent_lease_id: "sql-parent".to_string(),
                delegatee_actor_id: "child-sql-exec".to_string(),
                requested_expiry_ns: 1_900_000_000_000,
                sublease_id: "sublease-sql-001".to_string(),
                identity_proof_hash: vec![0x99; 32],
            };
            let frame = encode_delegate_sublease_request(&request);

            let response = dispatcher.dispatch(&frame, &ctx).unwrap();
            match response {
                PrivilegedResponse::DelegateSublease(resp) => {
                    assert_eq!(resp.sublease_id, "sublease-sql-001");
                    assert_eq!(resp.gate_id, "gate-sql-ds");
                    assert!(
                        !resp.event_id.is_empty(),
                        "Event ID must be non-empty on sqlite success"
                    );
                },
                PrivilegedResponse::Error(err) => {
                    panic!(
                        "DelegateSublease should succeed on sqlite path, got error: {}",
                        err.message
                    );
                },
                other => panic!("Expected DelegateSublease, got {other:?}"),
            }
        }

        #[test]
        fn test_delegate_sublease_sqlite_invalid_parent_rejected() {
            let (dispatcher, ctx, _conn) = setup_sqlite_dispatcher();

            let signer = Arc::new(apm2_core::crypto::Signer::generate());
            let orch = Arc::new(crate::gate::GateOrchestrator::new(
                crate::gate::GateOrchestratorConfig::default(),
                signer,
            ));

            let dispatcher = dispatcher.with_gate_orchestrator(orch);

            // Don't register any parent lease — should fail
            let request = DelegateSubleaseRequest {
                parent_lease_id: "nonexistent-parent".to_string(),
                delegatee_actor_id: "child-sql".to_string(),
                requested_expiry_ns: 1_900_000_000_000,
                sublease_id: "sublease-sql-invalid".to_string(),
                identity_proof_hash: vec![0x99; 32],
            };
            let frame = encode_delegate_sublease_request(&request);

            let response = dispatcher.dispatch(&frame, &ctx).unwrap();
            match response {
                PrivilegedResponse::Error(err) => {
                    assert!(
                        err.message.contains("parent gate lease not found"),
                        "Must reject when parent lease not in sqlite, got: {}",
                        err.message
                    );
                },
                other => panic!("Expected parent-not-found rejection, got {other:?}"),
            }
        }

        #[test]
        fn test_delegate_sublease_sqlite_missing_ancestor_lease_row_denied() {
            let parent_lease_id = "sql-parent-missing-ancestor";
            let missing_ancestor_lease_id = "sql-ancestor-missing";
            let (dispatcher, ctx, _conn, caller_actor) = setup_delegate_sublease_sqlite_fixture(
                parent_lease_id,
                "W-SQL-DS-MISSING-ANCESTOR",
                "gate-sql-ds",
            );
            emit_sublease_lineage_event(
                &dispatcher,
                parent_lease_id,
                missing_ancestor_lease_id,
                &caller_actor,
                1_700_000_110,
            );

            let request = DelegateSubleaseRequest {
                parent_lease_id: parent_lease_id.to_string(),
                delegatee_actor_id: "child-sql-missing-ancestor".to_string(),
                requested_expiry_ns: 1_900_000_000_000,
                sublease_id: "sublease-sql-missing-ancestor".to_string(),
                identity_proof_hash: vec![0x44; 32],
            };
            let frame = encode_delegate_sublease_request(&request);
            let response = dispatcher.dispatch(&frame, &ctx).unwrap();

            match response {
                PrivilegedResponse::Error(err) => {
                    assert!(
                        err.message.contains("references ancestor"),
                        "missing ancestor denial must identify broken lineage edge, got: {}",
                        err.message
                    );
                    assert!(
                        err.message
                            .contains("no authoritative gate_lease_issued record"),
                        "missing ancestor denial must explain missing authoritative lease row, got: {}",
                        err.message
                    );
                },
                other => panic!("expected missing-ancestor denial, got {other:?}"),
            }
        }

        #[test]
        fn test_delegate_sublease_sqlite_malformed_gate_lease_payload_denied() {
            let parent_lease_id = "sql-parent-malformed-ancestor";
            let ancestor_lease_id = "sql-ancestor-malformed";
            let (dispatcher, ctx, conn, caller_actor) = setup_delegate_sublease_sqlite_fixture(
                parent_lease_id,
                "W-SQL-DS-MALFORMED-ANCESTOR",
                "gate-sql-ds",
            );
            let cas = dispatcher
                .cas
                .as_ref()
                .expect("CAS should be configured by sqlite fixture");
            super::ingest_review_receipt::register_full_test_lease(
                &super::ingest_review_receipt::TestLeaseConfig {
                    dispatcher: &dispatcher,
                    cas: cas.as_ref(),
                    lease_id: ancestor_lease_id,
                    work_id: "W-SQL-DS-MALFORMED-ROOT",
                    gate_id: "gate-sql-ds",
                    executor_actor_id: &caller_actor,
                    policy_hash: [0xAB; 32],
                    wall_time_source: WallTimeSource::AuthenticatedNts,
                    include_attestation: true,
                },
            );
            emit_sublease_lineage_event(
                &dispatcher,
                parent_lease_id,
                ancestor_lease_id,
                &caller_actor,
                1_700_000_120,
            );

            {
                let conn = conn
                    .lock()
                    .expect("sqlite connection lock should be available");
                let payload_bytes: Vec<u8> = conn
                    .query_row(
                        "SELECT payload FROM ledger_events \
                         WHERE event_type = 'gate_lease_issued' \
                         AND json_extract(CAST(payload AS TEXT), '$.lease_id') = ?1 \
                         AND json_extract(CAST(payload AS TEXT), '$.full_lease') IS NOT NULL \
                         ORDER BY rowid DESC LIMIT 1",
                        rusqlite::params![ancestor_lease_id],
                        |row| row.get(0),
                    )
                    .expect("ancestor gate_lease_issued payload should exist");
                let mut payload_value: serde_json::Value = serde_json::from_slice(&payload_bytes)
                    .expect("ancestor payload should deserialize");
                payload_value["delegated_parent_lease_id"] = serde_json::json!(1337);
                let malformed_payload = serde_json::to_vec(&payload_value)
                    .expect("malformed payload fixture should serialize");
                conn.execute(
                    "UPDATE ledger_events \
                     SET payload = ?1 \
                     WHERE event_type = 'gate_lease_issued' \
                     AND json_extract(CAST(payload AS TEXT), '$.lease_id') = ?2 \
                     AND json_extract(CAST(payload AS TEXT), '$.full_lease') IS NOT NULL",
                    rusqlite::params![malformed_payload, ancestor_lease_id],
                )
                .expect("ancestor payload mutation should succeed");
            }

            let request = DelegateSubleaseRequest {
                parent_lease_id: parent_lease_id.to_string(),
                delegatee_actor_id: "child-sql-malformed-ancestor".to_string(),
                requested_expiry_ns: 1_900_000_000_000,
                sublease_id: "sublease-sql-malformed-ancestor".to_string(),
                identity_proof_hash: vec![0x45; 32],
            };
            let frame = encode_delegate_sublease_request(&request);
            let response = dispatcher.dispatch(&frame, &ctx).unwrap();

            match response {
                PrivilegedResponse::Error(err) => {
                    assert!(
                        err.message.contains("lineage metadata lookup failed"),
                        "malformed payload denial must indicate metadata lookup failure, got: {}",
                        err.message
                    );
                    assert!(
                        err.message.contains("non-string delegated_parent_lease_id"),
                        "malformed payload denial must identify parse/type defect, got: {}",
                        err.message
                    );
                },
                other => panic!("expected malformed lineage metadata denial, got {other:?}"),
            }
        }

        #[test]
        fn test_delegate_sublease_sqlite_valid_ancestor_chain_succeeds() {
            let parent_lease_id = "sql-parent-valid-ancestor-chain";
            let ancestor_lease_id = "sql-ancestor-root";
            let (dispatcher, ctx, _conn, caller_actor) = setup_delegate_sublease_sqlite_fixture(
                parent_lease_id,
                "W-SQL-DS-VALID-ANCESTOR",
                "gate-sql-ds",
            );
            let cas = dispatcher
                .cas
                .as_ref()
                .expect("CAS should be configured by sqlite fixture");
            super::ingest_review_receipt::register_full_test_lease(
                &super::ingest_review_receipt::TestLeaseConfig {
                    dispatcher: &dispatcher,
                    cas: cas.as_ref(),
                    lease_id: ancestor_lease_id,
                    work_id: "W-SQL-DS-VALID-ANCESTOR-ROOT",
                    gate_id: "gate-sql-ds",
                    executor_actor_id: &caller_actor,
                    policy_hash: [0xAB; 32],
                    wall_time_source: WallTimeSource::AuthenticatedNts,
                    include_attestation: true,
                },
            );
            emit_sublease_lineage_event(
                &dispatcher,
                parent_lease_id,
                ancestor_lease_id,
                &caller_actor,
                1_700_000_130,
            );

            let request = DelegateSubleaseRequest {
                parent_lease_id: parent_lease_id.to_string(),
                delegatee_actor_id: "child-sql-valid-ancestor".to_string(),
                requested_expiry_ns: 1_900_000_000_000,
                sublease_id: "sublease-sql-valid-ancestor".to_string(),
                identity_proof_hash: vec![0x46; 32],
            };
            let frame = encode_delegate_sublease_request(&request);
            let response = dispatcher.dispatch(&frame, &ctx).unwrap();

            match response {
                PrivilegedResponse::DelegateSublease(resp) => {
                    assert_eq!(resp.sublease_id, "sublease-sql-valid-ancestor");
                    assert_eq!(resp.parent_lease_id, parent_lease_id);
                    assert_eq!(resp.gate_id, "gate-sql-ds");
                    assert!(
                        !resp.event_id.is_empty(),
                        "valid lineage chain response must carry event_id"
                    );
                },
                PrivilegedResponse::Error(err) => {
                    panic!(
                        "valid lineage chain should pass sqlite delegation checks, got: {}",
                        err.message
                    );
                },
                other => panic!("expected DelegateSublease success, got {other:?}"),
            }
        }

        /// Quality MAJOR: Integration test using `DispatcherState` production
        /// wiring to exercise `DelegateSublease` and `IngestReviewReceipt`
        /// through the production composition path. This proves that
        /// `DispatcherState` properly wires `gate_orchestrator` into the
        /// privileged dispatcher.
        #[test]
        fn test_dispatcher_state_delegate_sublease_production_wiring() {
            use std::os::unix::fs::PermissionsExt;

            use crate::cas::{DurableCas, DurableCasConfig};
            use crate::state::DispatcherState;

            let conn = Connection::open_in_memory().unwrap();
            SqliteLedgerEventEmitter::init_schema_for_test(&conn).unwrap();
            SqliteWorkRegistry::init_schema(&conn).unwrap();
            let conn = Arc::new(Mutex::new(conn));

            let session_registry: Arc<dyn SessionRegistry> =
                Arc::new(InMemorySessionRegistry::new());

            // TCK-00408: Use with_persistence_and_cas so CAS is wired —
            // IngestReviewReceipt now requires CAS (fail-closed).
            let cas_dir = tempfile::tempdir().expect("tempdir for CAS");
            std::fs::set_permissions(cas_dir.path(), std::fs::Permissions::from_mode(0o700))
                .expect("set CAS dir permissions");
            // Pre-populate CAS with test artifact before state construction.
            {
                let cas = DurableCas::new(DurableCasConfig::new(cas_dir.path().to_path_buf()))
                    .expect("pre-populate CAS");
                cas.store(TEST_ARTIFACT_CONTENT)
                    .expect("store test artifact");
            }
            let state = DispatcherState::with_persistence_and_cas(
                session_registry,
                None, // no metrics
                Arc::clone(&conn),
                cas_dir.path(),
            )
            .expect("CAS initialization must succeed");

            // Wire gate orchestrator via production path
            let signer = Arc::new(apm2_core::crypto::Signer::generate());
            let orch = Arc::new(crate::gate::GateOrchestrator::new(
                crate::gate::GateOrchestratorConfig::default(),
                Arc::clone(&signer),
            ));
            let state = state.with_gate_orchestrator(orch);

            // Derive caller actor from test peer credentials
            let test_creds = PeerCredentials {
                uid: 1000,
                gid: 1000,
                pid: Some(12345),
            };
            let caller_actor = derive_actor_id(&test_creds);

            // Register parent lease in the sqlite-backed dispatcher
            let cas = state
                .privileged_dispatcher()
                .cas
                .as_ref()
                .expect("CAS should be configured by DispatcherState");
            super::ingest_review_receipt::register_full_test_lease(
                &super::ingest_review_receipt::TestLeaseConfig {
                    dispatcher: state.privileged_dispatcher(),
                    cas: cas.as_ref(),
                    lease_id: "ds-parent-prod",
                    work_id: "W-PROD-001",
                    gate_id: "gate-prod",
                    executor_actor_id: &caller_actor,
                    policy_hash: [0xAB; 32],
                    wall_time_source: WallTimeSource::AuthenticatedNts,
                    include_attestation: true,
                },
            );
            let mut ds_policy_resolution = test_policy_resolution_with_lineage(
                "W-PROD-001",
                &caller_actor,
                WorkRole::Reviewer,
                0,
            );
            ds_policy_resolution.resolved_policy_hash = [0xAB; 32];
            ds_policy_resolution.pcac_policy = Some(apm2_core::pcac::PcacPolicyKnobs::default());
            seed_policy_lineage_for_test(
                cas.as_ref(),
                "W-PROD-001",
                &caller_actor,
                WorkRole::Reviewer,
                &ds_policy_resolution,
            );
            state
                .privileged_dispatcher()
                .work_registry
                .register_claim(WorkClaim {
                    work_id: "W-PROD-001".to_string(),
                    lease_id: "ds-parent-prod".to_string(),
                    actor_id: caller_actor.clone(),
                    role: WorkRole::Reviewer,
                    policy_resolution: ds_policy_resolution,
                    executor_custody_domains: vec![],
                    author_custody_domains: vec![],
                    permeability_receipt: None,
                })
                .expect("delegate work claim registration should succeed");

            let ctx = ConnectionContext::privileged_session_open(Some(test_creds));

            // Exercise DelegateSublease through production-wired dispatcher
            let request = DelegateSubleaseRequest {
                parent_lease_id: "ds-parent-prod".to_string(),
                delegatee_actor_id: "child-prod-exec".to_string(),
                requested_expiry_ns: 1_900_000_000_000,
                sublease_id: "sublease-prod-001".to_string(),
                identity_proof_hash: vec![0x99; 32],
            };
            let frame = encode_delegate_sublease_request(&request);

            let response = state
                .privileged_dispatcher()
                .dispatch(&frame, &ctx)
                .unwrap();
            match response {
                PrivilegedResponse::DelegateSublease(resp) => {
                    assert_eq!(resp.sublease_id, "sublease-prod-001");
                    assert_eq!(resp.gate_id, "gate-prod");
                    assert_eq!(resp.delegatee_actor_id, "child-prod-exec");
                    assert!(
                        !resp.event_id.is_empty(),
                        "Event ID must be non-empty in production wiring path"
                    );
                },
                PrivilegedResponse::Error(err) => {
                    panic!(
                        "DelegateSublease must succeed via DispatcherState wiring, got: {}",
                        err.message
                    );
                },
                other => panic!("Expected DelegateSublease via production path, got {other:?}"),
            }

            // Also exercise IngestReviewReceipt through production wiring
            // to verify get_lease_work_id works with SqliteLeaseValidator.
            //
            // v6 Finding 1: The lease executor must be the derived actor_id
            // from peer credentials, since the handler now authenticates the
            // reviewer identity via peer credentials (not the request field).
            super::ingest_review_receipt::register_full_test_lease(
                &super::ingest_review_receipt::TestLeaseConfig {
                    dispatcher: state.privileged_dispatcher(),
                    cas: cas.as_ref(),
                    lease_id: "review-lease-prod",
                    work_id: "W-REVIEW-001",
                    gate_id: "gate-review",
                    executor_actor_id: &caller_actor,
                    policy_hash: [0u8; 32],
                    wall_time_source: WallTimeSource::AuthenticatedNts,
                    include_attestation: true,
                },
            );

            let mut policy_resolution = test_policy_resolution_with_lineage(
                "W-REVIEW-001",
                &caller_actor,
                WorkRole::Reviewer,
                0,
            );
            policy_resolution.pcac_policy = Some(apm2_core::pcac::PcacPolicyKnobs::default());
            seed_policy_lineage_for_test(
                cas.as_ref(),
                "W-REVIEW-001",
                &caller_actor,
                WorkRole::Reviewer,
                &policy_resolution,
            );
            let claim = WorkClaim {
                work_id: "W-REVIEW-001".to_string(),
                lease_id: "review-lease-prod".to_string(),
                actor_id: caller_actor,
                role: WorkRole::Reviewer,
                policy_resolution,
                executor_custody_domains: vec![],
                author_custody_domains: vec![],
                permeability_receipt: None,
            };
            state
                .privileged_dispatcher()
                .work_registry
                .register_claim(claim)
                .unwrap();

            let review_request = IngestReviewReceiptRequest {
                lease_id: "review-lease-prod".to_string(),
                receipt_id: "RR-PROD-001".to_string(),
                reviewer_actor_id: "reviewer-prod".to_string(),
                changeset_digest: vec![0x42; 32],
                artifact_bundle_hash: test_artifact_bundle_hash(),
                verdict: ReviewReceiptVerdict::Approve.into(),
                blocked_reason_code: 0,
                blocked_log_hash: vec![],
                identity_proof_hash: vec![0x99; 32],
            };
            let review_frame = encode_ingest_review_receipt_request(&review_request);

            let review_response = state
                .privileged_dispatcher()
                .dispatch(&review_frame, &ctx)
                .unwrap();
            match review_response {
                PrivilegedResponse::IngestReviewReceipt(resp) => {
                    assert_eq!(resp.receipt_id, "RR-PROD-001");
                    assert_eq!(
                        resp.event_type, "ReviewReceiptRecorded",
                        "Tier0 SelfSigned must pass through DispatcherState production path"
                    );
                    assert!(
                        !resp.event_id.is_empty(),
                        "Event ID must be non-empty on production path success"
                    );
                },
                PrivilegedResponse::Error(err) => {
                    panic!(
                        "IngestReviewReceipt must succeed via DispatcherState wiring, got: {}",
                        err.message
                    );
                },
                other => panic!("Expected IngestReviewReceipt via production path, got {other:?}"),
            }
        }

        #[test]
        fn test_delegate_sublease_privileged_pcac_lifecycle_enabled_succeeds() {
            let policy = crate::protocol::dispatch::PrivilegedPcacPolicy {};
            let (state, ctx, caller_actor, _cas_dir) =
                setup_dispatcher_state_with_privileged_pcac(policy, false);

            register_test_lease_and_claim(
                &state,
                "pcac-parent-ok",
                "W-PCAC-DS-OK",
                "gate-pcac-ok",
                &caller_actor,
                [0xAB; 32],
                0,
            );

            let request = DelegateSubleaseRequest {
                parent_lease_id: "pcac-parent-ok".to_string(),
                delegatee_actor_id: "pcac-child-ok".to_string(),
                requested_expiry_ns: 1_900_000_000_000,
                sublease_id: "pcac-sublease-ok".to_string(),
                identity_proof_hash: vec![0x44; 32],
            };
            let frame = encode_delegate_sublease_request(&request);
            let response = state
                .privileged_dispatcher()
                .dispatch(&frame, &ctx)
                .unwrap();

            match response {
                PrivilegedResponse::DelegateSublease(resp) => {
                    assert_eq!(resp.sublease_id, "pcac-sublease-ok");
                    assert!(
                        !resp.event_id.is_empty(),
                        "DelegateSublease must return non-empty event_id"
                    );
                },
                other => panic!("expected DelegateSublease success, got {other:?}"),
            }

            let persisted_sublease = state
                .privileged_dispatcher()
                .lease_validator()
                .get_gate_lease("pcac-sublease-ok");
            assert!(
                persisted_sublease.is_some(),
                "sublease must persist after successful delegation"
            );

            let sublease_events = state
                .privileged_dispatcher()
                .event_emitter()
                .get_events_by_work_id("pcac-sublease-ok");
            let sublease_event_count = sublease_events
                .iter()
                .filter(|event| event.event_type == "SubleaseIssued")
                .count();
            assert_eq!(
                sublease_event_count, 1,
                "exactly one SubleaseIssued event must be emitted"
            );

            let sublease_event = sublease_events
                .iter()
                .find(|event| event.event_type == "SubleaseIssued")
                .expect("SubleaseIssued event must be present");
            let payload = super::ingest_review_receipt::decode_wrapped_or_direct_event_payload(
                sublease_event,
            );
            assert!(
                payload.get("ajc_id").is_some(),
                "SubleaseIssued payload must include ajc_id lifecycle selector"
            );
            assert!(
                payload.get("intent_digest").is_some(),
                "SubleaseIssued payload must include intent_digest lifecycle selector"
            );
            assert!(
                payload.get("consume_tick").is_some(),
                "SubleaseIssued payload must include consume_tick lifecycle selector"
            );
            assert!(
                payload.get("pcac_time_envelope_ref").is_some(),
                "SubleaseIssued payload must include pcac_time_envelope_ref lifecycle selector"
            );
            assert!(
                payload.get("consume_selector_digest").is_some(),
                "SubleaseIssued payload must include consume_selector_digest lifecycle selector"
            );
        }

        #[test]
        fn test_ingest_review_receipt_privileged_pcac_lifecycle_enabled_succeeds() {
            let policy = crate::protocol::dispatch::PrivilegedPcacPolicy {};
            let (state, ctx, caller_actor, _cas_dir) =
                setup_dispatcher_state_with_privileged_pcac(policy, false);

            register_test_lease_and_claim(
                &state,
                "pcac-review-lease-ok",
                "W-PCAC-RR-OK",
                "gate-pcac-review",
                &caller_actor,
                [0xBC; 32],
                0,
            );

            let request = IngestReviewReceiptRequest {
                lease_id: "pcac-review-lease-ok".to_string(),
                receipt_id: "RR-PCAC-OK".to_string(),
                reviewer_actor_id: caller_actor,
                changeset_digest: vec![0x42; 32],
                artifact_bundle_hash: test_artifact_bundle_hash(),
                verdict: ReviewReceiptVerdict::Approve.into(),
                blocked_reason_code: 0,
                blocked_log_hash: vec![],
                identity_proof_hash: vec![0x55; 32],
            };
            let frame = encode_ingest_review_receipt_request(&request);
            let response = state
                .privileged_dispatcher()
                .dispatch(&frame, &ctx)
                .unwrap();

            match response {
                PrivilegedResponse::IngestReviewReceipt(resp) => {
                    assert_eq!(resp.receipt_id, "RR-PCAC-OK");
                    assert_eq!(resp.event_type, "ReviewReceiptRecorded");
                    assert!(
                        !resp.event_id.is_empty(),
                        "IngestReviewReceipt must return non-empty event_id"
                    );
                },
                other => panic!("expected IngestReviewReceipt success, got {other:?}"),
            }

            let review_events = state
                .privileged_dispatcher()
                .event_emitter()
                .get_events_by_work_id("W-PCAC-RR-OK");
            let review_event_count = review_events
                .iter()
                .filter(|event| event.event_type == "review_receipt_recorded")
                .count();
            assert_eq!(
                review_event_count, 1,
                "exactly one review_receipt_recorded event must be emitted"
            );

            let review_event = review_events
                .iter()
                .find(|event| event.event_type == "review_receipt_recorded")
                .expect("review_receipt_recorded event must be present");
            let payload =
                super::ingest_review_receipt::decode_wrapped_or_direct_event_payload(review_event);
            assert!(
                payload.get("ajc_id").is_some(),
                "review_receipt_recorded payload must include ajc_id lifecycle selector"
            );
            assert!(
                payload.get("intent_digest").is_some(),
                "review_receipt_recorded payload must include intent_digest lifecycle selector"
            );
            assert!(
                payload.get("consume_tick").is_some(),
                "review_receipt_recorded payload must include consume_tick lifecycle selector"
            );
            assert!(
                payload.get("pcac_time_envelope_ref").is_some(),
                "review_receipt_recorded payload must include pcac_time_envelope_ref lifecycle selector"
            );
            assert!(
                payload.get("consume_selector_digest").is_some(),
                "review_receipt_recorded payload must include consume_selector_digest lifecycle selector"
            );
        }

        #[test]
        fn test_ingest_review_blocked_privileged_pcac_lifecycle_enabled_succeeds() {
            let policy = crate::protocol::dispatch::PrivilegedPcacPolicy {};
            let (state, ctx, caller_actor, _cas_dir) =
                setup_dispatcher_state_with_privileged_pcac(policy, false);

            register_test_lease_and_claim(
                &state,
                "pcac-review-lease-blocked-ok",
                "W-PCAC-RR-BLOCKED-OK",
                "gate-pcac-review-blocked",
                &caller_actor,
                [0xBE; 32],
                0,
            );

            let request = IngestReviewReceiptRequest {
                lease_id: "pcac-review-lease-blocked-ok".to_string(),
                receipt_id: "RR-PCAC-BLOCKED-OK".to_string(),
                reviewer_actor_id: caller_actor,
                changeset_digest: vec![0x42; 32],
                artifact_bundle_hash: test_artifact_bundle_hash(),
                verdict: ReviewReceiptVerdict::Blocked.into(),
                blocked_reason_code: 73,
                blocked_log_hash: vec![0x91; 32],
                identity_proof_hash: vec![0x57; 32],
            };
            let frame = encode_ingest_review_receipt_request(&request);
            let response = state
                .privileged_dispatcher()
                .dispatch(&frame, &ctx)
                .unwrap();

            match response {
                PrivilegedResponse::IngestReviewReceipt(resp) => {
                    assert_eq!(resp.receipt_id, "RR-PCAC-BLOCKED-OK");
                    assert_eq!(resp.event_type, "ReviewBlockedRecorded");
                    assert!(
                        !resp.event_id.is_empty(),
                        "IngestReviewReceipt blocked verdict must return non-empty event_id"
                    );
                },
                other => panic!("expected IngestReviewReceipt blocked success, got {other:?}"),
            }

            let blocked_events = state
                .privileged_dispatcher()
                .event_emitter()
                .get_events_by_work_id("W-PCAC-RR-BLOCKED-OK");
            let blocked_event_count = blocked_events
                .iter()
                .filter(|event| event.event_type == "review_blocked_recorded")
                .count();
            assert_eq!(
                blocked_event_count, 1,
                "exactly one review_blocked_recorded event must be emitted"
            );

            let blocked_event = blocked_events
                .iter()
                .find(|event| event.event_type == "review_blocked_recorded")
                .expect("review_blocked_recorded event must be present");
            let payload =
                super::ingest_review_receipt::decode_wrapped_or_direct_event_payload(blocked_event);
            assert!(
                payload.get("ajc_id").is_some(),
                "review_blocked_recorded payload must include ajc_id lifecycle selector"
            );
            assert!(
                payload.get("intent_digest").is_some(),
                "review_blocked_recorded payload must include intent_digest lifecycle selector"
            );
            assert!(
                payload.get("consume_tick").is_some(),
                "review_blocked_recorded payload must include consume_tick lifecycle selector"
            );
            assert!(
                payload.get("pcac_time_envelope_ref").is_some(),
                "review_blocked_recorded payload must include pcac_time_envelope_ref lifecycle selector"
            );
            assert!(
                payload.get("consume_selector_digest").is_some(),
                "review_blocked_recorded payload must include consume_selector_digest lifecycle selector"
            );
        }

        #[test]
        fn test_delegate_sublease_privileged_pcac_tier2_pointer_only_denied() {
            let policy = crate::protocol::dispatch::PrivilegedPcacPolicy {};
            let (state, ctx, caller_actor, _cas_dir) =
                setup_dispatcher_state_with_privileged_pcac(policy, false);

            register_test_lease_and_claim(
                &state,
                "pcac-parent-tier2-pointer-only",
                "W-PCAC-DS-T2-PO",
                "gate-pcac-tier2-pointer-only",
                &caller_actor,
                [0xC1; 32],
                2,
            );

            let request = DelegateSubleaseRequest {
                parent_lease_id: "pcac-parent-tier2-pointer-only".to_string(),
                delegatee_actor_id: "pcac-child-tier2-pointer-only".to_string(),
                requested_expiry_ns: 1_900_000_000_000,
                sublease_id: "pcac-sublease-tier2-pointer-only".to_string(),
                identity_proof_hash: vec![0x66; 32],
            };
            let frame = encode_delegate_sublease_request(&request);
            let response = state
                .privileged_dispatcher()
                .dispatch(&frame, &ctx)
                .unwrap();

            match response {
                PrivilegedResponse::Error(err) => {
                    assert!(
                        err.message
                            .contains("pointer-only identity denied at Tier2+")
                            || err.message.contains("waiver expired or invalid"),
                        "Tier2 DelegateSublease must deny pointer-only identity evidence, got: {}",
                        err.message
                    );
                },
                other => panic!("expected Tier2 pointer-only denial, got {other:?}"),
            }

            let persisted_sublease = state
                .privileged_dispatcher()
                .lease_validator()
                .get_gate_lease("pcac-sublease-tier2-pointer-only");
            assert!(
                persisted_sublease.is_none(),
                "Tier2 pointer-only denial must not persist a sublease"
            );
        }

        #[test]
        fn test_ingest_review_receipt_privileged_pcac_join_uses_pointer_only_evidence() {
            use apm2_core::pcac::{
                AuthorityConsumeRecordV1, AuthorityConsumedV1, AuthorityDenyV1,
                AuthorityJoinCertificateV1, AuthorityJoinInputV1, AuthorityJoinKernel,
            };

            struct CapturingKernel {
                inner: crate::pcac::InProcessKernel,
                observed_levels: Arc<Mutex<Vec<IdentityEvidenceLevel>>>,
            }

            impl CapturingKernel {
                fn new(observed_levels: Arc<Mutex<Vec<IdentityEvidenceLevel>>>) -> Self {
                    Self {
                        inner: crate::pcac::InProcessKernel::new(1),
                        observed_levels,
                    }
                }
            }

            impl AuthorityJoinKernel for CapturingKernel {
                fn join(
                    &self,
                    input: &AuthorityJoinInputV1,
                    policy: &apm2_core::pcac::PcacPolicyKnobs,
                ) -> Result<AuthorityJoinCertificateV1, Box<AuthorityDenyV1>> {
                    self.observed_levels
                        .lock()
                        .expect("lock poisoned")
                        .push(input.identity_evidence_level);
                    AuthorityJoinKernel::join(&self.inner, input, policy)
                }

                fn revalidate(
                    &self,
                    cert: &AuthorityJoinCertificateV1,
                    current_time_envelope_ref: [u8; 32],
                    current_ledger_anchor: [u8; 32],
                    current_revocation_head_hash: [u8; 32],
                    policy: &apm2_core::pcac::PcacPolicyKnobs,
                ) -> Result<(), Box<AuthorityDenyV1>> {
                    AuthorityJoinKernel::revalidate(
                        &self.inner,
                        cert,
                        current_time_envelope_ref,
                        current_ledger_anchor,
                        current_revocation_head_hash,
                        policy,
                    )
                }

                fn consume(
                    &self,
                    cert: &AuthorityJoinCertificateV1,
                    intent_digest: [u8; 32],
                    boundary_intent_class: apm2_core::pcac::BoundaryIntentClass,
                    requires_authoritative_acceptance: bool,
                    current_time_envelope_ref: [u8; 32],
                    current_revocation_head_hash: [u8; 32],
                    policy: &apm2_core::pcac::PcacPolicyKnobs,
                ) -> Result<(AuthorityConsumedV1, AuthorityConsumeRecordV1), Box<AuthorityDenyV1>>
                {
                    AuthorityJoinKernel::consume(
                        &self.inner,
                        cert,
                        intent_digest,
                        boundary_intent_class,
                        requires_authoritative_acceptance,
                        current_time_envelope_ref,
                        current_revocation_head_hash,
                        policy,
                    )
                }
            }

            let (dispatcher, ctx, _conn) = setup_sqlite_dispatcher();

            let peer_creds = PeerCredentials {
                uid: 1000,
                gid: 1000,
                pid: Some(12345),
            };
            let caller_actor = derive_actor_id(&peer_creds);

            let cas = dispatcher
                .cas
                .as_ref()
                .expect("CAS should be configured by setup helper");
            super::ingest_review_receipt::register_full_test_lease(
                &super::ingest_review_receipt::TestLeaseConfig {
                    dispatcher: &dispatcher,
                    cas: cas.as_ref(),
                    lease_id: "pcac-review-pointer-only-observe",
                    work_id: "W-PCAC-RR-PO-OBSERVE",
                    gate_id: "gate-pcac-review-pointer-only-observe",
                    executor_actor_id: &caller_actor,
                    policy_hash: [0xC2; 32],
                    wall_time_source: WallTimeSource::AuthenticatedNts,
                    include_attestation: true,
                },
            );
            let mut policy_resolution = test_policy_resolution_with_lineage(
                "W-PCAC-RR-PO-OBSERVE",
                &caller_actor,
                WorkRole::Reviewer,
                0,
            );
            policy_resolution.resolved_policy_hash = [0xC2; 32];
            policy_resolution.pcac_policy = Some(apm2_core::pcac::PcacPolicyKnobs::default());
            seed_policy_lineage_for_test(
                cas.as_ref(),
                "W-PCAC-RR-PO-OBSERVE",
                &caller_actor,
                WorkRole::Reviewer,
                &policy_resolution,
            );
            dispatcher
                .work_registry
                .register_claim(WorkClaim {
                    work_id: "W-PCAC-RR-PO-OBSERVE".to_string(),
                    lease_id: "pcac-review-pointer-only-observe".to_string(),
                    actor_id: caller_actor.clone(),
                    role: WorkRole::Reviewer,
                    policy_resolution,
                    executor_custody_domains: vec![],
                    author_custody_domains: vec![],
                    permeability_receipt: None,
                })
                .expect("work claim registration");

            let observed_levels = Arc::new(Mutex::new(Vec::new()));
            let kernel: Arc<dyn AuthorityJoinKernel> =
                Arc::new(CapturingKernel::new(Arc::clone(&observed_levels)));
            let gate = Arc::new(crate::pcac::LifecycleGate::new(kernel));

            let dispatcher = dispatcher
                .with_pcac_lifecycle_gate(gate)
                .with_privileged_pcac_policy(crate::protocol::dispatch::PrivilegedPcacPolicy {});

            let request = IngestReviewReceiptRequest {
                lease_id: "pcac-review-pointer-only-observe".to_string(),
                receipt_id: "RR-PCAC-PO-OBSERVE".to_string(),
                reviewer_actor_id: caller_actor,
                changeset_digest: vec![0x42; 32],
                artifact_bundle_hash: test_artifact_bundle_hash(),
                verdict: ReviewReceiptVerdict::Approve.into(),
                blocked_reason_code: 0,
                blocked_log_hash: vec![],
                identity_proof_hash: vec![0x55; 32],
            };
            let frame = encode_ingest_review_receipt_request(&request);
            let response = dispatcher.dispatch(&frame, &ctx).unwrap();
            assert!(
                matches!(response, PrivilegedResponse::IngestReviewReceipt(_)),
                "Tier0 ingest with policy-enabled PCAC should succeed"
            );

            let observed = observed_levels.lock().expect("lock poisoned");
            assert!(
                observed.contains(&IdentityEvidenceLevel::PointerOnly),
                "IngestReviewReceipt must construct join input with PointerOnly identity evidence (WVR-0103 active)"
            );
        }

        #[test]
        fn test_delegate_sublease_privileged_pcac_deny_delegation_widening() {
            let policy = crate::protocol::dispatch::PrivilegedPcacPolicy {};
            let (state, ctx, caller_actor, _cas_dir) =
                setup_dispatcher_state_with_privileged_pcac(policy, false);
            let cas = state
                .privileged_dispatcher()
                .cas
                .as_ref()
                .expect("CAS should be configured by DispatcherState");
            super::ingest_review_receipt::register_full_test_lease(
                &super::ingest_review_receipt::TestLeaseConfig {
                    dispatcher: state.privileged_dispatcher(),
                    cas: cas.as_ref(),
                    lease_id: "pcac-parent-wide",
                    work_id: "W-PCAC-DS-WIDE",
                    gate_id: "gate-pcac-wide",
                    executor_actor_id: &caller_actor,
                    policy_hash: [0xCD; 32],
                    wall_time_source: WallTimeSource::AuthenticatedNts,
                    include_attestation: true,
                },
            );

            // Bind the Tier2+ pointer-only waiver scope to a narrower expiry.
            let expected_scope_hash = {
                let expected_expiry_millis = 1_900_000_u64;
                let mut hasher = blake3::Hasher::new();
                hasher.update(b"pcac-privileged-delegate-sublease-scope-v1");
                hasher.update(b"pcac-parent-wide");
                hasher.update(b"pcac-sublease-wide");
                hasher.update(b"pcac-child-wide");
                hasher.update(&expected_expiry_millis.to_le_bytes());
                hasher.update(b"W-PCAC-DS-WIDE");
                hasher.update(b"gate-pcac-wide");
                *hasher.finalize().as_bytes()
            };
            let waiver = apm2_core::pcac::PointerOnlyWaiver {
                waiver_id: "WVR-PCAC-DS-WIDE".to_string(),
                expires_at_tick: u64::MAX,
                scope_binding_hash: expected_scope_hash,
            };
            let pcac_policy = apm2_core::pcac::PcacPolicyKnobs {
                pointer_only_waiver: Some(waiver.clone()),
                ..apm2_core::pcac::PcacPolicyKnobs::default()
            };

            let mut policy_resolution = test_policy_resolution_with_lineage(
                "W-PCAC-DS-WIDE",
                &caller_actor,
                WorkRole::Reviewer,
                2,
            );
            policy_resolution.resolved_policy_hash = [0xCD; 32];
            policy_resolution.pointer_only_waiver = Some(waiver);
            policy_resolution.pcac_policy = Some(pcac_policy);
            seed_policy_lineage_for_test(
                cas.as_ref(),
                "W-PCAC-DS-WIDE",
                &caller_actor,
                WorkRole::Reviewer,
                &policy_resolution,
            );
            state
                .privileged_dispatcher()
                .work_registry
                .register_claim(WorkClaim {
                    work_id: "W-PCAC-DS-WIDE".to_string(),
                    lease_id: "pcac-parent-wide".to_string(),
                    actor_id: caller_actor,
                    role: WorkRole::Reviewer,
                    policy_resolution,
                    executor_custody_domains: vec![],
                    author_custody_domains: vec![],
                    permeability_receipt: None,
                })
                .expect("work claim registration for widening denial");

            // Parent expires_at is 2_000_000ms in the test lease helper.
            // Request widening beyond parent + waiver-scoped expiry.
            let request = DelegateSubleaseRequest {
                parent_lease_id: "pcac-parent-wide".to_string(),
                delegatee_actor_id: "pcac-child-wide".to_string(),
                requested_expiry_ns: 2_100_000_000_000,
                sublease_id: "pcac-sublease-wide".to_string(),
                identity_proof_hash: vec![0x66; 32],
            };
            let frame = encode_delegate_sublease_request(&request);
            let response = state
                .privileged_dispatcher()
                .dispatch(&frame, &ctx)
                .unwrap();

            match response {
                PrivilegedResponse::Error(err) => {
                    assert!(
                        err.message.contains("WaiverScopeInvalid")
                            || err.message.contains("waiver")
                            || err.message.contains("strict expiry narrowing violated")
                            || err
                                .message
                                .contains("PCAC authority denied for DelegateSublease"),
                        "widening denial must fail closed, got: {}",
                        err.message
                    );
                },
                other => panic!("expected widening denial, got {other:?}"),
            }

            let persisted_sublease = state
                .privileged_dispatcher()
                .lease_validator()
                .get_gate_lease("pcac-sublease-wide");
            assert!(
                persisted_sublease.is_none(),
                "widening denial must not persist a sublease"
            );

            let sublease_events = state
                .privileged_dispatcher()
                .event_emitter()
                .get_events_by_work_id("pcac-sublease-wide");
            let sublease_event_count = sublease_events
                .iter()
                .filter(|event| event.event_type == "SubleaseIssued")
                .count();
            assert_eq!(
                sublease_event_count, 0,
                "widening denial must not emit SubleaseIssued events"
            );
        }

        #[test]
        fn test_delegate_sublease_privileged_pcac_deny_stale_authority_revalidate() {
            use apm2_core::pcac::{
                AuthorityConsumeRecordV1, AuthorityConsumedV1, AuthorityDenyV1,
                AuthorityJoinCertificateV1, AuthorityJoinInputV1, AuthorityJoinKernel,
            };

            struct StaleAfterJoinKernel {
                inner: crate::pcac::InProcessKernel,
            }

            impl StaleAfterJoinKernel {
                fn new() -> Self {
                    Self {
                        inner: crate::pcac::InProcessKernel::new(1),
                    }
                }
            }

            impl AuthorityJoinKernel for StaleAfterJoinKernel {
                fn join(
                    &self,
                    input: &AuthorityJoinInputV1,
                    policy: &apm2_core::pcac::PcacPolicyKnobs,
                ) -> Result<AuthorityJoinCertificateV1, Box<AuthorityDenyV1>> {
                    let cert = AuthorityJoinKernel::join(&self.inner, input, policy)?;
                    self.inner.advance_tick(
                        cert.issued_at_tick
                            .saturating_add(policy.freshness_max_age_ticks)
                            .saturating_add(1),
                    );
                    Ok(cert)
                }

                fn revalidate(
                    &self,
                    cert: &AuthorityJoinCertificateV1,
                    current_time_envelope_ref: [u8; 32],
                    current_ledger_anchor: [u8; 32],
                    current_revocation_head_hash: [u8; 32],
                    policy: &apm2_core::pcac::PcacPolicyKnobs,
                ) -> Result<(), Box<AuthorityDenyV1>> {
                    AuthorityJoinKernel::revalidate(
                        &self.inner,
                        cert,
                        current_time_envelope_ref,
                        current_ledger_anchor,
                        current_revocation_head_hash,
                        policy,
                    )
                }

                fn consume(
                    &self,
                    cert: &AuthorityJoinCertificateV1,
                    intent_digest: [u8; 32],
                    boundary_intent_class: apm2_core::pcac::BoundaryIntentClass,
                    requires_authoritative_acceptance: bool,
                    current_time_envelope_ref: [u8; 32],
                    current_revocation_head_hash: [u8; 32],
                    policy: &apm2_core::pcac::PcacPolicyKnobs,
                ) -> Result<(AuthorityConsumedV1, AuthorityConsumeRecordV1), Box<AuthorityDenyV1>>
                {
                    AuthorityJoinKernel::consume(
                        &self.inner,
                        cert,
                        intent_digest,
                        boundary_intent_class,
                        requires_authoritative_acceptance,
                        current_time_envelope_ref,
                        current_revocation_head_hash,
                        policy,
                    )
                }
            }

            let (dispatcher, ctx, _conn) = setup_sqlite_dispatcher();
            let caller_actor = derive_actor_id(&PeerCredentials {
                uid: 1000,
                gid: 1000,
                pid: Some(12345),
            });

            let cas = dispatcher
                .cas
                .as_ref()
                .expect("CAS should be configured by setup helper");
            super::ingest_review_receipt::register_full_test_lease(
                &super::ingest_review_receipt::TestLeaseConfig {
                    dispatcher: &dispatcher,
                    cas: cas.as_ref(),
                    lease_id: "pcac-parent-stale",
                    work_id: "W-PCAC-DS-STALE",
                    gate_id: "gate-pcac-stale",
                    executor_actor_id: &caller_actor,
                    policy_hash: [0xD2; 32],
                    wall_time_source: WallTimeSource::AuthenticatedNts,
                    include_attestation: true,
                },
            );

            let mut policy_resolution = test_policy_resolution_with_lineage(
                "W-PCAC-DS-STALE",
                &caller_actor,
                WorkRole::Reviewer,
                0,
            );
            policy_resolution.resolved_policy_hash = [0xD2; 32];
            let pcac_policy = apm2_core::pcac::PcacPolicyKnobs {
                freshness_max_age_ticks: 0,
                ..apm2_core::pcac::PcacPolicyKnobs::default()
            };
            policy_resolution.pcac_policy = Some(pcac_policy);
            seed_policy_lineage_for_test(
                cas.as_ref(),
                "W-PCAC-DS-STALE",
                &caller_actor,
                WorkRole::Reviewer,
                &policy_resolution,
            );
            dispatcher
                .work_registry
                .register_claim(WorkClaim {
                    work_id: "W-PCAC-DS-STALE".to_string(),
                    lease_id: "pcac-parent-stale".to_string(),
                    actor_id: caller_actor,
                    role: WorkRole::Reviewer,
                    policy_resolution,
                    executor_custody_domains: vec![],
                    author_custody_domains: vec![],
                    permeability_receipt: None,
                })
                .expect("work claim registration");

            let kernel: Arc<dyn AuthorityJoinKernel> = Arc::new(StaleAfterJoinKernel::new());
            let gate = Arc::new(crate::pcac::LifecycleGate::new(kernel));
            let signer = Arc::new(apm2_core::crypto::Signer::generate());
            let orchestrator = Arc::new(crate::gate::GateOrchestrator::new(
                crate::gate::GateOrchestratorConfig::default(),
                signer,
            ));
            let dispatcher = dispatcher
                .with_gate_orchestrator(orchestrator)
                .with_pcac_lifecycle_gate(gate)
                .with_privileged_pcac_policy(crate::protocol::dispatch::PrivilegedPcacPolicy {});

            let request = DelegateSubleaseRequest {
                parent_lease_id: "pcac-parent-stale".to_string(),
                delegatee_actor_id: "pcac-child-stale".to_string(),
                requested_expiry_ns: 1_900_000_000_000,
                sublease_id: "pcac-sublease-stale".to_string(),
                identity_proof_hash: vec![0x74; 32],
            };
            let frame = encode_delegate_sublease_request(&request);
            let response = dispatcher.dispatch(&frame, &ctx).unwrap();

            match response {
                PrivilegedResponse::Error(err) => {
                    assert!(
                        err.message
                            .contains("PCAC authority denied for DelegateSublease")
                            && err.message.contains("freshness"),
                        "stale authority denial must fail during revalidate, got: {}",
                        err.message
                    );
                },
                other => panic!("expected stale authority denial, got {other:?}"),
            }

            let persisted_sublease = dispatcher
                .lease_validator()
                .get_gate_lease("pcac-sublease-stale");
            assert!(
                persisted_sublease.is_none(),
                "stale authority denial must not persist a sublease"
            );
        }

        #[test]
        fn test_delegate_sublease_privileged_pcac_deny_duplicate_consume_from_durable_index() {
            use apm2_core::pcac::{
                AuthorityConsumeRecordV1, AuthorityConsumedV1, AuthorityDenyV1,
                AuthorityJoinCertificateV1, AuthorityJoinInputV1, AuthorityJoinKernel,
            };

            struct DeterministicKernel;

            impl AuthorityJoinKernel for DeterministicKernel {
                fn join(
                    &self,
                    input: &AuthorityJoinInputV1,
                    _policy: &apm2_core::pcac::PcacPolicyKnobs,
                ) -> Result<AuthorityJoinCertificateV1, Box<AuthorityDenyV1>> {
                    Ok(AuthorityJoinCertificateV1 {
                        ajc_id: [0xA5; 32],
                        authority_join_hash: [0x5A; 32],
                        intent_digest: input.intent_digest,
                        boundary_intent_class: input.boundary_intent_class,
                        risk_tier: input.risk_tier,
                        issued_time_envelope_ref: input.time_envelope_ref,
                        issued_at_tick: 1,
                        as_of_ledger_anchor: input.as_of_ledger_anchor,
                        expires_at_tick: u64::MAX,
                        revocation_head_hash: input.directory_head_hash,
                        identity_evidence_level: input.identity_evidence_level,
                        admission_capacity_token: None,
                    })
                }

                fn revalidate(
                    &self,
                    _cert: &AuthorityJoinCertificateV1,
                    _current_time_envelope_ref: [u8; 32],
                    _current_ledger_anchor: [u8; 32],
                    _current_revocation_head_hash: [u8; 32],
                    _policy: &apm2_core::pcac::PcacPolicyKnobs,
                ) -> Result<(), Box<AuthorityDenyV1>> {
                    Ok(())
                }

                fn consume(
                    &self,
                    cert: &AuthorityJoinCertificateV1,
                    intent_digest: [u8; 32],
                    _boundary_intent_class: apm2_core::pcac::BoundaryIntentClass,
                    _requires_authoritative_acceptance: bool,
                    current_time_envelope_ref: [u8; 32],
                    _current_revocation_head_hash: [u8; 32],
                    _policy: &apm2_core::pcac::PcacPolicyKnobs,
                ) -> Result<(AuthorityConsumedV1, AuthorityConsumeRecordV1), Box<AuthorityDenyV1>>
                {
                    Ok((
                        AuthorityConsumedV1 {
                            ajc_id: cert.ajc_id,
                            intent_digest,
                            consumed_time_envelope_ref: current_time_envelope_ref,
                            consumed_at_tick: 2,
                        },
                        AuthorityConsumeRecordV1 {
                            ajc_id: cert.ajc_id,
                            consumed_time_envelope_ref: current_time_envelope_ref,
                            consumed_at_tick: 2,
                            effect_selector_digest: [0xCC; 32],
                        },
                    ))
                }
            }

            let (dispatcher, ctx, _conn) = setup_sqlite_dispatcher();
            let caller_actor = derive_actor_id(&PeerCredentials {
                uid: 1000,
                gid: 1000,
                pid: Some(12345),
            });

            let cas = dispatcher
                .cas
                .as_ref()
                .expect("CAS should be configured by setup helper");
            super::ingest_review_receipt::register_full_test_lease(
                &super::ingest_review_receipt::TestLeaseConfig {
                    dispatcher: &dispatcher,
                    cas: cas.as_ref(),
                    lease_id: "pcac-parent-duplicate-consume",
                    work_id: "W-PCAC-DS-DUP-CONSUME",
                    gate_id: "gate-pcac-duplicate-consume",
                    executor_actor_id: &caller_actor,
                    policy_hash: [0xE2; 32],
                    wall_time_source: WallTimeSource::AuthenticatedNts,
                    include_attestation: true,
                },
            );

            let mut policy_resolution = test_policy_resolution_with_lineage(
                "W-PCAC-DS-DUP-CONSUME",
                &caller_actor,
                WorkRole::Reviewer,
                0,
            );
            policy_resolution.resolved_policy_hash = [0xE2; 32];
            policy_resolution.pcac_policy = Some(apm2_core::pcac::PcacPolicyKnobs::default());
            seed_policy_lineage_for_test(
                cas.as_ref(),
                "W-PCAC-DS-DUP-CONSUME",
                &caller_actor,
                WorkRole::Reviewer,
                &policy_resolution,
            );
            dispatcher
                .work_registry
                .register_claim(WorkClaim {
                    work_id: "W-PCAC-DS-DUP-CONSUME".to_string(),
                    lease_id: "pcac-parent-duplicate-consume".to_string(),
                    actor_id: caller_actor,
                    role: WorkRole::Reviewer,
                    policy_resolution,
                    executor_custody_domains: vec![],
                    author_custody_domains: vec![],
                    permeability_receipt: None,
                })
                .expect("work claim registration");

            let consume_dir = tempfile::tempdir().expect("durable consume tempdir");
            let consume_log_path = consume_dir.path().join("pcac_consume.log");
            let durable_index = crate::pcac::FileBackedConsumeIndex::open(&consume_log_path, None)
                .expect("open durable consume index");
            let durable_kernel =
                crate::pcac::DurableKernel::new(DeterministicKernel, Box::new(durable_index));
            let kernel: Arc<dyn AuthorityJoinKernel> = Arc::new(durable_kernel);
            let gate = Arc::new(crate::pcac::LifecycleGate::new(kernel));
            let signer = Arc::new(apm2_core::crypto::Signer::generate());
            let orchestrator = Arc::new(crate::gate::GateOrchestrator::new(
                crate::gate::GateOrchestratorConfig::default(),
                signer,
            ));
            let dispatcher = dispatcher
                .with_gate_orchestrator(orchestrator)
                .with_pcac_lifecycle_gate(gate)
                .with_privileged_pcac_policy(crate::protocol::dispatch::PrivilegedPcacPolicy {});

            let request = DelegateSubleaseRequest {
                parent_lease_id: "pcac-parent-duplicate-consume".to_string(),
                delegatee_actor_id: "pcac-child-duplicate-consume".to_string(),
                requested_expiry_ns: 1_900_000_000_000,
                sublease_id: "pcac-sublease-duplicate-consume".to_string(),
                identity_proof_hash: vec![0x91; 32],
            };
            let frame = encode_delegate_sublease_request(&request);

            let first_response = dispatcher.dispatch(&frame, &ctx).unwrap();
            assert!(
                matches!(first_response, PrivilegedResponse::DelegateSublease(_)),
                "first delegated sublease should succeed before consume replay"
            );

            let second_response = dispatcher.dispatch(&frame, &ctx).unwrap();
            match second_response {
                PrivilegedResponse::Error(err) => {
                    assert!(
                        err.message.contains("authority already consumed"),
                        "second request must be denied by durable consume index, got: {}",
                        err.message
                    );
                },
                other => panic!("expected duplicate consume denial, got {other:?}"),
            }

            let sublease_events = dispatcher
                .event_emitter()
                .get_events_by_work_id("pcac-sublease-duplicate-consume");
            let sublease_event_count = sublease_events
                .iter()
                .filter(|event| event.event_type == "SubleaseIssued")
                .count();
            assert_eq!(
                sublease_event_count, 1,
                "duplicate consume denial must not emit an additional SubleaseIssued event"
            );
        }

        #[test]
        fn test_delegate_sublease_privileged_pcac_deny_lifecycle_failure() {
            let policy = crate::protocol::dispatch::PrivilegedPcacPolicy {};
            let (state, ctx, caller_actor, _cas_dir) =
                setup_dispatcher_state_with_privileged_pcac(policy, true);

            register_test_lease_and_claim(
                &state,
                "pcac-parent-missing-gate",
                "W-PCAC-DS-MISSING-GATE",
                "gate-pcac-missing-gate",
                &caller_actor,
                [0xDE; 32],
                0,
            );

            let request = DelegateSubleaseRequest {
                parent_lease_id: "pcac-parent-missing-gate".to_string(),
                delegatee_actor_id: "pcac-child-missing-gate".to_string(),
                requested_expiry_ns: 1_900_000_000_000,
                sublease_id: "pcac-sublease-missing-gate".to_string(),
                identity_proof_hash: vec![0xAB; 32],
            };
            let frame = encode_delegate_sublease_request(&request);
            let response = state
                .privileged_dispatcher()
                .dispatch(&frame, &ctx)
                .unwrap();

            match response {
                PrivilegedResponse::Error(err) => {
                    assert!(
                        err.message.contains("PCAC authority gate not wired"),
                        "lifecycle failure must fail-closed on missing gate wiring, got: {}",
                        err.message
                    );
                },
                other => panic!("expected lifecycle failure denial, got {other:?}"),
            }

            let persisted_sublease = state
                .privileged_dispatcher()
                .lease_validator()
                .get_gate_lease("pcac-sublease-missing-gate");
            assert!(
                persisted_sublease.is_none(),
                "lifecycle denial must not persist a sublease"
            );

            let sublease_events = state
                .privileged_dispatcher()
                .event_emitter()
                .get_events_by_work_id("pcac-sublease-missing-gate");
            let sublease_event_count = sublease_events
                .iter()
                .filter(|event| event.event_type == "SubleaseIssued")
                .count();
            assert_eq!(
                sublease_event_count, 0,
                "lifecycle denial must not emit SubleaseIssued events"
            );
        }

        #[test]
        fn test_ingest_review_receipt_privileged_pcac_deny_lifecycle_failure() {
            let policy = crate::protocol::dispatch::PrivilegedPcacPolicy {};
            let (state, ctx, caller_actor, _cas_dir) =
                setup_dispatcher_state_with_privileged_pcac(policy, true);

            register_test_lease_and_claim(
                &state,
                "pcac-review-lease-missing-gate",
                "W-PCAC-RR-MISSING-GATE",
                "gate-pcac-review-missing-gate",
                &caller_actor,
                [0xEE; 32],
                0,
            );

            let request = IngestReviewReceiptRequest {
                lease_id: "pcac-review-lease-missing-gate".to_string(),
                receipt_id: "RR-PCAC-MISSING-GATE".to_string(),
                reviewer_actor_id: "ignored-by-handler".to_string(),
                changeset_digest: vec![0x42; 32],
                artifact_bundle_hash: test_artifact_bundle_hash(),
                verdict: ReviewReceiptVerdict::Approve.into(),
                blocked_reason_code: 0,
                blocked_log_hash: vec![],
                identity_proof_hash: vec![0x88; 32],
            };
            let frame = encode_ingest_review_receipt_request(&request);
            let response = state
                .privileged_dispatcher()
                .dispatch(&frame, &ctx)
                .unwrap();

            match response {
                PrivilegedResponse::Error(err) => {
                    assert!(
                        err.message.contains("PCAC authority gate not wired"),
                        "lifecycle failure must fail-closed on missing gate wiring, got: {}",
                        err.message
                    );
                },
                other => panic!("expected lifecycle failure denial, got {other:?}"),
            }

            let review_events = state
                .privileged_dispatcher()
                .event_emitter()
                .get_events_by_work_id("pcac-review-lease-missing-gate");
            let review_event_count = review_events
                .iter()
                .filter(|event| event.event_type == "review_receipt_recorded")
                .count();
            assert_eq!(
                review_event_count, 0,
                "lifecycle denial must not emit review_receipt_recorded events"
            );
        }

        #[test]
        fn test_delegate_sublease_pcac_mandatory_even_when_policy_default() {
            let policy = crate::protocol::dispatch::PrivilegedPcacPolicy::default();
            let (state, ctx, caller_actor, _cas_dir) =
                setup_dispatcher_state_with_privileged_pcac(policy, true);

            register_test_lease_and_claim(
                &state,
                "pcac-parent-rollout-off",
                "W-PCAC-DS-ROLLOUT-OFF",
                "gate-pcac-rollout-off",
                &caller_actor,
                [0xA1; 32],
                0,
            );
            register_test_lease_and_claim(
                &state,
                "pcac-review-rollout-off",
                "W-PCAC-RR-ROLLOUT-OFF",
                "gate-pcac-review-rollout-off",
                &caller_actor,
                [0xA2; 32],
                0,
            );

            let delegate_request = DelegateSubleaseRequest {
                parent_lease_id: "pcac-parent-rollout-off".to_string(),
                delegatee_actor_id: "pcac-child-rollout-off".to_string(),
                requested_expiry_ns: 1_900_000_000_000,
                sublease_id: "pcac-sublease-rollout-off".to_string(),
                identity_proof_hash: vec![0x99; 32],
            };
            let delegate_frame = encode_delegate_sublease_request(&delegate_request);
            let delegate_response = state
                .privileged_dispatcher()
                .dispatch(&delegate_frame, &ctx)
                .unwrap();
            match delegate_response {
                PrivilegedResponse::Error(err) => {
                    assert!(
                        err.message.contains("PCAC authority gate not wired"),
                        "DelegateSublease must fail-closed when PCAC lifecycle gate is missing, got: {}",
                        err.message
                    );
                },
                other => panic!("expected DelegateSublease fail-closed denial, got {other:?}"),
            }

            let delegate_events = state
                .privileged_dispatcher()
                .event_emitter()
                .get_events_by_work_id("pcac-sublease-rollout-off");
            let delegate_event_count = delegate_events
                .iter()
                .filter(|event| event.event_type == "SubleaseIssued")
                .count();
            assert_eq!(
                delegate_event_count, 0,
                "DelegateSublease must not emit SubleaseIssued when PCAC gate is missing"
            );

            let review_request = IngestReviewReceiptRequest {
                lease_id: "pcac-review-rollout-off".to_string(),
                receipt_id: "RR-PCAC-ROLLOUT-OFF".to_string(),
                reviewer_actor_id: "ignored-by-handler".to_string(),
                changeset_digest: vec![0x42; 32],
                artifact_bundle_hash: test_artifact_bundle_hash(),
                verdict: ReviewReceiptVerdict::Approve.into(),
                blocked_reason_code: 0,
                blocked_log_hash: vec![],
                identity_proof_hash: vec![0xAA; 32],
            };
            let review_frame = encode_ingest_review_receipt_request(&review_request);
            let review_response = state
                .privileged_dispatcher()
                .dispatch(&review_frame, &ctx)
                .unwrap();
            match review_response {
                PrivilegedResponse::Error(err) => {
                    assert!(
                        err.message.contains("PCAC authority gate not wired"),
                        "IngestReviewReceipt must fail-closed when PCAC lifecycle gate is missing, got: {}",
                        err.message
                    );
                },
                other => panic!("expected IngestReviewReceipt fail-closed denial, got {other:?}"),
            }

            let review_events = state
                .privileged_dispatcher()
                .event_emitter()
                .get_events_by_work_id("W-PCAC-RR-ROLLOUT-OFF");
            let review_event_count = review_events
                .iter()
                .filter(|event| event.event_type == "review_receipt_recorded")
                .count();
            assert_eq!(
                review_event_count, 0,
                "IngestReviewReceipt must not emit review_receipt_recorded when PCAC gate is missing"
            );
        }

        #[test]
        fn test_ingest_review_receipt_pcac_mandatory_even_when_policy_default() {
            let policy = crate::protocol::dispatch::PrivilegedPcacPolicy::default();
            let (state, ctx, caller_actor, _cas_dir) =
                setup_dispatcher_state_with_privileged_pcac(policy, false);

            let cas = state
                .privileged_dispatcher()
                .cas
                .as_ref()
                .expect("CAS should be configured by DispatcherState");
            super::ingest_review_receipt::register_full_test_lease(
                &super::ingest_review_receipt::TestLeaseConfig {
                    dispatcher: state.privileged_dispatcher(),
                    cas: cas.as_ref(),
                    lease_id: "pcac-review-lifecycle-disabled",
                    work_id: "W-PCAC-RR-LIFECYCLE-DISABLED",
                    gate_id: "gate-pcac-review-lifecycle-disabled",
                    executor_actor_id: &caller_actor,
                    policy_hash: [0xA3; 32],
                    wall_time_source: WallTimeSource::AuthenticatedNts,
                    include_attestation: true,
                },
            );

            let mut policy_resolution = test_policy_resolution_with_lineage(
                "W-PCAC-RR-LIFECYCLE-DISABLED",
                &caller_actor,
                WorkRole::Reviewer,
                0,
            );
            policy_resolution.resolved_policy_hash = [0xA3; 32];
            policy_resolution.pcac_policy = Some(apm2_core::pcac::PcacPolicyKnobs {
                lifecycle_enforcement: false,
                ..apm2_core::pcac::PcacPolicyKnobs::default()
            });
            seed_policy_lineage_for_test(
                cas.as_ref(),
                "W-PCAC-RR-LIFECYCLE-DISABLED",
                &caller_actor,
                WorkRole::Reviewer,
                &policy_resolution,
            );

            state
                .privileged_dispatcher()
                .work_registry
                .register_claim(WorkClaim {
                    work_id: "W-PCAC-RR-LIFECYCLE-DISABLED".to_string(),
                    lease_id: "pcac-review-lifecycle-disabled".to_string(),
                    actor_id: caller_actor.clone(),
                    role: WorkRole::Reviewer,
                    policy_resolution,
                    executor_custody_domains: vec![],
                    author_custody_domains: vec![],
                    permeability_receipt: None,
                })
                .expect("work claim registration");

            let request = IngestReviewReceiptRequest {
                lease_id: "pcac-review-lifecycle-disabled".to_string(),
                receipt_id: "RR-PCAC-LIFECYCLE-DISABLED".to_string(),
                reviewer_actor_id: caller_actor,
                changeset_digest: vec![0x42; 32],
                artifact_bundle_hash: test_artifact_bundle_hash(),
                verdict: ReviewReceiptVerdict::Approve.into(),
                blocked_reason_code: 0,
                blocked_log_hash: vec![],
                identity_proof_hash: vec![0x55; 32],
            };
            let frame = encode_ingest_review_receipt_request(&request);
            let response = state
                .privileged_dispatcher()
                .dispatch(&frame, &ctx)
                .unwrap();

            match response {
                PrivilegedResponse::Error(err) => {
                    assert!(
                        err.message
                            .contains("IngestReviewReceipt requires PCAC lifecycle evidence (mandatory cutover)"),
                        "mandatory cutover must deny when lifecycle_enforcement=false, got: {}",
                        err.message
                    );
                },
                other => {
                    panic!("expected IngestReviewReceipt mandatory PCAC denial, got {other:?}")
                },
            }

            let review_events = state
                .privileged_dispatcher()
                .event_emitter()
                .get_events_by_work_id("W-PCAC-RR-LIFECYCLE-DISABLED");
            let review_event_count = review_events
                .iter()
                .filter(|event| event.event_type == "review_receipt_recorded")
                .count();
            assert_eq!(
                review_event_count, 0,
                "mandatory PCAC denial must not emit review_receipt_recorded events"
            );
        }

        /// Integration test: exercises the production `GovernancePolicyResolver
        /// -> ClaimWork -> IngestReviewReceipt` path end-to-end.
        /// Verifies that the governance resolver's transitional Tier1
        /// mapping permits `SelfSigned` attestation through the review
        /// receipt handler.
        ///
        /// This test was added as part of TCK-00340 quality fix to prevent
        /// regression where hardcoded Tier4 would block all production claims.
        #[test]
        fn test_governance_resolver_claim_then_ingest_review_receipt_production_path() {
            use std::os::unix::fs::PermissionsExt;

            use crate::cas::{DurableCas, DurableCasConfig};
            use crate::governance::GovernancePolicyResolver;
            use crate::state::DispatcherState;

            let conn = Connection::open_in_memory().unwrap();
            SqliteLedgerEventEmitter::init_schema_for_test(&conn).unwrap();
            SqliteWorkRegistry::init_schema(&conn).unwrap();
            let conn = Arc::new(Mutex::new(conn));

            let session_registry: Arc<dyn SessionRegistry> =
                Arc::new(InMemorySessionRegistry::new());

            // TCK-00408: Use with_persistence_and_cas — IngestReviewReceipt
            // now requires CAS (fail-closed).
            let cas_dir = tempfile::tempdir().expect("tempdir for CAS");
            std::fs::set_permissions(cas_dir.path(), std::fs::Permissions::from_mode(0o700))
                .expect("set CAS dir permissions");
            {
                let cas = DurableCas::new(DurableCasConfig::new(cas_dir.path().to_path_buf()))
                    .expect("pre-populate CAS");
                cas.store(TEST_ARTIFACT_CONTENT)
                    .expect("store test artifact");
            }
            let state = DispatcherState::with_persistence_and_cas(
                session_registry,
                None, // no metrics
                Arc::clone(&conn),
                cas_dir.path(),
            )
            .expect("CAS initialization must succeed");

            // Derive caller actor from test peer credentials
            let test_creds = PeerCredentials {
                uid: 1000,
                gid: 1000,
                pid: Some(12345),
            };
            let caller_actor = derive_actor_id(&test_creds);
            let ctx = ConnectionContext::privileged_session_open(Some(test_creds));

            // Step 1: Use GovernancePolicyResolver to produce the PolicyResolution
            // (this is what ClaimWork calls in production with_persistence path)
            let governance_resolver = GovernancePolicyResolver::new();
            let policy_resolution = governance_resolver
                .resolve_for_claim("W-GOV-001", WorkRole::Reviewer, &caller_actor)
                .expect("GovernancePolicyResolver must succeed");

            // Verify the governance resolver returns Tier1 (not Tier4)
            assert_eq!(
                policy_resolution.resolved_risk_tier, 1,
                "GovernancePolicyResolver must return Tier1 for transitional mapping"
            );

            let cas = state
                .privileged_dispatcher()
                .cas
                .as_ref()
                .expect("CAS should be configured by DispatcherState");
            seed_policy_lineage_for_test(
                cas.as_ref(),
                "W-GOV-001",
                &caller_actor,
                WorkRole::Reviewer,
                &policy_resolution,
            );

            // Step 2: Register the work claim with the governance-produced resolution
            let claim = WorkClaim {
                work_id: "W-GOV-001".to_string(),
                lease_id: "lease-gov-001".to_string(),
                actor_id: caller_actor.clone(),
                role: WorkRole::Reviewer,
                policy_resolution,
                executor_custody_domains: vec![],
                author_custody_domains: vec![],
                permeability_receipt: None,
            };
            state
                .privileged_dispatcher()
                .work_registry
                .register_claim(claim)
                .expect("Claim registration must succeed");

            // Step 3: Register the lease for reviewer identity validation
            super::ingest_review_receipt::register_full_test_lease(
                &super::ingest_review_receipt::TestLeaseConfig {
                    dispatcher: state.privileged_dispatcher(),
                    cas: cas.as_ref(),
                    lease_id: "lease-gov-001",
                    work_id: "W-GOV-001",
                    gate_id: "gate-gov",
                    executor_actor_id: &caller_actor,
                    policy_hash: [0u8; 32],
                    wall_time_source: WallTimeSource::AuthenticatedNts,
                    include_attestation: true,
                },
            );

            // Step 4: Submit IngestReviewReceipt with SelfSigned attestation
            let review_request = IngestReviewReceiptRequest {
                lease_id: "lease-gov-001".to_string(),
                receipt_id: "RR-GOV-001".to_string(),
                reviewer_actor_id: caller_actor,
                changeset_digest: vec![0x42; 32],
                artifact_bundle_hash: test_artifact_bundle_hash(),
                verdict: ReviewReceiptVerdict::Approve.into(),
                blocked_reason_code: 0,
                blocked_log_hash: vec![],
                identity_proof_hash: vec![0x99; 32],
            };
            let review_frame = encode_ingest_review_receipt_request(&review_request);

            let review_response = state
                .privileged_dispatcher()
                .dispatch(&review_frame, &ctx)
                .unwrap();
            match review_response {
                PrivilegedResponse::IngestReviewReceipt(resp) => {
                    assert_eq!(resp.receipt_id, "RR-GOV-001");
                    assert_eq!(
                        resp.event_type, "ReviewReceiptRecorded",
                        "Governance-resolved Tier1 claim with SelfSigned attestation \
                         must pass through production path"
                    );
                    assert!(
                        !resp.event_id.is_empty(),
                        "Event ID must be non-empty for governance-resolved production path"
                    );
                },
                PrivilegedResponse::Error(err) => {
                    panic!(
                        "IngestReviewReceipt MUST succeed for governance-resolved Tier1 claim \
                         with SelfSigned attestation. Got error: {}. This is the TCK-00340 \
                         regression where hardcoded Tier4 blocked all production claims.",
                        err.message
                    );
                },
                other => panic!(
                    "Expected IngestReviewReceipt via governance-resolved path, got {other:?}"
                ),
            }
        }
    }

    // ========================================================================
    // TCK-00349: Session-typed state machine and fail-closed decoding tests
    // ========================================================================
    mod tck_00349_session_state_machine {
        use super::*;

        /// TCK-00349: Verify that `ConnectionContext` starts in `Connected`
        /// phase.
        #[test]
        fn test_context_starts_in_connected_phase() {
            use crate::protocol::connection_handler::ConnectionPhase;

            let ctx = ConnectionContext::privileged(Some(PeerCredentials {
                uid: 1000,
                gid: 1000,
                pid: Some(12345),
            }));
            assert_eq!(ctx.phase(), ConnectionPhase::Connected);
            assert!(!ctx.phase().allows_dispatch());
        }

        /// TCK-00349: Verify full phase progression on `ConnectionContext`.
        #[test]
        fn test_context_full_phase_progression() {
            use crate::protocol::connection_handler::ConnectionPhase;

            let mut ctx = ConnectionContext::privileged(Some(PeerCredentials {
                uid: 1000,
                gid: 1000,
                pid: Some(12345),
            }));

            // Connected -> HandshakeComplete
            ctx.advance_to_handshake_complete().unwrap();
            assert_eq!(ctx.phase(), ConnectionPhase::HandshakeComplete);
            assert!(!ctx.phase().allows_dispatch());

            // HandshakeComplete -> SessionOpen
            ctx.advance_to_session_open().unwrap();
            assert_eq!(ctx.phase(), ConnectionPhase::SessionOpen);
            assert!(ctx.phase().allows_dispatch());
        }

        /// TCK-00349: Verify that dispatch is rejected in Connected phase.
        #[test]
        fn test_dispatch_rejected_in_connected_phase() {
            let dispatcher = PrivilegedDispatcher::new();
            let ctx = ConnectionContext::privileged(Some(PeerCredentials {
                uid: 1000,
                gid: 1000,
                pid: Some(12345),
            }));

            // Create a valid ClaimWork frame
            let request = ClaimWorkRequest {
                actor_id: "test".to_string(),
                role: WorkRole::Implementer.into(),
                credential_signature: vec![],
                nonce: vec![],
            };
            let frame = encode_claim_work_request(&request);

            let response = dispatcher.dispatch(&frame, &ctx).unwrap();
            match response {
                PrivilegedResponse::Error(err) => {
                    assert!(
                        err.message.contains("not SessionOpen"),
                        "Expected phase error, got: {}",
                        err.message
                    );
                },
                other => panic!("Expected error for pre-SessionOpen dispatch, got {other:?}"),
            }
        }

        /// TCK-00349: Verify that dispatch is rejected in `HandshakeComplete`
        /// phase.
        #[test]
        fn test_dispatch_rejected_in_handshake_complete_phase() {
            let dispatcher = PrivilegedDispatcher::new();
            let mut ctx = ConnectionContext::privileged(Some(PeerCredentials {
                uid: 1000,
                gid: 1000,
                pid: Some(12345),
            }));
            ctx.advance_to_handshake_complete().unwrap();

            let request = ClaimWorkRequest {
                actor_id: "test".to_string(),
                role: WorkRole::Implementer.into(),
                credential_signature: vec![],
                nonce: vec![],
            };
            let frame = encode_claim_work_request(&request);

            let response = dispatcher.dispatch(&frame, &ctx).unwrap();
            match response {
                PrivilegedResponse::Error(err) => {
                    assert!(
                        err.message.contains("not SessionOpen"),
                        "Expected phase error, got: {}",
                        err.message
                    );
                },
                other => panic!("Expected error for HandshakeComplete dispatch, got {other:?}"),
            }
        }

        /// TCK-00349: Verify that dispatch succeeds in `SessionOpen` phase.
        #[test]
        fn test_dispatch_succeeds_in_session_open_phase() {
            let dispatcher = PrivilegedDispatcher::new();
            let ctx = ConnectionContext::privileged_session_open(Some(PeerCredentials {
                uid: 1000,
                gid: 1000,
                pid: Some(12345),
            }));

            let request = ClaimWorkRequest {
                actor_id: "test".to_string(),
                role: WorkRole::Implementer.into(),
                credential_signature: vec![],
                nonce: vec![],
            };
            let frame = encode_claim_work_request(&request);

            let response = dispatcher.dispatch(&frame, &ctx).unwrap();
            // Should route to handler (ClaimWork response, not error)
            assert!(
                matches!(response, PrivilegedResponse::ClaimWork(_)),
                "Dispatch in SessionOpen phase should route to handler"
            );
        }

        /// TCK-00349: Verify unknown tag is rejected as protocol error.
        #[test]
        fn test_unknown_tag_returns_protocol_error() {
            let dispatcher = PrivilegedDispatcher::new();
            let ctx = ConnectionContext::privileged_session_open(Some(PeerCredentials {
                uid: 1000,
                gid: 1000,
                pid: Some(12345),
            }));

            // Tag 200 is not a valid message type
            let frame = Bytes::from(vec![200u8, 0, 0, 0]);
            let result = dispatcher.dispatch(&frame, &ctx);
            assert!(
                result.is_err(),
                "Unknown tag must return protocol error (fail-closed)"
            );
        }

        /// TCK-00349: Verify `privileged_session_open` convenience constructor
        /// creates context in correct phase.
        #[test]
        fn test_privileged_session_open_constructor() {
            use crate::protocol::connection_handler::ConnectionPhase;

            let ctx = ConnectionContext::privileged_session_open(Some(PeerCredentials {
                uid: 1000,
                gid: 1000,
                pid: Some(12345),
            }));
            assert_eq!(ctx.phase(), ConnectionPhase::SessionOpen);
            assert!(ctx.is_privileged());
        }

        /// TCK-00349: Verify `session_open` convenience constructor creates
        /// context in correct phase.
        #[test]
        fn test_session_open_constructor() {
            use crate::protocol::connection_handler::ConnectionPhase;

            let ctx = ConnectionContext::session_open(
                Some(PeerCredentials {
                    uid: 1000,
                    gid: 1000,
                    pid: Some(12346),
                }),
                Some("sess-001".to_string()),
            );
            assert_eq!(ctx.phase(), ConnectionPhase::SessionOpen);
            assert!(!ctx.is_privileged());
        }
    }

    // ========================================================================
    // BLOCKER 2 v3: Risk tier ceiling from policy resolution (not manifest)
    // ========================================================================
    mod risk_tier_ceiling_policy_binding {
        use crate::episode::envelope::RiskTier;

        /// BLOCKER 2 v3: Risk tier ceiling MUST be derived from policy
        /// resolution, not from the manifest. This test proves that
        /// `RiskTier::from_u8(claim.policy_resolution.resolved_risk_tier)`
        /// correctly bounds the ceiling, and that an invalid tier value
        /// falls back to Tier4 (fail-closed).
        #[test]
        fn risk_tier_ceiling_from_policy_resolution_not_manifest() {
            // Policy says Tier1 -- even if manifest capabilities are Tier3,
            // the ceiling should be Tier1.
            let policy_resolved_tier: u8 = 1;
            let ceiling = RiskTier::from_u8(policy_resolved_tier).unwrap_or(RiskTier::Tier4);
            assert_eq!(
                ceiling,
                RiskTier::Tier1,
                "Ceiling should match policy-resolved tier, not manifest capabilities"
            );
        }

        /// BLOCKER 2 v3: Invalid risk tier in policy resolution fails closed
        /// to Tier4 (most restrictive).
        #[test]
        fn invalid_risk_tier_falls_back_to_tier4_fail_closed() {
            let invalid_tier: u8 = 255;
            let ceiling = RiskTier::from_u8(invalid_tier).unwrap_or(RiskTier::Tier4);
            assert_eq!(
                ceiling,
                RiskTier::Tier4,
                "Invalid risk tier must fail closed to Tier4"
            );
        }

        /// BLOCKER 2 v3: Each valid tier value round-trips correctly.
        #[test]
        fn all_valid_tiers_round_trip() {
            for tier_val in 0..=4u8 {
                let tier = RiskTier::from_u8(tier_val)
                    .unwrap_or_else(|| panic!("Tier {tier_val} should be valid"));
                assert_eq!(
                    tier.tier(),
                    tier_val,
                    "Tier value should round-trip: expected {tier_val}, got {}",
                    tier.tier()
                );
            }
        }
    }

    // ========================================================================
    // MAJOR 1 v3: Scope baseline from policy resolution (not manifest)
    // ========================================================================
    mod scope_baseline_policy_binding {
        use super::*;

        /// MAJOR 1 v3: When `resolved_scope_baseline` is None in
        /// `PolicyResolution`, V1 minting should not proceed (fail-closed).
        /// This test verifies the None check logic independently.
        #[test]
        fn none_scope_baseline_is_fail_closed() {
            let resolution = PolicyResolution {
                policy_resolved_ref: "test".to_string(),
                resolved_policy_hash: [0u8; 32],
                capability_manifest_hash: [0u8; 32],
                context_pack_hash: [0u8; 32],
                role_spec_hash: [0u8; 32],
                context_pack_recipe_hash: [0u8; 32],
                resolved_risk_tier: 0,
                resolved_scope_baseline: None,
                expected_adapter_profile_hash: None,
                pcac_policy: None,
                pointer_only_waiver: None,
            };

            assert!(
                resolution.resolved_scope_baseline.is_none(),
                "None baseline must trigger fail-closed in V1 minting path"
            );
        }

        /// MAJOR 1 v3: When `resolved_scope_baseline` is Some with a valid
        /// baseline, the scope validation should use that baseline (not the
        /// manifest being validated).
        #[test]
        fn some_scope_baseline_used_for_validation() {
            use crate::episode::ToolClass;
            use crate::episode::capability::ScopeBaseline;

            let baseline = ScopeBaseline {
                tools: vec![ToolClass::Read, ToolClass::Git],
                write_paths: vec![],
                shell_patterns: vec![],
            };
            let resolution = PolicyResolution {
                policy_resolved_ref: "test".to_string(),
                resolved_policy_hash: [0u8; 32],
                capability_manifest_hash: [0u8; 32],
                context_pack_hash: [0u8; 32],
                role_spec_hash: [0u8; 32],
                context_pack_recipe_hash: [0u8; 32],
                resolved_risk_tier: 0,
                resolved_scope_baseline: Some(baseline),
                expected_adapter_profile_hash: None,
                pcac_policy: None,
                pointer_only_waiver: None,
            };

            let resolved = resolution.resolved_scope_baseline.unwrap();
            assert_eq!(
                resolved.tools.len(),
                2,
                "Baseline should have 2 tools from policy"
            );
            assert_eq!(resolved.tools[0], ToolClass::Read);
            assert_eq!(resolved.tools[1], ToolClass::Git);
        }

        /// MAJOR 1 v3: A manifest with tools NOT in the policy baseline is
        /// rejected by `validate_manifest_scope_subset` when the baseline
        /// comes from `PolicyResolution`, not from the manifest itself.
        #[test]
        fn manifest_exceeding_policy_baseline_is_rejected() {
            use crate::episode::ToolClass;
            use crate::episode::capability::{ScopeBaseline, validate_manifest_scope_subset};

            // Policy only allows Read
            let policy_baseline = ScopeBaseline {
                tools: vec![ToolClass::Read],
                write_paths: vec![],
                shell_patterns: vec![],
            };

            // Manifest tries to use Read + Execute
            let manifest = crate::episode::CapabilityManifest::builder("attack")
                .delegator("attacker")
                .created_at(1000)
                .expires_at(2000)
                .tool_allowlist(vec![ToolClass::Read, ToolClass::Execute])
                .build()
                .unwrap();

            let result = validate_manifest_scope_subset(&manifest, &policy_baseline);
            assert!(
                result.is_err(),
                "manifest exceeding policy baseline must be rejected"
            );
        }
    }

    // ========================================================================
    // TCK-00352 v3 integration: SpawnEpisode with V1 store
    // ========================================================================
    mod v3_spawn_episode_v1_integration {
        use std::sync::Arc;

        use super::*;
        use crate::episode::capability::ScopeBaseline;
        use crate::episode::reviewer_manifest::reviewer_v0_manifest;
        use crate::episode::tool_class::ToolClass;
        use crate::protocol::session_dispatch::V1ManifestStore;

        fn priv_ctx() -> ConnectionContext {
            ConnectionContext::privileged_session_open(Some(PeerCredentials {
                uid: 1000,
                gid: 1000,
                pid: Some(12345),
            }))
        }

        /// TCK-00352 MAJOR 1 v3 integration: `SpawnEpisode` with V1 store
        /// enabled MUST reject when `resolved_scope_baseline` is `None`.
        #[test]
        fn spawn_v1_rejects_none_scope_baseline_integration() {
            let v1_store = Arc::new(V1ManifestStore::new());
            let dispatcher = PrivilegedDispatcher::new().with_v1_manifest_store(v1_store);
            let ctx = priv_ctx();

            // Pre-register claim with NO scope baseline
            // TCK-00416: Use non-zero hashes (REQ-HEF-0013 mandates non-zero).
            let policy_resolution = test_policy_resolution_with_lineage(
                "W-V3-SCOPE-NONE",
                "actor:test-impl",
                WorkRole::Implementer,
                1,
            );
            let claim = WorkClaim {
                work_id: "W-V3-SCOPE-NONE".to_string(),
                lease_id: "L-V3-001".to_string(),
                actor_id: "actor:test-impl".to_string(),
                role: WorkRole::Implementer,
                policy_resolution,
                author_custody_domains: vec![],
                executor_custody_domains: vec![],
                permeability_receipt: None,
            };
            dispatcher.work_registry.register_claim(claim).unwrap();

            let request = SpawnEpisodeRequest {
                workspace_root: "/tmp".to_string(),
                work_id: "W-V3-SCOPE-NONE".to_string(),
                role: WorkRole::Implementer.into(),
                lease_id: Some("L-V3-001".to_string()),
                adapter_profile_hash: None,
                max_episodes: None,
                escalation_predicate: None,
                permeability_receipt_hash: None,
            };
            let frame = encode_spawn_episode_request(&request);
            let response = dispatcher.dispatch(&frame, &ctx).unwrap();

            match response {
                PrivilegedResponse::Error(err) => {
                    assert_eq!(
                        err.code,
                        PrivilegedErrorCode::CapabilityRequestRejected as i32,
                        "Expected CapabilityRequestRejected for missing scope baseline"
                    );
                    assert!(
                        err.message.contains("scope baseline")
                            || err.message.contains("fail-closed"),
                        "Error should mention scope baseline or fail-closed: {}",
                        err.message
                    );
                },
                other => panic!("Expected rejection for None scope baseline, got: {other:?}"),
            }
        }

        /// TCK-00352 MAJOR 1 v3 integration: `SpawnEpisode` rejects a
        /// manifest whose tools exceed the policy-resolved scope baseline.
        #[test]
        fn spawn_v1_rejects_scope_exceeding_policy_baseline() {
            let v1_store = Arc::new(V1ManifestStore::new());
            let dispatcher = PrivilegedDispatcher::new().with_v1_manifest_store(v1_store);
            let ctx = priv_ctx();

            // Scope baseline only allows Read -- reviewer manifest has more
            let narrow_baseline = ScopeBaseline {
                tools: vec![ToolClass::Read],
                write_paths: Vec::new(),
                shell_patterns: Vec::new(),
            };
            // TCK-00416: Use non-zero hashes (REQ-HEF-0013 mandates non-zero).
            let mut policy_resolution = test_policy_resolution_with_lineage(
                "W-V3-SCOPE-NARROW",
                "actor:test-reviewer",
                WorkRole::Reviewer,
                1,
            );
            policy_resolution.resolved_scope_baseline = Some(narrow_baseline);
            let claim = WorkClaim {
                work_id: "W-V3-SCOPE-NARROW".to_string(),
                lease_id: "L-V3-002".to_string(),
                actor_id: "actor:test-reviewer".to_string(),
                role: WorkRole::Reviewer,
                policy_resolution,
                author_custody_domains: vec![],
                executor_custody_domains: vec![],
                permeability_receipt: None,
            };
            dispatcher.work_registry.register_claim(claim).unwrap();

            let request = SpawnEpisodeRequest {
                workspace_root: "/tmp".to_string(),
                work_id: "W-V3-SCOPE-NARROW".to_string(),
                role: WorkRole::Reviewer.into(),
                lease_id: Some("L-V3-002".to_string()),
                adapter_profile_hash: None,
                max_episodes: None,
                escalation_predicate: None,
                permeability_receipt_hash: None,
            };
            let frame = encode_spawn_episode_request(&request);
            let response = dispatcher.dispatch(&frame, &ctx).unwrap();

            match response {
                PrivilegedResponse::Error(err) => {
                    assert_eq!(
                        err.code,
                        PrivilegedErrorCode::CapabilityRequestRejected as i32,
                        "Expected CapabilityRequestRejected for overbroad scope"
                    );
                    assert!(
                        err.message.contains("scope")
                            || err.message.contains("OverbroadScope")
                            || err.message.contains("baseline"),
                        "Error should mention scope failure: {}",
                        err.message
                    );
                },
                other => panic!("Expected rejection for overbroad scope, got: {other:?}"),
            }
        }

        /// TCK-00352 BLOCKER 2 v3 integration: Risk tier ceiling comes
        /// from `policy_resolution.resolved_risk_tier`, verified through
        /// the actual `SpawnEpisode` path with V1 store.
        #[test]
        fn spawn_v1_risk_ceiling_from_policy_integration() {
            let v1_store = Arc::new(V1ManifestStore::new());
            let dispatcher = PrivilegedDispatcher::new().with_v1_manifest_store(v1_store.clone());
            let ctx = priv_ctx();

            let reviewer = reviewer_v0_manifest();
            let matching_baseline = ScopeBaseline {
                tools: reviewer.tool_allowlist.clone(),
                write_paths: reviewer.write_allowlist.clone(),
                shell_patterns: reviewer.shell_allowlist.clone(),
            };

            // Policy says Tier0 -- the V1 ceiling must be Tier0
            // TCK-00416: Use non-zero hashes (REQ-HEF-0013 mandates non-zero).
            let mut policy_resolution = test_policy_resolution_with_lineage(
                "W-V3-RISK-CEIL",
                "actor:test-reviewer",
                WorkRole::Reviewer,
                0,
            );
            policy_resolution.resolved_scope_baseline = Some(matching_baseline);
            let claim = WorkClaim {
                work_id: "W-V3-RISK-CEIL".to_string(),
                lease_id: "L-V3-003".to_string(),
                actor_id: "actor:test-reviewer".to_string(),
                role: WorkRole::Reviewer,
                policy_resolution,
                author_custody_domains: vec![],
                executor_custody_domains: vec![],
                permeability_receipt: None,
            };
            dispatcher.work_registry.register_claim(claim).unwrap();

            let request = SpawnEpisodeRequest {
                workspace_root: "/tmp".to_string(),
                work_id: "W-V3-RISK-CEIL".to_string(),
                role: WorkRole::Reviewer.into(),
                lease_id: Some("L-V3-003".to_string()),
                adapter_profile_hash: None,
                max_episodes: None,
                escalation_predicate: None,
                permeability_receipt_hash: None,
            };
            let frame = encode_spawn_episode_request(&request);
            let response = dispatcher.dispatch(&frame, &ctx).unwrap();

            match &response {
                PrivilegedResponse::SpawnEpisode(resp) => {
                    let v1 = v1_store
                        .get(&resp.session_id)
                        .expect("V1 manifest should be registered");
                    assert_eq!(
                        v1.risk_tier_ceiling(),
                        crate::episode::envelope::RiskTier::Tier0,
                        "Risk tier ceiling must come from policy (Tier0)"
                    );
                },
                PrivilegedResponse::Error(err) => {
                    panic!("SpawnEpisode should succeed, got error: {err:?}");
                },
                other => panic!("Expected SpawnEpisode, got: {other:?}"),
            }
        }

        /// TCK-00352 BLOCKER 2 v3 integration: Invalid risk tier (255)
        /// must fail closed to Tier4 through the actual `SpawnEpisode` path.
        #[test]
        fn spawn_v1_invalid_risk_tier_fails_to_tier4_integration() {
            let v1_store = Arc::new(V1ManifestStore::new());
            let dispatcher = PrivilegedDispatcher::new().with_v1_manifest_store(v1_store.clone());
            let ctx = priv_ctx();

            let reviewer = reviewer_v0_manifest();
            let matching_baseline = ScopeBaseline {
                tools: reviewer.tool_allowlist.clone(),
                write_paths: reviewer.write_allowlist.clone(),
                shell_patterns: reviewer.shell_allowlist.clone(),
            };

            // TCK-00416: Use non-zero hashes (REQ-HEF-0013 mandates non-zero).
            let mut policy_resolution = test_policy_resolution_with_lineage(
                "W-V3-RISK-INV",
                "actor:test-reviewer",
                WorkRole::Reviewer,
                255,
            );
            policy_resolution.resolved_scope_baseline = Some(matching_baseline);
            let claim = WorkClaim {
                work_id: "W-V3-RISK-INV".to_string(),
                lease_id: "L-V3-004".to_string(),
                actor_id: "actor:test-reviewer".to_string(),
                role: WorkRole::Reviewer,
                policy_resolution,
                author_custody_domains: vec![],
                executor_custody_domains: vec![],
                permeability_receipt: None,
            };
            dispatcher.work_registry.register_claim(claim).unwrap();

            let request = SpawnEpisodeRequest {
                workspace_root: "/tmp".to_string(),
                work_id: "W-V3-RISK-INV".to_string(),
                role: WorkRole::Reviewer.into(),
                lease_id: Some("L-V3-004".to_string()),
                adapter_profile_hash: None,
                max_episodes: None,
                escalation_predicate: None,
                permeability_receipt_hash: None,
            };
            let frame = encode_spawn_episode_request(&request);
            let response = dispatcher.dispatch(&frame, &ctx).unwrap();

            match &response {
                PrivilegedResponse::SpawnEpisode(resp) => {
                    let v1 = v1_store
                        .get(&resp.session_id)
                        .expect("V1 manifest should be registered");
                    assert_eq!(
                        v1.risk_tier_ceiling(),
                        crate::episode::envelope::RiskTier::Tier4,
                        "Invalid risk tier must fail closed to Tier4"
                    );
                },
                PrivilegedResponse::Error(err) => {
                    // Also acceptable: minting fails due to Tier4 constraint
                    assert_eq!(
                        err.code,
                        PrivilegedErrorCode::CapabilityRequestRejected as i32,
                        "Error must be CapabilityRequestRejected: {err:?}"
                    );
                },
                other => panic!("Expected SpawnEpisode or Error, got: {other:?}"),
            }
        }
    }

    // =========================================================================
    // TCK-00399: build_harness_config tests
    // =========================================================================

    mod build_harness_config_tests {
        use apm2_core::fac::AgentAdapterProfileV1;
        use secrecy::ExposeSecret;

        use super::*;

        /// Creates a test profile by mutating the builtin `claude_code_profile`
        /// (which already passes all validation). We just override templates.
        fn test_profile_with_templates() -> AgentAdapterProfileV1 {
            let mut profile = apm2_core::fac::builtin_profiles::claude_code_profile();
            profile.args_template = vec![
                "--workspace".to_string(),
                "{workspace}".to_string(),
                "--prompt".to_string(),
                "{prompt}".to_string(),
            ];
            profile.env_template = vec![
                ("MY_WORKSPACE".to_string(), "{workspace}".to_string()),
                ("MY_PROMPT".to_string(), "{prompt}".to_string()),
            ];
            profile
        }

        #[test]
        fn test_template_expansion() {
            let profile = test_profile_with_templates();
            let token = secrecy::SecretString::from("test-token-123".to_string());

            let config = PrivilegedDispatcher::build_harness_config(
                &profile,
                "ep-001",
                "/home/user/workspace",
                "do something",
                "claude-code-v1",
                &token,
            )
            .expect("build_harness_config should succeed");

            // Check args were expanded
            assert_eq!(config.args[0], "--workspace");
            assert_eq!(config.args[1], "/home/user/workspace");
            assert_eq!(config.args[2], "--prompt");
            assert_eq!(config.args[3], "do something");

            // Check env was expanded
            assert_eq!(
                config.env.get("MY_WORKSPACE").unwrap().expose_secret(),
                "/home/user/workspace"
            );
            assert_eq!(
                config.env.get("MY_PROMPT").unwrap().expose_secret(),
                "do something"
            );

            // Check session token is in env (WVR-0002)
            assert_eq!(
                config
                    .env
                    .get("APM2_SESSION_TOKEN")
                    .unwrap()
                    .expose_secret(),
                "test-token-123"
            );
        }

        #[test]
        fn test_forbidden_env_rejected() {
            let mut profile = apm2_core::fac::builtin_profiles::claude_code_profile();
            profile.env_template = vec![("PATH".to_string(), "/malicious/path".to_string())];
            let token = secrecy::SecretString::from("token".to_string());

            let result = PrivilegedDispatcher::build_harness_config(
                &profile,
                "ep-001",
                "/workspace",
                "",
                "claude-code-v1",
                &token,
            );
            assert!(result.is_err());
            assert!(
                result.unwrap_err().contains("forbidden env var"),
                "Error should mention forbidden env var"
            );
        }

        #[test]
        fn test_ld_preload_rejected() {
            let mut profile = apm2_core::fac::builtin_profiles::claude_code_profile();
            profile.env_template = vec![("LD_PRELOAD".to_string(), "/evil.so".to_string())];
            let token = secrecy::SecretString::from("token".to_string());

            let result = PrivilegedDispatcher::build_harness_config(
                &profile,
                "ep-001",
                "/workspace",
                "",
                "claude-code-v1",
                &token,
            );
            assert!(result.is_err());
            assert!(result.unwrap_err().contains("LD_PRELOAD"));
        }

        #[test]
        fn test_session_token_in_argv_rejected() {
            let mut profile = apm2_core::fac::builtin_profiles::claude_code_profile();
            profile.args_template = vec!["--token".to_string(), "secret-token".to_string()];
            // Token value matches an argv value
            let token = secrecy::SecretString::from("secret-token".to_string());

            let result = PrivilegedDispatcher::build_harness_config(
                &profile,
                "ep-001",
                "/workspace",
                "",
                "claude-code-v1",
                &token,
            );
            assert!(result.is_err());
            assert!(
                result.unwrap_err().contains("session_token found in argv"),
                "Error should mention session_token in argv"
            );
        }

        #[test]
        fn test_model_template_expansion() {
            let mut profile = apm2_core::fac::builtin_profiles::claude_code_profile();
            profile.args_template = vec!["run".to_string(), "{model}".to_string()];
            profile.env_template = vec![];
            let token = secrecy::SecretString::from("token".to_string());

            let config = PrivilegedDispatcher::build_harness_config(
                &profile,
                "ep-001",
                "/workspace",
                "",
                "llama3",
                &token,
            )
            .expect("build_harness_config should succeed");

            assert_eq!(config.args[0], "run");
            assert_eq!(config.args[1], "llama3");
        }

        /// Known placeholder `{episode_id}` is NOT expanded by
        /// `build_harness_config` (only `{workspace}`, `{prompt}`, `{model}`
        /// are), so it must be rejected as unresolved.
        #[test]
        fn test_unresolved_known_placeholder_rejected() {
            let mut profile = apm2_core::fac::builtin_profiles::claude_code_profile();
            profile.args_template = vec!["run".to_string(), "{episode_id}".to_string()];
            profile.env_template = vec![];
            let token = secrecy::SecretString::from("token".to_string());

            let result = PrivilegedDispatcher::build_harness_config(
                &profile,
                "ep-001",
                "/workspace",
                "",
                "llama3",
                &token,
            );
            assert!(result.is_err());
            assert!(
                result
                    .unwrap_err()
                    .contains("unresolved template placeholder"),
                "Error should mention unresolved template placeholder"
            );
        }

        /// `{{workspace}}` IS expanded during template processing, so the
        /// result should succeed (no unresolved placeholder remains).
        #[test]
        fn test_expanded_known_placeholder_accepted() {
            let mut profile = apm2_core::fac::builtin_profiles::claude_code_profile();
            profile.args_template = vec!["run".to_string(), "prefix-{workspace}".to_string()];
            profile.env_template = vec![];
            let token = secrecy::SecretString::from("token".to_string());

            let result = PrivilegedDispatcher::build_harness_config(
                &profile,
                "ep-001",
                "/workspace",
                "",
                "llama3",
                &token,
            );
            assert!(
                result.is_ok(),
                "expanded known placeholder should be accepted, got error: {result:?}"
            );
        }

        /// Literal braces in args (e.g., JSON) must NOT be rejected.
        /// Only known placeholder tokens (`{workspace}`, `{prompt}`, etc.)
        /// should trigger the unresolved-placeholder guard.
        #[test]
        fn test_literal_braces_in_args_accepted() {
            let mut profile = apm2_core::fac::builtin_profiles::claude_code_profile();
            profile.args_template = vec![
                "--config".to_string(),
                r#"{"key": "value", "nested": {"a": 1}}"#.to_string(),
            ];
            profile.env_template = vec![];
            let token = secrecy::SecretString::from("token".to_string());

            let result = PrivilegedDispatcher::build_harness_config(
                &profile,
                "ep-001",
                "/workspace",
                "",
                "llama3",
                &token,
            );

            assert!(
                result.is_ok(),
                "literal braces in JSON args should be accepted, got error: {result:?}"
            );
        }

        /// Unknown placeholders (not in the known set) must be accepted.
        /// Only `{workspace}`, `{prompt}`, `{model}`, and `{episode_id}`
        /// are flagged.
        #[test]
        fn test_unknown_placeholder_not_in_known_set_accepted() {
            let mut profile = apm2_core::fac::builtin_profiles::claude_code_profile();
            profile.args_template = vec!["run".to_string(), "{custom_flag}".to_string()];
            profile.env_template = vec![];
            let token = secrecy::SecretString::from("token".to_string());

            let result = PrivilegedDispatcher::build_harness_config(
                &profile,
                "ep-001",
                "/workspace",
                "",
                "llama3",
                &token,
            );

            assert!(
                result.is_ok(),
                "unknown placeholder not in known set should be accepted, got error: {result:?}"
            );
        }
    }

    // =========================================================================
    // TCK-00373: Delegated spawn gate dispatcher-level tests
    // =========================================================================
    mod delegated_spawn_gate {
        use super::*;

        /// Helper: create a privileged connection context.
        fn priv_ctx() -> ConnectionContext {
            ConnectionContext::privileged_session_open(Some(PeerCredentials {
                uid: 1000,
                gid: 1000,
                pid: Some(12345),
            }))
        }

        /// Helper: build a valid permeability receipt for testing.
        ///
        /// The receipt's delegated authority is `(Med, ReadWrite,
        /// Capped(10M), Extend, Untrusted, Secret)` which satisfies
        /// tier-0 and tier-1 ceilings.
        fn build_test_receipt(
            delegate_actor_id: &str,
            policy_root_hash: [u8; 32],
        ) -> apm2_core::policy::permeability::PermeabilityReceipt {
            use apm2_core::policy::permeability::{
                AuthorityVector, BudgetLevel, CapabilityLevel, ClassificationLevel,
                PermeabilityReceiptBuilder, RiskLevel, StopPredicateLevel, TaintCeiling,
            };

            let parent = AuthorityVector::top();
            // Authority that satisfies tier-0 ceiling (Low, ReadOnly,
            // Capped(1M), Inherit, Attested, Confidential) and tier-1
            // ceiling (Med, ReadWrite, Capped(10M), Extend, Untrusted,
            // Secret):
            let overlay = AuthorityVector::new(
                RiskLevel::Med,
                CapabilityLevel::ReadWrite,
                BudgetLevel::Capped(10_000_000),
                StopPredicateLevel::Extend,
                TaintCeiling::Untrusted,
                ClassificationLevel::Secret,
            );
            PermeabilityReceiptBuilder::new("test-receipt-001", parent, overlay)
                .delegator_actor_id("root-delegator")
                .delegate_actor_id(delegate_actor_id)
                .issued_at_ms(1_000_000)
                .expires_at_ms(u64::MAX / 2) // far-future expiry for tests
                .policy_root_hash(policy_root_hash)
                .build()
                .unwrap()
        }

        /// TCK-00373 BLOCKER 1: Claim carries `permeability_receipt` but
        /// request omits `permeability_receipt_hash` => MUST reject.
        #[test]
        fn delegated_claim_missing_request_receipt_hash_rejected() {
            let dispatcher = PrivilegedDispatcher::new();
            let ctx = priv_ctx();

            let policy_hash = [0xAA; 32];
            let receipt = build_test_receipt("actor:delegated-user", policy_hash);

            let claim = WorkClaim {
                work_id: "W-DELEGATED-NO-HASH".to_string(),
                lease_id: "L-DELEGATED-001".to_string(),
                actor_id: "actor:delegated-user".to_string(),
                role: WorkRole::Implementer,
                policy_resolution: PolicyResolution {
                    policy_resolved_ref: "resolved-delegated".to_string(),
                    pcac_policy: None,
                    pointer_only_waiver: None,
                    resolved_policy_hash: policy_hash,
                    capability_manifest_hash: [0u8; 32],
                    context_pack_hash: [0u8; 32],
                    role_spec_hash: [0u8; 32],
                    context_pack_recipe_hash: [0u8; 32],
                    resolved_risk_tier: 0,
                    resolved_scope_baseline: None,
                    expected_adapter_profile_hash: None,
                },
                author_custody_domains: vec![],
                executor_custody_domains: vec![],
                permeability_receipt: Some(receipt),
            };
            dispatcher.work_registry.register_claim(claim).unwrap();

            // Request WITHOUT permeability_receipt_hash
            let request = SpawnEpisodeRequest {
                workspace_root: test_workspace_root(),
                work_id: "W-DELEGATED-NO-HASH".to_string(),
                role: WorkRole::Implementer.into(),
                lease_id: Some("L-DELEGATED-001".to_string()),
                adapter_profile_hash: None,
                max_episodes: None,
                escalation_predicate: None,
                permeability_receipt_hash: None, // Missing!
            };
            let frame = encode_spawn_episode_request(&request);
            let response = dispatcher.dispatch(&frame, &ctx).unwrap();

            match response {
                PrivilegedResponse::Error(err) => {
                    assert_eq!(
                        err.code,
                        PrivilegedErrorCode::CapabilityRequestRejected as i32,
                        "Expected CapabilityRequestRejected for missing receipt hash"
                    );
                    assert!(
                        err.message.contains("mandatory binding")
                            || err.message.contains("permeability_receipt_hash"),
                        "Error should mention mandatory binding or receipt hash: {}",
                        err.message
                    );
                },
                other => panic!(
                    "Expected rejection when claim has receipt but request omits hash, got: {other:?}"
                ),
            }
        }

        /// TCK-00373 BLOCKER 2: Context mismatch -- wrong `actor_id` in
        /// consumption context causes scope binding rejection.
        #[test]
        fn delegated_claim_actor_mismatch_rejected() {
            let dispatcher = PrivilegedDispatcher::new();
            let ctx = priv_ctx();

            let policy_hash = [0xBB; 32];
            // Receipt is bound to "actor:receipted-user" but claim has different actor_id
            let receipt = build_test_receipt("actor:receipted-user", policy_hash);
            let receipt_hash = receipt.content_hash().to_vec();

            let claim = WorkClaim {
                work_id: "W-DELEGATED-MISMATCH".to_string(),
                lease_id: "L-DELEGATED-002".to_string(),
                actor_id: "actor:different-user".to_string(), // Different from receipt!
                role: WorkRole::Implementer,
                policy_resolution: PolicyResolution {
                    policy_resolved_ref: "resolved-delegated-2".to_string(),
                    pcac_policy: None,
                    pointer_only_waiver: None,
                    resolved_policy_hash: policy_hash,
                    capability_manifest_hash: [0u8; 32],
                    context_pack_hash: [0u8; 32],
                    role_spec_hash: [0u8; 32],
                    context_pack_recipe_hash: [0u8; 32],
                    resolved_risk_tier: 0,
                    resolved_scope_baseline: None,
                    expected_adapter_profile_hash: None,
                },
                author_custody_domains: vec![],
                executor_custody_domains: vec![],
                permeability_receipt: Some(receipt),
            };
            dispatcher.work_registry.register_claim(claim).unwrap();

            let request = SpawnEpisodeRequest {
                workspace_root: test_workspace_root(),
                work_id: "W-DELEGATED-MISMATCH".to_string(),
                role: WorkRole::Implementer.into(),
                lease_id: Some("L-DELEGATED-002".to_string()),
                adapter_profile_hash: None,
                max_episodes: None,
                escalation_predicate: None,
                permeability_receipt_hash: Some(receipt_hash),
            };
            let frame = encode_spawn_episode_request(&request);
            let response = dispatcher.dispatch(&frame, &ctx).unwrap();

            match response {
                PrivilegedResponse::Error(err) => {
                    assert_eq!(
                        err.code,
                        PrivilegedErrorCode::CapabilityRequestRejected as i32,
                        "Expected CapabilityRequestRejected for actor mismatch"
                    );
                    assert!(
                        err.message.contains("consumption binding failed")
                            || err.message.contains("scope")
                            || err.message.contains("actor"),
                        "Error should mention consumption binding or scope or actor: {}",
                        err.message
                    );
                },
                other => panic!(
                    "Expected rejection for actor mismatch in delegated path, got: {other:?}"
                ),
            }
        }

        /// TCK-00373 MAJOR: Valid delegated claim with matching receipt hash,
        /// correct actor, and correct policy root => MUST accept (pass
        /// through to subsequent validation stages).
        #[test]
        fn delegated_claim_valid_receipt_accepted() {
            let dispatcher = PrivilegedDispatcher::new();
            let ctx = priv_ctx();

            let policy_hash = blake3::hash(
                format!("policy:{}:{}", "W-DELEGATED-VALID", "actor:valid-delegate").as_bytes(),
            );
            let policy_hash_bytes: [u8; 32] = *policy_hash.as_bytes();
            let receipt = build_test_receipt("actor:valid-delegate", policy_hash_bytes);
            let receipt_hash = receipt.content_hash().to_vec();

            let claim = WorkClaim {
                work_id: "W-DELEGATED-VALID".to_string(),
                lease_id: "L-DELEGATED-003".to_string(),
                actor_id: "actor:valid-delegate".to_string(),
                role: WorkRole::Implementer,
                policy_resolution: PolicyResolution {
                    policy_resolved_ref: "resolved-delegated-3".to_string(),
                    pcac_policy: None,
                    pointer_only_waiver: None,
                    resolved_policy_hash: policy_hash_bytes,
                    capability_manifest_hash: [0u8; 32],
                    context_pack_hash: [0u8; 32],
                    role_spec_hash: [0u8; 32],
                    context_pack_recipe_hash: [0u8; 32],
                    resolved_risk_tier: 0,
                    resolved_scope_baseline: None,
                    expected_adapter_profile_hash: None,
                },
                author_custody_domains: vec![],
                executor_custody_domains: vec![],
                permeability_receipt: Some(receipt),
            };
            dispatcher.work_registry.register_claim(claim).unwrap();

            let request = SpawnEpisodeRequest {
                workspace_root: test_workspace_root(),
                work_id: "W-DELEGATED-VALID".to_string(),
                role: WorkRole::Implementer.into(),
                lease_id: Some("L-DELEGATED-003".to_string()),
                adapter_profile_hash: None,
                max_episodes: None,
                escalation_predicate: None,
                permeability_receipt_hash: Some(receipt_hash),
            };
            let frame = encode_spawn_episode_request(&request);
            let response = dispatcher.dispatch(&frame, &ctx).unwrap();

            // The delegated gate should pass; the response may still be an
            // error from a LATER validation stage (e.g., scope baseline
            // validation, V1 manifest check, etc.) but NOT from the
            // permeability gate. We verify it is NOT a permeability error.
            match response {
                PrivilegedResponse::Error(err) => {
                    assert!(
                        !err.message.contains("permeability"),
                        "Valid delegated receipt should not be rejected by permeability gate, \
                         but got permeability error: {}",
                        err.message
                    );
                    // It's OK to get a later-stage error (e.g., scope
                    // baseline), confirming the delegated
                    // gate passed.
                },
                PrivilegedResponse::SpawnEpisode(_) => {
                    // Success is also acceptable.
                },
                other => {
                    // Any non-error, non-spawn response is unexpected.
                    panic!("Unexpected response type: {other:?}");
                },
            }
        }

        /// Helper: build a low-authority permeability receipt for testing.
        ///
        /// The receipt's delegated authority is minimal (Low/ReadOnly/Public).
        /// Used by the authority-insufficient rejection test.
        fn build_low_authority_receipt(
            delegate_actor_id: &str,
            policy_root_hash: [u8; 32],
        ) -> apm2_core::policy::permeability::PermeabilityReceipt {
            use apm2_core::policy::permeability::{
                AuthorityVector, BudgetLevel, CapabilityLevel, ClassificationLevel,
                PermeabilityReceiptBuilder, RiskLevel, StopPredicateLevel, TaintCeiling,
            };

            let parent = AuthorityVector::new(
                RiskLevel::Low,
                CapabilityLevel::ReadOnly,
                BudgetLevel::Capped(100),
                StopPredicateLevel::Inherit,
                TaintCeiling::Attested,
                ClassificationLevel::Public,
            );
            let overlay = parent; // overlay == parent => delegated == parent
            PermeabilityReceiptBuilder::new("test-receipt-low-auth", parent, overlay)
                .delegator_actor_id("root-delegator")
                .delegate_actor_id(delegate_actor_id)
                .issued_at_ms(1_000_000)
                .expires_at_ms(u64::MAX / 2)
                .policy_root_hash(policy_root_hash)
                .build()
                .unwrap()
        }

        /// TCK-00373 BLOCKER 1: Low-authority receipt used with a
        /// high-risk-tier spawn MUST be rejected.
        ///
        /// The receipt carries `(Low, ReadOnly, Capped(100), ...)` but the
        /// spawn's risk tier 3 demands `(High, Full, Unlimited, ...)`.
        /// Because `required_authority = ceiling` and the receipt's delegated
        /// authority is NOT a superset of the ceiling, the
        /// `validate_consumption_binding` subset check rejects the spawn.
        #[test]
        fn low_authority_receipt_rejected_for_high_risk_tier_spawn() {
            let dispatcher = PrivilegedDispatcher::new();
            let ctx = priv_ctx();

            let policy_hash = [0xEE; 32];
            let receipt = build_low_authority_receipt("actor:low-auth-user", policy_hash);
            let receipt_hash = receipt.content_hash().to_vec();

            let claim = WorkClaim {
                work_id: "W-DELEGATED-LOWAUTH".to_string(),
                lease_id: "L-DELEGATED-LOWAUTH".to_string(),
                actor_id: "actor:low-auth-user".to_string(),
                role: WorkRole::Implementer,
                policy_resolution: PolicyResolution {
                    policy_resolved_ref: "resolved-low-auth".to_string(),
                    pcac_policy: None,
                    pointer_only_waiver: None,
                    resolved_policy_hash: policy_hash,
                    capability_manifest_hash: [0u8; 32],
                    context_pack_hash: [0u8; 32],
                    role_spec_hash: [0u8; 32],
                    context_pack_recipe_hash: [0u8; 32],
                    resolved_risk_tier: 3, // High risk tier
                    resolved_scope_baseline: None,
                    expected_adapter_profile_hash: None,
                },
                author_custody_domains: vec![],
                executor_custody_domains: vec![],
                permeability_receipt: Some(receipt),
            };
            dispatcher.work_registry.register_claim(claim).unwrap();

            let request = SpawnEpisodeRequest {
                workspace_root: test_workspace_root(),
                work_id: "W-DELEGATED-LOWAUTH".to_string(),
                role: WorkRole::Implementer.into(),
                lease_id: Some("L-DELEGATED-LOWAUTH".to_string()),
                adapter_profile_hash: None,
                max_episodes: None,
                escalation_predicate: None,
                permeability_receipt_hash: Some(receipt_hash),
            };
            let frame = encode_spawn_episode_request(&request);
            let response = dispatcher.dispatch(&frame, &ctx).unwrap();

            match response {
                PrivilegedResponse::Error(err) => {
                    assert_eq!(
                        err.code,
                        PrivilegedErrorCode::CapabilityRequestRejected as i32,
                        "Expected CapabilityRequestRejected for insufficient authority"
                    );
                    assert!(
                        err.message.contains("consumption binding failed")
                            || err.message.contains("DelegationWidening")
                            || err.message.contains("authority"),
                        "Error should mention consumption binding failure or authority: {}",
                        err.message
                    );
                },
                other => panic!(
                    "Expected rejection for low-authority receipt with high risk tier, got: {other:?}"
                ),
            }
        }

        /// TCK-00373: Policy root mismatch between receipt and claim's
        /// resolved policy hash => MUST reject.
        #[test]
        fn delegated_claim_policy_root_mismatch_rejected() {
            let dispatcher = PrivilegedDispatcher::new();
            let ctx = priv_ctx();

            // Receipt bound to policy_hash_a, claim has policy_hash_b
            let policy_hash_a = [0xCC; 32];
            let policy_hash_b = [0xDD; 32];
            let receipt = build_test_receipt("actor:policy-test", policy_hash_a);
            let receipt_hash = receipt.content_hash().to_vec();

            let claim = WorkClaim {
                work_id: "W-DELEGATED-POLICY".to_string(),
                lease_id: "L-DELEGATED-004".to_string(),
                actor_id: "actor:policy-test".to_string(),
                role: WorkRole::Implementer,
                policy_resolution: PolicyResolution {
                    policy_resolved_ref: "resolved-delegated-4".to_string(),
                    pcac_policy: None,
                    pointer_only_waiver: None,
                    resolved_policy_hash: policy_hash_b, // Different from receipt!
                    capability_manifest_hash: [0u8; 32],
                    context_pack_hash: [0u8; 32],
                    role_spec_hash: [0u8; 32],
                    context_pack_recipe_hash: [0u8; 32],
                    resolved_risk_tier: 0,
                    resolved_scope_baseline: None,
                    expected_adapter_profile_hash: None,
                },
                author_custody_domains: vec![],
                executor_custody_domains: vec![],
                permeability_receipt: Some(receipt),
            };
            dispatcher.work_registry.register_claim(claim).unwrap();

            let request = SpawnEpisodeRequest {
                workspace_root: test_workspace_root(),
                work_id: "W-DELEGATED-POLICY".to_string(),
                role: WorkRole::Implementer.into(),
                lease_id: Some("L-DELEGATED-004".to_string()),
                adapter_profile_hash: None,
                max_episodes: None,
                escalation_predicate: None,
                permeability_receipt_hash: Some(receipt_hash),
            };
            let frame = encode_spawn_episode_request(&request);
            let response = dispatcher.dispatch(&frame, &ctx).unwrap();

            match response {
                PrivilegedResponse::Error(err) => {
                    assert_eq!(
                        err.code,
                        PrivilegedErrorCode::CapabilityRequestRejected as i32,
                        "Expected CapabilityRequestRejected for policy root mismatch"
                    );
                    assert!(
                        err.message.contains("consumption binding failed")
                            || err.message.contains("policy")
                            || err.message.contains("PolicyRootMismatch"),
                        "Error should mention consumption binding or policy: {}",
                        err.message
                    );
                },
                other => panic!(
                    "Expected rejection for policy root mismatch in delegated path, got: {other:?}"
                ),
            }
        }

        /// TCK-00373 MAJOR 1: End-to-end claim->spawn success using
        /// `PermeabilityReceiptResolver` wiring.
        ///
        /// A dynamic resolver builds a valid receipt at claim-time using
        /// the actual `resolved_policy_hash` and `actor_id` provided by
        /// the claim handler. The subsequent `SpawnEpisode` carries the
        /// receipt hash and the delegated gate passes. This proves the
        /// production path is wired (non-dead code) and that claim-level
        /// receipt binding flows into the spawn gate.
        #[test]
        fn claim_to_spawn_delegated_success_via_resolver() {
            use std::sync::{Arc, Mutex};

            use apm2_core::policy::permeability::{
                AuthorityVector, BudgetLevel, CapabilityLevel, ClassificationLevel,
                PermeabilityReceiptBuilder, RiskLevel, StopPredicateLevel, TaintCeiling,
            };

            // Dynamic resolver: builds a receipt at resolve time using
            // the actual policy hash from the governance stub. This avoids
            // the chicken-and-egg problem of needing the policy hash before
            // ClaimWork generates it.
            struct DynamicReceiptResolver {
                /// Capture the built receipt so the test can read the hash.
                built_receipt: Mutex<Option<apm2_core::policy::permeability::PermeabilityReceipt>>,
            }

            impl PermeabilityReceiptResolver for DynamicReceiptResolver {
                fn resolve_receipt(
                    &self,
                    actor_id: &str,
                    _work_id: &str,
                    resolved_policy_hash: &[u8; 32],
                ) -> Result<Option<apm2_core::policy::permeability::PermeabilityReceipt>, String>
                {
                    let parent = AuthorityVector::top();
                    // Authority that satisfies tier-0 ceiling.
                    let overlay = AuthorityVector::new(
                        RiskLevel::Low,
                        CapabilityLevel::ReadOnly,
                        BudgetLevel::Capped(1_000_000),
                        StopPredicateLevel::Inherit,
                        TaintCeiling::Attested,
                        ClassificationLevel::Confidential,
                    );
                    let receipt =
                        PermeabilityReceiptBuilder::new("resolver-e2e-receipt", parent, overlay)
                            .delegator_actor_id("root-delegator")
                            .delegate_actor_id(actor_id)
                            .issued_at_ms(1_000_000)
                            .expires_at_ms(u64::MAX / 2)
                            .policy_root_hash(*resolved_policy_hash)
                            .build()
                            .map_err(|e| format!("receipt build failed: {e}"))?;

                    *self.built_receipt.lock().unwrap() = Some(receipt.clone());
                    Ok(Some(receipt))
                }
            }

            let resolver = Arc::new(DynamicReceiptResolver {
                built_receipt: Mutex::new(None),
            });

            let resolver_dyn: Arc<dyn PermeabilityReceiptResolver> = resolver;
            let dispatcher = PrivilegedDispatcher::new()
                .with_permeability_receipt_resolver(Arc::clone(&resolver_dyn));
            let ctx = priv_ctx();

            // Step 1: ClaimWork -- resolver dynamically builds and attaches receipt
            let claim_request = ClaimWorkRequest {
                actor_id: "delegated-actor".to_string(),
                role: WorkRole::Implementer.into(),
                credential_signature: vec![1, 2, 3],
                nonce: vec![4, 5, 6],
            };
            let claim_frame = encode_claim_work_request(&claim_request);
            let claim_response = dispatcher.dispatch(&claim_frame, &ctx).unwrap();

            let (work_id, lease_id) = match claim_response {
                PrivilegedResponse::ClaimWork(resp) => (resp.work_id, resp.lease_id),
                other => panic!("Expected ClaimWork response, got: {other:?}"),
            };

            // Verify the claim now carries the receipt
            let stored_claim = dispatcher.work_registry.get_claim(&work_id).unwrap();
            assert!(
                stored_claim.permeability_receipt.is_some(),
                "Claim MUST carry permeability_receipt after resolver wiring"
            );

            // Get the receipt hash from the stored claim for the spawn request
            let receipt_hash_vec = stored_claim
                .permeability_receipt
                .as_ref()
                .unwrap()
                .content_hash()
                .to_vec();

            // Step 2: SpawnEpisode with receipt hash
            let spawn_request = SpawnEpisodeRequest {
                workspace_root: test_workspace_root(),
                work_id,
                role: WorkRole::Implementer.into(),
                lease_id: Some(lease_id),
                adapter_profile_hash: None,
                max_episodes: None,
                escalation_predicate: None,
                permeability_receipt_hash: Some(receipt_hash_vec),
            };
            let spawn_frame = encode_spawn_episode_request(&spawn_request);
            let spawn_response = dispatcher.dispatch(&spawn_frame, &ctx).unwrap();

            // The delegated gate should pass. The response may be a later-stage
            // error (e.g., scope baseline) but NOT a permeability rejection.
            match spawn_response {
                PrivilegedResponse::Error(err) => {
                    assert!(
                        !err.message.contains("permeability"),
                        "Delegated gate should pass for valid resolver-wired claim, \
                         but got permeability error: {}",
                        err.message
                    );
                },
                PrivilegedResponse::SpawnEpisode(_) => {
                    // Full success is also acceptable.
                },
                other => {
                    panic!("Unexpected response type: {other:?}");
                },
            }
        }

        /// TCK-00373 MAJOR 1 (failure path): When the receipt resolver
        /// returns an error during `ClaimWork`, the claim MUST be rejected
        /// (fail-closed).
        #[test]
        fn claim_rejected_when_receipt_resolver_fails() {
            use std::sync::Arc;

            struct FailingReceiptResolver;
            impl PermeabilityReceiptResolver for FailingReceiptResolver {
                fn resolve_receipt(
                    &self,
                    _actor_id: &str,
                    _work_id: &str,
                    _resolved_policy_hash: &[u8; 32],
                ) -> Result<Option<apm2_core::policy::permeability::PermeabilityReceipt>, String>
                {
                    Err("governance store unavailable (simulated)".to_string())
                }
            }

            let resolver = Arc::new(FailingReceiptResolver);
            let dispatcher =
                PrivilegedDispatcher::new().with_permeability_receipt_resolver(resolver);
            let ctx = priv_ctx();

            let claim_request = ClaimWorkRequest {
                actor_id: "test-actor-fail".to_string(),
                role: WorkRole::Implementer.into(),
                credential_signature: vec![1, 2, 3],
                nonce: vec![4, 5, 6],
            };
            let claim_frame = encode_claim_work_request(&claim_request);
            let claim_response = dispatcher.dispatch(&claim_frame, &ctx).unwrap();

            match claim_response {
                PrivilegedResponse::Error(err) => {
                    assert_eq!(
                        err.code,
                        PrivilegedErrorCode::CapabilityRequestRejected as i32,
                        "Expected CapabilityRequestRejected for resolver failure"
                    );
                    assert!(
                        err.message
                            .contains("permeability receipt resolution failed")
                            || err.message.contains("governance store unavailable"),
                        "Error should mention resolution failure: {}",
                        err.message
                    );
                },
                other => panic!("Expected claim rejection when resolver fails, got: {other:?}"),
            }
        }

        /// TCK-00373 BLOCKER 1: Invalid risk tier (out of range 0-4) MUST
        /// be rejected fail-closed, not mapped to a permissive ceiling.
        #[test]
        fn invalid_risk_tier_rejected_fail_closed() {
            let dispatcher = PrivilegedDispatcher::new();
            let ctx = priv_ctx();

            let policy_hash = [0xFF; 32];
            let receipt = build_test_receipt("actor:invalid-tier", policy_hash);
            let receipt_hash = receipt.content_hash().to_vec();

            let claim = WorkClaim {
                work_id: "W-DELEGATED-BADTIER".to_string(),
                lease_id: "L-DELEGATED-BADTIER".to_string(),
                actor_id: "actor:invalid-tier".to_string(),
                role: WorkRole::Implementer,
                policy_resolution: PolicyResolution {
                    policy_resolved_ref: "resolved-bad-tier".to_string(),
                    pcac_policy: None,
                    pointer_only_waiver: None,
                    resolved_policy_hash: policy_hash,
                    capability_manifest_hash: [0u8; 32],
                    context_pack_hash: [0u8; 32],
                    role_spec_hash: [0u8; 32],
                    context_pack_recipe_hash: [0u8; 32],
                    resolved_risk_tier: 255, // Invalid!
                    resolved_scope_baseline: None,
                    expected_adapter_profile_hash: None,
                },
                author_custody_domains: vec![],
                executor_custody_domains: vec![],
                permeability_receipt: Some(receipt),
            };
            dispatcher.work_registry.register_claim(claim).unwrap();

            let request = SpawnEpisodeRequest {
                workspace_root: test_workspace_root(),
                work_id: "W-DELEGATED-BADTIER".to_string(),
                role: WorkRole::Implementer.into(),
                lease_id: Some("L-DELEGATED-BADTIER".to_string()),
                adapter_profile_hash: None,
                max_episodes: None,
                escalation_predicate: None,
                permeability_receipt_hash: Some(receipt_hash),
            };
            let frame = encode_spawn_episode_request(&request);
            let response = dispatcher.dispatch(&frame, &ctx).unwrap();

            match response {
                PrivilegedResponse::Error(err) => {
                    assert_eq!(
                        err.code,
                        PrivilegedErrorCode::CapabilityRequestRejected as i32,
                        "Expected CapabilityRequestRejected for invalid risk tier"
                    );
                    assert!(
                        err.message.contains("invalid resolved_risk_tier")
                            || err.message.contains("fail-closed deny"),
                        "Error should mention invalid risk tier: {}",
                        err.message
                    );
                },
                other => panic!("Expected rejection for invalid risk tier, got: {other:?}"),
            }
        }
    }
}
