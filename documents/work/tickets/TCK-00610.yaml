ticket_meta:
  schema_version: "2026-01-29"
  template_version: "2026-01-29"
  ticket:
    id: "TCK-00610"
    title: "Fix stale-verdict approve bypass, clear verdicts on restart, and repair worktree-less auto-merge"
    status: "OPEN"
  binds:
    prd_id: "PRD-PLACEHOLDER"
    rfc_id: "RFC-0019"
    requirements: []
    evidence_artifacts: []
  custody:
    agent_roles: ["AGENT_IMPLEMENTER"]
    responsibility_domains: ["DOMAIN_RUNTIME"]
  dependencies:
    tickets:
      - ticket_id: "TCK-00609"
        reason: "TCK-00609 introduced the `approve` action in build_recommended_action() that trusts all_verdicts_approve without cross-checking finding counts. The approve action is the immediate vector for the stale-verdict bypass."
      - ticket_id: "TCK-00605"
        reason: "TCK-00605 introduced build_recommended_action, auto-merge via maybe_auto_merge_if_ready, and try_fast_forward_main — all involved in this incident."
  root_cause_analysis:
    summary: |
      INCIDENT: PR #692 merged to main with 2 MAJOR security findings
      (worker heartbeat starvation, containment bypass) because stale
      verdicts from a previous review round persisted across a review
      restart, causing the TCK-00609 `approve` action to fire before
      the `dispatch_implementor` check could catch the findings.

      Three defects compound to create the bypass:

      DEFECT 1: APPROVE ACTION TRUSTS STALE VERDICTS (TCK-00609 regression)

      `build_recommended_action()` at
      `crates/apm2-cli/src/commands/fac_review/mod.rs:2527-2534`:

          if input.merge_readiness.all_verdicts_approve {
              return DoctorRecommendedAction {
                  action: "approve".to_string(), ...
              };
          }

      The `approve` action fires when `all_verdicts_approve=true`,
      which is checked BEFORE the `dispatch_implementor` branch at
      line 2554. The dispatch_implementor branch cross-checks
      `counts.blocker > 0 || counts.major > 0`, but `approve`
      short-circuits before reaching it.

      Before TCK-00609, there was no `approve` action. The only
      action that consumed `all_verdicts_approve` was `merge`, which
      required the full `merge_ready` conjunction (gates_pass +
      sha_fresh + no_merge_conflicts) — a much higher bar that
      rarely triggered on stale data.

      The `approve` action fires on `all_verdicts_approve` alone.
      It does not check:
        - Whether finding counts have blocker/major (inconsistency guard)
        - Whether active reviewer agents are running (freshness guard)

      DEFECT 2: STALE VERDICTS PERSIST ACROSS REVIEW RESTARTS (pre-existing)

      Two data stores retain old verdicts when reviews restart:

      (a) Lifecycle record.verdicts BTreeMap:
          `lifecycle.rs:1476-1489` — the reduce_event handler for
          VerdictSet inserts into record.verdicts at line 1483.
          But ReviewsDispatched (the restart event) hits the
          `_ => {}` catch-all at line 1488, which does NOT clear
          record.verdicts. Old verdicts from previous rounds survive.

          Impact: When the first dimension of a new round sets its
          verdict, `next_state_for_event()` at lines 2347-2366
          checks BOTH dimensions. If the other dimension still has
          "approve" from the previous round (because its new verdict
          hasn't been set yet), the state machine transitions to
          MergeReady prematurely.

      (b) Verdict projection record.dimensions BTreeMap:
          `verdict_projection.rs:437-465` —
          `persist_verdict_projection_impl()` does a read-modify-write:
          loads existing record (which has ALL previous dimension
          entries), inserts the new dimension verdict, saves. Old
          dimension entries are NOT cleared on restart.

          Impact: Between the first and second dimension of a new
          round, the projection shows one fresh verdict + one stale
          verdict. `build_doctor_merge_readiness()` at mod.rs:2339-2344
          computes `all_verdicts_approve` from the projection, seeing
          stale approve for the dimension that hasn't been re-reviewed
          yet.

      The stale-verdict window:
          T0: Round 1 completes — security=approve, code-quality=approve
          T1: Reviews restart for same SHA (apm2 fac restart)
              Projection still has: {security: approve, code-quality: approve}
              Lifecycle verdicts still have: {security: approve, code-quality: approve}
          T2: Code-quality round 2 sets approve
              Projection: {security: approve (STALE!), code-quality: approve}
              Lifecycle: both approve → MergeReady → auto-merge fires
              Doctor: all_verdicts_approve=true → action="approve"
          T3: Security round 2 sets deny (e.g., "prepare failed")
              Too late — orchestrator already saw "approve" at T2

      DEFECT 3: AUTO-MERGE BROKEN WHEN NO WORKTREE HAS MAIN (pre-existing)

      `try_fast_forward_main()` at lifecycle.rs:1908-1971 calls
      `resolve_main_worktree()` which parses `git worktree list
      --porcelain` looking for a worktree with
      `branch refs/heads/main`. If no worktree has main checked out,
      it returns Err and auto-merge fails with a MergeFailed
      lifecycle event.

      The primary worktree `/home/ubuntu/Projects/apm2` currently
      has `ticket/RFC-0019/TCK-00542` checked out (confirmed via
      `git -C /home/ubuntu/Projects/apm2 symbolic-ref HEAD`). No
      worktree has main. Auto-merge is dead.

      The function uses `git merge --ff-only` which REQUIRES main to
      be checked out. But with 11+ worktrees, the primary worktree
      is routinely repurposed for ticket work. The function should
      use `git update-ref refs/heads/main <sha>` instead, which
      updates the branch ref without requiring main to be checked out.

      This defect also causes "local main keeps getting stale" — no
      mechanism syncs local main with origin/main when auto-merge
      can't fire.

      INCIDENT TIMELINE FOR PR #692:

      1. Round 1 reviews ran for SHA 693adf4a — security likely
         approved (findings were NITs only in earlier rounds)
      2. Reviews restarted (new round with updated review inputs)
      3. Code-quality round 2 approved at 02:26:39Z
         - Projection: security=approve (stale from round 1),
           code-quality=approve (fresh)
         - Lifecycle: MergeReady (both verdicts "approve")
         - Auto-merge attempted but FAILED (no main worktree)
         - Doctor returned action="approve"
         - GitHub CI check updated to green (projection shows approve)
      4. PR #692 merged on GitHub (merge commit 71c05c93)
      5. Security round 2 finished at 02:29:23Z with deny
         ("prepare failed: no review inputs available")
         — 2 MAJOR findings (heartbeat starvation, containment bypass)
      6. But PR already merged to main with unresolved MAJOR findings

scope:
  in_scope:
    - id: S1_GUARD_APPROVE_AGAINST_FINDINGS
      title: "Guard approve action against inconsistent finding counts and active reviewers"
      detail: |
        In `build_recommended_action()` at
        `crates/apm2-cli/src/commands/fac_review/mod.rs:2527-2534`:

        The `approve` action currently fires on `all_verdicts_approve`
        alone. Add two guards:

        1. FINDINGS GUARD: Compute `has_actionable_findings` (any
           dimension with blocker > 0 or major > 0) BEFORE the approve
           check. Only emit approve if `!has_actionable_findings`.

        2. ACTIVE AGENTS GUARD: Only emit approve if no active reviewer
           agents are running (`active_agents == 0`). If reviewers are
           still active, verdicts may not be final.

        Move the `requires_implementor_remediation` and
        `all_verdicts_resolved` computations above the approve check:

            let has_actionable_findings = input.findings_summary.iter().any(|entry| {
                entry.counts.blocker > 0 || entry.counts.major > 0
            });
            if input.merge_readiness.all_verdicts_approve
                && !has_actionable_findings
                && input.agent_activity.active_agents == 0
            {
                return DoctorRecommendedAction {
                    action: "approve".to_string(), ...
                };
            }

        This ensures:
          - Stale approve + current MAJOR findings → dispatch_implementor
          - Active reviewers (verdict not final) → wait
          - Genuine all-approve with no actionable findings → approve

        Acceptance:
          - Projection shows all approve BUT findings have 2 MAJOR →
            doctor returns "dispatch_implementor", NOT "approve".
          - All approve, zero MAJOR/BLOCKER, zero active agents →
            doctor returns "approve".
          - All approve, zero findings, but 1 active reviewer agent →
            doctor returns "wait".

    - id: S2_CLEAR_VERDICTS_ON_RESTART_LIFECYCLE
      title: "Clear lifecycle verdicts BTreeMap when ReviewsDispatched event fires"
      detail: |
        In `lifecycle.rs:1476-1489`, the reduce_event handler for
        ReviewsDispatched currently falls through to `_ => {}`:

            match event {
                LifecycleEventKind::VerdictSet { ... } => {
                    record.verdicts.insert(dim, dec);
                },
                _ => {},
            }

        Change to explicitly clear verdicts on ReviewsDispatched:

            LifecycleEventKind::ReviewsDispatched => {
                record.verdicts.clear();
            },
            _ => {},

        This prevents stale verdicts from a previous round from
        persisting into the next round. When the first dimension of
        the new round sets its verdict, the other dimension will have
        no entry (not stale "approve"), so `next_state_for_event()`
        will correctly return VerdictPending instead of MergeReady.

        The `verdicts.clear()` is safe because:
          - ReviewsDispatched means NEW reviews are starting
          - Old verdicts are from a PREVIOUS round (possibly stale)
          - The new round will set fresh verdicts as reviewers complete
          - The "deny is sticky" safety check in
            persist_verdict_projection (line 473-481) already prevents
            approve from overwriting deny within a single round

        Acceptance:
          - After `apm2 fac restart --pr <N>`, lifecycle record.verdicts
            is empty.
          - Security approve (round 1) does NOT carry into round 2.
          - First dimension of round 2 setting approve → lifecycle state
            is VerdictPending (not MergeReady), because the other
            dimension has no verdict yet.

    - id: S3_CLEAR_VERDICTS_ON_RESTART_PROJECTION
      title: "Clear verdict projection dimensions when reviews restart for same SHA"
      detail: |
        In `verdict_projection.rs`, add a function to clear dimension
        entries from the projection record for a given SHA:

            pub fn clear_dimension_verdicts_for_sha(
                owner_repo: &str,
                pr_number: u32,
                head_sha: &str,
            ) -> Result<(), String>

        This function:
          1. Acquires the projection lock
          2. Loads the projection record for the SHA
          3. Clears record.dimensions (BTreeMap::clear())
          4. Saves the record

        Call this from the restart flow — wherever `apm2 fac restart
        --pr <N>` dispatches new reviews, BEFORE the ReviewsDispatched
        lifecycle event. This ensures the projection is clean before
        new verdicts arrive.

        The clear must happen under the projection lock to prevent
        a concurrent verdict-set from racing with the clear.

        Acceptance:
          - After `apm2 fac restart --pr <N>`, projection for the SHA
            shows both dimensions as "pending" (no stale entries).
          - Doctor polled between restart and first new verdict shows
            all_verdicts_approve=false.
          - No stale-verdict window exists between restart and new
            verdicts.

    - id: S4_WORKTREE_LESS_AUTO_MERGE
      title: "Replace git merge --ff-only with git update-ref for auto-merge"
      detail: |
        `try_fast_forward_main()` at lifecycle.rs:1908-1971 currently:
          1. Finds a worktree with main checked out (resolve_main_worktree)
          2. Runs git merge --ff-only in that worktree

        This fails when no worktree has main checked out. Replace with
        `git update-ref`:

            fn try_fast_forward_main(
                current_dir: &Path,
                branch: &str,
                expected_sha: &str,
            ) -> Result<(), String> {
                // 1. Verify main ref exists
                let main_sha = git_stdout_checked(
                    current_dir,
                    &["rev-parse", "--verify", "refs/heads/main"],
                )?;

                // 2. Verify branch head matches expected
                let branch_sha = git_stdout_checked(
                    current_dir,
                    &["rev-parse", "--verify", &format!("refs/heads/{branch}")],
                )?;
                if !branch_sha.eq_ignore_ascii_case(expected_sha) {
                    return Err(...);
                }

                // 3. Verify fast-forward (main is ancestor of branch)
                git_run_checked(current_dir, &[
                    "merge-base", "--is-ancestor",
                    "refs/heads/main", &format!("refs/heads/{branch}")
                ])?;

                // 4. Update main ref atomically (no checkout needed)
                git_run_checked(current_dir, &[
                    "update-ref", "refs/heads/main", &branch_sha, &main_sha
                ])?;

                Ok(())
            }

        `git update-ref` is atomic and does NOT require the branch
        to be checked out. The old-value argument (main_sha) ensures
        no concurrent update races.

        Remove `resolve_main_worktree()` entirely (dead code after
        this change).

        If a worktree DOES have main checked out, the working tree
        will be stale (HEAD updated but working tree not). Add a
        best-effort `git -C <main_worktree> reset --hard HEAD`
        if a main worktree exists, to sync its working tree.

        Acceptance:
          - Auto-merge succeeds even when no worktree has main
            checked out.
          - `git log main` shows the merged commit after auto-merge.
          - Concurrent update-ref calls are safe (old-value guard).
          - If a main worktree exists, its working tree is synced.

    - id: S5_SYNC_LOCAL_MAIN_WITH_REMOTE
      title: "Add remote main sync to doctor and post-merge flow"
      detail: |
        Local main falls behind origin/main when merges happen on
        GitHub (PR merge button) rather than via local auto-merge.
        Two sync points:

        1. POST-MERGE: After successful auto-merge (in
           maybe_auto_merge_if_ready_inner, after the update-ref),
           push local main to origin:

               git push origin refs/heads/main:refs/heads/main

           This keeps origin/main in sync with local auto-merges.

        2. PRE-MERGE CHECK: Before attempting auto-merge, fetch
           remote main and verify local main is not behind:

               git fetch origin refs/heads/main:refs/heads/main

           If main is already checked out in a worktree, use:

               git fetch origin main
               git update-ref refs/heads/main origin/main

           This ensures local main is current before the ff-only
           check, preventing "non-fast-forward" failures when
           origin/main has diverged.

        3. DOCTOR SYNC: In run_doctor_inner(), if local main is
           behind origin/main (detectable by comparing refs),
           emit a health item:

               severity: "medium"
               message: "local main is N commits behind origin/main"
               remediation: "run `git fetch origin main:main`"

        Acceptance:
          - After local auto-merge, origin/main is updated.
          - If origin/main has commits local main doesn't, fetch
            syncs them before auto-merge attempt.
          - Doctor warns when local main is stale.

    - id: S6_UPDATE_EXISTING_TESTS
      title: "Update existing approve and merge-readiness tests"
      detail: |
        1. Update any test that asserts approve action fires on
           all_verdicts_approve alone — must now also verify zero
           actionable findings and zero active agents.

        2. Add lifecycle reducer test: ReviewsDispatched after
           VerdictSet clears record.verdicts.

        3. Add merge-readiness test: stale security=approve in
           projection + fresh code-quality=approve → after restart →
           all_verdicts_approve is false.

        Acceptance:
          - All existing tests updated and passing.
          - No test asserts approve fires with MAJOR findings present.

    - id: S7_ADD_NEW_TESTS
      title: "Add tests for stale-verdict prevention and worktree-less merge"
      detail: |
        New tests for approve guard:
        1. test_approve_blocked_when_findings_have_major:
           all_verdicts_approve=true, counts.major=2 → action != "approve"
        2. test_approve_blocked_when_active_agents_running:
           all_verdicts_approve=true, active_agents=1 → action != "approve"
        3. test_approve_fires_when_clean:
           all_verdicts_approve=true, no findings, no agents → action="approve"

        New tests for lifecycle verdict clearing:
        4. test_reviews_dispatched_clears_verdicts:
           VerdictSet(security=approve) → ReviewsDispatched →
           record.verdicts is empty
        5. test_no_premature_merge_ready_after_restart:
           security=approve + code-quality=approve → ReviewsDispatched →
           VerdictSet(code-quality=approve) → state is VerdictPending
           (not MergeReady)

        New tests for projection verdict clearing:
        6. test_clear_dimension_verdicts_for_sha:
           Projection with both dimensions → clear → load → both pending
        7. test_restart_clears_projection_verdicts:
           Full round → restart → projection shows pending for both

        New tests for worktree-less merge:
        8. test_try_fast_forward_main_without_main_worktree:
           update-ref path works when main is not checked out
        9. test_try_fast_forward_main_rejects_non_ff:
           Non-ancestor → Err

        Acceptance:
          - All 9 new tests pass.
          - Coverage of stale-verdict window prevention.
          - Coverage of worktree-less auto-merge path.

    - id: S8_WAIT_TIMEOUT_AND_EXIT_CODE
      title: "Raise --wait-timeout-seconds default to 1200s, distinguish timeout from matched exit"
      detail: |
        The `--wait-for-recommended-action` poll loop at
        `crates/apm2-cli/src/commands/fac_review/mod.rs:606-649` has
        three problems:

        1. DEFAULT TOO LOW: `--wait-timeout-seconds` defaults to 60s
           with a clap range of 5..=180 (fac.rs:374). Real orchestration
           cycles (review dispatch → reviewer runs → verdict set) take
           5-20 minutes. The 60s default causes premature exit on nearly
           every poll.

        2. RANGE TOO NARROW: The clap max of 180s (3 minutes) prevents
           orchestrators from setting a reasonable timeout. Orchestrators
           need 20+ minutes for multi-dimension review cycles.

        3. TIMEOUT EXITS WITH SUCCESS: At line 624-627, timeout returns
           `exit_codes::SUCCESS` with the last poll result. The caller
           cannot distinguish "exited because action matched --exit-on"
           from "exited because timeout expired." This is a liveness
           hazard: the orchestrator treats a stale "wait" result as if
           it were a matched action.

        Fixes:

        (a) Change default to 1200 seconds (20 minutes) and cap range
            at 5..=1200 (20 minutes max):

                #[arg(long, default_value_t = 1200,
                      value_parser = clap::value_parser!(u64).range(5..=1200))]
                pub wait_timeout_seconds: u64,

            The 20-minute cap is deliberate: if nothing has happened for
            20 minutes, the system is in an abnormal state and the
            orchestrator must diagnose the problem — not wait longer.
            Allowing longer waits masks glitches in the overall system.

            When a monitor attempts to set a value above 1200, clap
            should emit an informative error message. Use a custom
            value_parser that wraps the range check and returns a
            descriptive error:

                fn parse_wait_timeout(s: &str) -> Result<u64, String> {
                    let val: u64 = s.parse().map_err(|e| format!("{e}"))?;
                    if val < 5 || val > 1200 {
                        return Err(format!(
                            "wait timeout must be between 5 and 1200 seconds \
                             (20 minutes max). If nothing has happened for 20 \
                             minutes, the orchestrator should diagnose the \
                             problem rather than wait longer."
                        ));
                    }
                    Ok(val)
                }

                #[arg(long, default_value_t = 1200,
                      value_parser = parse_wait_timeout)]
                pub wait_timeout_seconds: u64,

        (b) On timeout, return a DISTINCT exit code (e.g.,
            exit_codes::TIMEOUT or a new constant like 2) instead of
            SUCCESS. This lets the orchestrator detect timeout and
            decide whether to re-poll or escalate.

        (c) In JSON output, include a `timed_out` field in the final
            result so the orchestrator can programmatically detect
            timeout:

                {
                  "recommended_action": { ... },
                  "timed_out": true,
                  "elapsed_seconds": 1200,
                  "ticks": 240
                }

        (d) In the recommended_action.command emitted by "wait",
            include the timeout flag so the orchestrator's next poll
            inherits a reasonable timeout:

                "apm2 fac doctor --pr {pr} --json \
                    --wait-for-recommended-action \
                    --wait-timeout-seconds 1200"

        Acceptance:
          - Default wait timeout is 1200 seconds (20 minutes).
          - `--wait-timeout-seconds 1201` is rejected with an
            informative error explaining the 20-minute cap and that
            the orchestrator should diagnose the problem.
          - `--wait-timeout-seconds 1200` is accepted (max value).
          - Timeout exit code differs from success (exit_on match).
          - JSON output includes `timed_out: true` when timeout expires.
          - wait action command includes `--wait-timeout-seconds`.

    - id: S9_CONSOLIDATE_CI_STATUS_IN_PR_BODY
      title: "Move all CI/gate status into the PR body, YAML-only format, brief previous-SHA collapsible"
      detail: |
        GitHub rate limits are exhausted by the current dual-update
        pattern: CI status is posted as a separate PR comment
        (`ci_status.rs` → `create_issue_comment` / `update_issue_comment`)
        AND gate status is synced into the PR body (`projection.rs` →
        `edit_pr_body`). Every gate transition fires TWO GitHub API
        calls (one PATCH to the comment, one PATCH to the PR body).
        Consolidating into a single PR body update halves API traffic.

        Five changes:

        (a) MOVE CI STATUS INTO PR BODY:

            Retire `ci_status.rs` comment CRUD (`create_status_comment`,
            `update_status_comment`, `ThrottledUpdater`). Merge the
            `CiStatus` gate data into the existing gate status section
            of the PR body. All status updates go through a single
            `edit_pr_body()` call.

            The `CiStatus` struct and its RUNNING/PASS/FAIL semantics
            remain — only the transport changes (PR comment → PR body
            section).

            In `evidence.rs`, replace `ThrottledUpdater::new()` /
            `.update()` / `.force_update()` with a call to the
            unified PR body updater.

        (b) YAML EVERYTHING (NO JSON):

            Replace `render_status_metadata_block()` which emits
            ````json` blocks (projection.rs:939-953) with YAML blocks.

            Replace `render_gate_table()` markdown table
            (projection.rs:918-937) with a fenced YAML block.

            The current SHA section becomes:

                ```yaml
                # apm2-gate-status:v2
                sha: <full_sha>
                short_sha: <8char>
                timestamp: <ISO-8601>
                all_passed: true|false
                gates:
                  - name: <gate_name>
                    status: RUNNING|PASS|FAIL
                    duration_secs: <optional>
                    tokens_used: <optional>
                    model: <optional>
                ```

            One fenced YAML block per SHA — no separate table + metadata.

        (c) CURRENT SHA: FULL BREAKDOWN AFTER TICKET YAML + COMMIT HISTORY:

            Position the current SHA's gate status section immediately
            after the ticket YAML + `fac_push_metadata.commit_history`
            block (rendered by `render_ticket_body_markdown()` in
            push.rs:239-283), and before any previous-SHA collapsible.

            PR body layout:

                ```yaml
                ticket_meta: ...
                fac_push_metadata:
                  commit_history: ...
                ```

                <!-- apm2-gate-status:start -->
                ## FAC Gate Status

                ```yaml
                # apm2-gate-status:v2
                sha: <current_sha>
                ...full breakdown...
                ```

                <details>
                <summary>Previous SHAs</summary>
                ...brief pass/fail...
                </details>
                <!-- apm2-gate-status:end -->

        (d) PREVIOUS SHAs: BRIEF PASS/FAIL IN A SINGLE COLLAPSIBLE:

            Currently each previous SHA gets its own `<details>` with
            a full markdown table + JSON metadata block
            (`render_previous_status()` at projection.rs:955-968).

            Replace with a SINGLE `<details>` at the bottom containing
            ALL previous SHAs as a brief pass/fail summary:

                <details>
                <summary>Previous SHAs (3)</summary>

                ```yaml
                previous_shas:
                  - sha: abc12345
                    timestamp: "2026-02-15T..."
                    all_passed: false
                    gates: [lint: PASS, security: FAIL, code-quality: PASS]
                  - sha: def67890
                    timestamp: "2026-02-14T..."
                    all_passed: true
                    gates: [lint: PASS, security: PASS, code-quality: PASS]
                ```

                </details>

            One-line-per-gate format (inline YAML flow sequence) keeps
            it brief. The count in the summary shows how many previous
            SHAs are present.

        (e) REMOVE MARKDOWN TABLE:

            Delete `render_gate_table()` (projection.rs:918-937) and
            all call sites. The YAML block is the sole rendering format.

        Migration: The `parse_pr_body()` function must handle BOTH old
        format (table + JSON metadata) and new format (YAML-only) during
        the transition. Old-format sections are parsed and re-rendered
        in the new format on the next update.

        Acceptance:
          - CI status updates use `edit_pr_body()` only — no PR comment
            CRUD for status.
          - Gate status section contains fenced YAML blocks, not JSON.
          - No markdown table in the gate status section.
          - Current SHA has full gate breakdown (with duration, model,
            tokens) immediately after ticket YAML + commit history.
          - All previous SHAs are in a single `<details>` collapsible
            with a brief one-line-per-gate YAML summary.
          - PR body update count per gate transition is 1 (not 2).
          - Old-format PRs are migrated on next status sync.

    - id: S10_FULL_MACHINE_THROUGHPUT_POLICY
      title: "Use full host compute for FAC pipeline tests; rely on queue admission for contention"
      detail: |
        The throughput pivot requires gates and pipeline to use the same
        host-aware default compute policy:

        1. In `crates/apm2-cli/src/commands/fac_review/evidence.rs`
           (`build_pipeline_test_command`), remove the per-lane cap:

               lane_parallelism_cap = host_parallelism / lane_count
               execution_profile = cap_execution_parallelism(...)

           and resolve throughput directly from host parallelism:

               execution_profile = resolve_gate_execution_profile(Throughput)

        2. Keep containment bounds (`RuntimeMaxSec`, `MemoryMax`,
           `TasksMax`) unchanged. This change is about throughput policy,
           not removing containment.

        3. Keep contention control at queue admission / lane scheduling,
           not at per-job CPU throttling defaults.

        Acceptance:
          - Pipeline throughput defaults resolve to host parallelism
            (no `host/lane_count` split).
          - `NEXTEST_TEST_THREADS` and `CARGO_BUILD_JOBS` in pipeline
            env equal host parallelism under throughput defaults.
          - Default pipeline CPU quota equals `host_parallelism * 100%`.
          - Memory, PID, and timeout containment limits remain enforced.

  out_of_scope:
    - "Revoking the merge of PR #692. The PR is already on main; reverting requires a separate coordinated rollback."
    - "Adding a pre-merge verification hook on GitHub (GitHub-side protection rules are outside the CLI's control plane)."
    - "Clearing findings on restart (findings are tied to SHA, not review round — they remain valid across restarts)."
    - "Handling non-fast-forward merges (the current ff-only policy is correct; non-ff merges indicate branch divergence that requires rebase)."

plan:
  steps:
    - id: STEP_01
      title: "Guard approve action against findings and active agents"
      detail: |
        In build_recommended_action() at mod.rs:2527-2534:
        1. Compute has_actionable_findings BEFORE the approve check.
        2. Add active_agents == 0 guard.
        3. Only emit approve if all three conditions hold.

    - id: STEP_02
      title: "Clear lifecycle verdicts on ReviewsDispatched"
      detail: |
        In lifecycle.rs reduce_event handler:
        1. Add explicit arm for ReviewsDispatched that calls
           record.verdicts.clear().
        2. Move from _ => {} catch-all to named match arm.

    - id: STEP_03
      title: "Add clear_dimension_verdicts_for_sha to verdict_projection"
      detail: |
        1. Add pub fn clear_dimension_verdicts_for_sha().
        2. Acquires lock, loads record, clears dimensions, saves.
        3. Call from the restart flow before ReviewsDispatched event.

    - id: STEP_04
      title: "Replace git merge --ff-only with git update-ref"
      detail: |
        In lifecycle.rs try_fast_forward_main():
        1. Remove resolve_main_worktree() dependency.
        2. Use git update-ref with old-value guard.
        3. Best-effort sync main worktree if one exists.

    - id: STEP_05
      title: "Add remote main sync to auto-merge flow"
      detail: |
        1. Fetch origin main before merge attempt.
        2. Push local main to origin after successful merge.
        3. Add doctor health item when local main is behind.

    - id: STEP_06
      title: "Update existing tests"
      detail: |
        Update approve tests to include findings/agents guards.
        Add lifecycle reducer test for ReviewsDispatched clearing.

    - id: STEP_07
      title: "Add new tests"
      detail: |
        Add the 9 tests described in S7. Cover: approve guards (3),
        lifecycle clearing (2), projection clearing (2),
        worktree-less merge (2).

    - id: STEP_08
      title: "Fix wait timeout default, range, and exit code"
      detail: |
        1. In fac.rs: change wait_timeout_seconds default to 1200,
           range to 5..=1200 with custom value_parser that emits an
           informative error when monitors exceed the 20-minute cap.
        2. In mod.rs poll loop: return distinct exit code on timeout.
        3. Add timed_out field to JSON output on timeout.
        4. Include --wait-timeout-seconds in wait action command.

    - id: STEP_09
      title: "Consolidate CI status into PR body, YAML-only format"
      detail: |
        1. In projection.rs: replace render_gate_table() and
           render_status_metadata_block() with YAML rendering.
        2. Replace per-SHA <details> with a single collapsible
           containing brief pass/fail YAML for all previous SHAs.
        3. In ci_status.rs: retire comment CRUD. Merge CiStatus
           data into the PR body gate section.
        4. In evidence.rs: replace ThrottledUpdater calls with
           unified PR body updater.
        5. In push.rs: ensure gate status section is positioned
           after ticket YAML + commit history.
        6. In projection.rs parse_pr_body(): handle old format
           (table + JSON) for migration.

    - id: STEP_10
      title: "Verify all changes"
      detail: |
        1. cargo test -p apm2-cli — all tests pass.
        2. Verify approve action never fires with MAJOR findings.
        3. Verify restart clears both lifecycle verdicts and projection.
        4. Verify auto-merge works without main worktree.
        5. Verify timeout returns distinct exit code.
        6. Verify PR body contains YAML-only gate status (no JSON,
           no markdown table).
        7. Verify CI status updates produce 1 API call (not 2).
        8. Verify old-format PR bodies are migrated correctly.
        9. Manual: restart reviews for a PR, verify stale-verdict
           window is eliminated.

    - id: STEP_11
      title: "Align pipeline throughput defaults with full-machine policy"
      detail: |
        1. Remove per-lane CPU/test-thread cap from
           `build_pipeline_test_command`.
        2. Ensure throughput profile uses host parallelism directly
           in both `fac gates` and pipeline paths.
        3. Add/update tests to assert pipeline env (`NEXTEST_TEST_THREADS`,
           `CARGO_BUILD_JOBS`) and effective CPU quota match host
           parallelism defaults.
        4. Verify containment knobs (memory/pids/timeout) are unchanged.

definition_of_done:
  evidence_ids: []
  criteria:
    - "approve action is blocked when any dimension has blocker or major findings — stale formal verdicts cannot hide actionable findings."
    - "approve action is blocked when active reviewer agents are running — verdicts-in-flight cannot trigger premature approve."
    - "ReviewsDispatched lifecycle event clears record.verdicts — no stale verdicts persist across review restarts."
    - "apm2 fac restart clears verdict projection dimensions for the SHA — projection shows pending for both dimensions after restart."
    - "try_fast_forward_main uses git update-ref (no worktree with main checkout required)."
    - "Auto-merge succeeds when primary worktree is on a ticket branch (not main)."
    - "Local main is synced with origin/main via fetch before merge and push after merge."
    - "Doctor emits health warning when local main is behind origin/main."
    - "Existing approve tests updated with findings and active-agents guards."
    - "9 new tests cover stale-verdict prevention, verdict clearing, and worktree-less merge."
    - "`--wait-timeout-seconds` defaults to 1200 (20 min), range capped at 1200 (20 min max). Values above 1200 are rejected with an informative error explaining the orchestrator should diagnose the problem."
    - "Timeout exit code differs from exit-on-match success code — orchestrators can distinguish timeout from matched action."
    - "JSON output includes `timed_out: true` when wait times out."
    - "CI status updates use a single `edit_pr_body()` call — no PR comment CRUD (`create_issue_comment` / `update_issue_comment`) for status."
    - "Gate status section uses fenced YAML blocks — no JSON metadata blocks, no markdown tables."
    - "Current SHA full breakdown appears immediately after ticket YAML + commit history in the PR body."
    - "All previous SHAs are in a single `<details>` collapsible with brief one-line-per-gate YAML summary."
    - "Old-format PR bodies (table + JSON) are migrated to new format on next status sync."
    - "Pipeline throughput defaults use host parallelism directly (no per-lane CPU split)."
    - "Pipeline default `NEXTEST_TEST_THREADS`/`CARGO_BUILD_JOBS` and effective CPU quota match host parallelism."
    - "Containment remains bounded (memory, pids, timeout) despite throughput pivot."
    - "`cargo test -p apm2-cli` passes."

notes:
  context: |
    INCIDENT REPORT

    PR #692 (TCK-00525: warm phase containment hardening) was merged
    to main with 2 unresolved MAJOR security findings:

    1. Worker heartbeat starvation (MAJOR): execute_warm_job runs on
       the main thread, blocking the heartbeat loop. Long-running
       phases cause the worker to be marked dead → restart loops.

    2. Containment bypass (MAJOR): Warm jobs use Command::spawn
       directly, bypassing systemd resource limits (MemoryMax,
       CPUQuota). Unbounded resource usage → DoS vector.

    The merge happened because:
    - Stale security=approve from a previous review round persisted
      in the verdict projection and lifecycle after reviews restarted
    - The TCK-00609 `approve` action trusted all_verdicts_approve
      without checking finding counts
    - The orchestrator saw action="approve" and the PR was merged on
      GitHub (auto-merge was broken — no main worktree)

    The root cause pattern is a TOCTOU: the verdict projection has
    a stale-verdict window between review restart and the second
    dimension's new verdict. Any consumer reading
    all_verdicts_approve during this window sees stale data.

    The fix addresses three layers:
    1. IMMEDIATE: Guard approve against inconsistent findings
    2. ROOT CAUSE: Clear stale verdicts on restart
    3. INFRASTRUCTURE: Fix auto-merge to work without main worktree

    This is safety-critical: the merge gate exists specifically to
    prevent code with unresolved MAJOR/BLOCKER findings from reaching
    main. The stale-verdict bypass undermines this guarantee.
  amendments:
    - amendment_id: "AMD-2026-02-16-FAC-THROUGHPUT-PIVOT"
      summary: "Clarify that references to bounded CPUQuota in this incident are containment concerns, not normative low-throughput defaults."
      replacement:
        - "Bounded execution remains mandatory for containment and fail-closed behavior."
        - "Default gate throughput policy is profile-driven and host-aware, with queue admission controlling contention."
        - "Pipeline throughput defaults are aligned with gates: full host parallelism by default, no per-lane CPU split."
  security: "default-deny, least privilege, fail-closed"
