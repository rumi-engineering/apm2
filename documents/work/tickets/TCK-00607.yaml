ticket_meta:
  schema_version: "2026-01-29"
  template_version: "2026-01-29"
  ticket:
    id: "TCK-00607"
    title: "Fix doctor false positive, unify timeouts, add agent activity diagnostics, and auto-nudge incomplete reviewers"
    status: "OPEN"
  binds:
    prd_id: "PRD-PLACEHOLDER"
    rfc_id: "RFC-0019"
    requirements: []
    evidence_artifacts: []
  custody:
    agent_roles: ["AGENT_IMPLEMENTER"]
    responsibility_domains: ["DOMAIN_RUNTIME"]
  dependencies:
    tickets:
      - ticket_id: "TCK-00605"
        reason: "TCK-00605 introduced the doctor --pr checks including worker_liveness."
  root_cause_analysis:
    summary: |
      Four operational issues surfaced during field verification of the FAC
      execution substrate:

      1. DOCTOR worker_liveness CHECK IS A FALSE POSITIVE

         The `check_projection_worker_health()` function in
         `crates/apm2-cli/src/commands/daemon.rs:882` constructs the expected
         cache path as:

             let cache_path = state_dir.join("projection_cache.db");

         This always looks for `{state_dir}/projection_cache.db`.

         However, the daemon projection worker in
         `crates/apm2-daemon/src/main.rs:1508-1515` determines the cache path
         differently when `ledger_db_path` is configured:

             let cache_path = daemon_config.ledger_db_path.as_ref().map_or_else(
                 || { ... p.join("projection_cache.db") },
                 |p| p.with_extension("projection_cache.db"),
             );

         When `ledger_db_path` is set (e.g., to `ledger.db`), Rust's
         `Path::with_extension()` replaces the existing extension, producing
         `ledger.projection_cache.db` — NOT `projection_cache.db`.

         Result: doctor reports 13/14 checks green with one ERROR on
         worker_liveness, even though the projection worker is running
         correctly and the cache file exists (just under the wrong name from
         the doctor's perspective).

         The fix must unify the cache-path derivation so that the doctor
         check and the daemon use the same logic.

      2. TEST TIMEOUTS ARE INCONSISTENT (240s vs 600s)

         The bounded FAC test timeout policy in
         `crates/apm2-cli/src/commands/fac_review/timeout_policy.rs:10-11`
         standardised on 600s:

             pub const DEFAULT_BOUNDED_TEST_TIMEOUT_SECONDS: u64 = 600;
             pub const MAX_MANUAL_TIMEOUT_SECONDS: u64 = 600;

         However, the `LaneTimeouts` default in
         `crates/apm2-core/src/fac/lane.rs:283` still uses 240s:

             test_timeout_seconds: 240,

         This means any code path that constructs a `LaneTimeouts::default()`
         (e.g., lane profile deserialization fallback, systemd unit
         generation) will silently use a 4-minute timeout while the rest of
         the system expects 10 minutes. Tests relying on the lane profile
         default can be killed 6 minutes early, producing flaky failures that
         only reproduce under load.

         The fix must unify all timeout defaults to 600s (or, if a cold/warm
         distinction is desired, 600s cold / 240s warm with an explicit
         field).

      3. DOCTOR --PR LACKS AGENT ACTIVITY METRICS

         Orchestrator-monitor agents diagnosing PR state rely on
         `apm2 fac doctor --pr <N>` to determine whether a "pending" review
         is actively working or stuck. The current `DoctorAgentSnapshot`
         (fac_review/mod.rs:136-159) includes `elapsed_seconds`,
         `models_attempted`, and `last_activity_seconds_ago`, but does NOT
         include the two most useful activity proxies:

           a) Tool call count — how many tool invocations the agent has made
              (a monotonically increasing count that distinguishes "active
              but slow" from "stuck doing nothing").
           b) Log line count — total lines written to the agent's log/output
              file, which increases with any agent activity.

         The event log scanner (`scan_event_signals_for_pr()` at
         fac_review/mod.rs:1399-1498) already reads review_events.ndjson but
         only extracts model attempts and activity timestamps. Extending it
         to count tool-call events (and reading log file sizes) is
         straightforward.

         Without these metrics, orchestrator monitors must guess whether
         "pending" means the agent is working or dead-but-not-yet-reaped,
         leading to premature restarts or unnecessary waiting.

      4. REVIEWERS EXIT WITHOUT EXECUTING TERMINAL COMMAND (verdict set)

         Review agents (especially codex) consistently collect context,
         produce findings, but then exit cleanly without calling
         `apm2 fac verdict set`. The auto-verdict reaper (TCK-00605) handles
         this after the fact, but auto-derived verdicts are lower quality
         than agent-issued ones because they lack the agent's synthesized
         reasoning.

         The existing `SpawnMode::Resume { message }` mechanism
         (fac_review/orchestrator.rs:767-792, types.rs:549-552) already
         supports resuming an agent session with a message — but it is only
         triggered by SHA drift. There is no logic to detect "agent exited
         without verdict" and resume with a nudge.

         All three backends support session resume:
           - Codex: `codex exec resume --last ... "message"`
           - Gemini: `gemini --resume latest -p "message"`
           - Claude: `claude --resume -p "message"`

         The fix should detect clean-exit-without-verdict and, before
         falling through to auto-verdict, attempt ONE resume with a message
         that echoes the agent's original assigned prompt and instructs it
         to execute the terminal command. If the nudge also fails to produce
         a verdict, fall through to auto-verdict as today.

  scope:
    in_scope:
      - id: S1_DOCTOR_CACHE_PATH
        title: "Fix doctor worker_liveness cache-path derivation"
        detail: |
          The doctor check in `check_projection_worker_health()` must use the
          same cache-path derivation logic as the daemon projection worker.

          Current doctor logic (daemon.rs:876-882):
            let state_dir = config.daemon.state_file.parent()...;
            let cache_path = state_dir.join("projection_cache.db");

          Correct logic must mirror daemon (main.rs:1508-1515):
            - If `ledger_db_path` is set: `ledger_db_path.with_extension("projection_cache.db")`
            - Otherwise: `state_dir.join("projection_cache.db")`

          The doctor check needs access to `config.daemon.ledger_db_path` (or
          the resolved `DaemonConfig`) to replicate this branching.

          Acceptance:
            - `apm2 daemon doctor` reports worker_liveness as OK when the
              projection worker is running and the cache file exists at the
              ledger-adjacent path.
            - `apm2 daemon doctor` still reports ERROR when projection is
              enabled but the cache file genuinely does not exist.

      - id: S2_UNIFY_TIMEOUTS
        title: "Unify test timeout defaults to 600s"
        detail: |
          Change `LaneTimeouts::default()` in
          `crates/apm2-core/src/fac/lane.rs:283` from 240 to 600:

              test_timeout_seconds: 600,

          Update all test fixtures and assertions that hard-code 240:
            - `crates/apm2-core/src/fac/lane.rs:1572` (JSON fixture)
            - `crates/apm2-core/src/fac/systemd_properties.rs:132` (test)

          Verify consistency with:
            - `timeout_policy.rs` (already 600)
            - `.config/nextest.toml` (60s × 10 cycles = 600s, already correct)
            - RFC-0019 amendment A1 (mandates 600s uniform policy)

          If a cold/warm split is desired instead:
            - Add `test_timeout_cold_seconds: 600` and
              `test_timeout_warm_seconds: 240` to `LaneTimeouts`
            - Update the serialization format and lane profile schema
            - Gate selection on workspace-cache presence
            This is a larger change; the simple path is to unify to 600s.

      - id: S3_DOCTOR_AGENT_ACTIVITY
        title: "Add tool-call count and log-line count to doctor --pr agent snapshots"
        detail: |
          Extend `DoctorAgentSnapshot` (fac_review/mod.rs:136-159) with two
          new fields:

              tool_call_count: Option<u64>,
              log_line_count: Option<u64>,

          **Tool call count:**
          Extend `scan_event_signals_for_pr()` (fac_review/mod.rs:1399-1498)
          to count events that represent tool invocations. The review event
          log (review_events.ndjson) already contains structured events; any
          event with a `"tool"` or `"tool_call"` field (or event type
          indicating tool use) should increment a per-agent counter. The
          exact event shape depends on the backend, so the scanner should
          count liberally (any event that is not a pure lifecycle event
          like `run_start`, `model_fallback`, `run_complete` is likely a
          tool call or output event).

          If the event log does not contain tool-granularity events for a
          backend, fall back to counting total event lines for that
          agent's run_id as a proxy.

          **Log line count:**
          The `ReviewStateEntry` stores `log_file: PathBuf` — the temp file
          where agent stdout/stderr is captured. Read the file's line count
          (or byte size as a cheaper proxy) and surface it in the snapshot.
          For agents whose log file has been cleaned up, emit `None`.

          Wire both fields into the `DoctorAgentSnapshot` serialization so
          they appear in `apm2 fac doctor --pr <N> --json` output.

          Acceptance:
            - `apm2 fac doctor --pr <N> --json | jq '.agents.entries[].tool_call_count'`
              returns a non-null integer for active/completed agents.
            - `apm2 fac doctor --pr <N> --json | jq '.agents.entries[].log_line_count'`
              returns a non-null integer when the log file exists.
            - Orchestrator-monitor agents can distinguish "pending, 0 tool
              calls" (stuck) from "pending, 847 tool calls" (working).

      - id: S4_AUTO_NUDGE
        title: "Auto-nudge agents that exit without executing their terminal command"
        detail: |
          Add a nudge-before-auto-verdict mechanism to the review
          orchestrator. When a review agent process exits (clean exit, not
          crash/timeout) without having called `apm2 fac verdict set`, the
          system should attempt ONE session resume before falling through
          to auto-verdict.

          **Detection site:**
          In `run_single_review()` (orchestrator.rs:595), after the spawned
          process exits, the orchestrator already checks whether a verdict
          was set. If the process exited with code 0 but no verdict exists
          for this (pr, dimension, sha), this is the nudge trigger.

          **Nudge mechanism:**
          1. Read the agent's original prompt from `ReviewStateEntry.prompt_file`
             (or reconstruct it via `build_prompt_content()`).
          2. Construct a resume message:

                 "RESUME TASK — REQUIRED TERMINAL COMMAND NOT EXECUTED.
                  You exited without running your required terminal command.
                  You MUST execute this command before exiting:

                      apm2 fac verdict set --pr {pr} --sha {sha} \
                          --dimension {dimension} --verdict <approve|deny> \
                          --reason '<your synthesized reasoning>'

                  Your original assignment was:
                  ---
                  {original_prompt_content}
                  ---

                  Review your findings and execute the verdict command now."

          3. Spawn with `SpawnMode::Resume { message }` using the
             appropriate backend resume command (codex exec resume --last,
             gemini --resume latest, claude --resume).
          4. Track this as `nudge_attempt_count` on the `ReviewStateEntry`
             (or `TrackedAgent`). Max nudge attempts: 1.
          5. If the nudge attempt also exits without verdict, fall through
             to auto-verdict (existing reaper logic).

          **State tracking:**
          Add a field to `ReviewStateEntry` or `ReviewRunState`:

              nudge_count: u32,  // default 0, max 1

          The nudge should only fire once per review run to avoid infinite
          resume loops.

          **Scope boundaries:**
            - Nudge ONLY on clean exit (exit code 0) without verdict.
            - Do NOT nudge on crash, timeout, or SIGKILL — these go straight
              to auto-verdict as today.
            - Do NOT nudge if the agent already exceeded MAX_RESTART_ATTEMPTS.
            - The nudge uses the same model and backend as the original run.

          Acceptance:
            - Codex agent exits without verdict → system resumes session
              with nudge message → agent executes verdict set.
            - If nudge also fails → auto-verdict fires (no regression).
            - `nudge_count` is visible in doctor --pr agent snapshot.
            - Review event log contains a `nudge_resume` event for
              traceability.

    out_of_scope:
      - "Refactoring the projection worker cache path logic itself (only the doctor check needs to match it)."
      - "Changing nextest.toml or CI-level timeouts (already correct at 600s)."
      - "Multi-nudge retry loops (nudge is capped at 1 attempt)."
      - "Changing the auto-verdict reaper logic itself (it remains the fallback)."
      - "Adding resume support to backends that don't have it (all three already do)."

  plan:
    steps:
      - id: STEP_01
        title: "Fix doctor cache-path to mirror daemon logic"
        detail: |
          In `crates/apm2-cli/src/commands/daemon.rs`, update
          `check_projection_worker_health()`:

          1. Read `config.daemon.ledger_db_path` from the ecosystem config.
          2. If `ledger_db_path` is set, compute cache_path as
             `ledger_db_path.with_extension("projection_cache.db")`.
          3. Otherwise, fall back to `state_dir.join("projection_cache.db")`.
          4. Update the comment at line 875 to reflect the new logic.

      - id: STEP_02
        title: "Unify LaneTimeouts::default() to 600s"
        detail: |
          1. Change `crates/apm2-core/src/fac/lane.rs:283` from 240 to 600.
          2. Update the JSON fixture in lane.rs:1572 to use 600.
          3. Update the systemd_properties.rs test at line 132 to use 600.
          4. Grep for any remaining 240-second test timeout references
             and update them.

      - id: STEP_03
        title: "Add tool_call_count and log_line_count to DoctorAgentSnapshot"
        detail: |
          1. Add `tool_call_count: Option<u64>` and `log_line_count: Option<u64>`
             fields to `DoctorAgentSnapshot` in fac_review/mod.rs.
          2. Extend `DoctorEventSignals` with a `tool_call_counts: BTreeMap<String, u64>`
             keyed by agent run_id.
          3. In `scan_event_signals_from_reader()`, count events per run_id that
             are not pure lifecycle events (run_start, run_complete, model_fallback).
             These are tool calls or output events — count them all.
          4. In the doctor snapshot builder, read log file line count from
             `ReviewStateEntry.log_file` (use `BufReader::lines().count()` or
             metadata byte size as a cheaper proxy).
          5. Wire both into the JSON serialization.

      - id: STEP_04
        title: "Implement auto-nudge for clean-exit-without-verdict"
        detail: |
          1. Add `nudge_count: u32` field to `ReviewRunState` (types.rs) and
             `TrackedAgent` (lifecycle.rs). Default 0.
          2. In `run_single_review()` (orchestrator.rs), after the child process
             exits, add a check:
               - Exit code 0 AND no verdict set for (pr, dimension, sha)
               - AND nudge_count == 0
               - AND restart_count < MAX_RESTART_ATTEMPTS
             If all true, enter nudge path.
          3. Nudge path:
             a. Read original prompt from prompt_file or reconstruct via
                `build_prompt_content()`.
             b. Construct the nudge message (see S4 detail).
             c. Increment nudge_count in run state and persist.
             d. Emit a `nudge_resume` event to review_events.ndjson.
             e. Spawn with `SpawnMode::Resume { message: nudge_message }`.
             f. Wait for the resumed process to exit.
             g. If verdict still not set, fall through to auto-verdict.
          4. Surface `nudge_count` in `DoctorAgentSnapshot`.

      - id: STEP_05
        title: "Test and verify"
        detail: |
          1. Run `cargo test -p apm2-cli` — doctor and review tests pass.
          2. Run `cargo test -p apm2-core fac::lane` — lane profile tests pass.
          3. Run `cargo test -p apm2-core fac::systemd` — systemd tests pass.
          4. Run `apm2 daemon doctor` on a running daemon with
             ledger_db_path configured — worker_liveness reports OK.
          5. Verify `apm2 fac doctor --pr <N> --json` output includes
             tool_call_count and log_line_count for agent entries.
          6. Verify nudge logic: mock a clean-exit-without-verdict scenario
             and confirm resume is attempted once before auto-verdict.

  definition_of_done:
    evidence_ids: []
    criteria:
      - "`apm2 daemon doctor` reports worker_liveness=OK when projection worker is running (no false positive)."
      - "Doctor still detects genuinely missing projection cache (no false negative)."
      - "`LaneTimeouts::default().test_timeout_seconds` is 600."
      - "No remaining hard-coded 240s test timeout defaults in the codebase (grep-verified)."
      - "`apm2 fac doctor --pr <N> --json` agent entries include `tool_call_count` and `log_line_count`."
      - "Orchestrator-monitor agents can distinguish stuck (0 tool calls) from active (N tool calls) pending reviews."
      - "Clean-exit-without-verdict triggers exactly one nudge resume before falling through to auto-verdict."
      - "Nudge message echoes the agent's original prompt and specifies the required terminal command."
      - "`nudge_count` is tracked in run state and visible in doctor --pr output."
      - "Nudge does NOT fire on crash, timeout, or SIGKILL exits."
      - "`cargo test -p apm2-cli -p apm2-core` passes."

  notes:
    context: |
      Discovered during field verification of multi-PR orchestration runs:

      (1) `apm2 daemon doctor` reported 13/14 checks green with one ERROR
      on worker_liveness. The projection cache file existed as
      `ledger.projection_cache.db` but the doctor check was looking for
      `projection_cache.db`. Path-derivation mismatch from TCK-00322.

      (2) Timeout inconsistency (240s vs 600s) noticed during the same
      review. The bounded timeout policy and nextest config are already at
      600s, but LaneTimeouts::default() still uses the pre-RFC-0019 value
      of 240s.

      (3) Orchestrator-monitor agents diagnosing pending reviews have no
      quantitative activity signal. They see "pending" and
      "last_activity_seconds_ago" but cannot tell if the agent has made
      847 tool calls (healthy, just slow) or 0 tool calls (stuck). Adding
      tool_call_count and log_line_count to doctor --pr agent snapshots
      gives monitors a monotonically increasing proxy for agent progress.

      (4) Codex (and occasionally other backends) consistently exits after
      collecting context and producing findings but BEFORE calling
      `apm2 fac verdict set`. The auto-verdict reaper catches this
      eventually, but auto-derived verdicts lack the agent's synthesized
      reasoning. Since all three backends support session resume, we can
      nudge the agent once: resume the session with a message echoing the
      original prompt and explicitly instructing it to execute the verdict
      command. This is cheaper and higher quality than restarting from
      scratch or falling through to auto-verdict.
    security: "default-deny, least privilege, fail-closed"
