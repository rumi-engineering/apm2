rfc_risks_and_open_questions:
  schema_version: "2026-01-27"
  template_version: "2026-01-27"

  risks:
    - risk_id: RISK-DAEMON-001
      title: "Daemon availability becomes single point of failure"
      description: |
        All episode execution depends on daemon availability.
        Daemon crash terminates all running episodes.
      likelihood: MEDIUM
      impact: HIGH
      mitigation:
        - "Systemd restart with socket activation"
        - "Episode state recoverable from ledger"
        - "Health monitoring with alerting"
        - "Graceful degradation: CLI can query ledger directly for read-only operations"
      residual_risk: MEDIUM
      owner: AUTH_ARCHITECTURE

    - risk_id: RISK-DAEMON-002
      title: "PTY parsing fragility across harness versions"
      description: |
        Vendor CLI output formats may change between versions.
        Parser may fail to detect tool calls or misparse output.
      likelihood: HIGH
      impact: MEDIUM
      mitigation:
        - "Version-specific parser modules"
        - "Fallback to raw mode on parse failure"
        - "Parser failures logged as defects for improvement"
        - "Test fixtures from real harness output"
      residual_risk: MEDIUM
      owner: AUTH_ARCHITECTURE

    - risk_id: RISK-DAEMON-003
      title: "Cgroup v2 availability and permissions"
      description: |
        Telemetry collection assumes cgroup v2 on host.
        Daemon may lack permissions to read cgroup metrics.
      likelihood: LOW
      impact: MEDIUM
      mitigation:
        - "Fallback to /proc for basic metrics"
        - "Document cgroup v2 requirement"
        - "Capability check at daemon startup"
        - "Telemetry degradation alert"
      residual_risk: LOW
      owner: AUTH_ARCHITECTURE

    - risk_id: RISK-DAEMON-004
      title: "Evidence storage growth during failure cascades"
      description: |
        Many simultaneous failures may trigger evidence promotion,
        exceeding storage budget before compaction runs.
      likelihood: LOW
      impact: HIGH
      mitigation:
        - "Per-episode evidence quota (100MB default)"
        - "Global evidence budget with admission control"
        - "Emergency compaction trigger"
        - "Oldest unpinned evidence evicted under pressure"
      residual_risk: MEDIUM
      owner: AUTH_ARCHITECTURE

    - risk_id: RISK-DAEMON-005
      title: "Canonicalization divergence across versions"
      description: |
        Canonicalization changes could break receipt verification
        for historical receipts.
      likelihood: LOW
      impact: HIGH
      mitigation:
        - "Canonicalizer version in receipt"
        - "Golden vector suite for each version"
        - "Breaking changes require new canonicalizer ID"
        - "All canonicalizer versions preserved for verification"
      residual_risk: LOW
      owner: AUTH_ARCHITECTURE

    - risk_id: RISK-SECURITY-001
      title: "Signing key compromise"
      description: |
        Daemon signing key compromise allows receipt forgery.
        All receipts signed by that key become untrustworthy.
      likelihood: LOW
      impact: CRITICAL
      mitigation:
        - "Key stored in OS keychain (not file)"
        - "AD-KEY-001: 90-day rotation, versioning, and recovery procedures"
        - "Transparency log for receipt verification (future)"
        - "Key compromise detection via anomaly monitoring"
      residual_risk: MEDIUM
      owner: AUTH_SECURITY

  open_questions:
    - question_id: OQ-DAEMON-001
      question: "How should harness adapter detect tool calls from PTY output?"
      context: |
        Black-box mode requires parsing harness PTY output to identify tool calls.
        Vendor CLIs may have different output formats (JSON-ish, markdown, custom).
        Need to balance detection accuracy with parsing fragility.
      status: RESOLVED_V2
      proposed_answer: |
        Multi-strategy approach:
        1. JSON block detection for structured tool calls
        2. Regex patterns for known vendor formats
        3. Fallback to manual tool invocation via interposed commands
        4. Vendor-specific adapter modules
      resolution: |
        **v2 Resolution (2026-01-27):**

        Codebase investigation confirmed existing BlackBoxAdapter and ClaudeCodeAdapter
        infrastructure in apm2-core/src/adapter/. The HarnessAdapter trait will be implemented
        following the existing Adapter pattern with:

        1. **Parser State Machine**: Each vendor adapter implements deterministic parser
           that emits HarnessEvent::ToolRequest on detection.
        2. **Fallback Chain**: JSON blocks → vendor regex → raw mode (no tool interception)
        3. **Test Fixtures**: Each adapter includes fixtures from real harness sessions
        4. **Error Classification**: Parsing failures emit defect records, not crashes

        Key design decision: HarnessAdapter implements the Holon trait, enabling
        EpisodeController integration. Tool requests become artifact side-effects
        within execute_episode().
      evidence_refs:
        - "crates/apm2-core/src/adapter/black_box.rs"
        - "crates/apm2-core/src/adapter/claude_code.rs"
      owner: AUTH_ARCHITECTURE

    - question_id: OQ-DAEMON-002
      question: "What is the exact cgroup hierarchy for episode isolation?"
      context: |
        Episodes need cgroup isolation for telemetry and resource limits.
        Options:
        - Per-episode cgroup under daemon slice
        - Shared pool cgroup with per-episode tracking
        - Delegate to container runtime
      status: RESOLVED_V2
      proposed_answer: |
        Per-episode cgroup under /sys/fs/cgroup/apm2.slice/episode-<id>.scope
        Daemon creates scope units via systemd transient API.
        Resources limits set based on episode budget.
      resolution: |
        **v2 Resolution (2026-01-27):**

        Codebase investigation confirmed no existing cgroup integration. Design finalized:

        **Hierarchy Structure:**
        ```
        /sys/fs/cgroup/apm2.slice/
        ├── daemon.service/               # Daemon process cgroup
        └── episode-<uuid>.scope/         # Per-episode transient scope
            ├── memory.max               # From episode budget
            ├── cpu.max                  # From episode budget
            ├── memory.oom.group         # OOM killer priority
            └── tasks                    # Child PIDs (adapter + harness)
        ```

        **Implementation Strategy:**
        1. **Primary**: Systemd transient API via DBus (org.freedesktop.systemd1)
           - Create transient scopes dynamically with StartTransientUnit
           - Automatic cleanup on episode termination
           - Better integration with Linux ecosystem

        2. **Fallback**: Direct cgroup v2 writes using nix crate
           - mkdir /sys/fs/cgroup/apm2.slice/episode-<id>.scope
           - echo <limit> > memory.max
           - Manual cleanup required

        **Budget-to-Cgroup Mapping:**
        - memory_bytes → cgroup memory.max
        - cpu_ms → cgroup cpu.max (percentage calculation)

        **Degraded Mode (RISK-DAEMON-003):**
        - If cgroup v2 unavailable, fall back to /proc/<pid>/stat metrics
        - Log degraded telemetry level at daemon startup
        - Telemetry accuracy reduced but functional
      evidence_refs:
        - "No existing cgroup code - implementation required"
        - "nix crate available with signal/process/fs features"
      owner: AUTH_ARCHITECTURE

    - question_id: OQ-DAEMON-003
      question: "How should daemon handle adapter failures during episode?"
      context: |
        Adapter may crash, become unresponsive, or produce invalid output.
        Need recovery strategy that preserves evidence and emits proper events.
      status: RESOLVED_V2
      proposed_answer: |
        1. Adapter timeout triggers health check
        2. Unresponsive adapter -> episode quarantine
        3. Adapter crash -> restart with same episode context
        4. Repeated failures -> escalate to quarantine
        5. All failures trigger evidence promotion
      resolution: |
        **v2 Resolution (2026-01-27):**

        Codebase investigation revealed existing infrastructure to leverage:

        **Existing Infrastructure:**
        - StallDetectionConfig in adapter config (timeout threshold)
        - AdapterError with is_transient()/is_fatal() classification
        - ProcessState enum with crash/termination states
        - HealthChecker infrastructure with history tracking
        - CrashType classification (CleanExit, ErrorExit, Signal, Timeout, EntropyExceeded)
        - RestartCoordinator with circuit breaker and backoff

        **Adapter Failure State Machine:**
        ```
        Running --timeout--> HealthCheck
        HealthCheck --pass--> Running
        HealthCheck --fail(transient)--> FailureCount++
        FailureCount >= 3 --> Quarantine
        Running --crash(transient)--> RestartCoordinator
        RestartCoordinator --retry--> Running (with resume_cursor)
        RestartCoordinator --exhausted--> Quarantine
        ```

        **Implementation:**
        1. **AdapterHealthMonitor** wraps existing HealthChecker
           - Periodic liveness checks (configurable interval)
           - Track consecutive failures
           - Trigger quarantine after threshold (default: 3)

        2. **Evidence Promotion on Failure:**
           - Adapter crash → promote flight recorder (PTY, tool I/O, telemetry)
           - Store in CAS as AdapterFailure evidence category
           - Link to episode receipts

        3. **Episode FSM Integration:**
           - Add QUARANTINED state (exists in SessionState)
           - Emit SessionQuarantined event with reason
           - Pin evidence until manual review

        4. **Restart Recovery:**
           - Use resume_cursor from last checkpoint
           - Increment restart_attempt (monotonicity enforced)
           - Apply backoff delay from RestartCoordinator
      evidence_refs:
        - "crates/apm2-core/src/session/crash.rs (CrashType)"
        - "crates/apm2-core/src/session/restart_coordinator.rs"
        - "crates/apm2-core/src/health/mod.rs (HealthChecker)"
        - "crates/apm2-core/src/adapter/error.rs (AdapterError)"
      owner: AUTH_ARCHITECTURE

    - question_id: OQ-DAEMON-004
      question: "What is the protocol for CAC ContextPack enforcement?"
      context: |
        Weak local models may need deny-by-default read restrictions.
        ContextPack defines what reads are allowed.
        How is this integrated into tool plane?
      status: RESOLVED_V2
      proposed_answer: |
        1. context_pack_hash in episode envelope
        2. Read tool validates path against context pack manifest
        3. Unmanifested reads: deny by default, log as context miss
        4. Context miss triggers DCP (Deferred Context Pack) request
        5. Policy tier determines enforcement strictness
      resolution: |
        **v2 Resolution (2026-01-27):**

        Codebase investigation revealed two distinct integration points:

        **Integration Point 1: apm2-holon (Post-Execution Audit)**
        - EpisodeController already has pack miss tracking infrastructure
        - `record_pack_miss()` function exists (currently #[allow(dead_code)])
        - RunReceipt includes pack sufficiency evidence
        - DefectRecord emitted for pack misses
        - This enables post-hoc analysis of context discipline

        **Integration Point 2: apm2-daemon (Pre-Execution Enforcement)**
        - ToolBroker validates capability manifest before tool execution
        - context_pack_hash stored in EpisodeEnvelope.context_refs
        - Read tool requests validated against manifest paths
        - Unmanifested paths: DENY by default, emit context miss defect

        **Protocol Flow:**
        ```
        1. Episode created with context_pack_hash in envelope
        2. Tool request arrives (e.g., Read("/foo/bar.rs"))
        3. ToolBroker.validate_context():
           a. Load context pack manifest from CAS
           b. Check if path matches allowed patterns
           c. If match: proceed to capability validation
           d. If no match: DENY with "context_miss" reason
        4. Emit events:
           - ToolRequested (with context_miss flag if denied)
           - DefectRecord (for context discipline tracking)
        5. Post-episode: RunReceipt includes pack_sufficiency_score
        ```

        **Policy Tier Enforcement:**
        | Tier | Unmapped Read Behavior | Rationale |
        |------|------------------------|-----------|
        | 1 (Local) | Warn only, allow | Development flexibility |
        | 2 (Standard) | Deny, log defect | Production safety |
        | 3+ (High Risk) | Deny, quarantine | Maximum security |

        **Coordination with apm2-holon:**
        - Daemon ToolBroker handles real-time enforcement
        - EpisodeController RunReceipt captures pack sufficiency for audit
        - Both use same DefectRecord type for miss tracking
        - Signatures cross-reference for verification
      evidence_refs:
        - "crates/apm2-holon/src/episode/controller.rs:record_pack_miss"
        - "Proposed: apm2-daemon/src/tool/broker.rs:validate_context"
      owner: AUTH_ARCHITECTURE

    - question_id: OQ-DAEMON-005
      question: "What is the event vocabulary for evidence economics?"
      context: |
        Evidence economics (REQ-EVID-001) requires metrics that are evidence-backed,
        derived from events, and versioned. The architectural principle (AD-EVID-003)
        establishes that the ledger IS the substrate and reducers derive metrics.

        Ring buffers, TTL, and pinning must use a stable event vocabulary to 
        prevent disjoint subsystems or parallel observability systems that 
        bypass the ledger.
      status: RESOLVED
      proposed_answer: |
        **v2 Architectural Principle (established):**
        - Ring buffers are daemon-local transient capture
        - On trigger → emit events to ledger
        - Reducers compute all metrics (no parallel pipelines)
        - Ledger is the substrate for evidence economics

        **Normative Minimal v1 Vocabulary:**
        1. `telemetry.frame`: (Normative aggregate) CPU, Memory, IO stats.
        2. `evidence.pinned`: Cryptographically binds artifact hash to investigation.
        3. `evidence.ttl_expired`: Signals artifact eligibility for compaction.
        4. `compaction.completed`: Replaces raw artifacts with summary receipt.
        5. `episode.quarantined`: Authoritative state change pinning all episode evidence.

        **Reducer Semantics:**
        All metrics derived from these events are versioned and deterministic.
      resolution: |
        **v2 Resolution (2026-01-27):**

        The event vocabulary is frozen for v1 to prevent fragmentation. All 
        components MUST use these events for evidence and metrics.

        **Minimal v1 Events:**
        - `telemetry.frame`: High-rate metrics frames bound to receipts.
        - `evidence.pinned`: Binds artifact `bb3-256` hash to incident/defect.
        - `evidence.ttl_expired`: Deterministic signal for cleanup reducers.
        - `compaction.completed`: Finalizes the deletion of expired raw evidence.
        - `episode.quarantined`: (Authoritative) Prevents any cleanup of evidence.

        **Implementation:**
        - Events defined in `proto/apm2d_runtime_v1.proto`.
        - `EvidenceReducer` implements pin/TTL state machine.
        - `DefectRecord` wiring emits `evidence.pinned` events.
      evidence_refs:
        - "AD-EVID-003 (architectural principle)"
        - "proto/apm2d_runtime_v1.proto (Normative)"
        - "crates/apm2-core/src/cac/defect.rs"
      owner: AUTH_ARCHITECTURE

    - question_id: OQ-DAEMON-007
      question: "Remote access/auth policy (mTLS vs OAuth2.1) for TCP mode?"
      context: |
        V1 is UDS-only. Future versions may need remote access.
        Need to decide auth mechanism that aligns with OCAP model.
      status: DEFERRED_V2
      proposed_answer: |
        Likely mTLS with client certificates mapped to capability sets.
        OAuth2.1 for delegation tokens (time-limited capability grants).
        Explicit non-goal for v1.
      owner: AUTH_SECURITY

    - question_id: OQ-DAEMON-008
      question: "eBPF adoption for per-cgroup network attribution?"
      context: |
        V1 uses coarse network accounting. Fine-grained attribution
        requires eBPF programs in network path.
      status: DEFERRED_V2
      proposed_answer: |
        eBPF adoption when:
        1. Per-episode network budgets become blocking requirement
        2. Network exfiltration detection is priority
        V1 uses coarse accounting; v2 may add eBPF.
      owner: AUTH_ARCHITECTURE

    - question_id: OQ-DAEMON-009
      question: "Container vs Firecracker enforcement for sandbox tiering?"
      context: |
        Higher risk tiers may need stronger isolation.
        Container (Docker/Podman) vs microVM (Firecracker).
      status: DEFERRED_V2
      proposed_answer: |
        Protocol invariant (envelope + receipts) unchanged by sandbox type.
        Implementation detail for v2 security tiers.
        V1: namespace isolation sufficient for risk tier 1-2.
        V2+: container/firecracker for tier 3+.
      owner: AUTH_SECURITY

  deferred_items:
    - item_id: DEFER-DAEMON-001
      title: "Multi-node federation"
      reason: |
        Single-host daemon sufficient for Phase 1.
        Federation requires consensus, cross-machine routing.
        Deferred to Phase 2+.
      related_doctrine: "holarchy routing"

    - item_id: DEFER-DAEMON-002
      title: "TCP mode with remote authentication"
      reason: |
        UDS sufficient for local operation.
        Remote access requires auth infrastructure.
        Security review needed before exposing network.
      related_doctrine: "network exposure"

    - item_id: DEFER-DAEMON-003
      title: "Rekor-style transparency logging"
      reason: |
        Receipts are signed but not transparently logged.
        Transparency log provides non-repudiation.
        Infrastructure cost; deferred to Phase 2.
      related_doctrine: "measurement integrity"

    - item_id: DEFER-DAEMON-004
      title: "Parallel episode execution with resource pooling"
      reason: |
        V1 treats episodes as independent.
        Resource pooling requires coordination across episodes.
        Phase 2 optimization.
      related_doctrine: "coordination"

    - item_id: DEFER-DAEMON-005
      title: "Perfect inference determinism"
      reason: |
        Vendor models are non-deterministic oracles.
        Inputs/outputs captured as evidence.
        Replay may produce different outputs; acceptable.
      related_doctrine: "probabilistic_bounded determinism class"
