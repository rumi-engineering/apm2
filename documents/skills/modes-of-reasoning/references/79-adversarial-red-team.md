# Adversarial / Red-Team Reasoning

**Category:** Meta-Level and Reflective Modes

## What it is

Assume the role of an attacker/critic: try to break arguments, systems, incentives, and assumptions.

## What it outputs

- Failure modes
- Exploits
- Counterexamples
- "What could go wrong" maps
- Stress test results

## How it differs

It's intentionally antagonistic to your current plan; pairs with robust reasoning and assurance cases. You're trying to find holes, not defend your work.

## Best for

- Security
- Safety
- Governance
- Strategy stress-testing
- Design review

## Common failure mode

Cynicism theater (finding clever attacks without prioritizing real risk). Breaking things is easy; finding important vulnerabilities is harder.

## Related modes

- [Robust / worst-case reasoning](49-robust-worst-case.md) — defending against attacks
- [Assurance-case reasoning](36-assurance-case.md) — what red-teaming challenges
- [Counterexample-guided reasoning](08-counterexample-guided.md) — uses counterexamples constructively
